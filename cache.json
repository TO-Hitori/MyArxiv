{"2024-03-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.05535v1","updated":"2024-03-08T18:58:46Z","published":"2024-03-08T18:58:46Z","title":"Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in\n  Images and Videos","summary":"  We introduce LaGTran, a novel framework that utilizes readily available or\neasily acquired text descriptions to guide robust transfer of discriminative\nknowledge from labeled source to unlabeled target data with domain shifts.\nWhile unsupervised adaptation methods have been established to address this\nproblem, they show limitations in handling challenging domain shifts due to\ntheir exclusive operation within the pixel-space. Motivated by our observation\nthat semantically richer text modality has more favorable transfer properties,\nwe devise a transfer mechanism to use a source-trained text-classifier to\ngenerate predictions on the target text descriptions, and utilize these\npredictions as supervision for the corresponding images. Our approach driven by\nlanguage guidance is surprisingly easy and simple, yet significantly\noutperforms all prior approaches on challenging datasets like GeoNet and\nDomainNet, validating its extreme effectiveness. To further extend the scope of\nour study beyond images, we introduce a new benchmark to study ego-exo transfer\nin videos and find that our language-aided LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.\n","authors":["Tarun Kalluri","Bodhisattwa Prasad Majumder","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2403.05535v1.pdf","comment":"Project Page and Code: https://tarun005.github.io/lagtran/"},{"id":"http://arxiv.org/abs/2403.05534v1","updated":"2024-03-08T18:57:52Z","published":"2024-03-08T18:57:52Z","title":"Bayesian Preference Elicitation with Language Models","summary":"  Aligning AI systems to users' interests requires understanding and\nincorporating humans' complex values and preferences. Recently, language models\n(LMs) have been used to gather information about the preferences of human\nusers. This preference data can be used to fine-tune or guide other LMs and/or\nAI systems. However, LMs have been shown to struggle with crucial aspects of\npreference learning: quantifying uncertainty, modeling human mental states, and\nasking informative questions. These challenges have been addressed in other\nareas of machine learning, such as Bayesian Optimal Experimental Design (BOED),\nwhich focus on designing informative queries within a well-defined feature\nspace. But these methods, in turn, are difficult to scale and apply to\nreal-world problems where simply identifying the relevant features can be\ndifficult. We introduce OPEN (Optimal Preference Elicitation with Natural\nlanguage) a framework that uses BOED to guide the choice of informative\nquestions and an LM to extract features and translate abstract BOED queries\ninto natural language questions. By combining the flexibility of LMs with the\nrigor of BOED, OPEN can optimize the informativity of queries while remaining\nadaptable to real-world domains. In user studies, we find that OPEN outperforms\nexisting LM- and BOED-based methods for preference elicitation.\n","authors":["Kunal Handa","Yarin Gal","Ellie Pavlick","Noah Goodman","Jacob Andreas","Alex Tamkin","Belinda Z. Li"],"pdf_url":"https://arxiv.org/pdf/2403.05534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05530v1","updated":"2024-03-08T18:54:20Z","published":"2024-03-08T18:54:20Z","title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context","summary":"  In this report, we present the latest model of the Gemini family, Gemini 1.5\nPro, a highly compute-efficient multimodal mixture-of-experts model capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio.\nGemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks\nacross modalities, improves the state-of-the-art in long-document QA,\nlong-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's\nstate-of-the-art performance across a broad set of benchmarks. Studying the\nlimits of Gemini 1.5 Pro's long-context ability, we find continued improvement\nin next-token prediction and near-perfect retrieval (>99%) up to at least 10M\ntokens, a generational leap over existing models such as Claude 2.1 (200k) and\nGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large\nlanguage models at the frontier; when given a grammar manual for Kalamang, a\nlanguage with fewer than 200 speakers worldwide, the model learns to translate\nEnglish to Kalamang at a similar level to a person who learned from the same\ncontent.\n","authors":["Machel Reid","Nikolay Savinov","Denis Teplyashin","Dmitry Lepikhin","Timothy Lillicrap","Jean-baptiste Alayrac","Radu Soricut","Angeliki Lazaridou","Orhan Firat","Julian Schrittwieser","Ioannis Antonoglou","Rohan Anil","Sebastian Borgeaud","Andrew Dai","Katie Millican","Ethan Dyer","Mia Glaese","Thibault Sottiaux","Benjamin Lee","Fabio Viola","Malcolm Reynolds","Yuanzhong Xu","James Molloy","Jilin Chen","Michael Isard","Paul Barham","Tom Hennigan","Ross McIlroy","Melvin Johnson","Johan Schalkwyk","Eli Collins","Eliza Rutherford","Erica Moreira","Kareem Ayoub","Megha Goel","Clemens Meyer","Gregory Thornton","Zhen Yang","Henryk Michalewski","Zaheer Abbas","Nathan Schucher","Ankesh Anand","Richard Ives","James Keeling","Karel Lenc","Salem Haykal","Siamak Shakeri","Pranav Shyam","Aakanksha Chowdhery","Roman Ring","Stephen Spencer","Eren Sezener","Luke Vilnis","Oscar Chang","Nobuyuki Morioka","George Tucker","Ce Zheng","Oliver Woodman","Nithya Attaluri","Tomas Kocisky","Evgenii Eltyshev","Xi Chen","Timothy Chung","Vittorio Selo","Siddhartha Brahma","Petko Georgiev","Ambrose Slone","Zhenkai Zhu","James Lottes","Siyuan Qiao","Ben Caine","Sebastian Riedel","Alex Tomala","Martin Chadwick","Juliette Love","Peter Choy","Sid Mittal","Neil Houlsby","Yunhao Tang","Matthew Lamm","Libin Bai","Qiao Zhang","Luheng He","Yong Cheng","Peter Humphreys","Yujia Li","Sergey Brin","Albin Cassirer","Yingjie Miao","Lukas Zilka","Taylor Tobin","Kelvin Xu","Lev Proleev","Daniel Sohn","Alberto Magni","Lisa Anne Hendricks","Isabel Gao","Santiago Ontañón","Oskar Bunyan","Nathan Byrd","Abhanshu Sharma","Biao Zhang","Mario Pinto","Rishika Sinha","Harsh Mehta","Dawei Jia","Sergi Caelles","Albert Webson","Alex Morris","Becca Roelofs","Yifan Ding","Robin Strudel","Xuehan Xiong","Marvin Ritter","Mostafa Dehghani","Rahma Chaabouni","Abhijit Karmarkar","Guangda Lai","Fabian Mentzer","Bibo Xu","YaGuang Li","Yujing Zhang","Tom Le Paine","Alex Goldin","Behnam Neyshabur","Kate Baumli","Anselm Levskaya","Michael Laskin","Wenhao Jia","Jack W. Rae","Kefan Xiao","Antoine He","Skye Giordano","Lakshman Yagati","Jean-Baptiste Lespiau","Paul Natsev","Sanjay Ganapathy","Fangyu Liu","Danilo Martins","Nanxin Chen","Yunhan Xu","Megan Barnes","Rhys May","Arpi Vezer","Junhyuk Oh","Ken Franko","Sophie Bridgers","Ruizhe Zhao","Boxi Wu","Basil Mustafa","Sean Sechrist","Emilio Parisotto","Thanumalayan Sankaranarayana Pillai","Chris Larkin","Chenjie Gu","Christina Sorokin","Maxim Krikun","Alexey Guseynov","Jessica Landon","Romina Datta","Alexander Pritzel","Phoebe Thacker","Fan Yang","Kevin Hui","Anja Hauth","Chih-Kuan Yeh","David Barker","Justin Mao-Jones","Sophia Austin","Hannah Sheahan","Parker Schuh","James Svensson","Rohan Jain","Vinay Ramasesh","Anton Briukhov","Da-Woon Chung","Tamara von Glehn","Christina Butterfield","Priya Jhakra","Matthew Wiethoff","Justin Frye","Jordan Grimstad","Beer Changpinyo","Charline Le Lan","Anna Bortsova","Yonghui Wu","Paul Voigtlaender","Tara Sainath","Charlotte Smith","Will Hawkins","Kris Cao","James Besley","Srivatsan Srinivasan","Mark Omernick","Colin Gaffney","Gabriela Surita","Ryan Burnell","Bogdan Damoc","Junwhan Ahn","Andrew Brock","Mantas Pajarskas","Anastasia Petrushkina","Seb Noury","Lorenzo Blanco","Kevin Swersky","Arun Ahuja","Thi Avrahami","Vedant Misra","Raoul de Liedekerke","Mariko Iinuma","Alex Polozov","Sarah York","George van den Driessche","Paul Michel","Justin Chiu","Rory Blevins","Zach Gleicher","Adrià Recasens","Alban Rrustemi","Elena Gribovskaya","Aurko Roy","Wiktor Gworek","Séb Arnold","Lisa Lee","James Lee-Thorp","Marcello Maggioni","Enrique Piqueras","Kartikeya Badola","Sharad Vikram","Lucas Gonzalez","Anirudh Baddepudi","Evan Senter","Jacob Devlin","James Qin","Michael Azzam","Maja Trebacz","Martin Polacek","Kashyap Krishnakumar","Shuo-yiin Chang","Matthew Tung","Ivo Penchev","Rishabh Joshi","Kate Olszewska","Carrie Muir","Mateo Wirth","Ale Jakse Hartman","Josh Newlan","Sheleem Kashem","Vijay Bolina","Elahe Dabir","Joost van Amersfoort","Zafarali Ahmed","James Cobon-Kerr","Aishwarya Kamath","Arnar Mar Hrafnkelsson","Le Hou","Ian Mackinnon","Alexandre Frechette","Eric Noland","Xiance Si","Emanuel Taropa","Dong Li","Phil Crone","Anmol Gulati","Sébastien Cevey","Jonas Adler","Ada Ma","David Silver","Simon Tokumine","Richard Powell","Stephan Lee","Michael Chang","Samer Hassan","Diana Mincu","Antoine Yang","Nir Levine","Jenny Brennan","Mingqiu Wang","Sarah Hodkinson","Jeffrey Zhao","Josh Lipschultz","Aedan Pope","Michael B. Chang","Cheng Li","Laurent El Shafey","Michela Paganini","Sholto Douglas","Bernd Bohnet","Fabio Pardo","Seth Odoom","Mihaela Rosca","Cicero Nogueira dos Santos","Kedar Soparkar","Arthur Guez","Tom Hudson","Steven Hansen","Chulayuth Asawaroengchai","Ravi Addanki","Tianhe Yu","Wojciech Stokowiec","Mina Khan","Justin Gilmer","Jaehoon Lee","Carrie Grimes Bostock","Keran Rong","Jonathan Caton","Pedram Pejman","Filip Pavetic","Geoff Brown","Vivek Sharma","Mario Lučić","Rajkumar Samuel","Josip Djolonga","Amol Mandhane","Lars Lowe Sjösund","Elena Buchatskaya","Elspeth White","Natalie Clay","Jiepu Jiang","Hyeontaek Lim","Ross Hemsley","Jane Labanowski","Nicola De Cao","David Steiner","Sayed Hadi Hashemi","Jacob Austin","Anita Gergely","Tim Blyth","Joe Stanton","Kaushik Shivakumar","Aditya Siddhant","Anders Andreassen","Carlos Araya","Nikhil Sethi","Rakesh Shivanna","Steven Hand","Ankur Bapna","Ali Khodaei","Antoine Miech","Garrett Tanzer","Andy Swing","Shantanu Thakoor","Zhufeng Pan","Zachary Nado","Stephanie Winkler","Dian Yu","Mohammad Saleh","Loren Maggiore","Iain Barr","Minh Giang","Thais Kagohara","Ivo Danihelka","Amit Marathe","Vladimir Feinberg","Mohamed Elhawaty","Nimesh Ghelani","Dan Horgan","Helen Miller","Lexi Walker","Richard Tanburn","Mukarram Tariq","Disha Shrivastava","Fei Xia","Chung-Cheng Chiu","Zoe Ashwood","Khuslen Baatarsukh","Sina Samangooei","Fred Alcober","Axel Stjerngren","Paul Komarek","Katerina Tsihlas","Anudhyan Boral","Ramona Comanescu","Jeremy Chen","Ruibo Liu","Dawn Bloxwich","Charlie Chen","Yanhua Sun","Fangxiaoyu Feng","Matthew Mauger","Xerxes Dotiwalla","Vincent Hellendoorn","Michael Sharman","Ivy Zheng","Krishna Haridasan","Gabe Barth-Maron","Craig Swanson","Dominika Rogozińska","Alek Andreev","Paul Kishan Rubenstein","Ruoxin Sang","Dan Hurt","Gamaleldin Elsayed","Renshen Wang","Dave Lacey","Anastasija Ilić","Yao Zhao","Lora Aroyo","Chimezie Iwuanyanwu","Vitaly Nikolaev","Balaji Lakshminarayanan","Sadegh Jazayeri","Raphaël Lopez Kaufman","Mani Varadarajan","Chetan Tekur","Doug Fritz","Misha Khalman","David Reitter","Kingshuk Dasgupta","Shourya Sarcar","Tina Ornduff","Javier Snaider","Fantine Huot","Johnson Jia","Rupert Kemp","Nejc Trdin","Anitha Vijayakumar","Lucy Kim","Christof Angermueller","Li Lao","Tianqi Liu","Haibin Zhang","David Engel","Somer Greene","Anaïs White","Jessica Austin","Lilly Taylor","Shereen Ashraf","Dangyi Liu","Maria Georgaki","Irene Cai","Yana Kulizhskaya","Sonam Goenka","Brennan Saeta","Kiran Vodrahalli","Christian Frank","Dario de Cesare","Brona Robenek","Harry Richardson","Mahmoud Alnahlawi","Christopher Yew","Priya Ponnapalli","Marco Tagliasacchi","Alex Korchemniy","Yelin Kim","Dinghua Li","Bill Rosgen","Zoe Ashwood","Kyle Levin","Jeremy Wiesner","Praseem Banzal","Praveen Srinivasan","Hongkun Yu","Çağlar Ünlü","David Reid","Zora Tung","Daniel Finchelstein","Ravin Kumar","Andre Elisseeff","Jin Huang","Ming Zhang","Rui Zhu","Ricardo Aguilar","Mai Giménez","Jiawei Xia","Olivier Dousse","Willi Gierke","Soheil Hassas Yeganeh","Damion Yates","Komal Jalan","Lu Li","Eri Latorre-Chimoto","Duc Dung Nguyen","Ken Durden","Praveen Kallakuri","Yaxin Liu","Matthew Johnson","Tomy Tsai","Alice Talbert","Jasmine Liu","Alexander Neitz","Chen Elkind","Marco Selvi","Mimi Jasarevic","Livio Baldini Soares","Albert Cui","Pidong Wang","Alek Wenjiao Wang","Xinyu Ye","Krystal Kallarackal","Lucia Loher","Hoi Lam","Josef Broder","Dan Holtmann-Rice","Nina Martin","Bramandia Ramadhana","Daniel Toyama","Mrinal Shukla","Sujoy Basu","Abhi Mohan","Nick Fernando","Noah Fiedel","Kim Paterson","Hui Li","Ankush Garg","Jane Park","DongHyun Choi","Diane Wu","Sankalp Singh","Zhishuai Zhang","Amir Globerson","Lily Yu","John Carpenter","Félix de Chaumont Quitry","Carey Radebaugh","Chu-Cheng Lin","Alex Tudor","Prakash Shroff","Drew Garmon","Dayou Du","Neera Vats","Han Lu","Shariq Iqbal","Alex Yakubovich","Nilesh Tripuraneni","James Manyika","Haroon Qureshi","Nan Hua","Christel Ngani","Maria Abi Raad","Hannah Forbes","Anna Bulanova","Jeff Stanway","Mukund Sundararajan","Victor Ungureanu","Colton Bishop","Yunjie Li","Balaji Venkatraman","Bo Li","Chloe Thornton","Salvatore Scellato","Nishesh Gupta","Yicheng Wang","Ian Tenney","Xihui Wu","Ashish Shenoy","Gabriel Carvajal","Diana Gage Wright","Ben Bariach","Zhuyun Xiao","Peter Hawkins","Sid Dalmia","Clement Farabet","Pedro Valenzuela","Quan Yuan","Chris Welty","Ananth Agarwal","Mia Chen","Wooyeol Kim","Brice Hulse","Nandita Dukkipati","Adam Paszke","Andrew Bolt","Elnaz Davoodi","Kiam Choo","Jennifer Beattie","Jennifer Prendki","Harsha Vashisht","Rebeca Santamaria-Fernandez","Luis C. Cobo","Jarek Wilkiewicz","David Madras","Ali Elqursh","Grant Uy","Kevin Ramirez","Matt Harvey","Tyler Liechty","Heiga Zen","Jeff Seibert","Clara Huiyi Hu","Mohamed Elhawaty","Andrey Khorlin","Maigo Le","Asaf Aharoni","Megan Li","Lily Wang","Sandeep Kumar","Alejandro Lince","Norman Casagrande","Jay Hoover","Dalia El Badawy","David Soergel","Denis Vnukov","Matt Miecnikowski","Jiri Simsa","Anna Koop","Praveen Kumar","Thibault Sellam","Daniel Vlasic","Samira Daruki","Nir Shabat","John Zhang","Guolong Su","Jiageng Zhang","Jeremiah Liu","Yi Sun","Evan Palmer","Alireza Ghaffarkhah","Xi Xiong","Victor Cotruta","Michael Fink","Lucas Dixon","Ashwin Sreevatsa","Adrian Goedeckemeyer","Alek Dimitriev","Mohsen Jafari","Remi Crocker","Nicholas FitzGerald","Aviral Kumar","Sanjay Ghemawat","Ivan Philips","Frederick Liu","Yannie Liang","Rachel Sterneck","Alena Repina","Marcus Wu","Laura Knight","Marin Georgiev","Hyo Lee","Harry Askham","Abhishek Chakladar","Annie Louis","Carl Crous","Hardie Cate","Dessie Petrova","Michael Quinn","Denese Owusu-Afriyie","Achintya Singhal","Nan Wei","Solomon Kim","Damien Vincent","Milad Nasr","Christopher A. Choquette-Choo","Reiko Tojo","Shawn Lu","Diego de Las Casas","Yuchung Cheng","Tolga Bolukbasi","Katherine Lee","Saaber Fatehi","Rajagopal Ananthanarayanan","Miteyan Patel","Charbel Kaed","Jing Li","Jakub Sygnowski","Shreyas Rammohan Belle","Zhe Chen","Jaclyn Konzelmann","Siim Põder","Roopal Garg","Vinod Koverkathu","Adam Brown","Chris Dyer","Rosanne Liu","Azade Nova","Jun Xu","Slav Petrov","Demis Hassabis","Koray Kavukcuoglu","Jeffrey Dean","Oriol Vinyals"],"pdf_url":"https://arxiv.org/pdf/2403.05530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14877v2","updated":"2024-03-08T18:51:03Z","published":"2023-05-24T08:29:50Z","title":"Improving Probability-based Prompt Selection Through Unified Evaluation\n  and Analysis","summary":"  Previous works in prompt engineering for large language models have\nintroduced different gradient-free probability-based prompt selection methods\nthat aim to choose the optimal prompt among the candidates for a given task but\nhave failed to provide a comprehensive and fair comparison between each other.\nIn this paper, we propose a unified framework to interpret and evaluate the\nexisting probability-based prompt selection methods by performing extensive\nexperiments on 13 common and diverse NLP tasks. We find that each of the\nexisting methods can be interpreted as some variant of the method that\nmaximizes mutual information between the input and the predicted output (MI).\nUtilizing this finding, we develop several other combinatorial variants of MI\nand increase the effectiveness of the oracle prompt selection method from\n87.79% to 94.98%, measured as the ratio of the performance of the selected\nprompt to that of the optimal oracle prompt. Furthermore, considering that all\nthe methods rely on the output probability distribution of the model that might\nbe biased, we propose a novel calibration method called Calibration by\nMarginalization (CBM) that is orthogonal to the existing methods and helps\nincrease the prompt selection effectiveness of the best method to 96.85%,\nachieving 99.44% of the oracle prompt F1 without calibration.\n","authors":["Sohee Yang","Jonghyeon Kim","Joel Jang","Seonghyeon Ye","Hyunji Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2305.14877v2.pdf","comment":"TACL 2024 (Pre-MIT Press publication version)"},{"id":"http://arxiv.org/abs/2403.05527v1","updated":"2024-03-08T18:48:30Z","published":"2024-03-08T18:48:30Z","title":"GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless\n  Generative Inference of LLM","summary":"  Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.\n","authors":["Hao Kang","Qingru Zhang","Souvik Kundu","Geonhwa Jeong","Zaoxing Liu","Tushar Krishna","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.05527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05519v1","updated":"2024-03-08T18:42:59Z","published":"2024-03-08T18:42:59Z","title":"Authorship Attribution in Bangla Literature (AABL) via Transfer Learning\n  using ULMFiT","summary":"  Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.\n","authors":["Aisha Khatun","Anisur Rahman","Md Saiful Islam","Hemayet Ahmed Chowdhury","Ayesha Tasnim"],"pdf_url":"https://arxiv.org/pdf/2403.05519v1.pdf","comment":"Accepted in ACM TALLIP August 2022"},{"id":"http://arxiv.org/abs/2403.05518v1","updated":"2024-03-08T18:41:42Z","published":"2024-03-08T18:41:42Z","title":"Bias-Augmented Consistency Training Reduces Biased Reasoning in\n  Chain-of-Thought","summary":"  While chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning, it can systematically misrepresent\nthe factors influencing models' behavior--for example, rationalizing answers in\nline with a user's opinion without mentioning this bias. To mitigate this\nbiased reasoning problem, we introduce bias-augmented consistency training\n(BCT), an unsupervised fine-tuning scheme that trains models to give consistent\nreasoning across prompts with and without biasing features. We construct a\nsuite testing nine forms of biased reasoning on seven question-answering tasks,\nand find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of\nbiased reasoning by 86% on held-out tasks. Moreover, this model generalizes to\nother forms of bias, reducing biased reasoning on held-out biases by an average\nof 37%. As BCT generalizes to held-out biases and does not require gold labels,\nthis method may hold promise for reducing biased reasoning from as-of-yet\nunknown biases and on tasks where supervision for ground truth reasoning is\nunavailable.\n","authors":["James Chua","Edward Rees","Hunar Batra","Samuel R. Bowman","Julian Michael","Ethan Perez","Miles Turpin"],"pdf_url":"https://arxiv.org/pdf/2403.05518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14280v2","updated":"2024-03-08T18:04:24Z","published":"2024-01-25T16:11:41Z","title":"RomanSetu: Efficiently unlocking multilingual capabilities of Large\n  Language Models models via Romanization","summary":"  This study addresses the challenge of extending Large Language Models (LLMs)\nto non-English languages using non-Roman scripts. We propose an approach that\nutilizes the romanized form of text as an interface for LLMs, hypothesizing\nthat its frequent informal use and shared tokens with English enhance\ncross-lingual alignment. Our approach involves the continual pretraining of an\nEnglish LLM like Llama 2 on romanized text of non-English, non-Roman script\nlanguages, followed by instruction tuning on romanized data. The results\nindicate that romanized text not only reduces token fertility by 2x-4x but also\nmatches or outperforms native script representation across various NLU, NLG,\nand MT tasks. Moreover, the embeddings computed on romanized text exhibit\ncloser alignment with their English translations than those from the native\nscript. Our approach presents a promising direction for leveraging the power of\nEnglish LLMs in languages traditionally underrepresented in NLP.\n","authors":["Jaavid Aktar Husain","Raj Dabre","Aswanth Kumar","Jay Gala","Thanmay Jayakumar","Ratish Puduppully","Anoop Kunchukuttan"],"pdf_url":"https://arxiv.org/pdf/2401.14280v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.05493v1","updated":"2024-03-08T18:04:03Z","published":"2024-03-08T18:04:03Z","title":"To Err Is Human, but Llamas Can Learn It Too","summary":"  This study explores enhancing grammatical error correction (GEC) through\nartificial error generation (AEG) using language models (LMs). Specifically, we\nfine-tune Llama 2-based LMs for error generation and find that this approach\nyields synthetic errors akin to human errors. Next, we train GEC Llama models\nwith the help of these artificial errors and outperform previous\nstate-of-the-art error correction models, with gains ranging between 0.8 and 6\nF0.5 points across all tested languages (German, Ukrainian, and Estonian).\nMoreover, we demonstrate that generating errors by fine-tuning smaller\nsequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and\nGPT-4) also results in synthetic errors beneficially affecting error generation\nmodels.\n","authors":["Agnes Luhtaru","Taido Purason","Martin Vainikko","Maksym Del","Mark Fishel"],"pdf_url":"https://arxiv.org/pdf/2403.05493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05488v1","updated":"2024-03-08T17:53:58Z","published":"2024-03-08T17:53:58Z","title":"FFSTC: Fongbe to French Speech Translation Corpus","summary":"  In this paper, we introduce the Fongbe to French Speech Translation Corpus\n(FFSTC) for the first time. This corpus encompasses approximately 31 hours of\ncollected Fongbe language content, featuring both French transcriptions and\ncorresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset\ncompiled through various collection methods and the efforts of dedicated\nindividuals. Furthermore, we conduct baseline experiments using Fairseq's\ntransformer_s and conformer models to evaluate data quality and validity. Our\nresults indicate a score of 8.96 for the transformer_s model and 8.14 for the\nconformer model, establishing a baseline for the FFSTC corpus.\n","authors":["D. Fortune Kponou","Frejus A. A. Laleye","Eugene C. Ezin"],"pdf_url":"https://arxiv.org/pdf/2403.05488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05468v1","updated":"2024-03-08T17:30:41Z","published":"2024-03-08T17:30:41Z","title":"Will GPT-4 Run DOOM?","summary":"  We show that GPT-4's reasoning and planning capabilities extend to the 1993\nfirst-person shooter Doom. This large language model (LLM) is able to run and\nplay the game with only a few instructions, plus a textual\ndescription--generated by the model itself from screenshots--about the state of\nthe game being observed. We find that GPT-4 can play the game to a passable\ndegree: it is able to manipulate doors, combat enemies, and perform pathing.\nMore complex prompting strategies involving multiple model calls provide better\nresults. While further work is required to enable the LLM to play the game as\nwell as its classical, reinforcement learning-based counterparts, we note that\nGPT-4 required no training, leaning instead on its own reasoning and\nobservational capabilities. We hope our work pushes the boundaries on\nintelligent, LLM-based agents in video games. We conclude by discussing the\nethical implications of our work.\n","authors":["Adrian de Wynter"],"pdf_url":"https://arxiv.org/pdf/2403.05468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04235v3","updated":"2024-03-08T17:04:49Z","published":"2023-11-06T08:50:29Z","title":"Can LLMs Follow Simple Rules?","summary":"  As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Existing evaluations\nof adversarial attacks and defenses on LLMs generally require either expensive\nmanual review or unreliable heuristic checks. To address this issue, we propose\nRule-following Language Evaluation Scenarios (RuLES), a programmatic framework\nfor measuring rule-following ability in LLMs. RuLES consists of 14 simple text\nscenarios in which the model is instructed to obey various rules while\ninteracting with the user. Each scenario has a programmatic evaluation function\nto determine whether the model has broken any rules in a conversation. Our\nevaluations of proprietary and open models show that almost all current models\nstruggle to follow scenario rules, even on straightforward test cases. We also\ndemonstrate that simple optimization attacks suffice to significantly increase\nfailure rates on test cases. We conclude by exploring two potential avenues for\nimprovement: test-time steering and supervised fine-tuning.\n","authors":["Norman Mu","Sarah Chen","Zifan Wang","Sizhe Chen","David Karamardian","Lulwa Aljeraisy","Basel Alomair","Dan Hendrycks","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2311.04235v3.pdf","comment":"Project website: https://eecs.berkeley.edu/~normanmu/llm_rules;\n  revised content"},{"id":"http://arxiv.org/abs/2403.05434v1","updated":"2024-03-08T16:37:36Z","published":"2024-03-08T16:37:36Z","title":"Cost-Performance Optimization for Processing Low-Resource Language Tasks\n  Using Commercial LLMs","summary":"  Large Language Models (LLMs) exhibit impressive zero/few-shot inference and\ngeneration quality for high-resource languages(HRLs). A few of them have been\ntrained in low-resource languages (LRLs) and give decent performance. Owing to\nthe prohibitive costs of training LLMs, they are usually used as a network\nservice, with the client charged by the count of input and output tokens. The\nnumber of tokens strongly depends on the script and language, as well as the\nLLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage,\nbecause the well-known LLMs produce more tokens for LRLs than HRLs. This is\nbecause most currently popular LLMs are optimized for HRL vocabularies. Our\nobjective is to level the playing field: reduce the cost of processing LRLs in\ncontemporary LLMs while ensuring that predictive and generative qualities are\nnot compromised. As means to reduce the number of tokens processed by the LLM,\nwe consider code-mixing, translation, and transliteration of LRLs to HRLs. We\nperform an extensive study using the IndicXTREME dataset, covering 15 Indian\nlanguages, while using GPT-4 (one of the costliest LLM services released so\nfar) as a commercial LLM. We observe and analyze interesting patterns involving\ntoken count, cost,and quality across a multitude of languages and tasks. We\nshow that choosing the best policy to interact with the LLM can reduce cost by\n90% while giving better or comparable performance, compared to communicating\nwith the LLM in the original LRL.\n","authors":["Arijit Nag","Animesh Mukherjee","Niloy Ganguly","Soumen Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2403.05434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05396v1","updated":"2024-03-08T15:51:43Z","published":"2024-03-08T15:51:43Z","title":"HistGen: Histopathology Report Generation via Local-Global Feature\n  Encoding and Cross-modal Context Interaction","summary":"  Histopathology serves as the gold standard in cancer diagnosis, with clinical\nreports being vital in interpreting and understanding this process, guiding\ncancer treatment and patient care. The automation of histopathology report\ngeneration with deep learning stands to significantly enhance clinical\nefficiency and lessen the labor-intensive, time-consuming burden on\npathologists in report writing. In pursuit of this advancement, we introduce\nHistGen, a multiple instance learning-empowered framework for histopathology\nreport generation together with the first benchmark dataset for evaluation.\nInspired by diagnostic and report-writing workflows, HistGen features two\ndelicately designed modules, aiming to boost report generation by aligning\nwhole slide images (WSIs) and diagnostic reports from local and global\ngranularity. To achieve this, a local-global hierarchical encoder is developed\nfor efficient visual feature aggregation from a region-to-slide perspective.\nMeanwhile, a cross-modal context module is proposed to explicitly facilitate\nalignment and interaction between distinct modalities, effectively bridging the\ngap between the extensive visual sequences of WSIs and corresponding highly\nsummarized reports. Experimental results on WSI report generation show the\nproposed model outperforms state-of-the-art (SOTA) models by a large margin.\nMoreover, the results of fine-tuning our model on cancer subtyping and survival\nanalysis tasks further demonstrate superior performance compared to SOTA\nmethods, showcasing strong transfer learning capability. Dataset, model\nweights, and source code are available in\nhttps://github.com/dddavid4real/HistGen.\n","authors":["Zhengrui Guo","Jiabo Ma","Yingxue Xu","Yihui Wang","Liansheng Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.05396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12307v3","updated":"2024-03-08T15:26:38Z","published":"2023-09-21T17:59:11Z","title":"LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models","summary":"  We present LongLoRA, an efficient fine-tuning approach that extends the\ncontext sizes of pre-trained large language models (LLMs), with limited\ncomputation cost. Typically, training LLMs with long context sizes is\ncomputationally expensive, requiring extensive training hours and GPU\nresources. For example, training on the context length of 8192 needs 16x\ncomputational costs in self-attention layers as that of 2048. In this paper, we\nspeed up the context extension of LLMs in two aspects. On the one hand,\nalthough dense global attention is needed during inference, fine-tuning the\nmodel can be effectively and efficiently done by sparse local attention. The\nproposed shifted sparse attention effectively enables context extension,\nleading to non-trivial computation saving with similar performance to\nfine-tuning with vanilla attention. Particularly, it can be implemented with\nonly two lines of code in training, while being optional in inference. On the\nother hand, we revisit the parameter-efficient fine-tuning regime for context\nexpansion. Notably, we find that LoRA for context extension works well under\nthe premise of trainable embedding and normalization. LongLoRA combines this\nimproved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on\nvarious tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B\nfrom 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine.\nLongLoRA extends models' context while retaining their original architectures,\nand is compatible with most existing techniques, like Flash-Attention2. In\naddition, we further conduct supervised fine-tuning with LongLoRA and our long\ninstruction-following LongAlpaca dataset.\n","authors":["Yukang Chen","Shengju Qian","Haotian Tang","Xin Lai","Zhijian Liu","Song Han","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2309.12307v3.pdf","comment":"Code, models, dataset, and demo are available at\n  https://github.com/dvlab-research/LongLoRA"},{"id":"http://arxiv.org/abs/2402.14809v2","updated":"2024-03-08T15:15:47Z","published":"2024-02-22T18:59:02Z","title":"CriticBench: Benchmarking LLMs for Critique-Correct Reasoning","summary":"  The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.\n","authors":["Zicheng Lin","Zhibin Gou","Tian Liang","Ruilin Luo","Haowei Liu","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2402.14809v2.pdf","comment":"Corrected computation errors in Tables 1, 7-11; updated corresponding\n  figs"},{"id":"http://arxiv.org/abs/2401.14040v2","updated":"2024-03-08T15:00:31Z","published":"2024-01-25T09:36:58Z","title":"(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection","summary":"  In the universe of Natural Language Processing, Transformer-based language\nmodels like BERT and (Chat)GPT have emerged as lexical superheroes with great\npower to solve open research problems. In this paper, we specifically focus on\nthe temporal problem of semantic change, and evaluate their ability to solve\ntwo diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and\nHistoWiC. In particular, we investigate the potential of a novel, off-the-shelf\ntechnology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a\nfamily of models that currently stand as the state-of-the-art for modeling\nsemantic change. Our experiments represent the first attempt to assess the use\nof (Chat)GPT for studying semantic change. Our results indicate that ChatGPT\nperforms significantly worse than the foundational GPT version. Furthermore,\nour results demonstrate that (Chat)GPT achieves slightly lower performance than\nBERT in detecting long-term changes but performs significantly worse in\ndetecting short-term changes.\n","authors":["Francesco Periti","Haim Dubossarsky","Nina Tahmasebi"],"pdf_url":"https://arxiv.org/pdf/2401.14040v2.pdf","comment":"Accepted to the Findings of EACL 2024"},{"id":"http://arxiv.org/abs/2403.05365v1","updated":"2024-03-08T14:55:05Z","published":"2024-03-08T14:55:05Z","title":"The Impact of Quantization on the Robustness of Transformer-based Text\n  Classifiers","summary":"  Transformer-based models have made remarkable advancements in various NLP\nareas. Nevertheless, these models often exhibit vulnerabilities when confronted\nwith adversarial attacks. In this paper, we explore the effect of quantization\non the robustness of Transformer-based models. Quantization usually involves\nmapping a high-precision real number to a lower-precision value, aiming at\nreducing the size of the model at hand. To the best of our knowledge, this work\nis the first application of quantization on the robustness of NLP models. In\nour experiments, we evaluate the impact of quantization on BERT and DistilBERT\nmodels in text classification using SST-2, Emotion, and MR datasets. We also\nevaluate the performance of these models against TextFooler, PWWS, and PSO\nadversarial attacks. Our findings show that quantization significantly improves\n(by an average of 18.68%) the adversarial accuracy of the models. Furthermore,\nwe compare the effect of quantization versus that of the adversarial training\napproach on robustness. Our experiments indicate that quantization increases\nthe robustness of the model by 18.80% on average compared to adversarial\ntraining without imposing any extra computational overhead during training.\nTherefore, our results highlight the effectiveness of quantization in improving\nthe robustness of NLP models.\n","authors":["Seyed Parsa Neshaei","Yasaman Boreshban","Gholamreza Ghassem-Sani","Seyed Abolghasem Mirroshandel"],"pdf_url":"https://arxiv.org/pdf/2403.05365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12011v3","updated":"2024-03-08T14:50:40Z","published":"2024-02-19T10:04:59Z","title":"A Systematic Comparison of Contextualized Word Embeddings for Lexical\n  Semantic Change","summary":"  Contextualized embeddings are the preferred tool for modeling Lexical\nSemantic Change (LSC). Current evaluations typically focus on a specific task\nknown as Graded Change Detection (GCD). However, performance comparison across\nwork are often misleading due to their reliance on diverse settings. In this\npaper, we evaluate state-of-the-art models and approaches for GCD under equal\nconditions. We further break the LSC problem into Word-in-Context (WiC) and\nWord Sense Induction (WSI) tasks, and compare models across these different\nlevels. Our evaluation is performed across different languages on eight\navailable benchmarks for LSC, and shows that (i) APD outperforms other\napproaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for\nWiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need\nfor improving the modeling of word meanings, as well as focus on how, when, and\nwhy these meanings change, rather than solely focusing on the extent of\nsemantic change.\n","authors":["Francesco Periti","Nina Tahmasebi"],"pdf_url":"https://arxiv.org/pdf/2402.12011v3.pdf","comment":"Submitted to NAACL 2024"},{"id":"http://arxiv.org/abs/2402.13709v2","updated":"2024-03-08T14:35:30Z","published":"2024-02-21T11:23:21Z","title":"SaGE: Evaluating Moral Consistency in Large Language Models","summary":"  Despite recent advancements showcasing the impressive capabilities of Large\nLanguage Models (LLMs) in conversational systems, we show that even\nstate-of-the-art LLMs are morally inconsistent in their generations,\nquestioning their reliability (and trustworthiness in general). Prior works in\nLLM evaluation focus on developing ground-truth data to measure accuracy on\nspecific tasks. However, for moral scenarios that often lack universally\nagreed-upon answers, consistency in model responses becomes crucial for their\nreliability. To address this issue, we propose an information-theoretic measure\ncalled Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of\nThumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract\nprinciples learned by a model and can help explain their decision-making\nstrategies effectively. To this extent, we construct the Moral Consistency\nCorpus (MCC), containing 50K moral questions, responses to them by LLMs, and\nthe RoTs that these models followed. Furthermore, to illustrate the\ngeneralizability of SaGE, we use it to investigate LLM consistency on two\npopular datasets -- TruthfulQA and HellaSwag. Our results reveal that\ntask-accuracy and consistency are independent problems, and there is a dire\nneed to investigate these issues further.\n","authors":["Vamshi Krishna Bonagiri","Sreeram Vennam","Priyanshul Govil","Ponnurangam Kumaraguru","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2402.13709v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.09889v4","updated":"2024-03-08T14:23:49Z","published":"2023-11-16T13:37:21Z","title":"Language Generation from Brain Recordings","summary":"  Generating human language through non-invasive brain-computer interfaces\n(BCIs) has the potential to unlock many applications, such as serving disabled\npatients and improving communication. Currently, however, generating language\nvia BCIs has been previously successful only within a classification setup for\nselecting pre-generated sentence continuation candidates with the most likely\ncortical semantic representation. Inspired by recent research that revealed\nassociations between the brain and the large computational language models, we\npropose a generative language BCI that utilizes the capacity of a large\nlanguage model (LLM) jointly with a semantic brain decoder to directly generate\nlanguage from functional magnetic resonance imaging (fMRI) input. The proposed\nmodel can generate coherent language sequences aligned with the semantic\ncontent of visual or auditory language stimuli perceived, without prior\nknowledge of any pre-generated candidates. We compare the language generated\nfrom the presented model with a random control, pre-generated language\nselection approach, and a standard LLM, which generates common coherent text\nsolely based on the next word likelihood according to statistical language\ntraining data. The proposed model is found to generate language that is more\naligned with semantic stimulus in response to which brain input is sampled. Our\nfindings demonstrate the potential and feasibility of employing BCIs in direct\nlanguage generation.\n","authors":["Ziyi Ye","Qingyao Ai","Yiqun Liu","Maarten de Rijke","Min Zhang","Christina Lioma","Tuukka Ruotsalo"],"pdf_url":"https://arxiv.org/pdf/2311.09889v4.pdf","comment":"Preprint. Under Submission"},{"id":"http://arxiv.org/abs/2310.12557v2","updated":"2024-03-08T14:16:55Z","published":"2023-10-19T08:07:22Z","title":"DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial\n  Reasoning in Text","summary":"  Spatial reasoning in text plays a crucial role in various real-world\napplications. Existing approaches for spatial reasoning typically infer spatial\nrelations from pure text, which overlooks the gap between natural language and\nsymbolic structures. Graph neural networks (GNNs) have showcased exceptional\nproficiency in inducing and aggregating symbolic structures. However, classical\nGNNs face challenges in handling multi-hop spatial reasoning due to the\nover-smoothing issue, i.e., the performance decreases substantially as the\nnumber of graph layers increases. To cope with these challenges, we propose a\nnovel Depth-Wise Graph Neural Network (DepWiGNN). Specifically, we design a\nnovel node memory scheme and aggregate the information over the depth dimension\ninstead of the breadth dimension of the graph, which empowers the ability to\ncollect long dependencies without stacking multiple layers. Experimental\nresults on two challenging multi-hop spatial reasoning datasets show that\nDepWiGNN outperforms existing spatial reasoning methods. The comparisons with\nthe other three GNNs further demonstrate its superiority in capturing long\ndependency in the graph.\n","authors":["Shuaiyi Li","Yang Deng","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2310.12557v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2403.05338v1","updated":"2024-03-08T14:14:37Z","published":"2024-03-08T14:14:37Z","title":"Explaining Pre-Trained Language Models with Attribution Scores: An\n  Analysis in Low-Resource Settings","summary":"  Attribution scores indicate the importance of different input parts and can,\nthus, explain model behaviour. Currently, prompt-based models are gaining\npopularity, i.a., due to their easier adaptability in low-resource settings.\nHowever, the quality of attribution scores extracted from prompt-based models\nhas not been investigated yet. In this work, we address this topic by analyzing\nattribution scores extracted from prompt-based models w.r.t. plausibility and\nfaithfulness and comparing them with attribution scores extracted from\nfine-tuned models and large language models. In contrast to previous work, we\nintroduce training size as another dimension into the analysis. We find that\nusing the prompting paradigm (with either encoder-based or decoder-based\nmodels) yields more plausible explanations than fine-tuning the models in\nlow-resource settings and Shapley Value Sampling consistently outperforms\nattention and Integrated Gradients in terms of leading to more plausible and\nfaithful explanations.\n","authors":["Wei Zhou","Heike Adel","Hendrik Schuff","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.05338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05330v1","updated":"2024-03-08T14:07:44Z","published":"2024-03-08T14:07:44Z","title":"Consecutive Model Editing with Batch alongside HooK Layers","summary":"  As the typical retraining paradigm is unacceptably time- and\nresource-consuming, researchers are turning to model editing in order to seek\nan effective, consecutive, and batch-supportive way to edit the model behavior\ndirectly. Despite all these practical expectations, existing model editing\nmethods fail to realize all of them. Furthermore, the memory demands for such\nsuccession-supportive model editing approaches tend to be prohibitive,\nfrequently necessitating an external memory that grows incrementally over time.\nTo cope with these challenges, we propose COMEBA-HK, a model editing method\nthat is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as\nit only needs a small amount of it to store several hook layers with updated\nweights. Experimental results demonstrate the superiority of our method over\nother batch-supportive model editing methods under both single-round and\nconsecutive batch editing scenarios. Extensive analyses of COMEBA-HK have been\nconducted to verify the stability of our method over 1) the number of\nconsecutive steps and 2) the number of editing instance.\n","authors":["Shuaiyi Li","Yang Deng","Deng Cai","Hongyuan Lu","Liang Chen","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2403.05330v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.05326v1","updated":"2024-03-08T14:05:36Z","published":"2024-03-08T14:05:36Z","title":"ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in\n  Dialogues","summary":"  Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,\nQuestion-Answering and Dialogue) has attracted ever-more interest in recent\nyears and achieved important progresses. However, existing studies on\ninteractive ASU largely ignore the coreference issue for opinion targets (i.e.,\naspects), while this phenomenon is ubiquitous in interactive scenarios\nespecially dialogues, limiting the ASU performance. Recently, large language\nmodels (LLMs) shows the powerful ability to integrate various NLP tasks with\nthe chat paradigm. In this way, this paper proposes a new Chat-based Aspect\nSentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in\nunderstanding aspect sentiments in dialogue scenarios. Particularly, this\nChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to\naddress the aspect coreference issue. On this basis, we propose a Trusted\nSelf-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.\nSpecifically, this TSA treats the ACR task as an auxiliary task to boost the\nperformance of the primary ASU task, and further integrates trusted learning\ninto reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination\nproblem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to\nevaluate TSA, and extensive experiments show that our proposed TSA can\nsignificantly outperform several state-of-the-art baselines, justifying the\neffectiveness of TSA to ChatASU and the importance of considering the\ncoreference and hallucination issues in ChatASU.\n","authors":["Yiding Liu","Jingjing Wang","Jiaming Luo","Tao Zeng","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05313v1","updated":"2024-03-08T13:42:19Z","published":"2024-03-08T13:42:19Z","title":"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in\n  Long-Horizon Generation","summary":"  We explore how iterative revising a chain of thoughts with the help of\ninformation retrieval significantly improves large language models' reasoning\nand generation ability in long-horizon generation tasks, while hugely\nmitigating hallucination. In particular, the proposed method --\n*retrieval-augmented thoughts* (RAT) -- revises each thought step one by one\nwith retrieved information relevant to the task query, the current and the past\nthought steps, after the initial zero-shot CoT is generated. Applying RAT to\nGPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on\nvarious long-horizon generation tasks; on average of relatively increasing\nrating scores by 13.63% on code generation, 16.96% on mathematical reasoning,\n19.2% on creative writing, and 42.78% on embodied task planning. The demo page\ncan be found at https://craftjarvis.github.io/RAT\n","authors":["Zihao Wang","Anji Liu","Haowei Lin","Jiaqi Li","Xiaojian Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2403.05313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05303v1","updated":"2024-03-08T13:32:01Z","published":"2024-03-08T13:32:01Z","title":"ACLSum: A New Dataset for Aspect-based Summarization of Scientific\n  Publications","summary":"  Extensive efforts in the past have been directed toward the development of\nsummarization datasets. However, a predominant number of these resources have\nbeen (semi)-automatically generated, typically through web data crawling,\nresulting in subpar resources for training and evaluating summarization\nsystems, a quality compromise that is arguably due to the substantial costs\nassociated with generating ground-truth summaries, particularly for diverse\nlanguages and specialized domains. To address this issue, we present ACLSum, a\nnovel summarization dataset carefully crafted and evaluated by domain experts.\nIn contrast to previous datasets, ACLSum facilitates multi-aspect summarization\nof scientific papers, covering challenges, approaches, and outcomes in depth.\nThrough extensive experiments, we evaluate the quality of our resource and the\nperformance of models based on pretrained language models and state-of-the-art\nlarge language models (LLMs). Additionally, we explore the effectiveness of\nextractive versus abstractive summarization within the scholarly domain on the\nbasis of automatically discovered aspects. Our results corroborate previous\nfindings in the general domain and indicate the general superiority of\nend-to-end aspect-based summarization. Our data is released at\nhttps://github.com/sobamchan/aclsum.\n","authors":["Sotaro Takeshita","Tommaso Green","Ines Reinig","Kai Eckert","Simone Paolo Ponzetto"],"pdf_url":"https://arxiv.org/pdf/2403.05303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13116v3","updated":"2024-03-08T13:29:03Z","published":"2024-02-20T16:17:37Z","title":"A Survey on Knowledge Distillation of Large Language Models","summary":"  In the era of Large Language Models (LLMs), Knowledge Distillation (KD)\nemerges as a pivotal methodology for transferring advanced capabilities from\nleading proprietary LLMs, such as GPT-4, to their open-source counterparts like\nLLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a\ncrucial role in both compressing these models, and facilitating their\nself-improvement by employing themselves as teachers. This paper presents a\ncomprehensive survey of KD's role within the realm of LLM, highlighting its\ncritical function in imparting advanced knowledge to smaller models and its\nutility in model compression and self-improvement. Our survey is meticulously\nstructured around three foundational pillars: \\textit{algorithm},\n\\textit{skill}, and \\textit{verticalization} -- providing a comprehensive\nexamination of KD mechanisms, the enhancement of specific cognitive abilities,\nand their practical implications across diverse fields. Crucially, the survey\nnavigates the intricate interplay between data augmentation (DA) and KD,\nillustrating how DA emerges as a powerful paradigm within the KD framework to\nbolster LLMs' performance. By leveraging DA to generate context-rich,\nskill-specific training data, KD transcends traditional boundaries, enabling\nopen-source models to approximate the contextual adeptness, ethical alignment,\nand deep semantic insights characteristic of their proprietary counterparts.\nThis work aims to provide an insightful guide for researchers and\npractitioners, offering a detailed overview of current methodologies in KD and\nproposing future research directions. Importantly, we firmly advocate for\ncompliance with the legal terms that regulate the use of LLMs, ensuring ethical\nand lawful application of KD of LLMs. An associated Github repository is\navailable at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.\n","authors":["Xiaohan Xu","Ming Li","Chongyang Tao","Tao Shen","Reynold Cheng","Jinyang Li","Can Xu","Dacheng Tao","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.13116v3.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2403.05297v1","updated":"2024-03-08T13:24:46Z","published":"2024-03-08T13:24:46Z","title":"PEEB: Part-based Image Classifiers with an Explainable and Editable\n  Language Bottleneck","summary":"  CLIP-based classifiers rely on the prompt containing a {class name} that is\nknown to the text encoder. That is, CLIP performs poorly on new classes or the\nclasses whose names rarely appear on the Internet (e.g., scientific names of\nbirds). For fine-grained classification, we propose PEEB - an explainable and\neditable classifier to (1) express the class name into a set of pre-defined\ntext descriptors that describe the visual parts of that class; and (2) match\nthe embeddings of the detected parts to their textual descriptors in each class\nto compute a logit score for classification. In a zero-shot setting where the\nclass names are unknown, PEEB outperforms CLIP by a large margin (~10x in\naccuracy). Compared to part-based classifiers, PEEB is not only the\nstate-of-the-art on the supervised-learning setting (88.80% accuracy) but also\nthe first to enable users to edit the class definitions to form a new\nclassifier without retraining. Compared to concept bottleneck models, PEEB is\nalso the state-of-the-art in both zero-shot and supervised learning settings.\n","authors":["Thang M. Pham","Peijie Chen","Tin Nguyen","Seunghyun Yoon","Trung Bui","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.05297v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2312.09979v4","updated":"2024-03-08T13:13:54Z","published":"2023-12-15T17:45:06Z","title":"LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models\n  via MoE-Style Plugin","summary":"  Supervised fine-tuning (SFT) is a crucial step for large language models\n(LLMs), enabling them to align with human instructions and enhance their\ncapabilities in downstream tasks. Increasing instruction data substantially is\na direct solution to align the model with a broader range of downstream tasks\nor notably improve its performance on a specific task. However, we find that\nlarge-scale increases in instruction data can damage the world knowledge\npreviously stored in LLMs. To address this challenge, we propose LoRAMoE, a\nnovelty framework that introduces several low-rank adapters (LoRA) and\nintegrates them by using a router network, like a plugin version of Mixture of\nExperts (MoE). It freezes the backbone model and forces a portion of LoRAs to\nfocus on leveraging world knowledge to solve downstream tasks, to alleviate\nworld knowledge-edge forgetting. Experimental results show that, as the\ninstruction data increases, LoRAMoE can significantly improve the ability to\nprocess downstream tasks, while maintaining the world knowledge stored in the\nLLM.\n","authors":["Shihan Dou","Enyu Zhou","Yan Liu","Songyang Gao","Jun Zhao","Wei Shen","Yuhao Zhou","Zhiheng Xi","Xiao Wang","Xiaoran Fan","Shiliang Pu","Jiang Zhu","Rui Zheng","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2312.09979v4.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.05286v1","updated":"2024-03-08T13:10:59Z","published":"2024-03-08T13:10:59Z","title":"LLM4Decompile: Decompiling Binary Code with Large Language Models","summary":"  Decompilation aims to restore compiled code to human-readable source code,\nbut struggles with details like names and structure. Large language models\n(LLMs) show promise for programming tasks, motivating their application to\ndecompilation. However, there does not exist any open-source LLM for\ndecompilation. Moreover, existing decompilation evaluation systems mainly\nconsider token-level accuracy and largely ignore code executability, which is\nthe most important feature of any program. Therefore, we release the first\nopen-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion\ntokens of C source code and the corresponding assembly code. The open-source\nLLMs can serve as baselines for further development in the field. To ensure\npractical program evaluation, we introduce Decompile-Eval, the first dataset\nthat considers re-compilability and re-executability for decompilation. The\nbenchmark emphasizes the importance of evaluating the decompilation model from\nthe perspective of program semantics. Experiments indicate that our\nLLM4Decompile has demonstrated the capability to accurately decompile 21% of\nthe assembly code, which achieves a 50% improvement over GPT-4. Our code,\ndataset, and models are released at\nhttps://github.com/albertan017/LLM4Decompile\n","authors":["Hanzhuo Tan","Qi Luo","Jing Li","Yuqun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05286v1.pdf","comment":"on going"},{"id":"http://arxiv.org/abs/2310.09499v3","updated":"2024-03-08T13:01:36Z","published":"2023-10-14T05:43:09Z","title":"One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language\n  Models","summary":"  Various Large Language Models(LLMs) from the Generative Pretrained\nTransformer(GPT) family have achieved outstanding performances in a wide range\nof text generation tasks. However, the enormous model sizes have hindered their\npractical use in real-world applications due to high inference latency.\nTherefore, improving the efficiencies of LLMs through quantization, pruning,\nand other means has been a key issue in LLM studies. In this work, we propose a\nmethod based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs\nto at least 50% sparsity without the need of any retraining. It allocates\nsparsity adaptively based on sensitivity, allowing us to reduce pruning-induced\nerror while maintaining the overall sparsity level. The advantages of the\nproposed method exhibit even more when the sparsity is extremely high.\nFurthermore, our method is compatible with quantization, enabling further\ncompression of LLMs.\n","authors":["Hang Shao","Bei Liu","Bo Xiao","Ke Zeng","Guanglu Wan","Yanmin Qian"],"pdf_url":"https://arxiv.org/pdf/2310.09499v3.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2403.05268v1","updated":"2024-03-08T12:45:53Z","published":"2024-03-08T12:45:53Z","title":"Deep Prompt Multi-task Network for Abuse Language Detection","summary":"  The detection of abusive language remains a long-standing challenge with the\nextensive use of social networks. The detection task of abusive language\nsuffers from limited accuracy. We argue that the existing detection methods\nutilize the fine-tuning technique of the pre-trained language models (PLMs) to\nhandle downstream tasks. Hence, these methods fail to stimulate the general\nknowledge of the PLMs. To address the problem, we propose a novel Deep Prompt\nMulti-task Network (DPMN) for abuse language detection. Specifically, DPMN\nfirst attempts to design two forms of deep prompt tuning and light prompt\ntuning for the PLMs. The effects of different prompt lengths, tuning\nstrategies, and prompt initialization methods on detecting abusive language are\nstudied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which\ncan be used as a short text classifier. Eventually, DPMN utilizes multi-task\nlearning to improve detection metrics further. The multi-task network has the\nfunction of transferring effective knowledge. The proposed DPMN is evaluated\nagainst eight typical methods on three public datasets: OLID, SOLID, and\nAbuseAnalyzer. The experimental results show that our DPMN outperforms the\nstate-of-the-art methods.\n","authors":["Jian Zhu","Yuping Ruan","Jingfei Chang","Cheng Luo"],"pdf_url":"https://arxiv.org/pdf/2403.05268v1.pdf","comment":"Submitted to the International Conference on Pattern Recognition\n  (ICPR) 2024"},{"id":"http://arxiv.org/abs/2403.05266v1","updated":"2024-03-08T12:42:36Z","published":"2024-03-08T12:42:36Z","title":"ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models","summary":"  Large language models (LLMs) have achieved unprecedented performance in\nvarious applications, yet their evaluation remains a critical issue. Existing\nhallucination benchmarks are either static or lack adjustable complexity for\nthorough analysis. We contend that utilizing existing relational databases is a\npromising approach for constructing benchmarks due to their accurate knowledge\ndescription via functional dependencies. We propose ERBench to automatically\nconvert any relational database into a benchmark based on the\nentity-relationship (ER) model. Our key idea is to construct questions using\nthe database schema, records, and functional dependencies such that they can be\nautomatically verified. In addition, we use foreign key constraints to join\nrelations and construct multihop questions, which can be arbitrarily complex\nand used to debug the intermediate answers of LLMs. Finally, ERBench supports\ncontinuous evaluation, multimodal questions, and various prompt engineering\ntechniques. In our experiments, we construct an LLM benchmark using databases\nof multiple domains and make an extensive comparison of contemporary LLMs. We\nobserve that better LLMs like GPT-4 can handle a larger variety of question\ntypes, but are by no means perfect. Also, correct answers do not necessarily\nimply correct rationales, which is an important evaluation that ERBench does\nbetter than other benchmarks for various question types. Code is available at\nhttps: //github.com/DILAB-KAIST/ERBench.\n","authors":["Jio Oh","Soyeon Kim","Junseok Seo","Jindong Wang","Ruochen Xu","Xing Xie","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2403.05266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05257v1","updated":"2024-03-08T12:28:15Z","published":"2024-03-08T12:28:15Z","title":"Cross-lingual Transfer or Machine Translation? On Data Augmentation for\n  Monolingual Semantic Textual Similarity","summary":"  Learning better sentence embeddings leads to improved performance for natural\nlanguage understanding tasks including semantic textual similarity (STS) and\nnatural language inference (NLI). As prior studies leverage large-scale labeled\nNLI datasets for fine-tuning masked language models to yield sentence\nembeddings, task performance for languages other than English is often left\nbehind. In this study, we directly compared two data augmentation techniques as\npotential solutions for monolingual STS: (a) cross-lingual transfer that\nexploits English resources alone as training data to yield non-English sentence\nembeddings as zero-shot inference, and (b) machine translation that coverts\nEnglish data into pseudo non-English training data in advance. In our\nexperiments on monolingual STS in Japanese and Korean, we find that the two\ndata techniques yield performance on par. Rather, we find a superiority of the\nWikipedia domain over the NLI domain for these languages, in contrast to prior\nstudies that focused on NLI as training data. Combining our findings, we\ndemonstrate that the cross-lingual transfer of Wikipedia data exhibits improved\nperformance, and that native Wikipedia data can further improve performance for\nmonolingual STS.\n","authors":["Sho Hoshino","Akihiko Kato","Soichiro Murakami","Peinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05257v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.16787v2","updated":"2024-03-08T12:21:12Z","published":"2023-11-28T13:50:50Z","title":"Evaluating Optimal Reference Translations","summary":"  The overall translation quality reached by current machine translation (MT)\nsystems for high-resourced language pairs is remarkably good. Standard methods\nof evaluation are not suitable nor intended to uncover the many translation\nerrors and quality deficiencies that still persist. Furthermore, the quality of\nstandard reference translations is commonly questioned and comparable quality\nlevels have been reached by MT alone in several language pairs. Navigating\nfurther research in these high-resource settings is thus difficult. In this\narticle, we propose a methodology for creating more reliable document-level\nhuman reference translations, called \"optimal reference translations,\" with the\nsimple aim to raise the bar of what should be deemed \"human translation\nquality.\" We evaluate the obtained document-level optimal reference\ntranslations in comparison with \"standard\" ones, confirming a significant\nquality increase and also documenting the relationship between evaluation and\ntranslation editing.\n","authors":["Vilém Zouhar","Věra Kloudová","Martin Popel","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2311.16787v2.pdf","comment":"To appear in Natural Language Engineering 2024"},{"id":"http://arxiv.org/abs/2403.05217v1","updated":"2024-03-08T11:09:13Z","published":"2024-03-08T11:09:13Z","title":"Harnessing Multi-Role Capabilities of Large Language Models for\n  Open-Domain Question Answering","summary":"  Open-domain question answering (ODQA) has emerged as a pivotal research\nspotlight in information systems. Existing methods follow two main paradigms to\ncollect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves\npertinent documents from an external corpus; and (2) the\n\\textit{generate-then-read} paradigm employs large language models (LLMs) to\ngenerate relevant documents. However, neither can fully address multifaceted\nrequirements for evidence. To this end, we propose LLMQA, a generalized\nframework that formulates the ODQA process into three basic steps: query\nexpansion, document selection, and answer generation, combining the superiority\nof both retrieval-based and generation-based evidence. Since LLMs exhibit their\nexcellent capabilities to accomplish various tasks, we instruct LLMs to play\nmultiple roles as generators, rerankers, and evaluators within our framework,\nintegrating them to collaborate in the ODQA process. Furthermore, we introduce\na novel prompt optimization algorithm to refine role-playing prompts and steer\nLLMs to produce higher-quality evidence and answers. Extensive experimental\nresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that\nLLMQA achieves the best performance in terms of both answer accuracy and\nevidence quality, showcasing its potential for advancing ODQA research and\napplications.\n","authors":["Hongda Sun","Yuxuan Liu","Chengwei Wu","Haiyu Yan","Cheng Tai","Xin Gao","Shuo Shang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.05217v1.pdf","comment":"TheWebConf 2024 (WWW 2024) oral, code repo:\n  https://github.com/EthanLeo-LYX/LLMQA"},{"id":"http://arxiv.org/abs/2403.05216v1","updated":"2024-03-08T11:00:09Z","published":"2024-03-08T11:00:09Z","title":"SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot\n  Stance Detection in Social Media","summary":"  Stance detection, as the task of determining the viewpoint of a social media\npost towards a target as 'favor' or 'against', has been understudied in the\nchallenging yet realistic scenario where there is limited labeled data for a\ncertain target. Our work advances research in few-shot stance detection by\nintroducing SocialPET, a socially informed approach to leveraging language\nmodels for the task. Our proposed approach builds on the Pattern Exploiting\nTraining (PET) technique, which addresses classification tasks as cloze\nquestions through the use of language models. To enhance the approach with\nsocial awareness, we exploit the social network structure surrounding social\nmedia posts. We prove the effectiveness of SocialPET on two stance datasets,\nMulti-target and P-Stance, outperforming competitive stance detection models as\nwell as the base model, PET, where the labeled instances for the target under\nstudy is as few as 100. When we delve into the results, we observe that\nSocialPET is comparatively strong in identifying instances of the `against'\nclass, where baseline models underperform.\n","authors":["Parisa Jamadi Khiabani","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2403.05216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05189v1","updated":"2024-03-08T10:09:57Z","published":"2024-03-08T10:09:57Z","title":"Tracing the Roots of Facts in Multilingual Language Models: Independent,\n  Shared, and Transferred Knowledge","summary":"  Acquiring factual knowledge for language models (LMs) in low-resource\nlanguages poses a serious challenge, thus resorting to cross-lingual transfer\nin multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and\nrepresent factual knowledge. Using the multilingual factual knowledge probing\ndataset, mLAMA, we first conducted a neuron investigation of ML-LMs\n(specifically, multilingual BERT). We then traced the roots of facts back to\nthe knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire\nspecific facts. We finally identified three patterns of acquiring and\nrepresenting facts in ML-LMs: language-independent, cross-lingual shared and\ntransferred, and devised methods for differentiating them. Our findings\nhighlight the challenge of maintaining consistent factual knowledge across\nlanguages, underscoring the need for better fact representation learning in\nML-LMs.\n","authors":["Xin Zhao","Naoki Yoshinaga","Daisuke Oba"],"pdf_url":"https://arxiv.org/pdf/2403.05189v1.pdf","comment":"EACL 2024 main conference"},{"id":"http://arxiv.org/abs/2403.05188v1","updated":"2024-03-08T09:56:45Z","published":"2024-03-08T09:56:45Z","title":"CommitBench: A Benchmark for Commit Message Generation","summary":"  Writing commit messages is a tedious daily task for many software developers,\nand often remains neglected. Automating this task has the potential to save\ntime while ensuring that messages are informative. A high-quality dataset and\nan objective benchmark are vital preconditions for solid research and\nevaluation towards this goal. We show that existing datasets exhibit various\nproblems, such as the quality of the commit selection, small sample sizes,\nduplicates, privacy issues, and missing licenses for redistribution. This can\nlead to unusable models and skewed evaluations, where inferior models achieve\nhigher evaluation scores due to biases in the data. We compile a new\nlarge-scale dataset, CommitBench, adopting best practices for dataset creation.\nWe sample commits from diverse projects with licenses that permit\nredistribution and apply our filtering and dataset enhancements to improve the\nquality of generated commit messages. We use CommitBench to compare existing\nmodels and show that other approaches are outperformed by a Transformer model\npretrained on source code. We hope to accelerate future research by publishing\nthe source code( https://github.com/Maxscha/commitbench ).\n","authors":["Maximilian Schall","Tamara Czinczoll","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2403.05188v1.pdf","comment":"Submitted and accepted at SANER 2024"},{"id":"http://arxiv.org/abs/2403.05186v1","updated":"2024-03-08T09:54:56Z","published":"2024-03-08T09:54:56Z","title":"ROUGE-K: Do Your Summaries Have Keywords?","summary":"  Keywords, that is, content-relevant words in summaries play an important role\nin efficient information conveyance, making it critical to assess if\nsystem-generated summaries contain such informative words during evaluation.\nHowever, existing evaluation metrics for extreme summarization models do not\npay explicit attention to keywords in summaries, leaving developers ignorant of\ntheir presence. To address this issue, we present a keyword-oriented evaluation\nmetric, dubbed ROUGE-K, which provides a quantitative answer to the question of\n-- \\textit{How well do summaries include keywords?} Through the lens of this\nkeyword-aware metric, we surprisingly find that a current strong baseline model\noften misses essential information in their summaries. Our analysis reveals\nthat human annotators indeed find the summaries with more keywords to be more\nrelevant to the source documents. This is an important yet previously\noverlooked aspect in evaluating summarization systems. Finally, to enhance\nkeyword inclusion, we propose four approaches for incorporating word importance\ninto a transformer-based model and experimentally show that it enables guiding\nmodels to include more keywords while keeping the overall quality. Our code is\nreleased at https://github.com/sobamchan/rougek.\n","authors":["Sotaro Takeshita","Simone Paolo Ponzetto","Kai Eckert"],"pdf_url":"https://arxiv.org/pdf/2403.05186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02869v2","updated":"2024-03-08T08:54:08Z","published":"2023-05-04T14:28:39Z","title":"Masked Structural Growth for 2x Faster Language Model Pre-training","summary":"  Accelerating large language model pre-training is a critical issue in present\nresearch. In this paper, we focus on speeding up pre-training by progressively\ngrowing from a small Transformer structure to a large one. There are two main\nresearch problems associated with progressive growth: determining the optimal\ngrowth schedule, and designing efficient growth operators. In terms of growth\nschedule, the impact of each single dimension on a schedule's efficiency is\nunder-explored by existing work. Regarding the growth operators, existing\nmethods rely on the initialization of new weights to inherit knowledge, and\nachieve only non-strict function preservation, limiting further improvements on\ntraining dynamics. To address these issues, we propose Masked Structural Growth\n(MSG), including (i) growth schedules involving all possible dimensions and\n(ii) strictly function-preserving growth operators that is independent of the\ninitialization of new weights. Experiments show that MSG is significantly\nfaster than related work: we achieve up to 2.2x speedup in pre-training\ndifferent types of language models while maintaining comparable or better\ndownstream performances. Code is publicly available at\nhttps://github.com/cofe-ai/MSG.\n","authors":["Yiqun Yao","Zheng Zhang","Jing Li","Yequan Wang"],"pdf_url":"https://arxiv.org/pdf/2305.02869v2.pdf","comment":"ICLR 2024 camera ready"},{"id":"http://arxiv.org/abs/2403.05152v1","updated":"2024-03-08T08:41:14Z","published":"2024-03-08T08:41:14Z","title":"Towards a Psychology of Machines: Large Language Models Predict Human\n  Memory","summary":"  Large language models (LLMs) are demonstrating remarkable capabilities across\nvarious tasks despite lacking a foundation in human cognition. This raises the\nquestion: can these models, beyond simply mimicking human language patterns,\noffer insights into the mechanisms underlying human cognition? This study\nexplores the ability of ChatGPT to predict human performance in a\nlanguage-based memory task. Building upon theories of text comprehension, we\nhypothesize that recognizing ambiguous sentences (e.g., \"Because Bill drinks\nwine is never kept in the house\") is facilitated by preceding them with\ncontextually relevant information. Participants, both human and ChatGPT, were\npresented with pairs of sentences. The second sentence was always a garden-path\nsentence designed to be inherently ambiguous, while the first sentence either\nprovided a fitting (e.g., \"Bill has chronic alcoholism\") or an unfitting\ncontext (e.g., \"Bill likes to play golf\"). We measured both human's and\nChatGPT's ratings of sentence relatedness, ChatGPT's memorability ratings for\nthe garden-path sentences, and humans' spontaneous memory for the garden-path\nsentences. The results revealed a striking alignment between ChatGPT's\nassessments and human performance. Sentences deemed more related and assessed\nas being more memorable by ChatGPT were indeed better remembered by humans,\neven though ChatGPT's internal mechanisms likely differ significantly from\nhuman cognition. This finding, which was confirmed with a robustness check\nemploying synonyms, underscores the potential of generative AI models to\npredict human performance accurately. We discuss the broader implications of\nthese findings for leveraging LLMs in the development of psychological theories\nand for gaining a deeper understanding of human cognition.\n","authors":["Markus Huff","Elanur Ulakçı"],"pdf_url":"https://arxiv.org/pdf/2403.05152v1.pdf","comment":"32 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.05132v1","updated":"2024-03-08T07:59:19Z","published":"2024-03-08T07:59:19Z","title":"ChatUIE: Exploring Chat-based Unified Information Extraction using Large\n  Language Models","summary":"  Recent advancements in large language models have shown impressive\nperformance in general chat. However, their domain-specific capabilities,\nparticularly in information extraction, have certain limitations. Extracting\nstructured information from natural language that deviates from known schemas\nor instructions has proven challenging for previous prompt-based methods. This\nmotivated us to explore domain-specific modeling in chat-based language models\nas a solution for extracting structured information from natural language. In\nthis paper, we present ChatUIE, an innovative unified information extraction\nframework built upon ChatGLM. Simultaneously, reinforcement learning is\nemployed to improve and align various tasks that involve confusing and limited\nsamples. Furthermore, we integrate generation constraints to address the issue\nof generating elements that are not present in the input. Our experimental\nresults demonstrate that ChatUIE can significantly improve the performance of\ninformation extraction with a slight decrease in chatting ability.\n","authors":["Jun Xu","Mengshu Sun","Zhiqiang Zhang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05132v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2312.14197v3","updated":"2024-03-08T07:58:48Z","published":"2023-12-21T01:08:39Z","title":"Benchmarking and Defending Against Indirect Prompt Injection Attacks on\n  Large Language Models","summary":"  The integration of large language models (LLMs) with external content has\nenabled more up-to-date and wide-ranging applications of LLMs, such as\nMicrosoft Copilot. However, this integration has also exposed LLMs to the risk\nof indirect prompt injection attacks, where an attacker can embed malicious\ninstructions within external content, compromising LLM output and causing\nresponses to deviate from user expectations. To investigate this important but\nunderexplored issue, we introduce the first benchmark for indirect prompt\ninjection attacks, named BIPIA, to evaluate the risk of such attacks. Based on\nthe evaluation, our work makes a key analysis of the underlying reason for the\nsuccess of the attack, namely the inability of LLMs to distinguish between\ninstructions and external content and the absence of LLMs' awareness to not\nexecute instructions within external content. Building upon this analysis, we\ndevelop two black-box methods based on prompt learning and a white-box defense\nmethod based on fine-tuning with adversarial training accordingly. Experimental\nresults demonstrate that black-box defenses are highly effective in mitigating\nthese attacks, while the white-box defense reduces the attack success rate to\nnear-zero levels. Overall, our work systematically investigates indirect prompt\ninjection attacks by introducing a benchmark, analyzing the underlying reason\nfor the success of the attack, and developing an initial set of defenses.\n","authors":["Jingwei Yi","Yueqi Xie","Bin Zhu","Emre Kiciman","Guangzhong Sun","Xing Xie","Fangzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2312.14197v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05871v2","updated":"2024-03-08T07:56:57Z","published":"2024-01-11T12:27:33Z","title":"Enhancing Personality Recognition in Dialogue by Data Augmentation and\n  Heterogeneous Conversational Graph Networks","summary":"  Personality recognition is useful for enhancing robots' ability to tailor\nuser-adaptive responses, thus fostering rich human-robot interactions. One of\nthe challenges in this task is a limited number of speakers in existing\ndialogue corpora, which hampers the development of robust, speaker-independent\npersonality recognition models. Additionally, accurately modeling both the\ninterdependencies among interlocutors and the intra-dependencies within the\nspeaker in dialogues remains a significant issue. To address the first\nchallenge, we introduce personality trait interpolation for speaker data\naugmentation. For the second, we propose heterogeneous conversational graph\nnetworks to independently capture both contextual influences and inherent\npersonality traits. Evaluations on the RealPersonaChat corpus demonstrate our\nmethod's significant improvements over existing baselines.\n","authors":["Yahui Fu","Haiyue Song","Tianyu Zhao","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2401.05871v2.pdf","comment":"This paper has been accepted for presentation at International\n  Workshop on Spoken Dialogue Systems Technology 2024 (IWSDS 2024)"},{"id":"http://arxiv.org/abs/2403.05101v1","updated":"2024-03-08T07:06:43Z","published":"2024-03-08T07:06:43Z","title":"Rule-driven News Captioning","summary":"  News captioning task aims to generate sentences by describing named entities\nor concrete events for an image with its news article. Existing methods have\nachieved remarkable results by relying on the large-scale pre-trained models,\nwhich primarily focus on the correlations between the input news content and\nthe output predictions. However, the news captioning requires adhering to some\nfundamental rules of news reporting, such as accurately describing the\nindividuals and actions associated with the event. In this paper, we propose\nthe rule-driven news captioning method, which can generate image descriptions\nfollowing designated rule signal. Specifically, we first design the news-aware\nsemantic rule for the descriptions. This rule incorporates the primary action\ndepicted in the image (e.g., \"performing\") and the roles played by named\nentities involved in the action (e.g., \"Agent\" and \"Place\"). Second, we inject\nthis semantic rule into the large-scale pre-trained model, BART, with the\nprefix-tuning strategy, where multiple encoder layers are embedded with\nnews-aware semantic rule. Finally, we can effectively guide BART to generate\nnews sentences that comply with the designated rule. Extensive experiments on\ntwo widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the\neffectiveness of our method.\n","authors":["Ning Xu","Tingting Zhang","Hongshuo Tian","Yongdong Zhang","An-An Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05922v3","updated":"2024-03-08T06:51:43Z","published":"2023-11-10T08:12:00Z","title":"Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation\n  Extraction","summary":"  Few-shot relation extraction involves identifying the type of relationship\nbetween two specific entities within a text, using a limited number of\nannotated samples. A variety of solutions to this problem have emerged by\napplying meta-learning and neural graph techniques which typically necessitate\na training process for adaptation. Recently, the strategy of in-context\nlearning has been demonstrating notable results without the need of training.\nFew studies have already utilized in-context learning for zero-shot information\nextraction. Unfortunately, the evidence for inference is either not considered\nor implicitly modeled during the construction of chain-of-thought prompts. In\nthis paper, we propose a novel approach for few-shot relation extraction using\nlarge language models, named CoT-ER, chain-of-thought with explicit evidence\nreasoning. In particular, CoT-ER first induces large language models to\ngenerate evidences using task-specific and concept-level knowledge. Then these\nevidences are explicitly incorporated into chain-of-thought prompting for\nrelation extraction. Experimental results demonstrate that our CoT-ER approach\n(with 0% training data) achieves competitive performance compared to the\nfully-supervised (with 100% training data) state-of-the-art approach on the\nFewRel1.0 and FewRel2.0 datasets.\n","authors":["Xilai Ma","Jing Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.05922v3.pdf","comment":"Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2403.04732v2","updated":"2024-03-08T06:47:08Z","published":"2024-03-07T18:35:54Z","title":"How Far Are We from Intelligent Visual Deductive Reasoning?","summary":"  Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.\n","authors":["Yizhe Zhang","He Bai","Ruixiang Zhang","Jiatao Gu","Shuangfei Zhai","Josh Susskind","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2403.04732v2.pdf","comment":"ICLR 2024 AGI workshop. https://github.com/apple/ml-rpm-bench"},{"id":"http://arxiv.org/abs/2312.00849v2","updated":"2024-03-08T06:42:37Z","published":"2023-12-01T11:36:08Z","title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from\n  Fine-grained Correctional Human Feedback","summary":"  Multimodal Large Language Models (MLLMs) have recently demonstrated\nimpressive capabilities in multimodal understanding, reasoning, and\ninteraction. However, existing MLLMs prevalently suffer from serious\nhallucination problems, generating text that is not factually grounded in\nassociated images. The problem makes existing MLLMs untrustworthy and thus\nimpractical in real-world (especially high-stakes) applications. To address the\nchallenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior\nalignment from fine-grained correctional human feedback. Specifically, RLHF-V\ncollects human preference in the form of segment-level corrections on\nhallucinations, and performs dense direct preference optimization over the\nhuman feedback. Comprehensive experiments on five benchmarks in both automatic\nand human evaluation show that, RLHF-V can enable substantially more\ntrustworthy MLLM behaviors with promising data and computation efficiency.\nRemarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the\nhallucination rate of the base MLLM by 34.8%, outperforming the concurrent\nLLaVA-RLHF trained on 10k annotated data. The final model achieves\nstate-of-the-art performance in trustworthiness among open-source MLLMs, and\nshows better robustness than GPT-4V in preventing hallucinations aroused from\nover-generalization. We open-source our code, model, and data at\nhttps://github.com/RLHF-V/RLHF-V.\n","authors":["Tianyu Yu","Yuan Yao","Haoye Zhang","Taiwen He","Yifeng Han","Ganqu Cui","Jinyi Hu","Zhiyuan Liu","Hai-Tao Zheng","Maosong Sun","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2312.00849v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05065v1","updated":"2024-03-08T05:34:29Z","published":"2024-03-08T05:34:29Z","title":"Can we obtain significant success in RST discourse parsing by using\n  Large Language Models?","summary":"  Recently, decoder-only pre-trained large language models (LLMs), with several\ntens of billion parameters, have significantly impacted a wide range of natural\nlanguage processing (NLP) tasks. While encoder-only or encoder-decoder\npre-trained language models have already proved to be effective in discourse\nparsing, the extent to which LLMs can perform this task remains an open\nresearch question. Therefore, this paper explores how beneficial such LLMs are\nfor Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing\nprocess for both fundamental top-down and bottom-up strategies is converted\ninto prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with\nQLoRA, which has fewer parameters that can be tuned. Experimental results on\nthree benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate\nthat Llama 2 with 70 billion parameters in the bottom-up strategy obtained\nstate-of-the-art (SOTA) results with significant differences. Furthermore, our\nparsers demonstrated generalizability when evaluated on RST-DT, showing that,\nin spite of being trained with the GUM corpus, it obtained similar performances\nto those of existing parsers trained with RST-DT.\n","authors":["Aru Maekawa","Tsutomu Hirao","Hidetaka Kamigaito","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2403.05065v1.pdf","comment":"Accepted in the main conference of EACL 2024"},{"id":"http://arxiv.org/abs/2309.14556v3","updated":"2024-03-08T05:20:08Z","published":"2023-09-25T22:02:46Z","title":"Art or Artifice? Large Language Models and the False Promise of\n  Creativity","summary":"  Researchers have argued that large language models (LLMs) exhibit\nhigh-quality writing capabilities from blogs to stories. However, evaluating\nobjectively the creativity of a piece of writing is challenging. Inspired by\nthe Torrance Test of Creative Thinking (TTCT), which measures creativity as a\nprocess, we use the Consensual Assessment Technique [3] and propose the\nTorrance Test of Creative Writing (TTCW) to evaluate creativity as a product.\nTTCW consists of 14 binary tests organized into the original dimensions of\nFluency, Flexibility, Originality, and Elaboration. We recruit 10 creative\nwriters and implement a human assessment of 48 stories written either by\nprofessional authors or LLMs using TTCW. Our analysis shows that LLM-generated\nstories pass 3-10X less TTCW tests than stories written by professionals. In\naddition, we explore the use of LLMs as assessors to automate the TTCW\nevaluation, revealing that none of the LLMs positively correlate with the\nexpert assessments.\n","authors":["Tuhin Chakrabarty","Philippe Laban","Divyansh Agarwal","Smaranda Muresan","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2309.14556v3.pdf","comment":"ACM CHI 2024"},{"id":"http://arxiv.org/abs/2403.04460v2","updated":"2024-03-08T04:54:31Z","published":"2024-03-07T12:57:16Z","title":"Pearl: A Review-driven Persona-Knowledge Grounded Conversational\n  Recommendation Dataset","summary":"  Conversational recommender system is an emerging area that has garnered an\nincreasing interest in the community, especially with the advancements in large\nlanguage models (LLMs) that enable diverse reasoning over conversational input.\nDespite the progress, the field has many aspects left to explore. The currently\navailable public datasets for conversational recommendation lack specific user\npreferences and explanations for recommendations, hindering high-quality\nrecommendations. To address such challenges, we present a novel conversational\nrecommendation dataset named PEARL, synthesized with persona- and\nknowledge-augmented LLM simulators. We obtain detailed persona and knowledge\nfrom real-world reviews and construct a large-scale dataset with over 57k\ndialogues. Our experimental results demonstrate that utterances in PEARL\ninclude more specific user preferences, show expertise in the target domain,\nand provide recommendations more relevant to the dialogue context than those in\nprior datasets.\n","authors":["Minjin Kim","Minju Kim","Hana Kim","Beong-woo Kwak","Soyeon Chun","Hyunseo Kim","SeongKu Kang","Youngjae Yu","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2403.04460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04481v2","updated":"2024-03-08T04:47:16Z","published":"2024-03-07T13:30:52Z","title":"Do Large Language Model Understand Multi-Intent Spoken Language ?","summary":"  This study marks a significant advancement by harnessing Large Language\nModels (LLMs) for multi-intent spoken language understanding (SLU), proposing a\nunique methodology that capitalizes on the generative power of LLMs within an\nSLU context. Our innovative technique reconfigures entity slots specifically\nfor LLM application in multi-intent SLU environments and introduces the concept\nof Sub-Intent Instruction (SII), enhancing the dissection and interpretation of\nintricate, multi-intent communication within varied domains. The resultant\ndatasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing\nbenchmarks. Our research illustrates that LLMs can match and potentially excel\nbeyond the capabilities of current state-of-the-art multi-intent SLU models. It\nfurther explores LLM efficacy across various intent configurations and dataset\nproportions. Moreover, we introduce two pioneering metrics, Entity Slot\nAccuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth\nanalysis of LLM proficiency in this complex field.\n","authors":["Shangjian Yin","Peijie Huang","Yuhong Xu","Haojing Huang","Jiatian Chen"],"pdf_url":"https://arxiv.org/pdf/2403.04481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05045v1","updated":"2024-03-08T04:44:25Z","published":"2024-03-08T04:44:25Z","title":"Are Human Conversations Special? A Large Language Model Perspective","summary":"  This study analyzes changes in the attention mechanisms of large language\nmodels (LLMs) when used to understand natural conversations between humans\n(human-human). We analyze three use cases of LLMs: interactions over web\ncontent, code, and mathematical texts. By analyzing attention distance,\ndispersion, and interdependency across these domains, we highlight the unique\nchallenges posed by conversational data. Notably, conversations require nuanced\nhandling of long-term contextual relationships and exhibit higher complexity\nthrough their attention patterns. Our findings reveal that while language\nmodels exhibit domain-specific attention behaviors, there is a significant gap\nin their ability to specialize in human conversations. Through detailed\nattention entropy analysis and t-SNE visualizations, we demonstrate the need\nfor models trained with a diverse array of high-quality conversational data to\nenhance understanding and generation of human-like dialogue. This research\nhighlights the importance of domain specialization in language models and\nsuggests pathways for future advancement in modeling human conversational\nnuances.\n","authors":["Toshish Jawale","Chaitanya Animesh","Sekhar Vallath","Kartik Talamadupula","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.05045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04437v2","updated":"2024-03-08T04:03:27Z","published":"2024-02-06T22:15:09Z","title":"Structured Entity Extraction Using Large Language Models","summary":"  Recent advances in machine learning have significantly impacted the field of\ninformation extraction, with Large Language Models (LLMs) playing a pivotal\nrole in extracting structured information from unstructured text. This paper\nexplores the challenges and limitations of current methodologies in structured\nentity extraction and introduces a novel approach to address these issues. We\ncontribute to the field by first introducing and formalizing the task of\nStructured Entity Extraction (SEE), followed by proposing Approximate Entity\nSet OverlaP (AESOP) Metric designed to appropriately assess model performance\non this task. Later, we propose a new model that harnesses the power of LLMs\nfor enhanced effectiveness and efficiency through decomposing the entire\nextraction task into multiple stages. Quantitative evaluation and human\nside-by-side evaluation confirm that our model outperforms baselines, offering\npromising directions for future advancements in structured entity extraction.\n","authors":["Haolun Wu","Ye Yuan","Liana Mikaelyan","Alexander Meulemans","Xue Liu","James Hensman","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.04437v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.05023v1","updated":"2024-03-08T03:55:27Z","published":"2024-03-08T03:55:27Z","title":"Towards Multimodal Sentiment Analysis Debiasing via Bias Purification","summary":"  Multimodal Sentiment Analysis (MSA) aims to understand human intentions by\nintegrating emotion-related clues from diverse modalities, such as visual,\nlanguage, and audio. Unfortunately, the current MSA task invariably suffers\nfrom unplanned dataset biases, particularly multimodal utterance-level label\nbias and word-level context bias. These harmful biases potentially mislead\nmodels to focus on statistical shortcuts and spurious correlations, causing\nsevere performance bottlenecks. To alleviate these issues, we present a\nMultimodal Counterfactual Inference Sentiment (MCIS) analysis framework based\non causality rather than conventional likelihood. Concretely, we first\nformulate a causal graph to discover harmful biases from already-trained\nvanilla models. In the inference phase, given a factual multimodal input, MCIS\nimagines two counterfactual scenarios to purify and mitigate these biases.\nThen, MCIS can make unbiased decisions from biased observations by comparing\nfactual and counterfactual outcomes. We conduct extensive experiments on\nseveral standard MSA benchmarks. Qualitative and quantitative results show the\neffectiveness of the proposed framework.\n","authors":["Dingkang Yang","Mingcheng Li","Dongling Xiao","Yang Liu","Kun Yang","Zhaoyu Chen","Yuzheng Wang","Peng Zhai","Ke Li","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05023v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.05020v1","updated":"2024-03-08T03:49:17Z","published":"2024-03-08T03:49:17Z","title":"Is this the real life? Is this just fantasy? The Misleading Success of\n  Simulating Social Interactions With LLMs","summary":"  Recent advances in large language models (LLM) have enabled richer social\nsimulations, allowing for the study of various social phenomena with LLM-based\nagents. However, most work has used an omniscient perspective on these\nsimulations (e.g., single LLM to generate all interlocutors), which is\nfundamentally at odds with the non-omniscient, information asymmetric\ninteractions that humans have. To examine these differences, we develop an\nevaluation framework to simulate social interactions with LLMs in various\nsettings (omniscient, non-omniscient). Our experiments show that interlocutors\nsimulated omnisciently are much more successful at accomplishing social goals\ncompared to non-omniscient agents, despite the latter being the more realistic\nsetting. Furthermore, we demonstrate that learning from omniscient simulations\nimproves the apparent naturalness of interactions but scarcely enhances goal\nachievement in cooperative scenarios. Our findings indicate that addressing\ninformation asymmetry remains a fundamental challenge for LLM-based agents.\n","authors":["Xuhui Zhou","Zhe Su","Tiwalayo Eisape","Hyunwoo Kim","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2403.05020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14891v3","updated":"2024-03-08T03:47:32Z","published":"2024-02-22T12:36:31Z","title":"LLMBind: A Unified Modality-Task Integration Framework","summary":"  While recent progress in multimodal large language models tackles various\nmodality tasks, they posses limited integration capabilities for complex\nmulti-modality tasks, consequently constraining the development of the field.\nIn this work, we take the initiative to explore and propose the LLMBind, a\nunified framework for modality task integration, which binds Large Language\nModels and corresponding pre-trained task models with task-specific tokens.\nConsequently, LLMBind can interpret inputs and produce outputs in versatile\ncombinations of image, text, video, and audio. Specifically, we introduce a\nMixture-of-Experts technique to enable effective learning for different\nmultimodal tasks through collaboration among diverse experts. Furthermore, we\ncreate a multi-task dataset comprising 400k instruction data, which unlocks the\nability for interactive visual generation and editing tasks. Extensive\nexperiments show the effectiveness of our framework across various tasks,\nincluding image, video, audio generation, image segmentation, and image\nediting. More encouragingly, our framework can be easily extended to other\nmodality tasks, showcasing the promising potential of creating a unified AI\nagent for modeling universal modalities.\n","authors":["Bin Zhu","Peng Jin","Munan Ning","Bin Lin","Jinfa Huang","Qi Song","Jiaxi Cui","Junwu Zhang","Zhenyu Tang","Mingjun Pan","Xing Zhou","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.14891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11351v2","updated":"2024-03-08T03:07:18Z","published":"2023-08-22T11:00:09Z","title":"MMAPS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product\n  Summarization","summary":"  Given the long textual product information and the product image, Multi-modal\nProduct Summarization (MPS) aims to increase customers' desire to purchase by\nhighlighting product characteristics with a short textual summary. Existing MPS\nmethods can produce promising results. Nevertheless, they still 1) lack\nend-to-end product summarization, 2) lack multi-grained multi-modal modeling,\nand 3) lack multi-modal attribute modeling. To improve MPS, we propose an\nend-to-end multi-grained multi-modal attribute-aware product summarization\nmethod (MMAPS) for generating high-quality product summaries in e-commerce.\nMMAPS jointly models product attributes and generates product summaries. We\ndesign several multi-grained multi-modal tasks to better guide the multi-modal\nlearning of MMAPS. Furthermore, we model product attributes based on both text\nand image modalities so that multi-modal product characteristics can be\nmanifested in the generated summaries. Extensive experiments on a real\nlarge-scale Chinese e-commence dataset demonstrate that our model outperforms\nstate-of-the-art product summarization methods w.r.t. several summarization\nmetrics. Our code is publicly available at: https://github.com/KDEGroup/MMAPS.\n","authors":["Tao Chen","Ze Lin","Hui Li","Jiayi Ji","Yiyi Zhou","Guanbin Li","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2308.11351v2.pdf","comment":"LREC-COLING 2024.11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.05004v1","updated":"2024-03-08T03:03:20Z","published":"2024-03-08T03:03:20Z","title":"Can't Remember Details in Long Documents? You Need Some R&R","summary":"  Long-context large language models (LLMs) hold promise for tasks such as\nquestion-answering (QA) over long documents, but they tend to miss important\ninformation in the middle of context documents (arXiv:2307.03172v3). Here, we\nintroduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods\ncalled $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to\nalleviate this effect in document-based QA. In reprompting, we repeat the\nprompt instructions periodically throughout the context document to remind the\nLLM of its original task. In ICR, rather than instructing the LLM to answer the\nquestion directly, we instruct it to retrieve the top $k$ passage numbers most\nrelevant to the given question, which are then used as an abbreviated context\nin a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents\nup to 80k tokens in length and observe a 16-point boost in QA accuracy on\naverage. Our further analysis suggests that R&R improves performance on long\ndocument-based QA because it reduces the distance between relevant context and\nthe instructions. Finally, we show that compared to short-context chunkwise\nmethods, R&R enables the use of larger chunks that cost fewer LLM calls and\noutput tokens, while minimizing the drop in accuracy.\n","authors":["Devanshu Agrawal","Shang Gao","Martin Gajek"],"pdf_url":"https://arxiv.org/pdf/2403.05004v1.pdf","comment":"13 pages, 1 figure, 9 tables. For associated code repository see\n  https://github.com/casetext/r-and-r"},{"id":"http://arxiv.org/abs/2403.04997v1","updated":"2024-03-08T02:24:27Z","published":"2024-03-08T02:24:27Z","title":"DiffChat: Learning to Chat with Text-to-Image Synthesis Models for\n  Interactive Image Creation","summary":"  We present DiffChat, a novel method to align Large Language Models (LLMs) to\n\"chat\" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable\nDiffusion) for interactive image creation. Given a raw prompt/image and a\nuser-specified instruction, DiffChat can effectively make appropriate\nmodifications and generate the target prompt, which can be leveraged to create\nthe target image of high quality. To achieve this, we first collect an\ninstruction-following prompt engineering dataset named InstructPE for the\nsupervised training of DiffChat. Next, we propose a reinforcement learning\nframework with the feedback of three core criteria for image creation, i.e.,\naesthetics, user preference, and content integrity. It involves an action-space\ndynamic modification technique to obtain more relevant positive samples and\nharder negative samples during the off-policy sampling. Content integrity is\nalso introduced into the value estimation function for further improvement of\nproduced images. Our method can exhibit superior performance than baseline\nmodels and strong competitors based on both automatic and human evaluations,\nwhich fully demonstrates its effectiveness.\n","authors":["Jiapeng Wang","Chengyu Wang","Tingfeng Cao","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2403.04997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04964v1","updated":"2024-03-08T00:27:57Z","published":"2024-03-08T00:27:57Z","title":"Tell me the truth: A system to measure the trustworthiness of Large\n  Language Models","summary":"  Large Language Models (LLM) have taken the front seat in most of the news\nsince November 2023, when ChatGPT was introduced. After more than one year, one\nof the major reasons companies are resistant to adopting them is the limited\nconfidence they have in the trustworthiness of those systems. In a study by\n(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in\nidentifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics\nfound that ChatGPT has an accuracy rate of 17% percent when diagnosing\npediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust\nis a relative, subject condition that can change based on culture, domain,\nindividuals. And then, given a domain, how can the trustworthiness of a system\nbe measured? In this paper, I present a systematic approach to measure\ntrustworthiness based on a predefined ground truth, represented as a knowledge\ngraph of the domain. The approach is a process with humans in the loop to\nvalidate the representation of the domain and to fine-tune the system.\n  Measuring the trustworthiness would be essential for all the entities\noperating in critical environments, such as healthcare, defense, finance, but\nit would be very relevant for all the users of LLMs.\n","authors":["Carlo Lipizzi"],"pdf_url":"https://arxiv.org/pdf/2403.04964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04963v1","updated":"2024-03-08T00:19:24Z","published":"2024-03-08T00:19:24Z","title":"An In-depth Evaluation of GPT-4 in Sentence Simplification with\n  Error-based Human Assessment","summary":"  Sentence simplification, which rewrites a sentence to be easier to read and\nunderstand, is a promising technique to help people with various reading\ndifficulties. With the rise of advanced large language models (LLMs),\nevaluating their performance in sentence simplification has become imperative.\nRecent studies have used both automatic metrics and human evaluations to assess\nthe simplification abilities of LLMs. However, the suitability of existing\nevaluation methodologies for LLMs remains in question. First, the suitability\nof current automatic metrics on LLMs' simplification evaluation is still\nuncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the GPT-4's simplification capabilities. Results show that\nGPT-4 generally generates fewer erroneous simplification outputs compared to\nthe current state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that while these metrics are effective for significant quality\ndifferences, they lack sufficient sensitivity to assess the overall\nhigh-quality simplification by GPT-4.\n","authors":["Xuanxin Wu","Yuki Arase"],"pdf_url":"https://arxiv.org/pdf/2403.04963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16281v3","updated":"2024-03-08T00:15:02Z","published":"2023-03-28T19:49:58Z","title":"A \"Perspectival\" Mirror of the Elephant: Investigating Language Bias on\n  Google, ChatGPT, YouTube, and Wikipedia","summary":"  Contrary to Google Search's mission of delivering information from \"many\nangles so you can form your own understanding of the world,\" we find that\nGoogle and its most prominent returned results - Wikipedia and YouTube - simply\nreflect a narrow set of culturally dominant views tied to the search language\nfor complex topics like \"Buddhism,\" \"Liberalism,\" \"colonization,\" \"Iran\" and\n\"America.\" Simply stated, they present, to varying degrees, distinct\ninformation across the same search in different languages, a phenomenon we call\nlanguage bias. This paper presents evidence and analysis of language bias and\ndiscusses its larger social implications. We find that our online searches and\nemerging tools like ChatGPT turn us into the proverbial blind person touching a\nsmall portion of an elephant, ignorant of the existence of other cultural\nperspectives. Language bias sets a strong yet invisible cultural barrier\nonline, where each language group thinks they can see other groups through\nsearches, but in fact, what they see is their own reflection.\n","authors":["Queenie Luo","Michael J. Puett","Michael D. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.16281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00835v3","updated":"2024-03-08T00:13:31Z","published":"2024-02-28T20:17:04Z","title":"CLLMs: Consistency Large Language Models","summary":"  Parallel decoding methods such as Jacobi decoding show promise for more\nefficient LLM inference as it breaks the sequential nature of the LLM decoding\nprocess and transforms it into parallelizable computation. However, in\npractice, it achieves little speedup compared to traditional autoregressive\n(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\nmore than one token in a single fixed-point iteration step. To address this, we\ndevelop a new approach aimed at realizing fast convergence from any state to\nthe fixed point on a Jacobi trajectory. This is accomplished by refining the\ntarget LLM to consistently predict the fixed point given any state as input.\nExtensive experiments demonstrate the effectiveness of our method, showing\n2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving\ngeneration quality across both domain-specific and open-domain benchmarks.\n","authors":["Siqi Kou","Lanxiang Hu","Zhezhi He","Zhijie Deng","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.00835v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04960v1","updated":"2024-03-08T00:02:30Z","published":"2024-03-08T00:02:30Z","title":"SecGPT: An Execution Isolation Architecture for LLM-Based Systems","summary":"  Large language models (LLMs) extended as systems, such as ChatGPT, have begun\nsupporting third-party applications. These LLM apps leverage the de facto\nnatural language-based automated execution paradigm of LLMs: that is, apps and\ntheir interactions are defined in natural language, provided access to user\ndata, and allowed to freely interact with each other and the system. These LLM\napp ecosystems resemble the settings of earlier computing platforms, where\nthere was insufficient isolation between apps and the system. Because\nthird-party apps may not be trustworthy, and exacerbated by the imprecision of\nthe natural language interfaces, the current designs pose security and privacy\nrisks for users. In this paper, we propose SecGPT, an architecture for\nLLM-based systems that aims to mitigate the security and privacy issues that\narise with the execution of third-party apps. SecGPT's key idea is to isolate\nthe execution of apps and more precisely mediate their interactions outside of\ntheir isolated environments. We evaluate SecGPT against a number of case study\nattacks and demonstrate that it protects against many security, privacy, and\nsafety issues that exist in non-isolated LLM-based systems. The performance\noverhead incurred by SecGPT to improve security is under 0.3x for\nthree-quarters of the tested queries. To foster follow-up research, we release\nSecGPT's source code at https://github.com/llm-platform-security/SecGPT.\n","authors":["Yuhao Wu","Franziska Roesner","Tadayoshi Kohno","Ning Zhang","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2403.04960v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.05535v1","updated":"2024-03-08T18:58:46Z","published":"2024-03-08T18:58:46Z","title":"Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in\n  Images and Videos","summary":"  We introduce LaGTran, a novel framework that utilizes readily available or\neasily acquired text descriptions to guide robust transfer of discriminative\nknowledge from labeled source to unlabeled target data with domain shifts.\nWhile unsupervised adaptation methods have been established to address this\nproblem, they show limitations in handling challenging domain shifts due to\ntheir exclusive operation within the pixel-space. Motivated by our observation\nthat semantically richer text modality has more favorable transfer properties,\nwe devise a transfer mechanism to use a source-trained text-classifier to\ngenerate predictions on the target text descriptions, and utilize these\npredictions as supervision for the corresponding images. Our approach driven by\nlanguage guidance is surprisingly easy and simple, yet significantly\noutperforms all prior approaches on challenging datasets like GeoNet and\nDomainNet, validating its extreme effectiveness. To further extend the scope of\nour study beyond images, we introduce a new benchmark to study ego-exo transfer\nin videos and find that our language-aided LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.\n","authors":["Tarun Kalluri","Bodhisattwa Prasad Majumder","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2403.05535v1.pdf","comment":"Project Page and Code: https://tarun005.github.io/lagtran/"},{"id":"http://arxiv.org/abs/2403.05532v1","updated":"2024-03-08T18:57:00Z","published":"2024-03-08T18:57:00Z","title":"Tune without Validation: Searching for Learning Rate and Weight Decay on\n  Training Sets","summary":"  We introduce Tune without Validation (Twin), a pipeline for tuning learning\nrate and weight decay without validation sets. We leverage a recent theoretical\nframework concerning learning phases in hypothesis space to devise a heuristic\nthat predicts what hyper-parameter (HP) combinations yield better\ngeneralization. Twin performs a grid search of trials according to an\nearly-/non-early-stopping scheduler and then segments the region that provides\nthe best results in terms of training loss. Among these trials, the weight norm\nstrongly correlates with predicting generalization. To assess the effectiveness\nof Twin, we run extensive experiments on 20 image classification datasets and\ntrain several families of deep networks, including convolutional, transformer,\nand feed-forward models. We demonstrate proper HP selection when training from\nscratch and fine-tuning, emphasizing small-sample scenarios.\n","authors":["Lorenzo Brigato","Stavroula Mougiakakou"],"pdf_url":"https://arxiv.org/pdf/2403.05532v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2403.05523v1","updated":"2024-03-08T18:44:23Z","published":"2024-03-08T18:44:23Z","title":"Beyond Finite Data: Towards Data-free Out-of-distribution Generalization\n  via Extrapola","summary":"  Out-of-distribution (OOD) generalization is a favorable yet challenging\nproperty for deep neural networks. The core challenges lie in the limited\navailability of source domains that help models learn an invariant\nrepresentation from the spurious features. Various domain augmentation have\nbeen proposed but largely rely on interpolating existing domains and frequently\nface difficulties in creating truly \"novel\" domains. Humans, on the other hand,\ncan easily extrapolate novel domains, thus, an intriguing question arises: How\ncan neural networks extrapolate like humans and achieve OOD generalization?\n  We introduce a novel approach to domain extrapolation that leverages\nreasoning ability and the extensive knowledge encapsulated within large\nlanguage models (LLMs) to synthesize entirely new domains. Starting with the\nclass of interest, we query the LLMs to extract relevant knowledge for these\nnovel domains. We then bridge the gap between the text-centric knowledge\nderived from LLMs and the pixel input space of the model using text-to-image\ngeneration techniques. By augmenting the training set of domain generalization\ndatasets with high-fidelity, photo-realistic images of these new domains, we\nachieve significant improvements over all existing methods, as demonstrated in\nboth single and multi-domain generalization across various benchmarks.\n  With the ability to extrapolate any domains for any class, our method has the\npotential to learn a generalized model for any task without any data. To\nillustrate, we put forth a much more difficult setting termed, data-free domain\ngeneralization, that aims to learn a generalized model in the absence of any\ncollected data. Our empirical findings support the above argument and our\nmethods exhibit commendable performance in this setting, even surpassing the\nsupervised setting by approximately 1-2\\% on datasets such as VLCS.\n","authors":["Yijiang Li","Sucheng Ren","Weipeng Deng","Yuzhi Xu","Ying Gao","Edith Ngai","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05523v1.pdf","comment":"Preprint. Paper under review"},{"id":"http://arxiv.org/abs/2403.05521v1","updated":"2024-03-08T18:43:28Z","published":"2024-03-08T18:43:28Z","title":"Probabilistic Image-Driven Traffic Modeling via Remote Sensing","summary":"  This work addresses the task of modeling spatiotemporal traffic patterns\ndirectly from overhead imagery, which we refer to as image-driven traffic\nmodeling. We extend this line of work and introduce a multi-modal, multi-task\ntransformer-based segmentation architecture that can be used to create dense\ncity-scale traffic models. Our approach includes a geo-temporal positional\nencoding module for integrating geo-temporal context and a probabilistic\nobjective function for estimating traffic speeds that naturally models temporal\nvariations. We evaluate our method extensively using the Dynamic Traffic Speeds\n(DTS) benchmark dataset and significantly improve the state-of-the-art.\nFinally, we introduce the DTS++ dataset to support mobility-related location\nadaptation experiments.\n","authors":["Scott Workman","Armin Hadzic"],"pdf_url":"https://arxiv.org/pdf/2403.05521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03762v5","updated":"2024-03-08T18:42:34Z","published":"2024-02-06T07:07:33Z","title":"MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction","summary":"  Monocular SLAM has received a lot of attention due to its simple RGB inputs\nand the lifting of complex sensor constraints. However, existing monocular SLAM\nsystems are designed for bounded scenes, restricting the applicability of SLAM\nsystems. To address this limitation, we propose MoD-SLAM, the first monocular\nNeRF-based dense mapping method that allows 3D reconstruction in real-time in\nunbounded scenes. Specifically, we introduce a Gaussian-based unbounded scene\nrepresentation approach to solve the challenge of mapping scenes without\nboundaries. This strategy is essential to extend the SLAM application.\nMoreover, a depth estimation module in the front-end is designed to extract\naccurate priori depth values to supervise mapping and tracking processes. By\nintroducing a robust depth loss term into the tracking process, our SLAM system\nachieves more precise pose estimation in large-scale scenes. Our experiments on\ntwo standard datasets show that MoD-SLAM achieves competitive performance,\nimproving the accuracy of the 3D reconstruction and localization by up to 30%\nand 15% respectively compared with existing state-of-the-art monocular SLAM\nsystems.\n","authors":["Heng Zhou","Zhetao Guo","Shuhong Liu","Lechen Zhang","Qihao Wang","Yuxiang Ren","Mingrui Li"],"pdf_url":"https://arxiv.org/pdf/2402.03762v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04634v2","updated":"2024-03-08T18:28:28Z","published":"2024-03-07T16:18:28Z","title":"Pix2Gif: Motion-Guided Diffusion for GIF Generation","summary":"  We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)\ngeneration. We tackle this problem differently by formulating the task as an\nimage translation problem steered by text and motion magnitude prompts, as\nshown in teaser fig. To ensure that the model adheres to motion guidance, we\npropose a new motion-guided warping module to spatially transform the features\nof the source image conditioned on the two types of prompts. Furthermore, we\nintroduce a perceptual loss to ensure the transformed feature map remains\nwithin the same space as the target image, ensuring content consistency and\ncoherence. In preparation for the model training, we meticulously curated data\nby extracting coherent image frames from the TGIF video-caption dataset, which\nprovides rich information about the temporal changes of subjects. After\npretraining, we apply our model in a zero-shot manner to a number of video\ndatasets. Extensive qualitative and quantitative experiments demonstrate the\neffectiveness of our model -- it not only captures the semantic prompt from\ntext but also the spatial ones from motion guidance. We train all our models\nusing a single node of 16xV100 GPUs. Code, dataset and models are made public\nat: https://hiteshk03.github.io/Pix2Gif/.\n","authors":["Hitesh Kandala","Jianfeng Gao","Jianwei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.04634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05490v1","updated":"2024-03-08T17:55:41Z","published":"2024-03-08T17:55:41Z","title":"Poly-View Contrastive Learning","summary":"  Contrastive learning typically matches pairs of related views among a number\nof unrelated negative views. Views can be generated (e.g. by augmentations) or\nbe observed. We investigate matching when there are more than two related views\nwhich we call poly-view tasks, and derive new representation learning\nobjectives using information maximization and sufficient statistics. We show\nthat with unlimited computation, one should maximize the number of related\nviews, and with a fixed compute budget, it is beneficial to decrease the number\nof unique samples whilst increasing the number of views of those samples. In\nparticular, poly-view contrastive models trained for 128 epochs with batch size\n256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,\nchallenging the belief that contrastive models require large batch sizes and\nmany training epochs.\n","authors":["Amitis Shidani","Devon Hjelm","Jason Ramapuram","Russ Webb","Eeshan Gunesh Dhekane","Dan Busbridge"],"pdf_url":"https://arxiv.org/pdf/2403.05490v1.pdf","comment":"Accepted to ICLR 2024. 42 pages, 7 figures, 3 tables, loss\n  pseudo-code included in appendix"},{"id":"http://arxiv.org/abs/2403.05489v1","updated":"2024-03-08T17:54:38Z","published":"2024-03-08T17:54:38Z","title":"JointMotion: Joint Self-supervision for Joint Motion Prediction","summary":"  We present JointMotion, a self-supervised learning method for joint motion\nprediction in autonomous driving. Our method includes a scene-level objective\nconnecting motion and environments, and an instance-level objective to refine\nlearned representations. Our evaluations show that these objectives are\ncomplementary and outperform recent contrastive and autoencoding methods as\npre-training for joint motion prediction. Furthermore, JointMotion adapts to\nall common types of environment representations used for motion prediction\n(i.e., agent-centric, scene-centric, and pairwise relative), and enables\neffective transfer learning between the Waymo Open Motion and the Argoverse 2\nForecasting datasets. Notably, our method improves the joint final displacement\nerror of Wayformer, Scene Transformer, and HPTR by 3%, 7%, and 11%,\nrespectively.\n","authors":["Royden Wagner","Ömer Şahin Taş","Marvin Klemp","Carlos Fernandez"],"pdf_url":"https://arxiv.org/pdf/2403.05489v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.17987v2","updated":"2024-03-08T17:47:21Z","published":"2024-02-28T02:11:47Z","title":"Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A\n  Bayesian Fusion Approach","summary":"  Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs)\ninvolves transmitting Electromagnetic Waves (EMWs) and performing target type\nrecognition on the received radar echo, crucial for defense and aerospace\napplications. Previous studies highlighted the advantages of multistatic radar\nconfigurations over monostatic ones in RATR. However, fusion methods in\nmultistatic radar configurations often suboptimally combine classification\nvectors from individual radars probabilistically. To address this, we propose a\nfully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to\naggregate classification probability vectors from multiple radars. OBF, based\non expected 0-1 loss, updates a Recursive Bayesian Classification (RBC)\nposterior distribution for target UAV type, conditioned on historical\nobservations across multiple time steps. We evaluate the approach using\nsimulated random walk trajectories for seven drones, correlating target aspect\nangles to Radar Cross Section (RCS) measurements in an anechoic chamber.\nComparing against single radar Automated Target Recognition (ATR) systems and\nsuboptimal fusion methods, our empirical results demonstrate that the OBF\nmethod integrated with RBC significantly enhances classification accuracy\ncompared to other fusion methods and single radar configurations.\n","authors":["Michael Potter","Murat Akcakaya","Marius Necsoiu","Gunar Schirner","Deniz Erdogmus","Tales Imbiriba"],"pdf_url":"https://arxiv.org/pdf/2402.17987v2.pdf","comment":"To be submitted to IEEE Transactions on Aerospace and Electronic\n  Systems"},{"id":"http://arxiv.org/abs/2403.05468v1","updated":"2024-03-08T17:30:41Z","published":"2024-03-08T17:30:41Z","title":"Will GPT-4 Run DOOM?","summary":"  We show that GPT-4's reasoning and planning capabilities extend to the 1993\nfirst-person shooter Doom. This large language model (LLM) is able to run and\nplay the game with only a few instructions, plus a textual\ndescription--generated by the model itself from screenshots--about the state of\nthe game being observed. We find that GPT-4 can play the game to a passable\ndegree: it is able to manipulate doors, combat enemies, and perform pathing.\nMore complex prompting strategies involving multiple model calls provide better\nresults. While further work is required to enable the LLM to play the game as\nwell as its classical, reinforcement learning-based counterparts, we note that\nGPT-4 required no training, leaning instead on its own reasoning and\nobservational capabilities. We hope our work pushes the boundaries on\nintelligent, LLM-based agents in video games. We conclude by discussing the\nethical implications of our work.\n","authors":["Adrian de Wynter"],"pdf_url":"https://arxiv.org/pdf/2403.05468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05466v1","updated":"2024-03-08T17:29:51Z","published":"2024-03-08T17:29:51Z","title":"Grasping Trajectory Optimization with Point Clouds","summary":"  We introduce a new trajectory optimization method for robotic grasping based\non a point-cloud representation of robots and task spaces. In our method,\nrobots are represented by 3D points on their link surfaces. The task space of a\nrobot is represented by a point cloud that can be obtained from depth sensors.\nUsing the point-cloud representation, goal reaching in grasping can be\nformulated as point matching, while collision avoidance can be efficiently\nachieved by querying the signed distance values of the robot points in the\nsigned distance field of the scene points. Consequently, a constrained\nnon-linear optimization problem is formulated to solve the joint motion and\ngrasp planning problem. The advantage of our method is that the point-cloud\nrepresentation is general to be used with any robot in any environment. We\ndemonstrate the effectiveness of our method by conducting experiments on a\ntabletop scene and a shelf scene for grasping with a Fetch mobile manipulator\nand a Franka Panda arm.\n","authors":["Yu Xiang","Sai Haneesh Allu","Rohith Peddi","Tyler Summers","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2403.05466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04527v4","updated":"2024-03-08T17:28:47Z","published":"2023-06-07T15:36:26Z","title":"ContriMix: Scalable stain color augmentation for domain generalization\n  without domain labels in digital pathology","summary":"  Differences in staining and imaging procedures can cause significant color\nvariations in histopathology images, leading to poor generalization when\ndeploying deep-learning models trained from a different data source. Various\ncolor augmentation methods have been proposed to generate synthetic images\nduring training to make models more robust, eliminating the need for stain\nnormalization during test time. Many color augmentation methods leverage domain\nlabels to generate synthetic images. This approach causes three significant\nchallenges to scaling such a model. Firstly, incorporating data from a new\ndomain into deep-learning models trained on existing domain labels is not\nstraightforward. Secondly, dependency on domain labels prevents the use of\npathology images without domain labels to improve model performance. Finally,\nimplementation of these methods becomes complicated when multiple domain labels\n(e.g., patient identification, medical center, etc) are associated with a\nsingle image. We introduce ContriMix, a novel domain label free stain color\naugmentation method based on DRIT++, a style-transfer method. Contrimix\nleverages sample stain color variation within a training minibatch and random\nmixing to extract content and attribute information from pathology images. This\ninformation can be used by a trained ContriMix model to create synthetic images\nto improve the performance of existing classifiers. ContriMix outperforms\ncompeting methods on the Camelyon17-WILDS dataset. Its performance is\nconsistent across different slides in the test set while being robust to the\ncolor variation from rare substances in pathology images. We make our code and\ntrained ContriMix models available for research use. The code for ContriMix can\nbe found at https://gitlab.com/huutan86/contrimix\n","authors":["Tan H. Nguyen","Dinkar Juyal","Jin Li","Aaditya Prakash","Shima Nofallah","Chintan Shah","Sai Chowdary Gullapally","Limin Yu","Michael Griffin","Anand Sampat","John Abel","Justin Lee","Amaro Taylor-Weiner"],"pdf_url":"https://arxiv.org/pdf/2306.04527v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05452v1","updated":"2024-03-08T16:57:54Z","published":"2024-03-08T16:57:54Z","title":"The R2D2 deep neural network series paradigm for fast precision imaging\n  in radio astronomy","summary":"  Radio-interferometric (RI) imaging entails solving high-resolution\nhigh-dynamic range inverse problems from large data volumes. Recent image\nreconstruction techniques grounded in optimization theory have demonstrated\nremarkable capability for imaging precision, well beyond CLEAN's capability.\nThese range from advanced proximal algorithms propelled by handcrafted\nregularization operators, such as the SARA family, to hybrid plug-and-play\n(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.\nOptimization and PnP structures are however highly iterative, which hinders\ntheir ability to handle the extreme data sizes expected from future\ninstruments. To address this scalability challenge, we introduce a novel deep\nlearning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic\nrange imaging'. R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. It\nthus takes a hybrid structure between a PnP algorithm and a learned version of\nthe matching pursuit algorithm that underpins CLEAN. We present a comprehensive\nstudy of our approach, featuring its multiple incarnations distinguished by\ntheir DNN architectures. We provide a detailed description of its training\nprocess, targeting a telescope-specific approach. R2D2's capability to deliver\nhigh precision is demonstrated in simulation, across a variety of image and\nobservation settings using the Very Large Array (VLA). Its reconstruction speed\nis also demonstrated: with only few iterations required to clean data residuals\nat dynamic ranges up to 105, R2D2 opens the door to fast precision imaging.\nR2D2 codes are available in the BASPLib library on GitHub.\n","authors":["Amir Aghabiglou","Chung San Chu","Arwa Dabbech","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.05452v1.pdf","comment":"20 pages, 10 figures, submitted to APJ"},{"id":"http://arxiv.org/abs/2403.05451v1","updated":"2024-03-08T16:57:47Z","published":"2024-03-08T16:57:47Z","title":"Attention-guided Feature Distillation for Semantic Segmentation","summary":"  In contrast to existing complex methodologies commonly employed for\ndistilling knowledge from a teacher to a student, the pro-posed method\nshowcases the efficacy of a simple yet powerful method for utilizing refined\nfeature maps to transfer attention. The proposed method has proven to be\neffective in distilling rich information, outperforming existing methods in\nsemantic segmentation as a dense prediction task. The proposed Attention-guided\nFeature Distillation (AttnFD) method, em-ploys the Convolutional Block\nAttention Module (CBAM), which refines feature maps by taking into account both\nchannel-specific and spatial information content. By only using the Mean\nSquared Error (MSE) loss function between the refined feature maps of the\nteacher and the student,AttnFD demonstrates outstanding performance in semantic\nsegmentation, achieving state-of-the-art results in terms of mean Intersection\nover Union (mIoU) on the PascalVoc 2012 and Cityscapes datasets. The Code is\navailable at https://github.com/AmirMansurian/AttnFD.\n","authors":["Amir M. Mansourian","Arya Jalali","Rozhan Ahmadi","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2403.05451v1.pdf","comment":"17 pages, 8 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2403.00939v2","updated":"2024-03-08T16:55:11Z","published":"2024-03-01T19:36:11Z","title":"G3DR: Generative 3D Reconstruction in ImageNet","summary":"  We introduce a novel 3D generative method, Generative 3D Reconstruction\n(G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects\nfrom single images, addressing the limitations of existing methods. At the\nheart of our framework is a novel depth regularization technique that enables\nthe generation of scenes with high-geometric fidelity. G3DR also leverages a\npretrained language-vision model, such as CLIP, to enable reconstruction in\nnovel views and improve the visual realism of generations. Additionally, G3DR\ndesigns a simple but effective sampling procedure to further improve the\nquality of generations. G3DR offers diverse and efficient 3D asset generation\nbased on class or text conditioning. Despite its simplicity, G3DR is able to\nbeat state-of-theart methods, improving over them by up to 22% in perceptual\nmetrics and 90% in geometry scores, while needing only half of the training\ntime. Code is available at https://github.com/preddy5/G3DR\n","authors":["Pradyumna Reddy","Ismail Elezi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2403.00939v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05438v1","updated":"2024-03-08T16:44:54Z","published":"2024-03-08T16:44:54Z","title":"VideoElevator: Elevating Video Generation Quality with Versatile\n  Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models (T2I) have demonstrated unprecedented\ncapabilities in creating realistic and aesthetic images. On the contrary,\ntext-to-video diffusion models (T2V) still lag far behind in frame quality and\ntext alignment, owing to insufficient quality and quantity of training videos.\nIn this paper, we introduce VideoElevator, a training-free and plug-and-play\nmethod, which elevates the performance of T2V using superior capabilities of\nT2I. Different from conventional T2V sampling (i.e., temporal and spatial\nmodeling), VideoElevator explicitly decomposes each sampling step into temporal\nmotion refining and spatial quality elevating. Specifically, temporal motion\nrefining uses encapsulated T2V to enhance temporal consistency, followed by\ninverting to the noise distribution required by T2I. Then, spatial quality\nelevating harnesses inflated T2I to directly predict less noisy latent, adding\nmore photo-realistic details. We have conducted experiments in extensive\nprompts under the combination of various T2V and T2I. The results show that\nVideoElevator not only improves the performance of T2V baselines with\nfoundational T2I, but also facilitates stylistic video synthesis with\npersonalized T2I. Our code is available at\nhttps://github.com/YBYBZhang/VideoElevator.\n","authors":["Yabo Zhang","Yuxiang Wei","Xianhui Lin","Zheng Hui","Peiran Ren","Xuansong Xie","Xiangyang Ji","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2403.05438v1.pdf","comment":"Project page: https://videoelevator.github.io Code:\n  https://github.com/YBYBZhang/VideoElevator"},{"id":"http://arxiv.org/abs/2403.05435v1","updated":"2024-03-08T16:38:11Z","published":"2024-03-08T16:38:11Z","title":"OmniCount: Multi-label Object Counting with Semantic-Geometric Priors","summary":"  Object counting is pivotal for understanding the composition of scenes.\nPreviously, this task was dominated by class-specific methods, which have\ngradually evolved into more adaptable class-agnostic strategies. However, these\nstrategies come with their own set of limitations, such as the need for manual\nexemplar input and multiple passes for multiple categories, resulting in\nsignificant inefficiencies. This paper introduces a new, more practical\napproach enabling simultaneous counting of multiple object categories using an\nopen vocabulary framework. Our solution, OmniCount, stands out by using\nsemantic and geometric insights from pre-trained models to count multiple\ncategories of objects as specified by users, all without additional training.\nOmniCount distinguishes itself by generating precise object masks and\nleveraging point prompts via the Segment Anything Model for efficient counting.\nTo evaluate OmniCount, we created the OmniCount-191 benchmark, a\nfirst-of-its-kind dataset with multi-label object counts, including points,\nbounding boxes, and VQA annotations. Our comprehensive evaluation in\nOmniCount-191, alongside other leading benchmarks, demonstrates OmniCount's\nexceptional performance, significantly outpacing existing solutions and\nheralding a new era in object counting technology.\n","authors":["Anindya Mondal","Sauradip Nag","Xiatian Zhu","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2403.05435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05433v1","updated":"2024-03-08T16:34:30Z","published":"2024-03-08T16:34:30Z","title":"Part-aware Personalized Segment Anything Model for Patient-Specific\n  Segmentation","summary":"  Precision medicine, such as patient-adaptive treatments utilizing medical\nimages, poses new challenges for image segmentation algorithms due to (1) the\nlarge variability across different patients and (2) the limited availability of\nannotated data for each patient. In this work, we propose a data-efficient\nsegmentation method to address these challenges, namely Part-aware Personalized\nSegment Anything Model (P^2SAM). Without any model fine-tuning, P^2SAM enables\nseamless adaptation to any new patients relying only on one-shot\npatient-specific data. We introduce a novel part-aware prompt mechanism to\nselect multiple-point prompts based on part-level features of the one-shot\ndata. To further promote the robustness of the selected prompt, we propose a\nretrieval approach to handle outlier prompts. Extensive experiments demonstrate\nthat P^2SAM improves the performance by +8.0% and +2.0% mean Dice score within\ntwo patient-specific segmentation settings, and exhibits impressive generality\nacross different application domains, e.g., +6.4% mIoU on the PerSeg benchmark.\nCode will be released upon acceptance.\n","authors":["Chenhui Zhao","Liyue Shen"],"pdf_url":"https://arxiv.org/pdf/2403.05433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05422v1","updated":"2024-03-08T16:19:39Z","published":"2024-03-08T16:19:39Z","title":"EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in\n  UAV","summary":"  Vehicle detection in Unmanned Aerial Vehicle (UAV) captured images has wide\napplications in aerial photography and remote sensing. There are many public\nbenchmark datasets proposed for the vehicle detection and tracking in UAV\nimages. Recent studies show that adding an adversarial patch on objects can\nfool the well-trained deep neural networks based object detectors, posing\nsecurity concerns to the downstream tasks. However, the current public UAV\ndatasets might ignore the diverse altitudes, vehicle attributes, fine-grained\ninstance-level annotation in mostly side view with blurred vehicle roof, so\nnone of them is good to study the adversarial patch based vehicle detection\nattack problem. In this paper, we propose a new dataset named EVD4UAV as an\naltitude-sensitive benchmark to evade vehicle detection in UAV with 6,284\nimages and 90,886 fine-grained annotated vehicles. The EVD4UAV dataset has\ndiverse altitudes (50m, 70m, 90m), vehicle attributes (color, type),\nfine-grained annotation (horizontal and rotated bounding boxes, instance-level\nmask) in top view with clear vehicle roof. One white-box and two black-box\npatch based attack methods are implemented to attack three classic deep neural\nnetworks based object detectors on EVD4UAV. The experimental results show that\nthese representative attack methods could not achieve the robust\naltitude-insensitive attack performance.\n","authors":["Huiming Sun","Jiacheng Guo","Zibo Meng","Tianyun Zhang","Jianwu Fang","Yuewei Lin","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.05422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05419v1","updated":"2024-03-08T16:18:04Z","published":"2024-03-08T16:18:04Z","title":"Rethinking Transformers Pre-training for Multi-Spectral Satellite\n  Imagery","summary":"  Recent advances in unsupervised learning have demonstrated the ability of\nlarge vision models to achieve promising results on downstream tasks by\npre-training on large amount of unlabelled data. Such pre-training techniques\nhave also been explored recently in the remote sensing domain due to the\navailability of large amount of unlabelled data. Different from standard\nnatural image datasets, remote sensing data is acquired from various sensor\ntechnologies and exhibit diverse range of scale variations as well as\nmodalities. Existing satellite image pre-training methods either ignore the\nscale information present in the remote sensing imagery or restrict themselves\nto use only a single type of data modality. In this paper, we re-visit\ntransformers pre-training and leverage multi-scale information that is\neffectively utilized with multiple modalities. Our proposed approach, named\nSatMAE++, performs multi-scale pre-training and utilizes convolution based\nupsampling blocks to reconstruct the image at higher scales making it\nextensible to include more scales. Compared to existing works, the proposed\nSatMAE++ with multi-scale pre-training is equally effective for both optical as\nwell as multi-spectral imagery. Extensive experiments on six datasets reveal\nthe merits of proposed contributions, leading to state-of-the-art performance\non all datasets. SatMAE++ achieves mean average precision (mAP) gain of 2.5\\%\nfor multi-label classification task on BigEarthNet dataset. Our code and\npre-trained models are available at \\url{https://github.com/techmn/satmae_pp}.\n","authors":["Mubashir Noman","Muzammal Naseer","Hisham Cholakkal","Rao Muhammad Anwar","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.05419v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05416v1","updated":"2024-03-08T16:14:54Z","published":"2024-03-08T16:14:54Z","title":"SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised\n  Learning for Robust Infrared Small Target Detection","summary":"  Single-frame infrared small target (SIRST) detection aims to recognize small\ntargets from clutter backgrounds. Recently, convolutional neural networks have\nachieved significant advantages in general object detection. With the\ndevelopment of Transformer, the scale of SIRST models is constantly increasing.\nDue to the limited training samples, performance has not been improved\naccordingly. The quality, quantity, and diversity of the infrared dataset are\ncritical to the detection of small targets. To highlight this issue, we propose\na negative sample augmentation method in this paper. Specifically, a negative\naugmentation approach is proposed to generate massive negatives for\nself-supervised learning. Firstly, we perform a sequential noise modeling\ntechnology to generate realistic infrared data. Secondly, we fuse the extracted\nnoise with the original data to facilitate diversity and fidelity in the\ngenerated data. Lastly, we proposed a negative augmentation strategy to enrich\ndiversity as well as maintain semantic invariance. The proposed algorithm\nproduces a synthetic SIRST-5K dataset, which contains massive pseudo-data and\ncorresponding labels. With a rich diversity of infrared small target data, our\nalgorithm significantly improves the model performance and convergence speed.\nCompared with other state-of-the-art (SOTA) methods, our method achieves\noutstanding performance in terms of probability of detection (Pd), false-alarm\nrate (Fa), and intersection over union (IoU).\n","authors":["Yahao Lu","Yupei Lin","Han Wu","Xiaoyu Xian","Yukai Shi","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2403.05416v1.pdf","comment":"We address the quality, quantity, and diversity of the infrared data\n  in SIRST, the dataset is available at: https://github.com/luy0222/SIRST-5K"},{"id":"http://arxiv.org/abs/2211.14456v5","updated":"2024-03-08T16:14:10Z","published":"2022-11-26T02:15:35Z","title":"TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis","summary":"  In many practical applications, 3D point cloud analysis requires rotation\ninvariance. In this paper, we present a learnable descriptor invariant under 3D\nrotations and reflections, i.e., the O(3) actions, utilizing the recently\nintroduced steerable 3D spherical neurons and vector neurons. Specifically, we\npropose an embedding of the 3D spherical neurons into 4D vector neurons, which\nleverages end-to-end training of the model. In our approach, we perform\nTetraTransform--an equivariant embedding of the 3D input into 4D, constructed\nfrom the steerable neurons--and extract deeper O(3)-equivariant features using\nvector neurons. This integration of the TetraTransform into the VN-DGCNN\nframework, termed TetraSphere, negligibly increases the number of parameters by\nless than 0.0002%. TetraSphere sets a new state-of-the-art performance\nclassifying randomly rotated real-world object scans of the challenging subsets\nof ScanObjectNN. Additionally, TetraSphere outperforms all equivariant methods\non randomly rotated synthetic data: classifying objects from ModelNet40 and\nsegmenting parts of the ShapeNet shapes. Thus, our results reveal the practical\nvalue of steerable 3D spherical neurons for learning in 3D Euclidean space. The\ncode is available at \\url{https://github.com/pavlo-melnyk/tetrasphere}.\n","authors":["Pavlo Melnyk","Andreas Robinson","Michael Felsberg","Mårten Wadenbäck"],"pdf_url":"https://arxiv.org/pdf/2211.14456v5.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05408v1","updated":"2024-03-08T16:06:54Z","published":"2024-03-08T16:06:54Z","title":"FedFMS: Exploring Federated Foundation Models for Medical Image\n  Segmentation","summary":"  Medical image segmentation is crucial for clinical diagnosis. The\nSegmentation Anything Model (SAM) serves as a powerful foundation model for\nvisual segmentation and can be adapted for medical image segmentation. However,\nmedical imaging data typically contain privacy-sensitive information, making it\nchallenging to train foundation models with centralized storage and sharing. To\ndate, there are few foundation models tailored for medical image deployment\nwithin the federated learning framework, and the segmentation performance, as\nwell as the efficiency of communication and training, remain unexplored. In\nresponse to these issues, we developed Federated Foundation models for Medical\nimage Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a\ncommunication and training-efficient Federated SAM with Medical SAM Adapter\n(FedMSA). Comprehensive experiments on diverse datasets are conducted to\ninvestigate the performance disparities between centralized training and\nfederated learning across various configurations of FedFMS. The experiments\nrevealed that FedFMS could achieve performance comparable to models trained via\ncentralized training methods while maintaining privacy. Furthermore, FedMSA\ndemonstrated the potential to enhance communication and training efficiency.\nOur model implementation codes are available at\nhttps://github.com/LIU-YUXI/FedFMS.\n","authors":["Yuxi Liu","Guibo Luo","Yuesheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.05408v1.pdf","comment":"Medical image segmentation, Federated learning and Foundation model"},{"id":"http://arxiv.org/abs/2403.05402v1","updated":"2024-03-08T15:58:00Z","published":"2024-03-08T15:58:00Z","title":"DualBEV: CNN is All You Need in View Transformation","summary":"  Camera-based Bird's-Eye-View (BEV) perception often struggles between\nadopting 3D-to-2D or 2D-to-3D view transformation (VT). The 3D-to-2D VT\ntypically employs resource intensive Transformer to establish robust\ncorrespondences between 3D and 2D feature, while the 2D-to-3D VT utilizes the\nLift-Splat-Shoot (LSS) pipeline for real-time application, potentially missing\ndistant information. To address these limitations, we propose DualBEV, a\nunified framework that utilizes a shared CNN-based feature transformation\nincorporating three probabilistic measurements for both strategies. By\nconsidering dual-view correspondences in one-stage, DualBEV effectively bridges\nthe gap between these strategies, harnessing their individual strengths. Our\nmethod achieves state-of-the-art performance without Transformer, delivering\ncomparable efficiency to the LSS approach, with 55.2% mAP and 63.4% NDS on the\nnuScenes test set. Code will be released at\nhttps://github.com/PeidongLi/DualBEV.\n","authors":["Peidong Li","Wancheng Shen","Qihao Huang","Dixiao Cui"],"pdf_url":"https://arxiv.org/pdf/2403.05402v1.pdf","comment":"16 pages, 6 figures, Tech Report"},{"id":"http://arxiv.org/abs/2403.05396v1","updated":"2024-03-08T15:51:43Z","published":"2024-03-08T15:51:43Z","title":"HistGen: Histopathology Report Generation via Local-Global Feature\n  Encoding and Cross-modal Context Interaction","summary":"  Histopathology serves as the gold standard in cancer diagnosis, with clinical\nreports being vital in interpreting and understanding this process, guiding\ncancer treatment and patient care. The automation of histopathology report\ngeneration with deep learning stands to significantly enhance clinical\nefficiency and lessen the labor-intensive, time-consuming burden on\npathologists in report writing. In pursuit of this advancement, we introduce\nHistGen, a multiple instance learning-empowered framework for histopathology\nreport generation together with the first benchmark dataset for evaluation.\nInspired by diagnostic and report-writing workflows, HistGen features two\ndelicately designed modules, aiming to boost report generation by aligning\nwhole slide images (WSIs) and diagnostic reports from local and global\ngranularity. To achieve this, a local-global hierarchical encoder is developed\nfor efficient visual feature aggregation from a region-to-slide perspective.\nMeanwhile, a cross-modal context module is proposed to explicitly facilitate\nalignment and interaction between distinct modalities, effectively bridging the\ngap between the extensive visual sequences of WSIs and corresponding highly\nsummarized reports. Experimental results on WSI report generation show the\nproposed model outperforms state-of-the-art (SOTA) models by a large margin.\nMoreover, the results of fine-tuning our model on cancer subtyping and survival\nanalysis tasks further demonstrate superior performance compared to SOTA\nmethods, showcasing strong transfer learning capability. Dataset, model\nweights, and source code are available in\nhttps://github.com/dddavid4real/HistGen.\n","authors":["Zhengrui Guo","Jiabo Ma","Yingxue Xu","Yihui Wang","Liansheng Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.05396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05394v1","updated":"2024-03-08T15:45:07Z","published":"2024-03-08T15:45:07Z","title":"A Deep Learning Method for Classification of Biophilic Artworks","summary":"  Biophilia is an innate love for living things and nature itself that has been\nassociated with a positive impact on mental health and well-being. This study\nexplores the application of deep learning methods for the classification of\nBiophilic artwork, in order to learn and explain the different Biophilic\ncharacteristics present in a visual representation of a painting. Using the\nconcept of Biophilia that postulates the deep connection of human beings with\nnature, we use an artificially intelligent algorithm to recognise the different\npatterns underlying the Biophilic features in an artwork. Our proposed method\nuses a lower-dimensional representation of an image and a decoder model to\nextract salient features of the image of each Biophilic trait, such as plants,\nwater bodies, seasons, animals, etc., based on learnt factors such as shape,\ntexture, and illumination. The proposed classification model is capable of\nextracting Biophilic artwork that not only helps artists, collectors, and\nresearchers studying to interpret and exploit the effects of mental well-being\non exposure to nature-inspired visual aesthetics but also enables a methodical\nexploration of the study of Biophilia and Biophilic artwork for aesthetic\npreferences. Using the proposed algorithms, we have also created a gallery of\nBiophilic collections comprising famous artworks from different European and\nAmerican art galleries, which will soon be published on the Vieunite@ online\ncommunity.\n","authors":["Purna Kar","Jordan J. Bird","Yangang Xing","Alexander Sumich","Andrew Knight","Ahmad Lotfi","Benedict Carpenter van Barthold"],"pdf_url":"https://arxiv.org/pdf/2403.05394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.05986v3","updated":"2024-03-08T15:41:34Z","published":"2022-04-12T17:52:09Z","title":"Machine Learning Security against Data Poisoning: Are We There Yet?","summary":"  The recent success of machine learning (ML) has been fueled by the increasing\navailability of computing power and large amounts of data in many different\napplications. However, the trustworthiness of the resulting models can be\ncompromised when such data is maliciously manipulated to mislead the learning\nprocess. In this article, we first review poisoning attacks that compromise the\ntraining data used to learn ML models, including attacks that aim to reduce the\noverall performance, manipulate the predictions on specific test samples, and\neven implant backdoors in the model. We then discuss how to mitigate these\nattacks using basic security principles, or by deploying ML-oriented defensive\nmechanisms. We conclude our article by formulating some relevant open\nchallenges which are hindering the development of testing methods and\nbenchmarks suitable for assessing and improving the trustworthiness of ML\nmodels against data poisoning attacks\n","authors":["Antonio Emanuele Cinà","Kathrin Grosse","Ambra Demontis","Battista Biggio","Fabio Roli","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2204.05986v3.pdf","comment":"preprint, 10 pages, 3 figures. Paper accepted to the IEEE Computer -\n  Special Issue on Trustworthy AI"},{"id":"http://arxiv.org/abs/2401.16822v3","updated":"2024-03-08T15:36:11Z","published":"2024-01-30T08:57:48Z","title":"EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor\n  Image Comprehension in Remote Sensing Domain","summary":"  Multi-modal large language models (MLLMs) have demonstrated remarkable\nsuccess in vision and visual-language tasks within the natural image domain.\nOwing to the significant diversities between the natural and remote sensing\n(RS) images, the development of MLLMs in the RS domain is still in the infant\nstage. To fill the gap, a pioneer MLLM named EarthGPT integrating various\nmulti-sensor RS interpretation tasks uniformly is proposed in this paper for\nuniversal RS image comprehension. In EarthGPT, three key techniques are\ndeveloped including a visual-enhanced perception mechanism, a cross-modal\nmutual comprehension approach, and a unified instruction tuning method for\nmulti-sensor multi-task in the RS domain. More importantly, a dataset named\nMMRS-1M featuring large-scale multi-sensor multi-modal RS instruction-following\nis constructed, comprising over 1M image-text pairs based on 34 existing\ndiverse RS datasets and including multi-sensor images such as optical,\nsynthetic aperture radar (SAR), and infrared. The MMRS-1M dataset addresses the\ndrawback of MLLMs on RS expert knowledge and stimulates the development of\nMLLMs in the RS domain. Extensive experiments are conducted, demonstrating the\nEarthGPT's superior performance in various RS visual interpretation tasks\ncompared with the other specialist models and MLLMs, proving the effectiveness\nof the proposed EarthGPT and offering a versatile paradigm for open-set\nreasoning tasks.\n","authors":["Wei Zhang","Miaoxin Cai","Tong Zhang","Yin Zhuang","Xuerui Mao"],"pdf_url":"https://arxiv.org/pdf/2401.16822v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05388v1","updated":"2024-03-08T15:32:18Z","published":"2024-03-08T15:32:18Z","title":"Generalized Correspondence Matching via Flexible Hierarchical Refinement\n  and Patch Descriptor Distillation","summary":"  Correspondence matching plays a crucial role in numerous robotics\napplications. In comparison to conventional hand-crafted methods and recent\ndata-driven approaches, there is significant interest in plug-and-play\nalgorithms that make full use of pre-trained backbone networks for multi-scale\nfeature extraction and leverage hierarchical refinement strategies to generate\nmatched correspondences. The primary focus of this paper is to address the\nlimitations of deep feature matching (DFM), a state-of-the-art (SoTA)\nplug-and-play correspondence matching approach. First, we eliminate the\npre-defined threshold employed in the hierarchical refinement process of DFM by\nleveraging a more flexible nearest neighbor search strategy, thereby preventing\nthe exclusion of repetitive yet valid matches during the early stages. Our\nsecond technical contribution is the integration of a patch descriptor, which\nextends the applicability of DFM to accommodate a wide range of backbone\nnetworks pre-trained across diverse computer vision tasks, including image\nclassification, semantic segmentation, and stereo matching. Taking into account\nthe practical applicability of our method in real-world robotics applications,\nwe also propose a novel patch descriptor distillation strategy to further\nreduce the computational complexity of correspondence matching. Extensive\nexperiments conducted on three public datasets demonstrate the superior\nperformance of our proposed method. Specifically, it achieves an overall\nperformance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 with\nrespect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatches\ndataset, outperforming all other SoTA algorithms. Our source code, demo video,\nand supplement are publicly available at mias.group/GCM.\n","authors":["Yu Han","Ziwei Long","Yanting Zhang","Jin Wu","Zhijun Fang","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2403.05388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05384v1","updated":"2024-03-08T15:26:27Z","published":"2024-03-08T15:26:27Z","title":"A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of\n  3D Echocardiography Images using a GAN","summary":"  Due to privacy issues and limited amount of publicly available labeled\ndatasets in the domain of medical imaging, we propose an image generation\npipeline to synthesize 3D echocardiographic images with corresponding ground\ntruth labels, to alleviate the need for data collection and for laborious and\nerror-prone human labeling of images for subsequent Deep Learning (DL) tasks.\nThe proposed method utilizes detailed anatomical segmentations of the heart as\nground truth label sources. This initial dataset is combined with a second\ndataset made up of real 3D echocardiographic images to train a Generative\nAdversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasound\nimages paired with ground truth labels. To generate the synthetic 3D dataset,\nthe trained GAN uses high resolution anatomical models from Computed Tomography\n(CT) as input. A qualitative analysis of the synthesized images showed that the\nmain structures of the heart are well delineated and closely follow the labels\nobtained from the anatomical models. To assess the usability of these synthetic\nimages for DL tasks, segmentation algorithms were trained to delineate the left\nventricle, left atrium, and myocardium. A quantitative analysis of the 3D\nsegmentations given by the models trained with the synthetic images indicated\nthe potential use of this GAN approach to generate 3D synthetic data, use the\ndata to train DL models for different clinical tasks, and therefore tackle the\nproblem of scarcity of 3D labeled echocardiography datasets.\n","authors":["Cristiana Tiago","Andrew Gilbert","Ahmed S. Beela","Svein Arne Aase","Sten Roar Snare","Jurica Sprem"],"pdf_url":"https://arxiv.org/pdf/2403.05384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05381v1","updated":"2024-03-08T15:20:27Z","published":"2024-03-08T15:20:27Z","title":"Exploring Robust Features for Few-Shot Object Detection in Satellite\n  Imagery","summary":"  The goal of this paper is to perform object detection in satellite imagery\nwith only a few examples, thus enabling users to specify any object class with\nminimal annotation. To this end, we explore recent methods and ideas from\nopen-vocabulary detection for the remote sensing domain. We develop a few-shot\nobject detector based on a traditional two-stage architecture, where the\nclassification block is replaced by a prototype-based classifier. A large-scale\npre-trained model is used to build class-reference embeddings or prototypes,\nwhich are compared to region proposal contents for label prediction. In\naddition, we propose to fine-tune prototypes on available training images to\nboost performance and learn differences between similar classes, such as\naircraft types. We perform extensive evaluations on two remote sensing datasets\ncontaining challenging and rare objects. Moreover, we study the performance of\nboth visual and image-text features, namely DINOv2 and CLIP, including two CLIP\nmodels specifically tailored for remote sensing applications. Results indicate\nthat visual features are largely superior to vision-language models, as the\nlatter lack the necessary domain-specific vocabulary. Lastly, the developed\ndetector outperforms fully supervised and few-shot methods evaluated on the\nSIMD and DIOR datasets, despite minimal training parameters.\n","authors":["Xavier Bou","Gabriele Facciolo","Rafael Grompone von Gioi","Jean-Michel Morel","Thibaud Ehret"],"pdf_url":"https://arxiv.org/pdf/2403.05381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05380v1","updated":"2024-03-08T15:19:26Z","published":"2024-03-08T15:19:26Z","title":"Spectrogram-Based Detection of Auto-Tuned Vocals in Music Recordings","summary":"  In the domain of music production and audio processing, the implementation of\nautomatic pitch correction of the singing voice, also known as Auto-Tune, has\nsignificantly transformed the landscape of vocal performance. While auto-tuning\ntechnology has offered musicians the ability to tune their vocal pitches and\nachieve a desired level of precision, its use has also sparked debates\nregarding its impact on authenticity and artistic integrity. As a result,\ndetecting and analyzing Auto-Tuned vocals in music recordings has become\nessential for music scholars, producers, and listeners. However, to the best of\nour knowledge, no prior effort has been made in this direction. This study\nintroduces a data-driven approach leveraging triplet networks for the detection\nof Auto-Tuned songs, backed by the creation of a dataset composed of original\nand Auto-Tuned audio clips. The experimental results demonstrate the\nsuperiority of the proposed method in both accuracy and robustness compared to\nRawnet2, an end-to-end model proposed for anti-spoofing and widely used for\nother audio forensic tasks.\n","authors":["Mahyar Gohari","Paolo Bestagini","Sergio Benini","Nicola Adami"],"pdf_url":"https://arxiv.org/pdf/2403.05380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14418v3","updated":"2024-03-08T15:17:30Z","published":"2023-08-28T08:54:27Z","title":"Multi-Scale and Multi-Layer Contrastive Learning for Domain\n  Generalization","summary":"  During the past decade, deep neural networks have led to fast-paced progress\nand significant achievements in computer vision problems, for both academia and\nindustry. Yet despite their success, state-of-the-art image classification\napproaches fail to generalize well in previously unseen visual contexts, as\nrequired by many real-world applications. In this paper, we focus on this\ndomain generalization (DG) problem and argue that the generalization ability of\ndeep convolutional neural networks can be improved by taking advantage of\nmulti-layer and multi-scaled representations of the network. We introduce a\nframework that aims at improving domain generalization of image classifiers by\ncombining both low-level and high-level features at multiple scales, enabling\nthe network to implicitly disentangle representations in its latent space and\nlearn domain-invariant attributes of the depicted objects. Additionally, to\nfurther facilitate robust representation learning, we propose a novel objective\nfunction, inspired by contrastive learning, which aims at constraining the\nextracted representations to remain invariant under distribution shifts. We\ndemonstrate the effectiveness of our method by evaluating on the domain\ngeneralization datasets of PACS, VLCS, Office-Home and NICO. Through extensive\nexperimentation, we show that our model is able to surpass the performance of\nprevious DG methods and consistently produce competitive and state-of-the-art\nresults in all datasets\n","authors":["Aristotelis Ballas","Christos Diou"],"pdf_url":"https://arxiv.org/pdf/2308.14418v3.pdf","comment":"Manuscript accepted in: IEEE Transactions on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2403.05379v1","updated":"2024-03-08T15:16:15Z","published":"2024-03-08T15:16:15Z","title":"Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia\n  Classification","summary":"  Automated disease diagnosis using medical image analysis relies on deep\nlearning, often requiring large labeled datasets for supervised model training.\nDiseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and\ncostly annotations on a single-cell level. Multiple Instance Learning (MIL)\naddresses weakly labeled scenarios but necessitates powerful encoders typically\ntrained with labeled data. In this study, we explore Self-Supervised Learning\n(SSL) as a pre-training approach for MIL-based AML subtype classification from\nblood smears, removing the need for labeled data during encoder training. We\ninvestigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and\ncompare their performance against supervised pre-training. Our findings show\nthat SSL-pretrained encoders achieve comparable performance, showcasing the\npotential of SSL in MIL. This breakthrough offers a cost-effective and\ndata-efficient solution, propelling the field of AI-based disease diagnosis.\n","authors":["Salome Kazeminia","Max Joosten","Dragan Bosnacki","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2403.05379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05369v1","updated":"2024-03-08T15:00:44Z","published":"2024-03-08T15:00:44Z","title":"Frequency-Adaptive Dilated Convolution for Semantic Segmentation","summary":"  Dilated convolution, which expands the receptive field by inserting gaps\nbetween its consecutive elements, is widely employed in computer vision. In\nthis study, we propose three strategies to improve individual phases of dilated\nconvolution from the view of spectrum analysis. Departing from the conventional\npractice of fixing a global dilation rate as a hyperparameter, we introduce\nFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts\ndilation rates spatially based on local frequency components. Subsequently, we\ndesign two plug-in modules to directly enhance effective bandwidth and\nreceptive field size. The Adaptive Kernel (AdaKern) module decomposes\nconvolution weights into low-frequency and high-frequency components,\ndynamically adjusting the ratio between these components on a per-channel\nbasis. By increasing the high-frequency part of convolution weights, AdaKern\ncaptures more high-frequency components, thereby improving effective bandwidth.\nThe Frequency Selection (FreqSelect) module optimally balances high- and\nlow-frequency components in feature representations through spatially variant\nreweighting. It suppresses high frequencies in the background to encourage FADC\nto learn a larger dilation, thereby increasing the receptive field for an\nexpanded scope. Extensive experiments on segmentation and object detection\nconsistently validate the efficacy of our approach. The code is publicly\navailable at \\url{https://github.com/Linwei-Chen/FADC}.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.05369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12635v3","updated":"2024-03-08T14:57:48Z","published":"2023-12-19T22:33:42Z","title":"RealCraft: Attention Control as A Tool for Zero-Shot Consistent Video\n  Editing","summary":"  Even though large-scale text-to-image generative models show promising\nperformance in synthesizing high-quality images, applying these models directly\nto image editing remains a significant challenge. This challenge is further\namplified in video editing due to the additional dimension of time. This is\nespecially the case for editing real-world videos as it necessitates\nmaintaining a stable structural layout across frames while executing localized\nedits without disrupting the existing content. In this paper, we propose\nRealCraft, an attention-control-based method for zero-shot real-world video\nediting. By swapping cross-attention for new feature injection and relaxing\nspatial-temporal attention of the editing object, we achieve localized\nshape-wise edit along with enhanced temporal consistency. Our model directly\nuses Stable Diffusion and operates without the need for additional information.\nWe showcase the proposed zero-shot attention-control-based method across a\nrange of videos, demonstrating shape-wise, time-consistent and parameter-free\nediting in videos of up to 64 frames.\n","authors":["Shutong Jin","Ruiyu Wang","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2312.12635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05353v1","updated":"2024-03-08T14:34:32Z","published":"2024-03-08T14:34:32Z","title":"Hybridized Convolutional Neural Networks and Long Short-Term Memory for\n  Improved Alzheimer's Disease Diagnosis from MRI Scans","summary":"  Brain-related diseases are more sensitive than other diseases due to several\nfactors, including the complexity of surgical procedures, high costs, and other\nchallenges. Alzheimer's disease is a common brain disorder that causes memory\nloss and the shrinking of brain cells. Early detection is critical for\nproviding proper treatment to patients. However, identifying Alzheimer's at an\nearly stage using manual scanning of CT or MRI scans is challenging. Therefore,\nresearchers have delved into the exploration of computer-aided systems,\nemploying Machine Learning and Deep Learning methodologies, which entail the\ntraining of datasets to detect Alzheimer's disease. This study aims to present\na hybrid model that combines a CNN model's feature extraction capabilities with\nan LSTM model's detection capabilities. This study has applied the transfer\nlearning called VGG16 in the hybrid model to extract features from MRI images.\nThe LSTM detects features between the convolution layer and the fully connected\nlayer. The output layer of the fully connected layer uses the softmax function.\nThe training of the hybrid model involved utilizing the ADNI dataset. The trial\nfindings revealed that the model achieved a level of accuracy of 98.8%, a\nsensitivity rate of 100%, and a specificity rate of 76%. The proposed hybrid\nmodel outperforms its contemporary CNN counterparts, showcasing a superior\nperformance.\n","authors":["Maleka Khatun","Md Manowarul Islam","Habibur Rahman Rifat","Md. Shamim Bin Shahid","Md. Alamin Talukder","Md Ashraf Uddin"],"pdf_url":"https://arxiv.org/pdf/2403.05353v1.pdf","comment":"Accepted In The 26th International Conference on Computer and\n  Information Technology (ICCIT) On 13-15 December 2023"},{"id":"http://arxiv.org/abs/2403.05352v1","updated":"2024-03-08T14:32:01Z","published":"2024-03-08T14:32:01Z","title":"Enhancing Plausibility Evaluation for Generated Designs with Denoising\n  Autoencoder","summary":"  A great interest has arisen in using Deep Generative Models (DGM) for\ngenerative design. When assessing the quality of the generated designs, human\ndesigners focus more on structural plausibility, e.g., no missing component,\nrather than visual artifacts, e.g., noises in the images. Meanwhile, commonly\nused metrics such as Fr\\'echet Inception Distance (FID) may not evaluate\naccurately as they tend to penalize visual artifacts instead of structural\nimplausibility. As such, FID might not be suitable to assess the performance of\nDGMs for a generative design task. In this work, we propose to encode the input\ndesigns with a simple Denoising Autoencoder (DAE) and measure the distribution\ndistance in the latent space thereof. We experimentally test our DAE-based\nmetrics with FID and other state-of-the-art metrics on three data sets:\ncompared to FID and some more recent works, e.g., FD$_\\text{DINO-V2}$ and\ntopology distance, DAE-based metrics can effectively detect implausible\nstructures and are more consistent with structural inspection by human experts.\n","authors":["Jiajie Fan","Amal Trigui","Thomas Bäck","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05351v1","updated":"2024-03-08T14:31:40Z","published":"2024-03-08T14:31:40Z","title":"Multiple Instance Learning with random sampling for Whole Slide Image\n  Classification","summary":"  In computational pathology, random sampling of patches during training of\nMultiple Instance Learning (MIL) methods is computationally efficient and\nserves as a regularization strategy. Despite its promising benefits, questions\nconcerning performance trends for varying sample sizes and its influence on\nmodel interpretability remain. Addressing these, we reach an optimal\nperformance enhancement of 1.7% using thirty percent of patches on the\nCAMELYON16 dataset, and 3.7% with only eight samples on the TUPAC16 dataset. We\nalso find interpretability effects are strongly dataset-dependent, with\ninterpretability impacted on CAMELYON16, while remaining unaffected on TUPAC16.\nThis reinforces that both the performance and interpretability relationships\nwith sampling are closely task-specific. End-to-end training with 1024 samples\nreveals improvements across both datasets compared to pre-extracted features,\nfurther highlighting the potential of this efficient approach.\n","authors":["H. Keshvarikhojasteh","J. P. W. Pluim","M. Veta"],"pdf_url":"https://arxiv.org/pdf/2403.05351v1.pdf","comment":"SPIE Medical Imaging 2024"},{"id":"http://arxiv.org/abs/2403.05346v1","updated":"2024-03-08T14:23:00Z","published":"2024-03-08T14:23:00Z","title":"VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object\n  Detection with Vision-Language Model","summary":"  In the field of Class Incremental Object Detection (CIOD), creating models\nthat can continuously learn like humans is a major challenge. Pseudo-labeling\nmethods, although initially powerful, struggle with multi-scenario incremental\nlearning due to their tendency to forget past knowledge. To overcome this, we\nintroduce a new approach called Vision-Language Model assisted Pseudo-Labeling\n(VLM-PL). This technique uses Vision-Language Model (VLM) to verify the\ncorrectness of pseudo ground-truths (GTs) without requiring additional model\ntraining. VLM-PL starts by deriving pseudo GTs from a pre-trained detector.\nThen, we generate custom queries for each pseudo GT using carefully designed\nprompt templates that combine image and text features. This allows the VLM to\nclassify the correctness through its responses. Furthermore, VLM-PL integrates\nrefined pseudo and real GTs from upcoming training, effectively combining new\nand old knowledge. Extensive experiments conducted on the Pascal VOC and MS\nCOCO datasets not only highlight VLM-PL's exceptional performance in\nmulti-scenario but also illuminate its effectiveness in dual-scenario by\nachieving state-of-the-art results in both.\n","authors":["Junsu Kim","Yunhoe Ku","Jihyeon Kim","Junuk Cha","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2403.05346v1.pdf","comment":"pre-print, under-review"},{"id":"http://arxiv.org/abs/2403.05344v1","updated":"2024-03-08T14:21:43Z","published":"2024-03-08T14:21:43Z","title":"Federated Learning Method for Preserving Privacy in Face Recognition\n  System","summary":"  The state-of-the-art face recognition systems are typically trained on a\nsingle computer, utilizing extensive image datasets collected from various\nnumber of users. However, these datasets often contain sensitive personal\ninformation that users may hesitate to disclose. To address potential privacy\nconcerns, we explore the application of federated learning, both with and\nwithout secure aggregators, in the context of both supervised and unsupervised\nface recognition systems. Federated learning facilitates the training of a\nshared model without necessitating the sharing of individual private data,\nachieving this by training models on decentralized edge devices housing the\ndata. In our proposed system, each edge device independently trains its own\nmodel, which is subsequently transmitted either to a secure aggregator or\ndirectly to the central server. To introduce diverse data without the need for\ndata transmission, we employ generative adversarial networks to generate\nimposter data at the edge. Following this, the secure aggregator or central\nserver combines these individual models to construct a global model, which is\nthen relayed back to the edge devices. Experimental findings based on the\nCelebA datasets reveal that employing federated learning in both supervised and\nunsupervised face recognition systems offers dual benefits. Firstly, it\nsafeguards privacy since the original data remains on the edge devices.\nSecondly, the experimental results demonstrate that the aggregated model yields\nnearly identical performance compared to the individual models, particularly\nwhen the federated model does not utilize a secure aggregator. Hence, our\nresults shed light on the practical challenges associated with\nprivacy-preserving face image training, particularly in terms of the balance\nbetween privacy and accuracy.\n","authors":["Enoch Solomon","Abraham Woubie"],"pdf_url":"https://arxiv.org/pdf/2403.05344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05340v1","updated":"2024-03-08T14:17:07Z","published":"2024-03-08T14:17:07Z","title":"Embedded Deployment of Semantic Segmentation in Medicine through\n  Low-Resolution Inputs","summary":"  When deploying neural networks in real-life situations, the size and\ncomputational effort are often the limiting factors. This is especially true in\nenvironments where big, expensive hardware is not affordable, like in embedded\nmedical devices, where budgets are often tight. State-of-the-art proposed\nmultiple different lightweight solutions for such use cases, mostly by changing\nthe base model architecture, not taking the input and output resolution into\nconsideration. In this paper, we propose our architecture that takes advantage\nof the fact that in hardware-limited environments, we often refrain from using\nthe highest available input resolutions to guarantee a higher throughput.\nAlthough using lower-resolution input leads to a significant reduction in\ncomputing and memory requirements, it may also incur reduced prediction\nquality. Our architecture addresses this problem by exploiting the fact that we\ncan still utilize high-resolution ground-truths in training. The proposed model\ninputs lower-resolution images and high-resolution ground truths, which can\nimprove the prediction quality by 5.5% while adding less than 200 parameters to\nthe model. %reducing the frames per second only from 25 to 20. We conduct an\nextensive analysis to illustrate that our architecture enhances existing\nstate-of-the-art frameworks for lightweight semantic segmentation of cancer in\nMRI images. We also tested the deployment speed of state-of-the-art lightweight\nnetworks and our architecture on Nvidia's Jetson Nano to emulate deployment in\nresource-constrained embedded scenarios.\n","authors":["Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.05340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05329v1","updated":"2024-03-08T14:07:37Z","published":"2024-03-08T14:07:37Z","title":"OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy\n  Prediction","summary":"  3D occupancy prediction based on multi-sensor fusion, crucial for a reliable\nautonomous driving system, enables fine-grained understanding of 3D scenes.\nPrevious fusion-based 3D occupancy predictions relied on depth estimation for\nprocessing 2D image features. However, depth estimation is an ill-posed\nproblem, hindering the accuracy and robustness of these methods. Furthermore,\nfine-grained occupancy prediction demands extensive computational resources. We\nintroduce OccFusion, a multi-modal fusion method free from depth estimation,\nand a corresponding point cloud sampling algorithm for dense integration of\nimage features. Building on this, we propose an active training method and an\nactive coarse to fine pipeline, enabling the model to adaptively learn more\nfrom complex samples and optimize predictions specifically for challenging\nareas such as small or overlapping objects. The active methods we propose can\nbe naturally extended to any occupancy prediction model. Experiments on the\nOpenOccupancy benchmark show our method surpasses existing state-of-the-art\n(SOTA) multi-modal methods in IoU across all categories. Additionally, our\nmodel is more efficient during both the training and inference phases,\nrequiring far fewer computational resources. Comprehensive ablation studies\ndemonstrate the effectiveness of our proposed techniques.\n","authors":["Ji Zhang","Yiran Ding"],"pdf_url":"https://arxiv.org/pdf/2403.05329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05327v1","updated":"2024-03-08T14:06:15Z","published":"2024-03-08T14:06:15Z","title":"DiffSF: Diffusion Models for Scene Flow Estimation","summary":"  Scene flow estimation is an essential ingredient for a variety of real-world\napplications, especially for autonomous agents, such as self-driving cars and\nrobots. While recent scene flow estimation approaches achieve a reasonable\naccuracy, their applicability to real-world systems additionally benefits from\na reliability measure. Aiming at improving accuracy while additionally\nproviding an estimate for uncertainty, we propose DiffSF that combines\ntransformer-based scene flow estimation with denoising diffusion models. In the\ndiffusion process, the ground truth scene flow vector field is gradually\nperturbed by adding Gaussian noise. In the reverse process, starting from\nrandomly sampled Gaussian noise, the scene flow vector field prediction is\nrecovered by conditioning on a source and a target point cloud. We show that\nthe diffusion process greatly increases the robustness of predictions compared\nto prior approaches resulting in state-of-the-art performance on standard scene\nflow estimation benchmarks. Moreover, by sampling multiple times with different\ninitial states, the denoising process predicts multiple hypotheses, which\nenables measuring the output uncertainty, allowing our approach to detect a\nmajority of the inaccurate predictions.\n","authors":["Yushan Zhang","Bastian Wandt","Maria Magnusson","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2403.05327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05325v1","updated":"2024-03-08T14:04:30Z","published":"2024-03-08T14:04:30Z","title":"Fine-tuning a Multiple Instance Learning Feature Extractor with Masked\n  Context Modelling and Knowledge Distillation","summary":"  The first step in Multiple Instance Learning (MIL) algorithms for Whole Slide\nImage (WSI) classification consists of tiling the input image into smaller\npatches and computing their feature vectors produced by a pre-trained feature\nextractor model. Feature extractor models that were pre-trained with\nsupervision on ImageNet have proven to transfer well to this domain, however,\nthis pre-training task does not take into account that visual information in\nneighboring patches is highly correlated. Based on this observation, we propose\nto increase downstream MIL classification by fine-tuning the feature extractor\nmodel using \\textit{Masked Context Modelling with Knowledge Distillation}. In\nthis task, the feature extractor model is fine-tuned by predicting masked\npatches in a bigger context window. Since reconstructing the input image would\nrequire a powerful image generation model, and our goal is not to generate\nrealistically looking image patches, we predict instead the feature vectors\nproduced by a larger teacher network. A single epoch of the proposed task\nsuffices to increase the downstream performance of the feature-extractor model\nwhen used in a MIL scenario, even capable of outperforming the downstream\nperformance of the teacher model, while being considerably smaller and\nrequiring a fraction of its compute.\n","authors":["Juan I. Pisula","Katarzyna Bozek"],"pdf_url":"https://arxiv.org/pdf/2403.05325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03105v3","updated":"2024-03-08T13:30:58Z","published":"2023-05-04T18:39:14Z","title":"HAISTA-NET: Human Assisted Instance Segmentation Through Attention","summary":"  Instance segmentation is a form of image detection which has a range of\napplications, such as object refinement, medical image analysis, and\nimage/video editing, all of which demand a high degree of accuracy. However,\nthis precision is often beyond the reach of what even state-of-the-art, fully\nautomated instance segmentation algorithms can deliver. The performance gap\nbecomes particularly prohibitive for small and complex objects. Practitioners\ntypically resort to fully manual annotation, which can be a laborious process.\nIn order to overcome this problem, we propose a novel approach to enable more\nprecise predictions and generate higher-quality segmentation masks for\nhigh-curvature, complex and small-scale objects. Our human-assisted\nsegmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network\nto incorporate human-specified partial boundaries. We also present a dataset of\nhand-drawn partial object boundaries, which we refer to as human attention\nmaps. In addition, the Partial Sketch Object Boundaries (PSOB) dataset contains\nhand-drawn partial object boundaries which represent curvatures of an object's\nground truth mask with several pixels. Through extensive evaluation using the\nPSOB dataset, we show that HAISTA-NET outperforms state-of-the art methods such\nas Mask R-CNN, Strong Mask R-CNN, and Mask2Former, achieving respective\nincreases of +36.7, +29.6, and +26.5 points in AP-Mask metrics for these three\nmodels. We hope that our novel approach will set a baseline for future\nhuman-aided deep learning models by combining fully automated and interactive\ninstance segmentation architectures.\n","authors":["Muhammed Korkmaz","T. Metin Sezgin"],"pdf_url":"https://arxiv.org/pdf/2305.03105v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05297v1","updated":"2024-03-08T13:24:46Z","published":"2024-03-08T13:24:46Z","title":"PEEB: Part-based Image Classifiers with an Explainable and Editable\n  Language Bottleneck","summary":"  CLIP-based classifiers rely on the prompt containing a {class name} that is\nknown to the text encoder. That is, CLIP performs poorly on new classes or the\nclasses whose names rarely appear on the Internet (e.g., scientific names of\nbirds). For fine-grained classification, we propose PEEB - an explainable and\neditable classifier to (1) express the class name into a set of pre-defined\ntext descriptors that describe the visual parts of that class; and (2) match\nthe embeddings of the detected parts to their textual descriptors in each class\nto compute a logit score for classification. In a zero-shot setting where the\nclass names are unknown, PEEB outperforms CLIP by a large margin (~10x in\naccuracy). Compared to part-based classifiers, PEEB is not only the\nstate-of-the-art on the supervised-learning setting (88.80% accuracy) but also\nthe first to enable users to edit the class definitions to form a new\nclassifier without retraining. Compared to concept bottleneck models, PEEB is\nalso the state-of-the-art in both zero-shot and supervised learning settings.\n","authors":["Thang M. Pham","Peijie Chen","Tin Nguyen","Seunghyun Yoon","Trung Bui","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.05297v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2312.08338v2","updated":"2024-03-08T13:15:27Z","published":"2023-12-13T18:14:13Z","title":"Global Latent Neural Rendering","summary":"  A recent trend among generalizable novel view synthesis methods is to learn a\nrendering operator acting over single camera rays. This approach is promising\nbecause it removes the need for explicit volumetric rendering, but it\neffectively treats target images as collections of independent pixels. Here, we\npropose to learn a global rendering operator acting over all camera rays\njointly. We show that the right representation to enable such rendering is a\n5-dimensional plane sweep volume consisting of the projection of the input\nimages on a set of planes facing the target camera. Based on this\nunderstanding, we introduce our Convolutional Global Latent Renderer (ConvGLR),\nan efficient convolutional architecture that performs the rendering operation\nglobally in a low-resolution latent space. Experiments on various datasets\nunder sparse and generalizable setups show that our approach consistently\noutperforms existing methods by significant margins.\n","authors":["Thomas Tanay","Matteo Maggioni"],"pdf_url":"https://arxiv.org/pdf/2312.08338v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05280v1","updated":"2024-03-08T13:00:52Z","published":"2024-03-08T13:00:52Z","title":"ContrastDiagnosis: Enhancing Interpretability in Lung Nodule Diagnosis\n  Using Contrastive Learning","summary":"  With the ongoing development of deep learning, an increasing number of AI\nmodels have surpassed the performance levels of human clinical practitioners.\nHowever, the prevalence of AI diagnostic products in actual clinical practice\nremains significantly lower than desired. One crucial reason for this gap is\nthe so-called `black box' nature of AI models. Clinicians' distrust of black\nbox models has directly hindered the clinical deployment of AI products. To\naddress this challenge, we propose ContrastDiagnosis, a straightforward yet\neffective interpretable diagnosis framework. This framework is designed to\nintroduce inherent transparency and provide extensive post-hoc explainability\nfor deep learning model, making them more suitable for clinical medical\ndiagnosis. ContrastDiagnosis incorporates a contrastive learning mechanism to\nprovide a case-based reasoning diagnostic rationale, enhancing the model's\ntransparency and also offers post-hoc interpretability by highlighting similar\nareas. High diagnostic accuracy was achieved with AUC of 0.977 while maintain a\nhigh transparency and explainability.\n","authors":["Chenglong Wang","Yinqiao Yi","Yida Wang","Chengxiu Zhang","Yun Liu","Kensaku Mori","Mei Yuan","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.05280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00920v2","updated":"2024-03-08T12:51:28Z","published":"2023-10-02T06:17:24Z","title":"Every Dataset Counts: Scaling up Monocular 3D Object Detection with\n  Joint Datasets Training","summary":"  Monocular 3D object detection plays a crucial role in autonomous driving.\nHowever, existing monocular 3D detection algorithms depend on 3D labels derived\nfrom LiDAR measurements, which are costly to acquire for new datasets and\nchallenging to deploy in novel environments. Specifically, this study\ninvestigates the pipeline for training a monocular 3D object detection model on\na diverse collection of 3D and 2D datasets. The proposed framework comprises\nthree components: (1) a robust monocular 3D model capable of functioning across\nvarious camera settings, (2) a selective-training strategy to accommodate\ndatasets with differing class annotations, and (3) a pseudo 3D training\napproach using 2D labels to enhance detection performance in scenes containing\nonly 2D labels. With this framework, we could train models on a joint set of\nvarious open 3D/2D datasets to obtain models with significantly stronger\ngeneralization capability and enhanced performance on new dataset with only 2D\nlabels. We conduct extensive experiments on\nKITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling\nability of the proposed method.\n","authors":["Fulong Ma","Xiaoyang Yan","Guoyang Zhao","Xiaojie Xu","Yuxuan Liu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2310.00920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05262v1","updated":"2024-03-08T12:35:07Z","published":"2024-03-08T12:35:07Z","title":"Debiasing Large Visual Language Models","summary":"  In the realms of computer vision and natural language processing, Large\nVision-Language Models (LVLMs) have become indispensable tools, proficient in\ngenerating textual descriptions based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias in the generated\ncontent, where the output is primarily influenced by the underlying Large\nLanguage Models (LLMs) prior rather than the input image. Our empirical\nexperiments underscore the persistence of this bias, as LVLMs often provide\nconfident answers even in the absence of relevant images or given incongruent\nvisual input. To rectify these biases and redirect the model's focus toward\nvision information, we introduce two simple, training-free strategies. Firstly,\nfor tasks such as classification or multi-choice question-answering (QA), we\npropose a ``calibration'' step through affine transformation to adjust the\noutput distribution. This ``Post-Hoc debias'' approach ensures uniform scores\nfor each answer when the image is absent, serving as an effective\nregularization technique to alleviate the influence of LLM priors. For more\nintricate open-ended generation tasks, we extend this method to ``Debias\nsampling'', drawing inspirations from contrastive decoding methods.\nFurthermore, our investigation sheds light on the instability of LVLMs across\nvarious decoding configurations. Through systematic exploration of different\nsettings, we significantly enhance performance, surpassing reported results and\nraising concerns about the fairness of existing evaluations. Comprehensive\nexperiments substantiate the effectiveness of our proposed strategies in\nmitigating biases. These strategies not only prove beneficial in minimizing\nhallucinations but also contribute to the generation of more helpful and\nprecise illustrations.\n","authors":["Yi-Fan Zhang","Weichen Yu","Qingsong Wen","Xue Wang","Zhang Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.05262v1.pdf","comment":"38 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.05261v1","updated":"2024-03-08T12:32:14Z","published":"2024-03-08T12:32:14Z","title":"Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval","summary":"  Current image-text retrieval methods have demonstrated impressive performance\nin recent years. However, they still face two problems: the inter-modal\nmatching missing problem and the intra-modal semantic loss problem. These\nproblems can significantly affect the accuracy of image-text retrieval. To\naddress these challenges, we propose a novel method called Cross-modal and\nUni-modal Soft-label Alignment (CUSA). Our method leverages the power of\nuni-modal pre-trained models to provide soft-label supervision signals for the\nimage-text retrieval model. Additionally, we introduce two alignment\ntechniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label\nAlignment (USA), to overcome false negatives and enhance similarity recognition\nbetween uni-modal samples. Our method is designed to be plug-and-play, meaning\nit can be easily applied to existing image-text retrieval models without\nchanging their original architectures. Extensive experiments on various\nimage-text retrieval models and datasets, we demonstrate that our method can\nconsistently improve the performance of image-text retrieval and achieve new\nstate-of-the-art results. Furthermore, our method can also boost the uni-modal\nretrieval performance of image-text retrieval models, enabling it to achieve\nuniversal retrieval. The code and supplementary files can be found at\nhttps://github.com/lerogo/aaai24_itr_cusa.\n","authors":["Hailang Huang","Zhijie Nie","Ziqiao Wang","Ziyu Shang"],"pdf_url":"https://arxiv.org/pdf/2403.05261v1.pdf","comment":"9 pages, Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.05256v1","updated":"2024-03-08T12:26:48Z","published":"2024-03-08T12:26:48Z","title":"DuDoUniNeXt: Dual-domain unified hybrid model for single and\n  multi-contrast undersampled MRI reconstruction","summary":"  Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to\nincorporate a reference image of auxiliary modality to guide the reconstruction\nprocess of the target modality. Known MC reconstruction methods perform well\nwith a fully sampled reference image, but usually exhibit inferior performance,\ncompared to single-contrast (SC) methods, when the reference image is missing\nor of low quality. To address this issue, we propose DuDoUniNeXt, a unified\ndual-domain MRI reconstruction network that can accommodate to scenarios\ninvolving absent, low-quality, and high-quality reference images. DuDoUniNeXt\nadopts a hybrid backbone that combines CNN and ViT, enabling specific\nadjustment of image domain and k-space reconstruction. Specifically, an\nadaptive coarse-to-fine feature fusion module (AdaC2F) is devised to\ndynamically process the information from reference images of varying qualities.\nBesides, a partially shared shallow feature extractor (PaSS) is proposed, which\nuses shared and distinct parameters to handle consistent and discrepancy\ninformation among contrasts. Experimental results demonstrate that the proposed\nmodel surpasses state-of-the-art SC and MC models significantly. Ablation\nstudies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS,\nand the dual-domain unified learning scheme.\n","authors":["Ziqi Gao","Yue Zhang","Xinwen Liu","Kaiyan Li","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05256v1.pdf","comment":"11 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2402.17664v3","updated":"2024-03-08T12:16:31Z","published":"2024-02-27T16:35:07Z","title":"Bayesian Differentiable Physics for Cloth Digitalization","summary":"  We propose a new method for cloth digitalization. Deviating from existing\nmethods which learn from data captured under relatively casual settings, we\npropose to learn from data captured in strictly tested measuring protocols, and\nfind plausible physical parameters of the cloths. However, such data is\ncurrently absent, so we first propose a new dataset with accurate cloth\nmeasurements. Further, the data size is considerably smaller than the ones in\ncurrent deep learning, due to the nature of the data capture process. To learn\nfrom small data, we propose a new Bayesian differentiable cloth model to\nestimate the complex material heterogeneity of real cloths. It can provide\nhighly accurate digitalization from very limited data samples. Through\nexhaustive evaluation and comparison, we show our method is accurate in cloth\ndigitalization, efficient in learning from limited data samples, and general in\ncapturing material variations. Code and data are available\nhttps://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization\n","authors":["Deshan Gong","Ningtao Mao","He Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17664v3.pdf","comment":"9 pages, 8 figures, to be published in CVPR"},{"id":"http://arxiv.org/abs/2403.05247v1","updated":"2024-03-08T12:08:06Z","published":"2024-03-08T12:08:06Z","title":"Hide in Thicket: Generating Imperceptible and Rational Adversarial\n  Perturbations on 3D Point Clouds","summary":"  Adversarial attack methods based on point manipulation for 3D point cloud\nclassification have revealed the fragility of 3D models, yet the adversarial\nexamples they produce are easily perceived or defended against. The trade-off\nbetween the imperceptibility and adversarial strength leads most point attack\nmethods to inevitably introduce easily detectable outlier points upon a\nsuccessful attack. Another promising strategy, shape-based attack, can\neffectively eliminate outliers, but existing methods often suffer significant\nreductions in imperceptibility due to irrational deformations. We find that\nconcealing deformation perturbations in areas insensitive to human eyes can\nachieve a better trade-off between imperceptibility and adversarial strength,\nspecifically in parts of the object surface that are complex and exhibit\ndrastic curvature changes. Therefore, we propose a novel shape-based\nadversarial attack method, HiT-ADV, which initially conducts a two-stage search\nfor attack regions based on saliency and imperceptibility scores, and then adds\ndeformation perturbations in each attack region using Gaussian kernel\nfunctions. Additionally, HiT-ADV is extendable to physical attack. We propose\nthat by employing benign resampling and benign rigid transformations, we can\nfurther enhance physical adversarial strength with little sacrifice to\nimperceptibility. Extensive experiments have validated the superiority of our\nmethod in terms of adversarial and imperceptible properties in both digital and\nphysical spaces. Our code is avaliable at: https://github.com/TRLou/HiT-ADV.\n","authors":["Tianrui Lou","Xiaojun Jia","Jindong Gu","Li Liu","Siyuan Liang","Bangyan He","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2403.05247v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05246v1","updated":"2024-03-08T12:07:42Z","published":"2024-03-08T12:07:42Z","title":"LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image\n  Segmentation","summary":"  UNet and its variants have been widely used in medical image segmentation.\nHowever, these models, especially those based on Transformer architectures,\npose challenges due to their large number of parameters and computational\nloads, making them unsuitable for mobile health applications. Recently, State\nSpace Models (SSMs), exemplified by Mamba, have emerged as competitive\nalternatives to CNN and Transformer architectures. Building upon this, we\nemploy Mamba as a lightweight substitute for CNN and Transformer within UNet,\naiming at tackling challenges stemming from computational resource limitations\nin real medical settings. To this end, we introduce the Lightweight Mamba UNet\n(LightM-UNet) that integrates Mamba and UNet in a lightweight framework.\nSpecifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure\nMamba fashion to extract deep semantic features and model long-range spatial\ndependencies, with linear computational complexity. Extensive experiments\nconducted on two real-world 2D/3D datasets demonstrate that LightM-UNet\nsurpasses existing state-of-the-art literature. Notably, when compared to the\nrenowned nnU-Net, LightM-UNet achieves superior segmentation performance while\ndrastically reducing parameter and computation costs by 116x and 21x,\nrespectively. This highlights the potential of Mamba in facilitating model\nlightweighting. Our code implementation is publicly available at\nhttps://github.com/MrBlankness/LightM-UNet.\n","authors":["Weibin Liao","Yinghao Zhu","Xinyuan Wang","Cehngwei Pan","Yasha Wang","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2403.05246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05245v1","updated":"2024-03-08T12:07:18Z","published":"2024-03-08T12:07:18Z","title":"Noise Level Adaptive Diffusion Model for Robust Reconstruction of\n  Accelerated MRI","summary":"  In general, diffusion model-based MRI reconstruction methods incrementally\nremove artificially added noise while imposing data consistency to reconstruct\nthe underlying images. However, real-world MRI acquisitions already contain\ninherent noise due to thermal fluctuations. This phenomenon is particularly\nnotable when using ultra-fast, high-resolution imaging sequences for advanced\nresearch, or using low-field systems favored by low- and middle-income\ncountries. These common scenarios can lead to sub-optimal performance or\ncomplete failure of existing diffusion model-based reconstruction techniques.\nSpecifically, as the artificially added noise is gradually removed, the\ninherent MRI noise becomes increasingly pronounced, making the actual noise\nlevel inconsistent with the predefined denoising schedule and consequently\ninaccurate image reconstruction. To tackle this problem, we propose a posterior\nsampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC)\noperation. Extensive experiments are conducted on two public datasets and an\nin-house clinical dataset with field strength ranging from 0.3T to 3T, showing\nthat our method surpasses the state-of-the-art MRI reconstruction methods, and\nis highly robust against various noise levels. The code will be released after\nreview.\n","authors":["Shoujin Huang","Guanxiong Luo","Xi Wang","Ziran Chen","Yuwan Wang","Huaishui Yang","Pheng-Ann Heng","Lingyan Zhang","Mengye Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.05245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05239v1","updated":"2024-03-08T11:59:32Z","published":"2024-03-08T11:59:32Z","title":"Towards Effective Usage of Human-Centric Priors in Diffusion Models for\n  Text-based Human Image Generation","summary":"  Vanilla text-to-image diffusion models struggle with generating accurate\nhuman images, commonly resulting in imperfect anatomies such as unnatural\npostures or disproportionate limbs.Existing methods address this issue mostly\nby fine-tuning the model with extra images or adding additional controls --\nhuman-centric priors such as pose or depth maps -- during the image generation\nphase. This paper explores the integration of these human-centric priors\ndirectly into the model fine-tuning stage, essentially eliminating the need for\nextra conditions at the inference stage. We realize this idea by proposing a\nhuman-centric alignment loss to strengthen human-related information from the\ntextual prompts within the cross-attention maps. To ensure semantic detail\nrichness and human structural accuracy during fine-tuning, we introduce\nscale-aware and step-wise constraints within the diffusion process, according\nto an in-depth analysis of the cross-attention layer. Extensive experiments\nshow that our method largely improves over state-of-the-art text-to-image\nmodels to synthesize high-quality human images based on user-written prompts.\nProject page: \\url{https://hcplayercvpr2024.github.io}.\n","authors":["Junyan Wang","Zhenhong Sun","Zhiyu Tan","Xuanbai Chen","Weihua Chen","Hao Li","Cheng Zhang","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2403.05239v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05234v1","updated":"2024-03-08T11:48:44Z","published":"2024-03-08T11:48:44Z","title":"Benchmarking Micro-action Recognition: Dataset, Methods, and\n  Applications","summary":"  Micro-action is an imperceptible non-verbal behaviour characterised by\nlow-intensity movement. It offers insights into the feelings and intentions of\nindividuals and is important for human-oriented applications such as emotion\nrecognition and psychological assessment. However, the identification,\ndifferentiation, and understanding of micro-actions pose challenges due to the\nimperceptible and inaccessible nature of these subtle human behaviors in\neveryday life. In this study, we innovatively collect a new micro-action\ndataset designated as Micro-action-52 (MA-52), and propose a benchmark named\nmicro-action network (MANet) for micro-action recognition (MAR) task. Uniquely,\nMA-52 provides the whole-body perspective including gestures, upper- and\nlower-limb movements, attempting to reveal comprehensive micro-action cues. In\ndetail, MA-52 contains 52 micro-action categories along with seven body part\nlabels, and encompasses a full array of realistic and natural micro-actions,\naccounting for 205 participants and 22,422 video instances collated from the\npsychological interviews. Based on the proposed dataset, we assess MANet and\nother nine prevalent action recognition methods. MANet incorporates squeeze-and\nexcitation (SE) and temporal shift module (TSM) into the ResNet architecture\nfor modeling the spatiotemporal characteristics of micro-actions. Then a\njoint-embedding loss is designed for semantic matching between video and action\nlabels; the loss is used to better distinguish between visually similar yet\ndistinct micro-action categories. The extended application in emotion\nrecognition has demonstrated one of the important values of our proposed\ndataset and method. In the future, further exploration of human behaviour,\nemotion, and psychological assessment will be conducted in depth. The dataset\nand source code are released at https://github.com/VUT-HFUT/Micro-Action.\n","authors":["Dan Guo","Kun Li","Bin Hu","Yan Zhang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05234v1.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology"},{"id":"http://arxiv.org/abs/2403.05231v1","updated":"2024-03-08T11:41:48Z","published":"2024-03-08T11:41:48Z","title":"Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance","summary":"  Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language\nmodels, we propose LoRAT, a method that unveils the power of larger Vision\nTransformers (ViT) for tracking within laboratory-level resources. The essence\nof our work lies in adapting LoRA, a technique that fine-tunes a small subset\nof model parameters without adding inference latency, to the domain of visual\ntracking. However, unique challenges and potential domain gaps make this\ntransfer not as easy as the first intuition. Firstly, a transformer-based\ntracker constructs unshared position embedding for template and search image.\nThis poses a challenge for the transfer of LoRA, usually requiring consistency\nin the design when applied to the pre-trained backbone, to downstream tasks.\nSecondly, the inductive bias inherent in convolutional heads diminishes the\neffectiveness of parameter-efficient fine-tuning in tracking models. To\novercome these limitations, we first decouple the position embeddings in\ntransformer-based trackers into shared spatial ones and independent type ones.\nThe shared embeddings, which describe the absolute coordinates of\nmulti-resolution images (namely, the template and search images), are inherited\nfrom the pre-trained backbones. In contrast, the independent embeddings\nindicate the sources of each token and are learned from scratch. Furthermore,\nwe design an anchor-free head solely based on a multilayer perceptron (MLP) to\nadapt PETR, enabling better performance with less computational overhead. With\nour design, 1) it becomes practical to train trackers with the ViT-g backbone\non GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the\ntraining time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve\nthe LaSOT SUC score from 0.703 to 0.743 with the L-224 variant; 4) we fast the\ninference speed of the L-224 variant from 52 to 119 FPS. Code and models will\nbe released.\n","authors":["Liting Lin","Heng Fan","Zhipeng Zhang","Yaowei Wang","Yong Xu","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2403.05231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15832v2","updated":"2024-03-08T11:31:58Z","published":"2024-02-24T14:59:19Z","title":"Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and\n  Eosin Whole Slide Images: An Indian Cohort Study","summary":"  The effective management of brain tumors relies on precise typing, subtyping,\nand grading. This study advances patient care with findings from rigorous\nmultiple instance learning experimentations across various feature extractors\nand aggregators in brain tumor histopathology. It establishes new performance\nbenchmarks in glioma subtype classification across multiple datasets, including\na novel dataset focused on the Indian demographic (IPD- Brain), providing a\nvaluable resource for existing research. Using a ResNet-50, pretrained on\nhistopathology datasets for feature extraction, combined with the Double-Tier\nFeature Distillation (DTFD) feature aggregator, our approach achieves\nstate-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on the TCGA-Brain\ndataset, respectively, for three-way glioma subtype classification. Moreover,\nit establishes new benchmarks in grading and detecting IHC molecular biomarkers\n(IDH1R132H, TP53, ATRX, Ki-67) through H&E stained whole slide images for the\nIPD-Brain dataset. The work also highlights a significant correlation between\nthe model decision-making processes and the diagnostic reasoning of\npathologists, underscoring its capability to mimic professional diagnostic\nprocedures.\n","authors":["Ekansh Chauhan","Amit Sharma","Megha S Uppin","C. V. Jawahar","P. K. Vinod"],"pdf_url":"https://arxiv.org/pdf/2402.15832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05446v5","updated":"2024-03-08T11:31:28Z","published":"2023-10-09T06:43:38Z","title":"RetSeg: Retention-based Colorectal Polyps Segmentation Network","summary":"  Vision Transformers (ViTs) have revolutionized medical imaging analysis,\nshowcasing superior efficacy compared to conventional Convolutional Neural\nNetworks (CNNs) in vital tasks such as polyp classification, detection, and\nsegmentation. Leveraging attention mechanisms to focus on specific image\nregions, ViTs exhibit contextual awareness in processing visual data,\nculminating in robust and precise predictions, even for intricate medical\nimages. Moreover, the inherent self-attention mechanism in Transformers\naccommodates varying input sizes and resolutions, granting an unprecedented\nflexibility absent in traditional CNNs. However, Transformers grapple with\nchallenges like excessive memory usage and limited training parallelism due to\nself-attention, rendering them impractical for real-time disease detection on\nresource-constrained devices. In this study, we address these hurdles by\ninvestigating the integration of the recently introduced retention mechanism\ninto polyp segmentation, introducing RetSeg, an encoder-decoder network\nfeaturing multi-head retention blocks. Drawing inspiration from Retentive\nNetworks (RetNet), RetSeg is designed to bridge the gap between precise polyp\nsegmentation and resource utilization, particularly tailored for colonoscopy\nimages. We train and validate RetSeg for polyp segmentation employing two\npublicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we\nshowcase RetSeg's promising performance across diverse public datasets,\nincluding CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While\nour work represents an early-stage exploration, further in-depth studies are\nimperative to advance these promising findings.\n","authors":["Khaled ELKarazle","Valliappan Raman","Caslon Chua","Patrick Then"],"pdf_url":"https://arxiv.org/pdf/2310.05446v5.pdf","comment":"Updated PDF"},{"id":"http://arxiv.org/abs/2309.04836v3","updated":"2024-03-08T11:28:40Z","published":"2023-09-09T16:21:56Z","title":"Neural Semantic Surface Maps","summary":"  We present an automated technique for computing a map between two genus-zero\nshapes, which matches semantically corresponding regions to one another. Lack\nof annotated data prohibits direct inference of 3D semantic priors; instead,\ncurrent State-of-the-art methods predominantly optimize geometric properties or\nrequire varying amounts of manual annotation. To overcome the lack of annotated\ntraining data, we distill semantic matches from pre-trained vision models: our\nmethod renders the pair of 3D shapes from multiple viewpoints; the resulting\nrenders are then fed into an off-the-shelf image-matching method which\nleverages a pretrained visual model to produce feature points. This yields\nsemantic correspondences, which can be projected back to the 3D shapes,\nproducing a raw matching that is inaccurate and inconsistent between different\nviewpoints. These correspondences are refined and distilled into an\ninter-surface map by a dedicated optimization scheme, which promotes\nbijectivity and continuity of the output map. We illustrate that our approach\ncan generate semantic surface-to-surface maps, eliminating manual annotations\nor any 3D training data requirement. Furthermore, it proves effective in\nscenarios with high semantic complexity, where objects are non-isometrically\nrelated, as well as in situations where they are nearly isometric.\n","authors":["Luca Morreale","Noam Aigerman","Vladimir G. Kim","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2309.04836v3.pdf","comment":"Accepted at Eurographics 2024"},{"id":"http://arxiv.org/abs/2403.05220v1","updated":"2024-03-08T11:18:26Z","published":"2024-03-08T11:18:26Z","title":"Synthetic Privileged Information Enhances Medical Image Representation\n  Learning","summary":"  Multimodal self-supervised representation learning has consistently proven to\nbe a highly effective method in medical image analysis, offering strong task\nperformance and producing biologically informed insights. However, these\nmethods heavily rely on large, paired datasets, which is prohibitive for their\nuse in scenarios where paired data does not exist, or there is only a small\namount available. In contrast, image generation methods can work well on very\nsmall datasets, and can find mappings between unpaired datasets, meaning an\neffectively unlimited amount of paired synthetic data can be generated. In this\nwork, we demonstrate that representation learning can be significantly improved\nby synthetically generating paired information, both compared to training on\neither single-modality (up to 4.4x error reduction) or authentic multi-modal\npaired datasets (up to 5.6x error reduction).\n","authors":["Lucas Farndale","Chris Walsh","Robert Insall","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.05220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05218v1","updated":"2024-03-08T11:09:46Z","published":"2024-03-08T11:09:46Z","title":"3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder","summary":"  Monocular 3D face reconstruction plays a crucial role in avatar generation,\nwith significant demand in web-related applications such as generating virtual\nfinancial advisors in FinTech. Current reconstruction methods predominantly\nrely on deep learning techniques and employ 2D self-supervision as a means to\nguide model learning. However, these methods encounter challenges in capturing\nthe comprehensive 3D structural information of the face due to the utilization\nof 2D images for model training purposes. To overcome this limitation and\nenhance the reconstruction of 3D structural features, we propose an innovative\napproach that integrates existing 2D features with 3D features to guide the\nmodel learning process. Specifically, we introduce the 3D-ID Loss, which\nleverages the high-dimensional structure features extracted from a\nSpectral-Based Graph Convolution Encoder applied to the facial mesh. This\napproach surpasses the sole reliance on the 3D information provided by the\nfacial mesh vertices coordinates. Our model is trained using 2D-3D data pairs\nfrom a combination of datasets and achieves state-of-the-art performance on the\nNoW benchmark.\n","authors":["Haoxin Xu","Zezheng Zhao","Yuxin Cao","Chunyu Chen","Hao Ge","Ziyao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05218v1.pdf","comment":"4 pages, 3 figures. Accepted to WWW 2024"},{"id":"http://arxiv.org/abs/2310.06549v4","updated":"2024-03-08T11:07:51Z","published":"2023-10-10T11:51:12Z","title":"Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks","summary":"  Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06549v4.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2312.04885v2","updated":"2024-03-08T10:59:20Z","published":"2023-12-08T07:48:03Z","title":"VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement","summary":"  In recent years, online Video Instance Segmentation (VIS) methods have shown\nremarkable advancement with their powerful query-based detectors. Utilizing the\noutput queries of the detector at the frame-level, these methods achieve high\naccuracy on challenging benchmarks. However, our observations demonstrate that\nthese methods heavily rely on location information, which often causes\nincorrect associations between objects. This paper presents that a key axis of\nobject matching in trackers is appearance information, which becomes greatly\ninstructive under conditions where positional cues are insufficient for\ndistinguishing their identities. Therefore, we suggest a simple yet powerful\nextension to object decoders that explicitly extract embeddings from backbone\nfeatures and drive queries to capture the appearances of objects, which greatly\nenhances instance association accuracy. Furthermore, recognizing the\nlimitations of existing benchmarks in fully evaluating appearance awareness, we\nhave constructed a synthetic dataset to rigorously validate our method. By\neffectively resolving the over-reliance on location information, we achieve\nstate-of-the-art results on YouTube-VIS 2019/2021 and Occluded VIS (OVIS). Code\nis available at https://github.com/KimHanjung/VISAGE.\n","authors":["Hanjung Kim","Jaehyun Kang","Miran Heo","Sukjun Hwang","Seoung Wug Oh","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2312.04885v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2403.05211v1","updated":"2024-03-08T10:55:07Z","published":"2024-03-08T10:55:07Z","title":"Improving the Successful Robotic Grasp Detection Using Convolutional\n  Neural Networks","summary":"  Robotic grasp should be carried out in a real-time manner by proper accuracy.\nPerception is the first and significant step in this procedure. This paper\nproposes an improved pipeline model trying to detect grasp as a rectangle\nrepresentation for different seen or unseen objects. It helps the robot to\nstart control procedures from nearer to the proper part of the object. The main\nidea consists in pre-processing, output normalization, and data augmentation to\nimprove accuracy by 4.3 percent without making the system slow. Also, a\ncomparison has been conducted over different pre-trained models like AlexNet,\nResNet, Vgg19, which are the most famous feature extractors for image\nprocessing in object detection. Although AlexNet has less complexity than other\nones, it outperformed them, which helps the real-time property.\n","authors":["Hamed Hosseini","Mehdi Tale Masouleh","Ahmad Kalhor"],"pdf_url":"https://arxiv.org/pdf/2403.05211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05209v1","updated":"2024-03-08T10:49:37Z","published":"2024-03-08T10:49:37Z","title":"Overcoming Data Inequality across Domains with Semi-Supervised Domain\n  Generalization","summary":"  While there have been considerable advancements in machine learning driven by\nextensive datasets, a significant disparity still persists in the availability\nof data across various sources and populations. This inequality across domains\nposes challenges in modeling for those with limited data, which can lead to\nprofound practical and ethical concerns. In this paper, we address a\nrepresentative case of data inequality problem across domains termed\nSemi-Supervised Domain Generalization (SSDG), in which only one domain is\nlabeled while the rest are unlabeled. We propose a novel algorithm, ProUD,\nwhich can effectively learn domain-invariant features via domain-aware\nprototypes along with progressive generalization via uncertainty-adaptive\nmixing of labeled and unlabeled domains. Our experiments on three different\nbenchmark datasets demonstrate the effectiveness of ProUD, outperforming all\nbaseline models including single domain generalization and semi-supervised\nlearning. Source code will be released upon acceptance of the paper.\n","authors":["Jinha Park","Wonguk Cho","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2403.05209v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.04508v2","updated":"2024-03-08T10:34:48Z","published":"2024-03-07T14:08:01Z","title":"Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces","summary":"  Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D\nreconstruction and novel view synthesis in recent years due to their remarkable\nperformance. Despite the huge interest in NeRF methods, a practical use case of\nNeRFs has largely been ignored; the exploration of the scene space modelled by\na NeRF. In this paper, for the first time in the literature, we propose and\nformally define the scene exploration framework as the efficient discovery of\nNeRF model inputs (i.e. coordinates and viewing angles), using which one can\nrender novel views that adhere to user-selected criteria. To remedy the lack of\napproaches addressing scene exploration, we first propose two baseline methods\ncalled Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).\nWe then cast scene exploration as an optimization problem, and propose the\ncriteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient\nexploration. We test all three approaches with various criteria (e.g. saliency\nmaximization, image quality maximization, photo-composition quality\nimprovement) and show that our EGPS performs more favourably than other\nbaselines. We finally highlight key points and limitations, and outline\ndirections for future research in scene exploration.\n","authors":["Evangelos Skartados","Mehmet Kerim Yucel","Bruno Manganelli","Anastasios Drosou","Albert Saà-Garriga"],"pdf_url":"https://arxiv.org/pdf/2403.04508v2.pdf","comment":"Accepted at ACM MMSys'24"},{"id":"http://arxiv.org/abs/2403.05196v1","updated":"2024-03-08T10:19:00Z","published":"2024-03-08T10:19:00Z","title":"Denoising Autoregressive Representation Learning","summary":"  In this paper, we explore a new generative approach for learning visual\nrepresentations. Our method, DARL, employs a decoder-only Transformer to\npredict image patches autoregressively. We find that training with Mean Squared\nError (MSE) alone leads to strong representations. To enhance the image\ngeneration ability, we replace the MSE loss with the diffusion objective by\nusing a denoising patch decoder. We show that the learned representation can be\nimproved by using tailored noise schedules and longer training in larger\nmodels. Notably, the optimal schedule differs significantly from the typical\nones used in standard image diffusion models. Overall, despite its simple\narchitecture, DARL delivers performance remarkably close to state-of-the-art\nmasked prediction models under the fine-tuning protocol. This marks an\nimportant step towards a unified model capable of both visual perception and\ngeneration, effectively combining the strengths of autoregressive and denoising\ndiffusion models.\n","authors":["Yazhe Li","Jorg Bornschein","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.05196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02118v3","updated":"2024-03-08T10:03:44Z","published":"2024-03-04T15:21:51Z","title":"Towards Implicit Prompt For Text-To-Image Models","summary":"  Recent text-to-image (T2I) models have had great success, and many benchmarks\nhave been proposed to evaluate their performance and safety. However, they only\nconsider explicit prompts while neglecting implicit prompts (hint at a target\nwithout explicitly mentioning it). These prompts may get rid of safety\nconstraints and pose potential threats to the applications of these models.\nThis position paper highlights the current state of T2I models toward implicit\nprompts. We present a benchmark named ImplicitBench and conduct an\ninvestigation on the performance and impacts of implicit prompts with popular\nT2I models. Specifically, we design and collect more than 2,000 implicit\nprompts of three aspects: General Symbols, Celebrity Privacy, and\nNot-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models'\ncapabilities under these implicit prompts. Experiment results show that (1) T2I\nmodels are able to accurately create various target symbols indicated by\nimplicit prompts; (2) Implicit prompts bring potential risks of privacy leakage\nfor T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can\nbe bypassed with implicit prompts. We call for increased attention to the\npotential and risks of implicit prompts in the T2I community and further\ninvestigation into the capabilities and impacts of implicit prompts, advocating\nfor a balanced approach that harnesses their benefits while mitigating their\nrisks.\n","authors":["Yue Yang","Yuqi lin","Hong Liu","Wenqi Shao","Runjian Chen","Hailong Shang","Yu Wang","Yu Qiao","Kaipeng Zhang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2403.02118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02746v2","updated":"2024-03-08T09:59:40Z","published":"2024-03-05T08:02:00Z","title":"Learning without Exact Guidance: Updating Large-scale High-resolution\n  Land Cover Maps from Low-resolution Historical Labels","summary":"  Large-scale high-resolution (HR) land-cover mapping is a vital task to survey\nthe Earth's surface and resolve many challenges facing humanity. However, it is\nstill a non-trivial task hindered by complex ground details, various landforms,\nand the scarcity of accurate training labels over a wide-span geographic area.\nIn this paper, we propose an efficient, weakly supervised framework\n(Paraformer) to guide large-scale HR land-cover mapping with easy-access\nhistorical land-cover data of low resolution (LR). Specifically, existing\nland-cover mapping approaches reveal the dominance of CNNs in preserving local\nground details but still suffer from insufficient global modeling in various\nlandforms. Therefore, we design a parallel CNN-Transformer feature extractor in\nParaformer, consisting of a downsampling-free CNN branch and a Transformer\nbranch, to jointly capture local and global contextual information. Besides,\nfacing the spatial mismatch of training data, a pseudo-label-assisted training\n(PLAT) module is adopted to reasonably refine LR labels for weakly supervised\nsemantic segmentation of HR images. Experiments on two large-scale datasets\ndemonstrate the superiority of Paraformer over other state-of-the-art methods\nfor automatically updating HR land-cover maps from LR historical labels.\n","authors":["Zhuohong Li","Wei He","Jiepan Li","Fangxiao Lu","Hongyan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.02746v2.pdf","comment":"11 pages, 9 figures, accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.04115v2","updated":"2024-03-08T09:56:47Z","published":"2024-03-07T00:09:07Z","title":"DNAct: Diffusion Guided Multi-Task 3D Policy Learning","summary":"  This paper presents DNAct, a language-conditioned multi-task policy framework\nthat integrates neural rendering pre-training and diffusion training to enforce\nmulti-modality learning in action sequence spaces. To learn a generalizable\nmulti-task policy with few demonstrations, the pre-training phase of DNAct\nleverages neural rendering to distill 2D semantic features from foundation\nmodels such as Stable Diffusion to a 3D space, which provides a comprehensive\nsemantic understanding regarding the scene. Consequently, it allows various\napplications to challenging robotic tasks requiring rich 3D semantics and\naccurate geometry. Furthermore, we introduce a novel approach utilizing\ndiffusion training to learn a vision and language feature that encapsulates the\ninherent multi-modality in the multi-task demonstrations. By reconstructing the\naction sequences from different tasks via the diffusion process, the model is\ncapable of distinguishing different modalities and thus improving the\nrobustness and the generalizability of the learned representation. DNAct\nsignificantly surpasses SOTA NeRF-based multi-task manipulation approaches with\nover 30% improvement in success rate. Project website: dnact.github.io.\n","authors":["Ge Yan","Yueh-Hua Wu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.04115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17561v2","updated":"2024-03-08T09:56:43Z","published":"2024-02-27T14:59:48Z","title":"PHNet: Patch-based Normalization for Portrait Harmonization","summary":"  A common problem for composite images is the incompatibility of their\nforeground and background components. Image harmonization aims to solve this\nproblem, making the whole image look more authentic and coherent. Most existing\nsolutions predict lookup tables (LUTs) or reconstruct images, utilizing various\nattributes of composite images. Recent approaches have primarily focused on\nemploying global transformations like normalization and color curve rendering\nto achieve visual consistency, and they often overlook the importance of local\nvisual coherence. We present a patch-based harmonization network consisting of\nnovel Patch-based normalization (PN) blocks and a feature extractor based on\nstatistical color transfer. Extensive experiments demonstrate the network's\nhigh generalization capability for different domains. Our network achieves\nstate-of-the-art results on the iHarmony4 dataset. Also, we created a new human\nportrait harmonization dataset based on FFHQ and checked the proposed method to\nshow the generalization ability by achieving the best metrics on it. The\nbenchmark experiments confirm that the suggested patch-based normalization\nblock and feature extractor effectively improve the network's capability to\nharmonize portraits. Our code and model baselines are publicly available.\n","authors":["Karen Efremyan","Elizaveta Petrova","Evgeny Kaskov","Alexander Kapitanov"],"pdf_url":"https://arxiv.org/pdf/2402.17561v2.pdf","comment":"Image harmonization, Patch-based normalization, Portrait\n  harmonization"},{"id":"http://arxiv.org/abs/2307.09944v2","updated":"2024-03-08T09:54:12Z","published":"2023-07-19T12:39:40Z","title":"ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method","summary":"  Capsule Networks have emerged as a powerful class of deep learning\narchitectures, known for robust performance with relatively few parameters\ncompared to Convolutional Neural Networks (CNNs). However, their inherent\nefficiency is often overshadowed by their slow, iterative routing mechanisms\nwhich establish connections between Capsule layers, posing computational\nchallenges resulting in an inability to scale. In this paper, we introduce a\nnovel, non-iterative routing mechanism, inspired by trainable prototype\nclustering. This innovative approach aims to mitigate computational complexity,\nwhile retaining, if not enhancing, performance efficacy. Furthermore, we\nharness a shared Capsule subspace, negating the need to project each\nlower-level Capsule to each higher-level Capsule, thereby significantly\nreducing memory requisites during training. Our approach demonstrates superior\nresults compared to the current best non-iterative Capsule Network and tests on\nthe Imagewoof dataset, which is too computationally demanding to handle\nefficiently by iterative approaches. Our findings underscore the potential of\nour proposed methodology in enhancing the operational efficiency and\nperformance of Capsule Networks, paving the way for their application in\nincreasingly complex computational scenarios. Code is available at\nhttps://github.com/mileseverett/ProtoCaps.\n","authors":["Miles Everett","Mingjun Zhong","Georgios Leontidis"],"pdf_url":"https://arxiv.org/pdf/2307.09944v2.pdf","comment":"13 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.05181v1","updated":"2024-03-08T09:43:27Z","published":"2024-03-08T09:43:27Z","title":"Adversarial Sparse Teacher: Defense Against Distillation-Based Model\n  Stealing Attacks Using Adversarial Examples","summary":"  Knowledge Distillation (KD) facilitates the transfer of discriminative\ncapabilities from an advanced teacher model to a simpler student model,\nensuring performance enhancement without compromising accuracy. It is also\nexploited for model stealing attacks, where adversaries use KD to mimic the\nfunctionality of a teacher model. Recent developments in this domain have been\ninfluenced by the Stingy Teacher model, which provided empirical analysis\nshowing that sparse outputs can significantly degrade the performance of\nstudent models. Addressing the risk of intellectual property leakage, our work\nintroduces an approach to train a teacher model that inherently protects its\nlogits, influenced by the Nasty Teacher concept. Differing from existing\nmethods, we incorporate sparse outputs of adversarial examples with standard\ntraining data to strengthen the teacher's defense against student distillation.\nOur approach carefully reduces the relative entropy between the original and\nadversarially perturbed outputs, allowing the model to produce adversarial\nlogits with minimal impact on overall performance. The source codes will be\nmade publicly available soon.\n","authors":["Eda Yilmaz","Hacer Yalim Keles"],"pdf_url":"https://arxiv.org/pdf/2403.05181v1.pdf","comment":"12 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.05175v1","updated":"2024-03-08T09:32:43Z","published":"2024-03-08T09:32:43Z","title":"Continual Learning and Catastrophic Forgetting","summary":"  This book chapter delves into the dynamics of continual learning, which is\nthe process of incrementally learning from a non-stationary stream of data.\nAlthough continual learning is a natural skill for the human brain, it is very\nchallenging for artificial neural networks. An important reason is that, when\nlearning something new, these networks tend to quickly and drastically forget\nwhat they had learned before, a phenomenon known as catastrophic forgetting.\nEspecially in the last decade, continual learning has become an extensively\nstudied topic in deep learning. This book chapter reviews the insights that\nthis field has generated.\n","authors":["Gido M. van de Ven","Nicholas Soures","Dhireesha Kudithipudi"],"pdf_url":"https://arxiv.org/pdf/2403.05175v1.pdf","comment":"Preprint of a book chapter; 21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.05172v1","updated":"2024-03-08T09:25:48Z","published":"2024-03-08T09:25:48Z","title":"Learning Expressive And Generalizable Motion Features For Face Forgery\n  Detection","summary":"  Previous face forgery detection methods mainly focus on appearance features,\nwhich may be easily attacked by sophisticated manipulation. Considering the\nmajority of current face manipulation methods generate fake faces based on a\nsingle frame, which do not take frame consistency and coordination into\nconsideration, artifacts on frame sequences are more effective for face forgery\ndetection. However, current sequence-based face forgery detection methods use\ngeneral video classification networks directly, which discard the special and\ndiscriminative motion information for face manipulation detection. To this end,\nwe propose an effective sequence-based forgery detection framework based on an\nexisting video classification method. To make the motion features more\nexpressive for manipulation detection, we propose an alternative motion\nconsistency block instead of the original motion features module. To make the\nlearned features more generalizable, we propose an auxiliary anomaly detection\nblock. With these two specially designed improvements, we make a general video\nclassification network achieve promising results on three popular face forgery\ndatasets.\n","authors":["Jingyi Zhang","Peng Zhang","Jingjing Wang","Di Xie","Shiliang Pu"],"pdf_url":"https://arxiv.org/pdf/2403.05172v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2203.07976v4","updated":"2024-03-08T09:24:04Z","published":"2022-03-15T15:05:40Z","title":"On the Pitfalls of Batch Normalization for End-to-End Video Learning: A\n  Study on Surgical Workflow Analysis","summary":"  Batch Normalization's (BN) unique property of depending on other samples in a\nbatch is known to cause problems in several tasks, including sequence modeling.\nYet, BN-related issues are hardly studied for long video understanding, despite\nthe ubiquitous use of BN in CNNs (Convolutional Neural Networks) for feature\nextraction. Especially in surgical workflow analysis, where the lack of\npretrained feature extractors has led to complex, multi-stage training\npipelines, limited awareness of BN issues may have hidden the benefits of\ntraining CNNs and temporal models end to end. In this paper, we analyze\npitfalls of BN in video learning, including issues specific to online tasks\nsuch as a 'cheating' effect in anticipation. We observe that BN's properties\ncreate major obstacles for end-to-end learning. However, using BN-free\nbackbones, even simple CNN-LSTMs beat the state of the art\n{\\color{\\colorrevtwo}on three surgical workflow benchmarks} by utilizing\nadequate end-to-end training strategies which maximize temporal context. We\nconclude that awareness of BN's pitfalls is crucial for effective end-to-end\nlearning in surgical tasks. By reproducing results on natural-video datasets,\nwe hope our insights will benefit other areas of video learning as well. Code\nis available at: \\url{https://gitlab.com/nct_tso_public/pitfalls_bn}\n","authors":["Dominik Rivoir","Isabel Funke","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2203.07976v4.pdf","comment":"accepted at Medical Image Analysis (MedIA)\n  https://www.sciencedirect.com/science/article/pii/S1361841524000513"},{"id":"http://arxiv.org/abs/2403.05170v1","updated":"2024-03-08T09:19:29Z","published":"2024-03-08T09:19:29Z","title":"DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition","summary":"  This paper proposes a new pipeline for long-tail (LT) recognition. Instead of\nre-weighting or re-sampling, we utilize the long-tailed dataset itself to\ngenerate a balanced proxy that can be optimized through cross-entropy (CE).\nSpecifically, a randomly initialized diffusion model, trained exclusively on\nthe long-tailed dataset, is employed to synthesize new samples for\nunderrepresented classes. Then, we utilize the inherent information in the\noriginal dataset to filter out harmful samples and keep the useful ones. Our\nstrategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a\npioneering utilization of generative models in long-tail recognition. DiffuLT\nachieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT,\nsurpassing the best competitors with non-trivial margins. Abundant ablations\nmake our pipeline interpretable, too. The whole generation pipeline is done\nwithout any external data or pre-trained model weights, making it highly\ngeneralizable to real-world long-tailed settings.\n","authors":["Jie Shao","Ke Zhu","Hanxiao Zhang","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05168v1","updated":"2024-03-08T09:16:47Z","published":"2024-03-08T09:16:47Z","title":"Unlocking the Potential of Multimodal Unified Discrete Representation\n  through Training-Free Codebook Optimization and Hierarchical Alignment","summary":"  Recent advances in representation learning have demonstrated the significance\nof multimodal alignment. The Dual Cross-modal Information Disentanglement\n(DCID) model, utilizing a unified codebook, shows promising results in\nachieving fine-grained representation and cross-modal generalization. However,\nit is still hindered by equal treatment of all channels and neglect of minor\nevent information, resulting in interference from irrelevant channels and\nlimited performance in fine-grained tasks. Thus, in this work, We propose a\nTraining-free Optimization of Codebook (TOC) method to enhance model\nperformance by selecting important channels in the unified space without\nretraining. Additionally, we introduce the Hierarchical Dual Cross-modal\nInformation Disentanglement (H-DCID) approach to extend information separation\nand alignment to two levels, capturing more cross-modal details. The experiment\nresults demonstrate significant improvements across various downstream tasks,\nwith TOC contributing to an average improvement of 1.70% for DCID on four\ntasks, and H-DCID surpassing DCID by an average of 3.64%. The combination of\nTOC and H-DCID further enhances performance, exceeding DCID by 4.43%. These\nfindings highlight the effectiveness of our methods in facilitating robust and\nnuanced cross-modal learning, opening avenues for future enhancements. The\nsource code and pre-trained models can be accessed at\nhttps://github.com/haihuangcode/TOC_H-DCID.\n","authors":["Hai Huang","Yan Xia","Shengpeng Ji","Shulei Wang","Hanting Wang","Jieming Zhu","Zhenhua Dong","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.05168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15993v2","updated":"2024-03-08T09:12:57Z","published":"2023-11-27T16:41:31Z","title":"Unified Batch Normalization: Identifying and Alleviating the Feature\n  Condensation in Batch Normalization and a Unified Framework","summary":"  Batch Normalization (BN) has become an essential technique in contemporary\nneural network design, enhancing training stability. Specifically, BN employs\ncentering and scaling operations to standardize features along the batch\ndimension and uses an affine transformation to recover features. Although\nstandard BN has shown its capability to improve deep neural network training\nand convergence, it still exhibits inherent limitations in certain cases.\nCurrent enhancements to BN typically address only isolated aspects of its\nmechanism. In this work, we critically examine BN from a feature perspective,\nidentifying feature condensation during BN as a detrimental factor to test\nperformance. To tackle this problem, we propose a two-stage unified framework\ncalled Unified Batch Normalization (UBN). In the first stage, we employ a\nstraightforward feature condensation threshold to mitigate condensation\neffects, thereby preventing improper updates of statistical norms. In the\nsecond stage, we unify various normalization variants to boost each component\nof BN. Our experimental results reveal that UBN significantly enhances\nperformance across different visual backbones and different vision tasks, and\nnotably expedites network training convergence, particularly in early training\nstages. Notably, our method improved about 3% in accuracy on ImageNet\nclassification and 4% in mean average precision on both Object Detection and\nInstance Segmentation on COCO dataset, showing the effectiveness of our\napproach in real-world scenarios.\n","authors":["Shaobo Wang","Xiangdong Zhang","Dongrui Liu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.15993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05160v1","updated":"2024-03-08T09:02:13Z","published":"2024-03-08T09:02:13Z","title":"MamMIL: Multiple Instance Learning for Whole Slide Images with State\n  Space Models","summary":"  Recently, pathological diagnosis, the gold standard for cancer diagnosis, has\nachieved superior performance by combining the Transformer with the multiple\ninstance learning (MIL) framework using whole slide images (WSIs). However, the\ngiga-pixel nature of WSIs poses a great challenge for the quadratic-complexity\nself-attention mechanism in Transformer to be applied in MIL. Existing studies\nusually use linear attention to improve computing efficiency but inevitably\nbring performance bottlenecks. To tackle this challenge, we propose a MamMIL\nframework for WSI classification by cooperating the selective structured state\nspace model (i.e., Mamba) with MIL for the first time, enabling the modeling of\ninstance dependencies while maintaining linear complexity. Specifically, to\nsolve the problem that Mamba can only conduct unidirectional one-dimensional\n(1D) sequence modeling, we innovatively introduce a bidirectional state space\nmodel and a 2D context-aware block to enable MamMIL to learn the bidirectional\ninstance dependencies with 2D spatial relationships. Experiments on two\ndatasets show that MamMIL can achieve advanced classification performance with\nsmaller memory footprints than the state-of-the-art MIL frameworks based on the\nTransformer. The code will be open-sourced if accepted.\n","authors":["Zijie Fang","Yifeng Wang","Zhi Wang","Jian Zhang","Xiangyang Ji","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05160v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.07726v3","updated":"2024-03-08T08:58:10Z","published":"2023-09-27T06:32:00Z","title":"Warfare:Breaking the Watermark Protection of AI-Generated Content","summary":"  AI-Generated Content (AIGC) is gaining great popularity, with many emerging\ncommercial services and applications. These services leverage advanced\ngenerative models, such as latent diffusion models and large language models,\nto generate creative content (e.g., realistic images and fluent sentences) for\nusers. The usage of such generated content needs to be highly regulated, as the\nservice providers need to ensure the users do not violate the usage policies\n(e.g., abuse for commercialization, generating and distributing unsafe\ncontent). A promising solution to achieve this goal is watermarking, which adds\nunique and imperceptible watermarks on the content for service verification and\nattribution. Numerous watermarking approaches have been proposed recently.\nHowever, in this paper, we show that an adversary can easily break these\nwatermarking mechanisms. Specifically, we consider two possible attacks. (1)\nWatermark removal: the adversary can easily erase the embedded watermark from\nthe generated content and then use it freely bypassing the regulation of the\nservice provider. (2) Watermark forging: the adversary can create illegal\ncontent with forged watermarks from another user, causing the service provider\nto make wrong attributions. We propose Warfare, a unified methodology to\nachieve both attacks in a holistic way. The key idea is to leverage a\npre-trained diffusion model for content processing and a generative adversarial\nnetwork for watermark removal or forging. We evaluate Warfare on different\ndatasets and embedding setups. The results prove that it can achieve high\nsuccess rates while maintaining the quality of the generated content. Compared\nto existing diffusion model-based attacks, Warfare is 5,050~11,000x faster.\n","authors":["Guanlin Li","Yifei Chen","Jie Zhang","Jiwei Li","Shangwei Guo","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07726v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00343v4","updated":"2024-03-08T08:53:55Z","published":"2023-12-01T04:35:47Z","title":"OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline","summary":"  Stereo matching aims to estimate the disparity between matching pixels in a\nstereo image pair, which is of great importance to robotics, autonomous\ndriving, and other computer vision tasks. Despite the development of numerous\nimpressive methods in recent years, replicating their results and determining\nthe most suitable architecture for practical application remains challenging.\nAddressing this gap, our paper introduces a comprehensive benchmark focusing on\npractical applicability rather than solely on performance enhancement.\nSpecifically, we develop a flexible and efficient stereo matching codebase,\ncalled OpenStereo. OpenStereo includes training and inference codes of more\nthan 10 network models, making it, to our knowledge, the most complete stereo\nmatching toolbox available. Based on OpenStereo, we conducted experiments and\nhave achieved or surpassed the performance metrics reported in the original\npaper. Additionally, we carry out an exhaustive analysis and deconstruction of\nrecent developments in stereo matching through comprehensive ablative\nexperiments. These investigations inspired the creation of StereoBase, a strong\nbaseline model. Our StereoBase ranks 1st on SceneFlow, KITTI 2015, 2012\n(Reflective) among published methods and achieves the best performance across\nall metrics. In addition, StereoBase has strong cross-dataset\ngeneralization.Code is available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Juntao Lu","Chenming Zhang","Yiqi Wang","Yiqun Duan","Tian Yang","Zheng Zhu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00343v4.pdf","comment":"Code is available at: https://github.com/XiandaGuo/OpenStereo"},{"id":"http://arxiv.org/abs/2403.05159v1","updated":"2024-03-08T08:52:55Z","published":"2024-03-08T08:52:55Z","title":"LVIC: Multi-modality segmentation by Lifting Visual Info as Cue","summary":"  Multi-modality fusion is proven an effective method for 3d perception for\nautonomous driving. However, most current multi-modality fusion pipelines for\nLiDAR semantic segmentation have complicated fusion mechanisms. Point painting\nis a quite straight forward method which directly bind LiDAR points with visual\ninformation. Unfortunately, previous point painting like methods suffer from\nprojection error between camera and LiDAR. In our experiments, we find that\nthis projection error is the devil in point painting. As a result of that, we\npropose a depth aware point painting mechanism, which significantly boosts the\nmulti-modality fusion. Apart from that, we take a deeper look at the desired\nvisual feature for LiDAR to operate semantic segmentation. By Lifting Visual\nInformation as Cue, LVIC ranks 1st on nuScenes LiDAR semantic segmentation\nbenchmark. Our experiments show the robustness and effectiveness. Codes would\nbe make publicly available soon.\n","authors":["Zichao Dong","Bowen Pang","Xufeng Huang","Hang Ji","Xin Zhan","Junbo Chen"],"pdf_url":"https://arxiv.org/pdf/2403.05159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18920v3","updated":"2024-03-08T08:51:29Z","published":"2024-02-29T07:26:23Z","title":"Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation","summary":"  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n","authors":["Dongliang Cao","Marvin Eisenberger","Nafie El Amrani","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2402.18920v3.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.05155v1","updated":"2024-03-08T08:45:42Z","published":"2024-03-08T08:45:42Z","title":"LanePtrNet: Revisiting Lane Detection as Point Voting and Grouping on\n  Curves","summary":"  Lane detection plays a critical role in the field of autonomous driving.\nPrevailing methods generally adopt basic concepts (anchors, key points, etc.)\nfrom object detection and segmentation tasks, while these approaches require\nmanual adjustments for curved objects, involve exhaustive searches on\npredefined anchors, require complex post-processing steps, and may lack\nflexibility when applied to real-world scenarios.In this paper, we propose a\nnovel approach, LanePtrNet, which treats lane detection as a process of point\nvoting and grouping on ordered sets: Our method takes backbone features as\ninput and predicts a curve-aware centerness, which represents each lane as a\npoint and assigns the most probable center point to it. A novel point sampling\nmethod is proposed to generate a set of candidate points based on the votes\nreceived. By leveraging features from local neighborhoods, and cross-instance\nattention score, we design a grouping module that further performs lane-wise\nclustering between neighboring and seeding points. Furthermore, our method can\naccommodate a point-based framework, (PointNet++ series, etc.) as an\nalternative to the backbone. This flexibility enables effortless extension to\n3D lane detection tasks. We conduct comprehensive experiments to validate the\neffectiveness of our proposed approach, demonstrating its superior performance.\n","authors":["Jiayan Cao","Xueyu Zhu","Cheng Qian"],"pdf_url":"https://arxiv.org/pdf/2403.05155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05154v1","updated":"2024-03-08T08:42:23Z","published":"2024-03-08T08:42:23Z","title":"GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian\n  Splatting","summary":"  We present GSEdit, a pipeline for text-guided 3D object editing based on\nGaussian Splatting models. Our method enables the editing of the style and\nappearance of 3D objects without altering their main details, all in a matter\nof minutes on consumer hardware. We tackle the problem by leveraging Gaussian\nsplatting to represent 3D scenes, and we optimize the model while progressively\nvarying the image supervision by means of a pretrained image-based diffusion\nmodel. The input object may be given as a 3D triangular mesh, or directly\nprovided as Gaussians from a generative model such as DreamGaussian. GSEdit\nensures consistency across different viewpoints, maintaining the integrity of\nthe original object's information. Compared to previously proposed methods\nrelying on NeRF-like MLP models, GSEdit stands out for its efficiency, making\n3D editing tasks much faster. Our editing process is refined via the\napplication of the SDS loss, ensuring that our edits are both precise and\naccurate. Our comprehensive evaluation demonstrates that GSEdit effectively\nalters object shape and appearance following the given textual instructions\nwhile preserving their coherence and detail.\n","authors":["Francesco Palandra","Andrea Sanchietti","Daniele Baieri","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2403.05154v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.05146v1","updated":"2024-03-08T08:31:46Z","published":"2024-03-08T08:31:46Z","title":"Motion-Guided Dual-Camera Tracker for Low-Cost Skill Evaluation of\n  Gastric Endoscopy","summary":"  Gastric simulators with objective educational feedback have been proven\nuseful for endoscopy training. Existing electronic simulators with feedback are\nhowever not commonly adopted due to their high cost. In this work, a\nmotion-guided dual-camera tracker is proposed to provide reliable endoscope tip\nposition feedback at a low cost inside a mechanical simulator for endoscopy\nskill evaluation, tackling several unique challenges. To address the issue of\nsignificant appearance variation of the endoscope tip while keeping dual-camera\ntracking consistency, the cross-camera mutual template strategy (CMT) is\nproposed to introduce dynamic transient mutual templates to dual-camera\ntracking. To alleviate disturbance from large occlusion and distortion by the\nlight source from the endoscope tip, the Mamba-based motion-guided prediction\nhead (MMH) is presented to aggregate visual tracking with historical motion\ninformation modeled by the state space model. The proposed tracker was\nevaluated on datasets captured by low-cost camera pairs during endoscopy\nprocedures performed inside the mechanical simulator. The tracker achieves SOTA\nperformance with robust and consistent tracking on dual cameras. Further\ndownstream evaluation proves that the 3D tip position determined by the\nproposed tracker enables reliable skill differentiation. The code and dataset\nwill be released upon acceptance.\n","authors":["Yuelin Zhang","Wanquan Yan","Kim Yan","Chun Ping Lam","Yufu Qiu","Pengyu Zheng","Raymond Shing-Yan Tang","Shing Shin Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.05146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03135v3","updated":"2024-03-08T08:16:54Z","published":"2023-08-06T15:05:42Z","title":"EventBind: Learning a Unified Representation to Bind Them All for\n  Event-based Open-world Understanding","summary":"  In this paper, we propose EventBind, a novel and effective framework that\nunleashes the potential of vision-language models (VLMs) for event-based\nrecognition to compensate for the lack of large-scale event-based datasets. In\nparticular, due to the distinct modality gap with the image-text data and the\nlack of large-scale datasets, learning a common representation space for\nimages, texts, and events is non-trivial.Intuitively, we need to address two\nkey challenges: 1) how to generalize CLIP's visual encoder to event data while\nfully leveraging events' unique properties, e.g., sparsity and high temporal\nresolution; 2) how to effectively align the multi-modal embeddings, i.e.,\nimage, text, and events. Accordingly, we first introduce a novel event encoder\nthat subtly models the temporal information from events and meanwhile,\ngenerates event prompts for modality bridging. We then design a text encoder\nthat generates content prompts and utilizes hybrid text prompts to enhance\nEventBind's generalization ability across diverse datasets.With the proposed\nevent encoder, text encoder, and image encoder, a novel Hierarchical Triple\nContrastive Alignment (HTCA) module is introduced to jointly optimize the\ncorrelation and enable efficient knowledge transfer among the three modalities.\nWe evaluate various settings, including fine-tuning and few-shot on three\nbenchmarks, and our EventBind achieves new state-of-the-art accuracy compared\nwith the previous methods, such as on N-Caltech 101 +5.34% and +1.70%) and\nN-Imagenet(+5.65% and +1.99%) with fine-tuning and 20-shot settings,\nrespectively. Moreover, our EventBind can be flexibly extended to the event\nretrieval task using text or image queries, showing plausible performance. Our\nproject code will be made publicly available.\n","authors":["Jiazhou Zhou","Xu Zheng","Yuanhuiyi Lyu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.03135v3.pdf","comment":"Conferemve version"},{"id":"http://arxiv.org/abs/2208.10769v2","updated":"2024-03-08T08:16:34Z","published":"2022-08-23T07:00:44Z","title":"PIFu for the Real World: A Self-supervised Framework to Reconstruct\n  Dressed Human from Single-view Images","summary":"  It is very challenging to accurately reconstruct sophisticated human geometry\ncaused by various poses and garments from a single image. Recently, works based\non pixel-aligned implicit function (PIFu) have made a big step and achieved\nstate-of-the-art fidelity on image-based 3D human digitization. However, the\ntraining of PIFu relies heavily on expensive and limited 3D ground truth data\n(i.e. synthetic data), thus hindering its generalization to more diverse real\nworld images. In this work, we propose an end-to-end self-supervised network\nnamed SelfPIFu to utilize abundant and diverse in-the-wild images, resulting in\nlargely improved reconstructions when tested on unconstrained in-the-wild\nimages. At the core of SelfPIFu is the depth-guided volume-/surface-aware\nsigned distance fields (SDF) learning, which enables self-supervised learning\nof a PIFu without access to GT mesh. The whole framework consists of a normal\nestimator, a depth estimator, and a SDF-based PIFu and better utilizes extra\ndepth GT during training. Extensive experiments demonstrate the effectiveness\nof our self-supervised framework and the superiority of using depth as input.\nOn synthetic data, our Intersection-Over-Union (IoU) achieves to 93.5%, 18%\nhigher compared with PIFuHD. For in-the-wild images, we conduct user studies on\nthe reconstructed results, the selection rate of our results is over 68%\ncompared with other state-of-the-art methods.\n","authors":["Zhangyang Xiong","Dong Du","Yushuang Wu","Jingqi Dong","Di Kang","Linchao Bao","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2208.10769v2.pdf","comment":"CVM 2024"},{"id":"http://arxiv.org/abs/2403.05141v1","updated":"2024-03-08T08:15:53Z","published":"2024-03-08T08:15:53Z","title":"Med3DInsight: Enhancing 3D Medical Image Understanding with 2D\n  Multi-Modal Large Language Models","summary":"  Understanding 3D medical image volumes is a critical task in the medical\ndomain. However, existing 3D convolution and transformer-based methods have\nlimited semantic understanding of an image volume and also need a large set of\nvolumes for training. Recent advances in multi-modal large language models\n(MLLMs) provide a new and promising way to understand images with the help of\ntext descriptions. However, most current MLLMs are designed for 2D natural\nimages. To enhance the 3D medical image understanding with 2D MLLMs, we propose\na novel pre-training framework called Med3DInsight, which marries existing 3D\nimage encoders with 2D MLLMs and bridges them via a designed Plane-Slice-Aware\nTransformer (PSAT) module. Extensive experiments demonstrate our SOTA\nperformance on two downstream segmentation and classification tasks, including\nthree public datasets with CT and MRI modalities and comparison to more than\nten baselines. Med3DInsight can be easily integrated into any current 3D\nmedical image understanding network and improves its performance by a good\nmargin.\n","authors":["Qiuhui Chen","Huping Ye","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2403.05141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05139v1","updated":"2024-03-08T08:12:18Z","published":"2024-03-08T08:12:18Z","title":"Improving Diffusion Models for Virtual Try-on","summary":"  This paper considers image-based virtual try-on, which renders an image of a\nperson wearing a curated garment, given a pair of images depicting the person\nand the garment, respectively. Previous works adapt existing exemplar-based\ninpainting diffusion models for virtual try-on to improve the naturalness of\nthe generated visuals compared to other methods (e.g., GAN-based), but they\nfail to preserve the identity of the garments. To overcome this limitation, we\npropose a novel diffusion model that improves garment fidelity and generates\nauthentic virtual try-on images. Our method, coined IDM-VTON, uses two\ndifferent modules to encode the semantics of garment image; given the base UNet\nof the diffusion model, 1) the high-level semantics extracted from a visual\nencoder are fused to the cross-attention layer, and then 2) the low-level\nfeatures extracted from parallel UNet are fused to the self-attention layer. In\naddition, we provide detailed textual prompts for both garment and person\nimages to enhance the authenticity of the generated visuals. Finally, we\npresent a customization method using a pair of person-garment images, which\nsignificantly improves fidelity and authenticity. Our experimental results show\nthat our method outperforms previous approaches (both diffusion-based and\nGAN-based) in preserving garment details and generating authentic virtual\ntry-on images, both qualitatively and quantitatively. Furthermore, the proposed\ncustomization method demonstrates its effectiveness in a real-world scenario.\n","authors":["Yisol Choi","Sangkyung Kwak","Kyungmin Lee","Hyungwon Choi","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2403.05139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05135v1","updated":"2024-03-08T08:08:10Z","published":"2024-03-08T08:08:10Z","title":"ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment","summary":"  Diffusion models have demonstrated remarkable performance in the domain of\ntext-to-image generation. However, most widely used models still employ CLIP as\ntheir text encoder, which constrains their ability to comprehend dense prompts,\nencompassing multiple objects, detailed attributes, complex relationships,\nlong-text alignment, etc. In this paper, we introduce an Efficient Large\nLanguage Model Adapter, termed ELLA, which equips text-to-image diffusion\nmodels with powerful Large Language Models (LLM) to enhance text alignment\nwithout training of either U-Net or LLM. To seamlessly bridge two pre-trained\nmodels, we investigate a range of semantic alignment connector designs and\npropose a novel module, the Timestep-Aware Semantic Connector (TSC), which\ndynamically extracts timestep-dependent conditions from LLM. Our approach\nadapts semantic features at different stages of the denoising process,\nassisting diffusion models in interpreting lengthy and intricate prompts over\nsampling timesteps. Additionally, ELLA can be readily incorporated with\ncommunity models and tools to improve their prompt-following capabilities. To\nassess text-to-image models in dense prompt following, we introduce Dense\nPrompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K\ndense prompts. Extensive experiments demonstrate the superiority of ELLA in\ndense prompt following compared to state-of-the-art methods, particularly in\nmultiple object compositions involving diverse attributes and relationships.\n","authors":["Xiwei Hu","Rui Wang","Yixiao Fang","Bin Fu","Pei Cheng","Gang Yu"],"pdf_url":"https://arxiv.org/pdf/2403.05135v1.pdf","comment":"Project Page: https://ella-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2203.08612v2","updated":"2024-03-08T08:02:59Z","published":"2022-03-16T13:28:17Z","title":"CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer\n  Learning","summary":"  Generating artistic portraits is a challenging problem in computer vision.\nExisting portrait stylization models that generate good quality results are\nbased on Image-to-Image Translation and require abundant data from both source\nand target domains. However, without enough data, these methods would result in\noverfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits\ngeneration model with a novel contrastive transfer learning strategy. We adapt\na pretrained StyleGAN in the source domain to a target artistic domain with no\nmore than 10 artistic faces. To reduce overfitting to the few training\nexamples, we introduce a novel Cross-Domain Triplet loss which explicitly\nencourages the target instances generated from different latent codes to be\ndistinguishable. We propose a new encoder which embeds real faces into Z+ space\nand proposes a dual-path training strategy to better cope with the adapted\ndecoder and eliminate the artifacts. Extensive qualitative, quantitative\ncomparisons and a user study show our method significantly outperforms\nstate-of-the-arts under 10-shot and 1-shot settings and generates high quality\nartistic portraits. The code will be made publicly available.\n","authors":["Yue Wang","Ran Yi","Luying Li","Ying Tai","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2203.08612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08138v2","updated":"2024-03-08T08:00:47Z","published":"2024-02-13T00:23:31Z","title":"H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object\n  Surface Fields","summary":"  Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance\nFields (SDF), and Occupancy Fields have recently emerged as solutions for 3D\nindoor scene reconstruction. We introduce a novel two-phase learning approach,\nH2O-SDF, that discriminates between object and non-object regions within indoor\nenvironments. This method achieves a nuanced balance, carefully preserving the\ngeometric integrity of room layouts while also capturing intricate surface\ndetails of specific objects. A cornerstone of our two-phase learning framework\nis the introduction of the Object Surface Field (OSF), a novel concept designed\nto mitigate the persistent vanishing gradient problem that has previously\nhindered the capture of high-frequency details in other methods. Our proposed\napproach is validated through several experiments that include ablation\nstudies.\n","authors":["Minyoung Park","Mirae Do","YeonJae Shin","Jaeseok Yoo","Jongkwang Hong","Joongrock Kim","Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2402.08138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05131v1","updated":"2024-03-08T07:58:13Z","published":"2024-03-08T07:58:13Z","title":"Sora as an AGI World Model? A Complete Survey on Text-to-Video\n  Generation","summary":"  Text-to-video generation marks a significant frontier in the rapidly evolving\ndomain of generative AI, integrating advancements in text-to-image synthesis,\nvideo captioning, and text-guided editing. This survey critically examines the\nprogression of text-to-video technologies, focusing on the shift from\ntraditional generative models to the cutting-edge Sora model, highlighting\ndevelopments in scalability and generalizability. Distinguishing our analysis\nfrom prior works, we offer an in-depth exploration of the technological\nframeworks and evolutionary pathways of these models. Additionally, we delve\ninto practical applications and address ethical and technological challenges\nsuch as the inability to perform multiple entity handling, comprehend\ncausal-effect learning, understand physical interaction, perceive object\nscaling and proportioning, and combat object hallucination which is also a\nlong-standing problem in generative models. Our comprehensive discussion covers\nthe topic of enablement of text-to-video generation models as human-assistive\ntools and world models, as well as eliciting model's shortcomings and\nsummarizing future improvement direction that mainly centers around training\ndatasets and evaluation metrics (both automatic and human-centered). Aimed at\nboth newcomers and seasoned researchers, this survey seeks to catalyze further\ninnovation and discussion in the growing field of text-to-video generation,\npaving the way for more reliable and practical generative artificial\nintelligence technologies.\n","authors":["Joseph Cho","Fachrina Dewi Puspitasari","Sheng Zheng","Jingyao Zheng","Lik-Hang Lee","Tae-Ho Kim","Choong Seon Hong","Chaoning Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05131v1.pdf","comment":"First complete survey on Text-to-Video Generation, 36 pages, 16\n  figures"},{"id":"http://arxiv.org/abs/2310.10051v2","updated":"2024-03-08T07:42:25Z","published":"2023-10-16T04:21:07Z","title":"EAR-Net: Pursuing End-to-End Absolute Rotations from Multi-View Images","summary":"  Absolute rotation estimation is an important topic in 3D computer vision.\nExisting works in literature generally employ a multi-stage (at least\ntwo-stage) estimation strategy where multiple independent operations (feature\nmatching, two-view rotation estimation, and rotation averaging) are implemented\nsequentially. However, such a multi-stage strategy inevitably leads to the\naccumulation of the errors caused by each involved operation, and degrades its\nfinal estimation on global rotations accordingly. To address this problem, we\npropose an End-to-end method for estimating Absolution Rotations from\nmulti-view images based on deep neural Networks, called EAR-Net. The proposed\nEAR-Net consists of an epipolar confidence graph construction module and a\nconfidence-aware rotation averaging module. The epipolar confidence graph\nconstruction module is explored to simultaneously predict pairwise relative\nrotations among the input images and their corresponding confidences, resulting\nin a weighted graph (called epipolar confidence graph). Based on this graph,\nthe confidence-aware rotation averaging module, which is differentiable, is\nexplored to predict the absolute rotations. Thanks to the introduced\nconfidences of the relative rotations, the proposed EAR-Net could effectively\nhandle outlier cases. Experimental results on three public datasets demonstrate\nthat EAR-Net outperforms the state-of-the-art methods by a large margin in\nterms of accuracy and speed.\n","authors":["Yuzhen Liu","Qiulei Dong"],"pdf_url":"https://arxiv.org/pdf/2310.10051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05125v1","updated":"2024-03-08T07:41:47Z","published":"2024-03-08T07:41:47Z","title":"Evaluating Text-to-Image Generative Models: An Empirical Study on Human\n  Image Synthesis","summary":"  In this paper, we present an empirical study introducing a nuanced evaluation\nframework for text-to-image (T2I) generative models, applied to human image\nsynthesis. Our framework categorizes evaluations into two distinct groups:\nfirst, focusing on image qualities such as aesthetics and realism, and second,\nexamining text conditions through concept coverage and fairness. We introduce\nan innovative aesthetic score prediction model that assesses the visual appeal\nof generated images and unveils the first dataset marked with low-quality\nregions in generated human images to facilitate automatic defect detection. Our\nexploration into concept coverage probes the model's effectiveness in\ninterpreting and rendering text-based concepts accurately, while our analysis\nof fairness reveals biases in model outputs, with an emphasis on gender, race,\nand age. While our study is grounded in human imagery, this dual-faceted\napproach is designed with the flexibility to be applicable to other forms of\nimage generation, enhancing our understanding of generative models and paving\nthe way to the next generation of more sophisticated, contextually aware, and\nethically attuned generative models. We will release our code, the data used\nfor evaluating generative models and the dataset annotated with defective areas\nsoon.\n","authors":["Muxi Chen","Yi Liu","Jian Yi","Changran Xu","Qiuxia Lai","Hongliang Wang","Tsung-Yi Ho","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.05125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05124v1","updated":"2024-03-08T07:37:21Z","published":"2024-03-08T07:37:21Z","title":"CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model","summary":"  Gaze estimation methods often experience significant performance degradation\nwhen evaluated across different domains, due to the domain gap between the\ntesting and training data. Existing methods try to address this issue using\nvarious domain generalization approaches, but with little success because of\nthe limited diversity of gaze datasets, such as appearance, wearable, and image\nquality. To overcome these limitations, we propose a novel framework called\nCLIP-Gaze that utilizes a pre-trained vision-language model to leverage its\ntransferable knowledge. Our framework is the first to leverage the\nvision-and-language cross-modality approach for gaze estimation task.\nSpecifically, we extract gaze-relevant feature by pushing it away from\ngaze-irrelevant features which can be flexibly constructed via language\ndescriptions. To learn more suitable prompts, we propose a personalized context\noptimization method for text prompt tuning. Furthermore, we utilize the\nrelationship among gaze samples to refine the distribution of gaze-relevant\nfeatures, thereby improving the generalization capability of the gaze\nestimation model. Extensive experiments demonstrate the excellent performance\nof CLIP-Gaze over existing methods on four cross-domain evaluations.\n","authors":["Pengwei Yin","Guanzhong Zeng","Jingjing Wang","Di Xie"],"pdf_url":"https://arxiv.org/pdf/2403.05124v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.06198v3","updated":"2024-03-08T07:36:58Z","published":"2023-12-11T08:22:24Z","title":"Optimized View and Geometry Distillation from Multi-view Diffuser","summary":"  Generating multi-view images from a single input view using image-conditioned\ndiffusion models is a recent advancement and has shown considerable potential.\nHowever, issues such as the lack of consistency in synthesized views and\nover-smoothing in extracted geometry persist. Previous methods integrate\nmulti-view consistency modules or impose additional supervisory to enhance view\nconsistency while compromising on the flexibility of camera positioning and\nlimiting the versatility of view synthesis. In this study, we consider the\nradiance field optimized during geometry extraction as a more rigid consistency\nprior, compared to volume and ray aggregation used in previous works. We\nfurther identify and rectify a critical bias in the traditional radiance field\noptimization process through score distillation from a multi-view diffuser. We\nintroduce an Unbiased Score Distillation (USD) that utilizes unconditioned\nnoises from a 2D diffusion model, greatly refining the radiance field fidelity.\nWe leverage the rendered views from the optimized radiance field as the basis\nand develop a two-step specialization process of a 2D diffusion model, which is\nadept at conducting object-specific denoising and generating high-quality\nmulti-view images. Finally, we recover faithful geometry and texture directly\nfrom the refined multi-view images. Empirical evaluations demonstrate that our\noptimized geometry and view distillation technique generates comparable results\nto the state-of-the-art models trained on extensive datasets, all while\nmaintaining freedom in camera positioning. Please see our project page at\nhttps://youjiazhang.github.io/USD/.\n","authors":["Youjia Zhang","Zikai Song","Junqing Yu","Yawei Luo","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2312.06198v3.pdf","comment":"Project page: https://youjiazhang.github.io/USD/"},{"id":"http://arxiv.org/abs/2403.05123v1","updated":"2024-03-08T07:36:46Z","published":"2024-03-08T07:36:46Z","title":"ECToNAS: Evolutionary Cross-Topology Neural Architecture Search","summary":"  We present ECToNAS, a cost-efficient evolutionary cross-topology neural\narchitecture search algorithm that does not require any pre-trained meta\ncontrollers. Our framework is able to select suitable network architectures for\ndifferent tasks and hyperparameter settings, independently performing\ncross-topology optimisation where required. It is a hybrid approach that fuses\ntraining and topology optimisation together into one lightweight,\nresource-friendly process. We demonstrate the validity and power of this\napproach with six standard data sets (CIFAR-10, CIFAR-100, EuroSAT, Fashion\nMNIST, MNIST, SVHN), showcasing the algorithm's ability to not only optimise\nthe topology within an architectural type, but also to dynamically add and\nremove convolutional cells when and where required, thus crossing boundaries\nbetween different network types. This enables researchers without a background\nin machine learning to make use of appropriate model types and topologies and\nto apply machine learning methods in their domains, with a computationally\ncheap, easy-to-use cross-topology neural architecture search framework that\nfully encapsulates the topology optimisation within the training process.\n","authors":["Elisabeth J. Schiessler","Roland C. Aydin","Christian J. Cyron"],"pdf_url":"https://arxiv.org/pdf/2403.05123v1.pdf","comment":"15 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.01753v2","updated":"2024-03-08T07:33:32Z","published":"2024-03-04T06:19:27Z","title":"Training-Free Pretrained Model Merging","summary":"  Recently, model merging techniques have surfaced as a solution to combine\nmultiple single-talent models into a single multi-talent model. However,\nprevious endeavors in this field have either necessitated additional training\nor fine-tuning processes, or require that the models possess the same\npre-trained initialization. In this work, we identify a common drawback in\nprior works w.r.t. the inconsistency of unit similarity in the weight space and\nthe activation space. To address this inconsistency, we propose an innovative\nmodel merging framework, coined as merging under dual-space constraints\n(MuDSC). Specifically, instead of solely maximizing the objective of a single\nspace, we advocate for the exploration of permutation matrices situated in a\nregion with a unified high similarity in the dual space, achieved through the\nlinear combination of activation and weight similarity matrices. In order to\nenhance usability, we have also incorporated adaptations for group structure,\nincluding Multi-Head Attention and Group Normalization. Comprehensive\nexperimental comparisons demonstrate that MuDSC can significantly boost the\nperformance of merged models with various task combinations and architectures.\nFurthermore, the visualization of the merged model within the multi-task loss\nlandscape reveals that MuDSC enables the merged model to reside in the\noverlapping segment, featuring a unified lower loss for each task. Our code is\npublicly available at https://github.com/zju-vipa/training_free_model_merging.\n","authors":["Zhengqi Xu","Ke Yuan","Huiqiong Wang","Yong Wang","Mingli Song","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2403.01753v2.pdf","comment":"CVPR2024 accepted"},{"id":"http://arxiv.org/abs/2403.05121v1","updated":"2024-03-08T07:32:50Z","published":"2024-03-08T07:32:50Z","title":"CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion","summary":"  Recent advancements in text-to-image generative systems have been largely\ndriven by diffusion models. However, single-stage text-to-image diffusion\nmodels still face challenges, in terms of computational efficiency and the\nrefinement of image details. To tackle the issue, we propose CogView3, an\ninnovative cascaded framework that enhances the performance of text-to-image\ndiffusion. CogView3 is the first model implementing relay diffusion in the\nrealm of text-to-image generation, executing the task by first creating\nlow-resolution images and subsequently applying relay-based super-resolution.\nThis methodology not only results in competitive text-to-image outputs but also\ngreatly reduces both training and inference costs. Our experimental results\ndemonstrate that CogView3 outperforms SDXL, the current state-of-the-art\nopen-source text-to-image diffusion model, by 77.0\\% in human evaluations, all\nwhile requiring only about 1/2 of the inference time. The distilled variant of\nCogView3 achieves comparable performance while only utilizing 1/10 of the\ninference time by SDXL.\n","authors":["Wendi Zheng","Jiayan Teng","Zhuoyi Yang","Weihan Wang","Jidong Chen","Xiaotao Gu","Yuxiao Dong","Ming Ding","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.05121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05117v1","updated":"2024-03-08T07:31:14Z","published":"2024-03-08T07:31:14Z","title":"Arbitrary-Scale Point Cloud Upsampling by Voxel-Based Network with\n  Latent Geometric-Consistent Learning","summary":"  Recently, arbitrary-scale point cloud upsampling mechanism became\nincreasingly popular due to its efficiency and convenience for practical\napplications. To achieve this, most previous approaches formulate it as a\nproblem of surface approximation and employ point-based networks to learn\nsurface representations. However, learning surfaces from sparse point clouds is\nmore challenging, and thus they often suffer from the low-fidelity geometry\napproximation. To address it, we propose an arbitrary-scale Point cloud\nUpsampling framework using Voxel-based Network (\\textbf{PU-VoxelNet}). Thanks\nto the completeness and regularity inherited from the voxel representation,\nvoxel-based networks are capable of providing predefined grid space to\napproximate 3D surface, and an arbitrary number of points can be reconstructed\naccording to the predicted density distribution within each grid cell. However,\nwe investigate the inaccurate grid sampling caused by imprecise density\npredictions. To address this issue, a density-guided grid resampling method is\ndeveloped to generate high-fidelity points while effectively avoiding sampling\noutliers. Further, to improve the fine-grained details, we present an auxiliary\ntraining supervision to enforce the latent geometric consistency among local\nsurface patches. Extensive experiments indicate the proposed approach\noutperforms the state-of-the-art approaches not only in terms of fixed\nupsampling rates but also for arbitrary-scale upsampling.\n","authors":["Hang Du","Xuejun Yan","Jingjing Wang","Di Xie","Shiliang Pu"],"pdf_url":"https://arxiv.org/pdf/2403.05117v1.pdf","comment":"Accepted to AAAI 2024. The source code is available at\n  https://github.com/hikvision-research/3DVision"},{"id":"http://arxiv.org/abs/2309.15278v2","updated":"2024-03-08T07:29:13Z","published":"2023-09-26T21:31:24Z","title":"Out of Sight, Still in Mind: Reasoning and Planning about Unobserved\n  Objects with Video Tracking Enabled Memory Models","summary":"  Robots need to have a memory of previously observed, but currently occluded\nobjects to work reliably in realistic environments. We investigate the problem\nof encoding object-oriented memory into a multi-object manipulation reasoning\nand planning framework. We propose DOOM and LOOM, which leverage transformer\nrelational dynamics to encode the history of trajectories given partial-view\npoint clouds and an object discovery and tracking engine. Our approaches can\nperform multiple challenging tasks including reasoning with occluded objects,\nnovel objects appearance, and object reappearance. Throughout our extensive\nsimulation and real-world experiments, we find that our approaches perform well\nin terms of different numbers of objects and different numbers of distractor\nactions. Furthermore, we show our approaches outperform an implicit memory\nbaseline.\n","authors":["Yixuan Huang","Jialin Yuan","Chanho Kim","Pupul Pradhan","Bryan Chen","Li Fuxin","Tucker Hermans"],"pdf_url":"https://arxiv.org/pdf/2309.15278v2.pdf","comment":"Accepted at IEEE Conference on Robotics and Automation (ICRA) 2024.\n  Website: https://sites.google.com/view/rdmemory"},{"id":"http://arxiv.org/abs/2403.05114v1","updated":"2024-03-08T07:22:48Z","published":"2024-03-08T07:22:48Z","title":"APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for\n  Unfairness Mitigation","summary":"  Ensuring fairness in deep-learning-based segmentors is crucial for health\nequity. Much effort has been dedicated to mitigating unfairness in the training\ndatasets or procedures. However, with the increasing prevalence of foundation\nmodels in medical image analysis, it is hard to train fair models from scratch\nwhile preserving utility. In this paper, we propose a novel method, Adversarial\nPrivacy-aware Perturbations on Latent Embedding (APPLE), that can improve the\nfairness of deployed segmentors by introducing a small latent feature perturber\nwithout updating the weights of the original model. By adding perturbation to\nthe latent vector, APPLE decorates the latent vector of segmentors such that no\nfairness-related features can be passed to the decoder of the segmentors while\npreserving the architecture and parameters of the segmentor. Experiments on two\nsegmentation datasets and five segmentors (three U-Net-like and two SAM-like)\nillustrate the effectiveness of our proposed method compared to several\nunfairness mitigation methods.\n","authors":["Zikang Xu","Fenghe Tang","Quan Quan","Qingsong Yao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05111v1","updated":"2024-03-08T07:16:14Z","published":"2024-03-08T07:16:14Z","title":"From Registration Uncertainty to Segmentation Uncertainty","summary":"  Understanding the uncertainty inherent in deep learning-based image\nregistration models has been an ongoing area of research. Existing methods have\nbeen developed to quantify both transformation and appearance uncertainties\nrelated to the registration process, elucidating areas where the model may\nexhibit ambiguity regarding the generated deformation. However, our study\nreveals that neither uncertainty effectively estimates the potential errors\nwhen the registration model is used for label propagation. Here, we propose a\nnovel framework to concurrently estimate both the epistemic and aleatoric\nsegmentation uncertainties for image registration. To this end, we implement a\ncompact deep neural network (DNN) designed to transform the appearance\ndiscrepancy in the warping into aleatoric segmentation uncertainty by\nminimizing a negative log-likelihood loss function. Furthermore, we present\nepistemic segmentation uncertainty within the label propagation process as the\nentropy of the propagated labels. By introducing segmentation uncertainty along\nwith existing methods for estimating registration uncertainty, we offer vital\ninsights into the potential uncertainties at different stages of image\nregistration. We validated our proposed framework using publicly available\ndatasets, and the results prove that the segmentation uncertainties estimated\nwith the proposed method correlate well with errors in label propagation, all\nwhile achieving superior registration performance.\n","authors":["Junyu Chen","Yihao Liu","Shuwen Wei","Zhangxing Bian","Aaron Carass","Yong Du"],"pdf_url":"https://arxiv.org/pdf/2403.05111v1.pdf","comment":"Accepted by IEEE ISBI'24 ((c) IEEE). Code available at\n  https://bit.ly/42VOZER"},{"id":"http://arxiv.org/abs/2403.05105v1","updated":"2024-03-08T07:09:30Z","published":"2024-03-08T07:09:30Z","title":"Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval","summary":"  Collecting well-matched multimedia datasets is crucial for training\ncross-modal retrieval models. However, in real-world scenarios, massive\nmultimodal data are harvested from the Internet, which inevitably contains\nPartially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data\nwill remarkably harm the cross-modal retrieval performance. Previous efforts\ntend to mitigate this problem by estimating a soft correspondence to\ndown-weight the contribution of PMPs. In this paper, we aim to address this\nchallenge from a new perspective: the potential semantic similarity among\nunpaired samples makes it possible to excavate useful knowledge from mismatched\npairs. To achieve this, we propose L2RM, a general framework based on Optimal\nTransport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to\ngenerate refined alignments by seeking a minimal-cost transport plan across\ndifferent modalities. To formalize the rematching idea in OT, first, we propose\na self-supervised cost function that automatically learns from explicit\nsimilarity-cost mapping relation. Second, we present to model a partial OT\nproblem while restricting the transport among false positives to further boost\nrefined alignments. Extensive experiments on three benchmarks demonstrate our\nL2RM significantly improves the robustness against PMPs for existing models.\nThe code is available at https://github.com/hhc1997/L2RM.\n","authors":["Haochen Han","Qinghua Zheng","Guang Dai","Minnan Luo","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05105v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05102v1","updated":"2024-03-08T07:07:28Z","published":"2024-03-08T07:07:28Z","title":"Enhancing Texture Generation with High-Fidelity Using Advanced Texture\n  Priors","summary":"  The recent advancements in 2D generation technology have sparked a widespread\ndiscussion on using 2D priors for 3D shape and texture content generation.\nHowever, these methods often overlook the subsequent user operations, such as\ntexture aliasing and blurring that occur when the user acquires the 3D model\nand simplifies its structure. Traditional graphics methods partially alleviate\nthis issue, but recent texture synthesis technologies fail to ensure\nconsistency with the original model's appearance and cannot achieve\nhigh-fidelity restoration. Moreover, background noise frequently arises in\nhigh-resolution texture synthesis, limiting the practical application of these\ngeneration technologies.In this work, we propose a high-resolution and\nhigh-fidelity texture restoration technique that uses the rough texture as the\ninitial input to enhance the consistency between the synthetic texture and the\ninitial texture, thereby overcoming the issues of aliasing and blurring caused\nby the user's structure simplification operations. Additionally, we introduce a\nbackground noise smoothing technique based on a self-supervised scheme to\naddress the noise problem in current high-resolution texture synthesis schemes.\nOur approach enables high-resolution texture synthesis, paving the way for\nhigh-definition and high-detail texture synthesis technology. Experiments\ndemonstrate that our scheme outperforms currently known schemes in\nhigh-fidelity texture recovery under high-resolution conditions.\n","authors":["Kuo Xu","Maoyu Wang","Muyu Wang","Lincong Feng","Tianhui Zhang","Xiaoli Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17862v3","updated":"2024-03-08T07:03:57Z","published":"2024-02-27T19:54:30Z","title":"REPrune: Channel Pruning via Kernel Representative Selection","summary":"  Channel pruning is widely accepted to accelerate modern convolutional neural\nnetworks (CNNs). The resulting pruned model benefits from its immediate\ndeployment on general-purpose software and hardware resources. However, its\nlarge pruning granularity, specifically at the unit of a convolution filter,\noften leads to undesirable accuracy drops due to the inflexibility of deciding\nhow and where to introduce sparsity to the CNNs. In this paper, we propose\nREPrune, a novel channel pruning technique that emulates kernel pruning, fully\nexploiting the finer but structured granularity. REPrune identifies similar\nkernels within each channel using agglomerative clustering. Then, it selects\nfilters that maximize the incorporation of kernel representatives while\noptimizing the maximum cluster coverage problem. By integrating with a\nsimultaneous training-pruning paradigm, REPrune promotes efficient, progressive\npruning throughout training CNNs, avoiding the conventional\ntrain-prune-finetune sequence. Experimental results highlight that REPrune\nperforms better in computer vision tasks than existing methods, effectively\nachieving a balance between acceleration ratio and performance retention.\n","authors":["Mincheol Park","Dongjin Kim","Cheonjun Park","Yuna Park","Gyeong Eun Gong","Won Woo Ro","Suhyun Kim"],"pdf_url":"https://arxiv.org/pdf/2402.17862v3.pdf","comment":"Published at AAAI2024"},{"id":"http://arxiv.org/abs/2403.05100v1","updated":"2024-03-08T07:03:18Z","published":"2024-03-08T07:03:18Z","title":"Exploring the Adversarial Frontier: Quantifying Robustness via\n  Adversarial Hypervolume","summary":"  The escalating threat of adversarial attacks on deep learning models,\nparticularly in security-critical fields, has underscored the need for robust\ndeep learning systems. Conventional robustness evaluations have relied on\nadversarial accuracy, which measures a model's performance under a specific\nperturbation intensity. However, this singular metric does not fully\nencapsulate the overall resilience of a model against varying degrees of\nperturbation. To address this gap, we propose a new metric termed adversarial\nhypervolume, assessing the robustness of deep learning models comprehensively\nover a range of perturbation intensities from a multi-objective optimization\nstandpoint. This metric allows for an in-depth comparison of defense mechanisms\nand recognizes the trivial improvements in robustness afforded by less potent\ndefensive strategies. Additionally, we adopt a novel training algorithm that\nenhances adversarial robustness uniformly across various perturbation\nintensities, in contrast to methods narrowly focused on optimizing adversarial\naccuracy. Our extensive empirical studies validate the effectiveness of the\nadversarial hypervolume metric, demonstrating its ability to reveal subtle\ndifferences in robustness that adversarial accuracy overlooks. This research\ncontributes a new measure of robustness and establishes a standard for\nassessing and benchmarking the resilience of current and future defensive\nmodels against adversarial threats.\n","authors":["Ping Guo","Cheng Gong","Xi Lin","Zhiyuan Yang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04732v2","updated":"2024-03-08T06:47:08Z","published":"2024-03-07T18:35:54Z","title":"How Far Are We from Intelligent Visual Deductive Reasoning?","summary":"  Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.\n","authors":["Yizhe Zhang","He Bai","Ruixiang Zhang","Jiatao Gu","Shuangfei Zhai","Josh Susskind","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2403.04732v2.pdf","comment":"ICLR 2024 AGI workshop. https://github.com/apple/ml-rpm-bench"},{"id":"http://arxiv.org/abs/2403.05094v1","updated":"2024-03-08T06:46:01Z","published":"2024-03-08T06:46:01Z","title":"Face2Diffusion for Fast and Editable Face Personalization","summary":"  Face personalization aims to insert specific faces, taken from images, into\npretrained text-to-image diffusion models. However, it is still challenging for\nprevious methods to preserve both the identity similarity and editability due\nto overfitting to training samples. In this paper, we propose Face2Diffusion\n(F2D) for high-editability face personalization. The core idea behind F2D is\nthat removing identity-irrelevant information from the training pipeline\nprevents the overfitting problem and improves editability of encoded faces. F2D\nconsists of the following three novel components: 1) Multi-scale identity\nencoder provides well-disentangled identity features while keeping the benefits\nof multi-scale information, which improves the diversity of camera poses. 2)\nExpression guidance disentangles face expressions from identities and improves\nthe controllability of face expressions. 3) Class-guided denoising\nregularization encourages models to learn how faces should be denoised, which\nboosts the text-alignment of backgrounds. Extensive experiments on the\nFaceForensics++ dataset and diverse prompts demonstrate our method greatly\nimproves the trade-off between the identity- and text-fidelity compared to\nprevious state-of-the-art methods.\n","authors":["Kaede Shiohara","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2403.05094v1.pdf","comment":"CVPR2024. Code: https://github.com/mapooon/Face2Diffusion, Webpage:\n  https://mapooon.github.io/Face2DiffusionPage/"},{"id":"http://arxiv.org/abs/2209.11795v2","updated":"2024-03-08T06:45:47Z","published":"2022-09-23T18:22:04Z","title":"Descriptor Distillation: a Teacher-Student-Regularized Framework for\n  Learning Local Descriptors","summary":"  Learning a fast and discriminative patch descriptor is a challenging topic in\ncomputer vision. Recently, many existing works focus on training various\ndescriptor learning networks by minimizing a triplet loss (or its variants),\nwhich is expected to decrease the distance between each positive pair and\nincrease the distance between each negative pair. However, such an expectation\nhas to be lowered due to the non-perfect convergence of network optimizer to a\nlocal solution. Addressing this problem and the open computational speed\nproblem, we propose a Descriptor Distillation framework for local descriptor\nlearning, called DesDis, where a student model gains knowledge from a\npre-trained teacher model, and it is further enhanced via a designed\nteacher-student regularizer. This teacher-student regularizer is to constrain\nthe difference between the positive (also negative) pair similarity from the\nteacher model and that from the student model, and we theoretically prove that\na more effective student model could be trained by minimizing a weighted\ncombination of the triplet loss and this regularizer, than its teacher which is\ntrained by minimizing the triplet loss singly. Under the proposed DesDis, many\nexisting descriptor networks could be embedded as the teacher model, and\naccordingly, both equal-weight and light-weight student models could be\nderived, which outperform their teacher in either accuracy or speed.\nExperimental results on 3 public datasets demonstrate that the equal-weight\nstudent models, derived from the proposed DesDis framework by utilizing three\ntypical descriptor learning networks as teacher models, could achieve\nsignificantly better performances than their teachers and several other\ncomparative methods. In addition, the derived light-weight models could achieve\n8 times or even faster speeds than the comparative methods under similar patch\nverification performances\n","authors":["Yuzhen Liu","Qiulei Dong"],"pdf_url":"https://arxiv.org/pdf/2209.11795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00849v2","updated":"2024-03-08T06:42:37Z","published":"2023-12-01T11:36:08Z","title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from\n  Fine-grained Correctional Human Feedback","summary":"  Multimodal Large Language Models (MLLMs) have recently demonstrated\nimpressive capabilities in multimodal understanding, reasoning, and\ninteraction. However, existing MLLMs prevalently suffer from serious\nhallucination problems, generating text that is not factually grounded in\nassociated images. The problem makes existing MLLMs untrustworthy and thus\nimpractical in real-world (especially high-stakes) applications. To address the\nchallenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior\nalignment from fine-grained correctional human feedback. Specifically, RLHF-V\ncollects human preference in the form of segment-level corrections on\nhallucinations, and performs dense direct preference optimization over the\nhuman feedback. Comprehensive experiments on five benchmarks in both automatic\nand human evaluation show that, RLHF-V can enable substantially more\ntrustworthy MLLM behaviors with promising data and computation efficiency.\nRemarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the\nhallucination rate of the base MLLM by 34.8%, outperforming the concurrent\nLLaVA-RLHF trained on 10k annotated data. The final model achieves\nstate-of-the-art performance in trustworthiness among open-source MLLMs, and\nshows better robustness than GPT-4V in preventing hallucinations aroused from\nover-generalization. We open-source our code, model, and data at\nhttps://github.com/RLHF-V/RLHF-V.\n","authors":["Tianyu Yu","Yuan Yao","Haoye Zhang","Taiwen He","Yifeng Han","Ganqu Cui","Jinyi Hu","Zhiyuan Liu","Hai-Tao Zheng","Maosong Sun","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2312.00849v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05093v1","updated":"2024-03-08T06:39:24Z","published":"2024-03-08T06:39:24Z","title":"Spectrum Translation for Refinement of Image Generation (STIG) Based on\n  Contrastive Learning and Spectral Filter Profile","summary":"  Currently, image generation and synthesis have remarkably progressed with\ngenerative models. Despite photo-realistic results, intrinsic discrepancies are\nstill observed in the frequency domain. The spectral discrepancy appeared not\nonly in generative adversarial networks but in diffusion models. In this study,\nwe propose a framework to effectively mitigate the disparity in frequency\ndomain of the generated images to improve generative performance of both GAN\nand diffusion models. This is realized by spectrum translation for the\nrefinement of image generation (STIG) based on contrastive learning. We adopt\ntheoretical logic of frequency components in various generative networks. The\nkey idea, here, is to refine the spectrum of the generated image via the\nconcept of image-to-image translation and contrastive learning in terms of\ndigital signal processing. We evaluate our framework across eight fake image\ndatasets and various cutting-edge models to demonstrate the effectiveness of\nSTIG. Our framework outperforms other cutting-edges showing significant\ndecreases in FID and log frequency distance of spectrum. We further emphasize\nthat STIG improves image quality by decreasing the spectral anomaly.\nAdditionally, validation results present that the frequency-based deepfake\ndetector confuses more in the case where fake spectrums are manipulated by\nSTIG.\n","authors":["Seokjun Lee","Seung-Won Jung","Hyunseok Seo"],"pdf_url":"https://arxiv.org/pdf/2403.05093v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2403.03691v2","updated":"2024-03-08T06:32:12Z","published":"2024-03-06T13:17:41Z","title":"MolNexTR: A Generalized Deep Learning Model for Molecular Image\n  Recognition","summary":"  In the field of chemical structure recognition, the task of converting\nmolecular images into graph structures and SMILES string stands as a\nsignificant challenge, primarily due to the varied drawing styles and\nconventions prevalent in chemical literature. To bridge this gap, we proposed\nMolNexTR, a novel image-to-graph deep learning model that collaborates to fuse\nthe strengths of ConvNext, a powerful Convolutional Neural Network variant, and\nVision-TRansformer. This integration facilitates a more nuanced extraction of\nboth local and global features from molecular images. MolNexTR can predict\natoms and bonds simultaneously and understand their layout rules. It also\nexcels at flexibly integrating symbolic chemistry principles to discern\nchirality and decipher abbreviated structures. We further incorporate a series\nof advanced algorithms, including improved data augmentation module, image\ncontamination module, and a post-processing module to get the final SMILES\noutput. These modules synergistically enhance the model's robustness against\nthe diverse styles of molecular imagery found in real literature. In our test\nsets, MolNexTR has demonstrated superior performance, achieving an accuracy\nrate of 81-97%, marking a significant advancement in the domain of molecular\nstructure recognition. Scientific contribution: MolNexTR is a novel\nimage-to-graph model that incorporates a unique dual-stream encoder to extract\ncomplex molecular image features, and combines chemical rules to predict atoms\nand bonds while understanding atom and bond layout rules. In addition, it\nemploys a series of novel augmentation algorithms to significantly enhance the\nrobustness and performance of the model.\n","authors":["Yufan Chen","Ching Ting Leung","Yong Huang","Jianwei Sun","Hao Chen","Hanyu Gao"],"pdf_url":"https://arxiv.org/pdf/2403.03691v2.pdf","comment":"Submitted to the Journal of Cheminformatics"},{"id":"http://arxiv.org/abs/2403.05087v1","updated":"2024-03-08T06:28:09Z","published":"2024-03-08T06:28:09Z","title":"SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded\n  Gaussian Splatting","summary":"  We present SplattingAvatar, a hybrid 3D representation of photorealistic\nhuman avatars with Gaussian Splatting embedded on a triangle mesh, which\nrenders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We\ndisentangle the motion and appearance of a virtual human with explicit mesh\ngeometry and implicit appearance modeling with Gaussian Splatting. The\nGaussians are defined by barycentric coordinates and displacement on a triangle\nmesh as Phong surfaces. We extend lifted optimization to simultaneously\noptimize the parameters of the Gaussians while walking on the triangle mesh.\nSplattingAvatar is a hybrid representation of virtual humans where the mesh\nrepresents low-frequency motion and surface deformation, while the Gaussians\ntake over the high-frequency geometry and detailed appearance. Unlike existing\ndeformation methods that rely on an MLP-based linear blend skinning (LBS) field\nfor motion, we control the rotation and translation of the Gaussians directly\nby mesh, which empowers its compatibility with various animation techniques,\ne.g., skeletal animation, blend shapes, and mesh editing. Trainable from\nmonocular videos for both full-body and head avatars, SplattingAvatar shows\nstate-of-the-art rendering quality across multiple datasets.\n","authors":["Zhijing Shao","Zhaolong Wang","Zhuang Li","Duotun Wang","Xiangru Lin","Yu Zhang","Mingming Fan","Zeyu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05087v1.pdf","comment":"[CVPR 2024] Code and data are available at\n  https://github.com/initialneil/SplattingAvatar"},{"id":"http://arxiv.org/abs/2403.05086v1","updated":"2024-03-08T06:27:13Z","published":"2024-03-08T06:27:13Z","title":"UFORecon: Generalizable Sparse-View Surface Reconstruction from\n  Arbitrary and UnFavOrable Data Sets","summary":"  Generalizable neural implicit surface reconstruction aims to obtain an\naccurate underlying geometry given a limited number of multi-view images from\nunseen scenes. However, existing methods select only informative and relevant\nviews using predefined scores for training and testing phases. This constraint\nrenders the model impractical in real-world scenarios, where the availability\nof favorable combinations cannot always be ensured. We introduce and validate a\nview-combination score to indicate the effectiveness of the input view\ncombination. We observe that previous methods output degenerate solutions under\narbitrary and unfavorable sets. Building upon this finding, we propose\n\\textbf{UFORecon}, a robust view-combination generalizable surface\nreconstruction framework. To achieve this, we apply cross-view matching\ntransformers to model interactions between source images and build correlation\nfrustums to capture global correlations. Additionally, we explicitly encode\npairwise feature similarities as view-consistent priors. Our proposed framework\nsignificantly outperforms previous methods in terms of view-combination\ngeneralizability and also in the conventional generalizable protocol trained\nwith favorable view-combinations. The code is available at\n\\url{https://github.com/Youngju-Na/UFORecon}.\n","authors":["Youngju Na","Woo Jae Kim","Kyu Beom Han","Suhyeon Ha","Sung-eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.05086v1.pdf","comment":"accepted at CVPR 2024, \\url{Project\n  Page}{https://youngju-na.github.io/uforecon.github.io/}"},{"id":"http://arxiv.org/abs/2402.14300v3","updated":"2024-03-08T05:48:41Z","published":"2024-02-22T05:34:22Z","title":"A Simple Framework Uniting Visual In-context Learning with Masked Image\n  Modeling to Improve Ultrasound Segmentation","summary":"  Conventional deep learning models deal with images one-by-one, requiring\ncostly and time-consuming expert labeling in the field of medical imaging, and\ndomain-specific restriction limits model generalizability. Visual in-context\nlearning (ICL) is a new and exciting area of research in computer vision.\nUnlike conventional deep learning, ICL emphasizes the model's ability to adapt\nto new tasks based on given examples quickly. Inspired by MAE-VQGAN, we\nproposed a new simple visual ICL method called SimICL, combining visual ICL\npairing images with masked image modeling (MIM) designed for self-supervised\nlearning. We validated our method on bony structures segmentation in a wrist\nultrasound (US) dataset with limited annotations, where the clinical objective\nwas to segment bony structures to help with further fracture detection. We used\na test set containing 3822 images from 18 patients for bony region\nsegmentation. SimICL achieved an remarkably high Dice coeffient (DC) of 0.96\nand Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and\nvisual ICL models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU\nincreasing up to 0.10 and 0.16. This remarkably high agreement with limited\nmanual annotations indicates SimICL could be used for training AI models even\non small US datasets. This could dramatically decrease the human expert time\nrequired for image labeling compared to conventional approaches, and enhance\nthe real-world use of AI assistance in US image analysis.\n","authors":["Yuyue Zhou","Banafshe Felfeliyan","Shrimanti Ghosh","Jessica Knight","Fatima Alves-Pereira","Christopher Keen","Jessica Küpper","Abhilash Rakkunedeth Hareendranathan","Jacob L. Jaremko"],"pdf_url":"https://arxiv.org/pdf/2402.14300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04492v2","updated":"2024-03-08T05:47:40Z","published":"2024-03-07T13:49:29Z","title":"Discriminative Sample-Guided and Parameter-Efficient Feature Space\n  Adaptation for Cross-Domain Few-Shot Learning","summary":"  In this paper, we look at cross-domain few-shot classification which presents\nthe challenging task of learning new classes in unseen domains with few\nlabelled examples. Existing methods, though somewhat effective, encounter\nseveral limitations, which we address in this work through two significant\nimprovements. First, to address overfitting associated with fine-tuning a large\nnumber of parameters on small datasets, we introduce a lightweight\nparameter-efficient adaptation strategy. This strategy employs a linear\ntransformation of pre-trained features, significantly reducing the trainable\nparameter count. Second, we replace the traditional nearest centroid classifier\nwith a variance-aware loss function, enhancing the model's sensitivity to the\ninter- and intra-class variances within the training set for improved\nclustering in feature space. Empirical evaluations on the Meta-Dataset\nbenchmark showcase that our approach not only improves accuracy up to 7.7% and\n5.3% on seen and unseen datasets respectively but also achieves this\nperformance while being at least ~3x more parameter-efficient than existing\nmethods, establishing a new state-of-the-art in cross-domain few-shot learning.\nOur code can be found at https://github.com/rashindrie/DIPA.\n","authors":["Rashindrie Perera","Saman Halgamuge"],"pdf_url":"https://arxiv.org/pdf/2403.04492v2.pdf","comment":"Code will be released at this link:\n  https://github.com/rashindrie/DIPA"},{"id":"http://arxiv.org/abs/2403.05069v1","updated":"2024-03-08T05:43:00Z","published":"2024-03-08T05:43:00Z","title":"Improving Diffusion-Based Generative Models via Approximated Optimal\n  Transport","summary":"  We introduce the Approximated Optimal Transport (AOT) technique, a novel\ntraining scheme for diffusion-based generative models. Our approach aims to\napproximate and integrate optimal transport into the training process,\nsignificantly enhancing the ability of diffusion models to estimate the\ndenoiser outputs accurately. This improvement leads to ODE trajectories of\ndiffusion models with lower curvature and reduced truncation errors during\nsampling. We achieve superior image quality and reduced sampling steps by\nemploying AOT in training. Specifically, we achieve FID scores of 1.88 with\njust 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional\ngenerations, respectively. Furthermore, when applying AOT to train the\ndiscriminator for guidance, we establish new state-of-the-art FID scores of\n1.68 and 1.58 for unconditional and conditional generations, respectively, each\nwith 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing\nthe performance of diffusion models.\n","authors":["Daegyu Kim","Jooyoung Choi","Chaehun Shin","Uiwon Hwang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.05069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09934v6","updated":"2024-03-08T05:40:12Z","published":"2022-07-20T14:20:35Z","title":"DeepIPC: Deeply Integrated Perception and Control for an Autonomous\n  Vehicle in Real Environments","summary":"  In this work, we introduce DeepIPC, a novel end-to-end model tailored for\nautonomous driving, which seamlessly integrates perception and control tasks.\nUnlike traditional models that handle these tasks separately, DeepIPC\ninnovatively combines a perception module, which processes RGBD images for\nsemantic segmentation and generates bird's eye view (BEV) mappings, with a\ncontroller module that utilizes these insights along with GNSS and angular\nspeed measurements to accurately predict navigational waypoints. This\nintegration allows DeepIPC to efficiently translate complex environmental data\ninto actionable driving commands. Our comprehensive evaluation demonstrates\nDeepIPC's superior performance in terms of drivability and multi-task\nefficiency across diverse real-world scenarios, setting a new benchmark for\nend-to-end autonomous driving systems with a leaner model architecture. The\nexperimental results underscore DeepIPC's potential to significantly enhance\nautonomous vehicular navigation, promising a step forward in the development of\nautonomous driving technologies. For further insights and replication, we will\nmake our code and datasets available at https://github.com/oskarnatan/DeepIPC.\n","authors":["Oskar Natan","Jun Miura"],"pdf_url":"https://arxiv.org/pdf/2207.09934v6.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2312.08488v3","updated":"2024-03-08T05:19:33Z","published":"2023-12-13T20:08:26Z","title":"A PnP Algorithm for Two-Dimensional Pose Estimation","summary":"  We propose a PnP algorithm for a camera constrained to two-dimensional motion\n(applicable, for instance, to many wheeled robotics platforms). Leveraging this\nassumption allows accuracy and performance improvements over 3D PnP algorithms\ndue to the reduction in search space dimensionality. It also reduces the\nincidence of ambiguous pose estimates (as, in most cases, the spurious\nsolutions fall outside the plane of movement). Our algorithm finds an\napproximate solution by solving a polynomial system and refines its prediction\niteratively to minimize the reprojection error. The algorithm compares\nfavorably to existing 3D PnP algorithms in terms of accuracy, performance, and\nrobustness to noise.\n","authors":["Joshua Wang"],"pdf_url":"https://arxiv.org/pdf/2312.08488v3.pdf","comment":"5 pages, 3 figures. New initialization strategy from version 2"},{"id":"http://arxiv.org/abs/2403.05062v1","updated":"2024-03-08T05:17:10Z","published":"2024-03-08T05:17:10Z","title":"Agile Multi-Source-Free Domain Adaptation","summary":"  Efficiently utilizing rich knowledge in pretrained models has become a\ncritical topic in the era of large models. This work focuses on adaptively\nutilizing knowledge from multiple source-pretrained models to an unlabeled\ntarget domain without accessing the source data. Despite being a practically\nuseful setting, existing methods require extensive parameter tuning over each\nsource model, which is computationally expensive when facing abundant source\ndomains or larger source models. To address this challenge, we propose a novel\napproach which is free of the parameter tuning over source backbones. Our\ntechnical contribution lies in the Bi-level ATtention ENsemble (Bi-ATEN)\nmodule, which learns both intra-domain weights and inter-domain ensemble\nweights to achieve a fine balance between instance specificity and domain\nconsistency. By slightly tuning source bottlenecks, we achieve comparable or\neven superior performance on a challenging benchmark DomainNet with less than\n3% trained parameters and 8 times of throughput compared with SOTA method.\nFurthermore, with minor modifications, the proposed module can be easily\nequipped to existing methods and gain more than 4% performance boost. Code is\navailable at https://github.com/TL-UESTC/Bi-ATEN.\n","authors":["Xinyao Li","Jingjing Li","Fengling Li","Lei Zhu","Ke Lu"],"pdf_url":"https://arxiv.org/pdf/2403.05062v1.pdf","comment":"Accepted to AAAI2024"},{"id":"http://arxiv.org/abs/2403.05061v1","updated":"2024-03-08T05:15:48Z","published":"2024-03-08T05:15:48Z","title":"RadarDistill: Boosting Radar-based Object Detection Performance via\n  Knowledge Distillation from LiDAR Features","summary":"  The inherent noisy and sparse characteristics of radar data pose challenges\nin finding effective representations for 3D object detection. In this paper, we\npropose RadarDistill, a novel knowledge distillation (KD) method, which can\nimprove the representation of radar data by leveraging LiDAR data. RadarDistill\nsuccessfully transfers desirable characteristics of LiDAR features into radar\nfeatures using three key components: Cross-Modality Alignment (CMA),\nActivation-based Feature Distillation (AFD), and Proposal-based Feature\nDistillation (PFD). CMA enhances the density of radar features through multiple\nlayers of dilation operations, effectively addressing the challenges of\ninefficient knowledge transfer from LiDAR to radar. AFD is designed to transfer\nknowledge from significant areas of the LiDAR features, specifically those\nregions where activation intensity exceeds a predetermined threshold. PFD\nguides the radar network to mimic LiDAR network features in the object\nproposals for accurately detected results while moderating features for\nmisdetected proposals like false positives. Our comparative analyses conducted\non the nuScenes datasets demonstrate that RadarDistill achieves\nstate-of-the-art (SOTA) performance for radar-only object detection task,\nrecording 20.5% in mAP and 43.7% in NDS. Also, RadarDistill significantly\nimproves the performance of the camera-radar fusion model.\n","authors":["Geonho Bang","Kwangjin Choi","Jisong Kim","Dongsuk Kum","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2403.05061v1.pdf","comment":"accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024, 10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.05056v1","updated":"2024-03-08T05:06:31Z","published":"2024-03-08T05:06:31Z","title":"Stealing Stable Diffusion Prior for Robust Monocular Depth Estimation","summary":"  Monocular depth estimation is a crucial task in computer vision. While\nexisting methods have shown impressive results under standard conditions, they\noften face challenges in reliably performing in scenarios such as low-light or\nrainy conditions due to the absence of diverse training data. This paper\nintroduces a novel approach named Stealing Stable Diffusion (SSD) prior for\nrobust monocular depth estimation. The approach addresses this limitation by\nutilizing stable diffusion to generate synthetic images that mimic challenging\nconditions. Additionally, a self-training mechanism is introduced to enhance\nthe model's depth estimation capability in such challenging environments. To\nenhance the utilization of the stable diffusion prior further, the DINOv2\nencoder is integrated into the depth model architecture, enabling the model to\nleverage rich semantic priors and improve its scene understanding. Furthermore,\na teacher loss is introduced to guide the student models in acquiring\nmeaningful knowledge independently, thus reducing their dependency on the\nteacher models. The effectiveness of the approach is evaluated on nuScenes and\nOxford RobotCar, two challenging public datasets, with the results showing the\nefficacy of the method. Source code and weights are available at:\nhttps://github.com/hitcslj/SSD.\n","authors":["Yifan Mao","Jian Liu","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05055v1","updated":"2024-03-08T05:03:25Z","published":"2024-03-08T05:03:25Z","title":"MUC: Mixture of Uncalibrated Cameras for Robust 3D Human Body\n  Reconstruction","summary":"  Multiple cameras can provide multi-view video coverage of a person. It is\nnecessary to fuse multi-view data, e.g., for subsequent behavioral analysis,\nwhile such fusion often relies on calibration of cameras in traditional\nsolutions. However, it is non-trivial to calibrate multiple cameras. In this\nwork, we propose a method to reconstruct 3D human body from multiple\nuncalibrated camera views. First, we adopt a pre-trained human body encoder to\nprocess each individual camera view, such that human body models and parameters\ncan be reconstructed for each view. Next, instead of simply averaging models\nacross views, we train a network to determine the weights of individual views\nfor their fusion, based on the parameters estimated for joints and hands of\nhuman body as well as camera positions. Further, we turn to the mesh surface of\nhuman body for dynamic fusion, such that facial expression can be seamlessly\nintegrated into the model of human body. Our method has demonstrated superior\nperformance in reconstructing human body upon two public datasets. More\nimportantly, our method can flexibly support ad-hoc deployment of an arbitrary\nnumber of cameras, which has significant potential in related applications. We\nwill release source code upon acceptance of the paper.\n","authors":["Yitao Zhu","Sheng Wang","Mengjie Xu","Zixu Zhuang","Zhixin Wang","Kaidong Wang","Han Zhang","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01663v2","updated":"2024-03-08T05:00:48Z","published":"2024-03-04T01:18:29Z","title":"PillarGen: Enhancing Radar Point Cloud Density and Quality via\n  Pillar-based Point Generation Network","summary":"  In this paper, we present a novel point generation model, referred to as\nPillar-based Point Generation Network (PillarGen), which facilitates the\ntransformation of point clouds from one domain into another. PillarGen can\nproduce synthetic point clouds with enhanced density and quality based on the\nprovided input point clouds. The PillarGen model performs the following three\nsteps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar\nto Point Generation (PPG). The input point clouds are encoded using a pillar\ngrid structure to generate pillar features. Then, OPP determines the active\npillars used for point generation and predicts the center of points and the\nnumber of points to be generated for each active pillar. PPG generates the\nsynthetic points for each active pillar based on the information provided by\nOPP. We evaluate the performance of PillarGen using our proprietary radar\ndataset, focusing on enhancing the density and quality of short-range radar\ndata using the long-range radar data as supervision. Our experiments\ndemonstrate that PillarGen outperforms traditional point upsampling methods in\nquantitative and qualitative measures. We also confirm that when PillarGen is\nincorporated into bird's eye view object detection, a significant improvement\nin detection accuracy is achieved.\n","authors":["Jisong Kim","Geonho Bang","Kwangjin Choi","Minjae Seong","Jaechang Yoo","Eunjong Pyo","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2403.01663v2.pdf","comment":"Accepted by IEEE International Conference on Robotics and Automation\n  (ICRA 2024), 8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.05053v1","updated":"2024-03-08T04:58:49Z","published":"2024-03-08T04:58:49Z","title":"PrimeComposer: Faster Progressively Combined Diffusion for Image\n  Composition with Attention Steering","summary":"  Image composition involves seamlessly integrating given objects into a\nspecific visual context. The current training-free methods rely on composing\nattention weights from several samplers to guide the generator. However, since\nthese weights are derived from disparate contexts, their combination leads to\ncoherence confusion in synthesis and loss of appearance information. These\nissues worsen with their excessive focus on background generation, even when\nunnecessary in this task. This not only slows down inference but also\ncompromises foreground generation quality. Moreover, these methods introduce\nunwanted artifacts in the transition area. In this paper, we formulate image\ncomposition as a subject-based local editing task, solely focusing on\nforeground generation. At each step, the edited foreground is combined with the\nnoisy background to maintain scene consistency. To address the remaining\nissues, we propose PrimeComposer, a faster training-free diffuser that\ncomposites the images by well-designed attention steering across different\nnoise levels. This steering is predominantly achieved by our Correlation\nDiffuser, utilizing its self-attention layers at each step. Within these\nlayers, the synthesized subject interacts with both the referenced object and\nbackground, capturing intricate details and coherent relationships. This prior\ninformation is encoded into the attention weights, which are then integrated\ninto the self-attention layers of the generator to guide the synthesis process.\nBesides, we introduce a Region-constrained Cross-Attention to confine the\nimpact of specific subject-related words to desired regions, addressing the\nunwanted artifacts shown in the prior method thereby further improving the\ncoherence in the transition area. Our method exhibits the fastest inference\nefficiency and extensive experiments demonstrate our superiority both\nqualitatively and quantitatively.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2403.05053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05050v1","updated":"2024-03-08T04:53:53Z","published":"2024-03-08T04:53:53Z","title":"DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for\n  Streaming Perception","summary":"  Autonomous driving systems demand real-time, accurate perception to navigate\ncomplex environments. Addressing this, we introduce the Dynamic Router Network\n(DyRoNet), a framework that innovates with low-rank dynamic routing for\nenhanced streaming perception. By integrating specialized pre-trained branch\nnetworks, fine-tuned for various environmental conditions, DyRoNet achieves a\nbalance between latency and precision. Its core feature, the speed router\nmodule, intelligently directs input data to the best-suited branch network,\noptimizing performance. The extensive evaluations reveal that DyRoNet adapts\neffectively to multiple branch selection strategies, setting a new benchmark in\nperformance across a range of scenarios. DyRoNet not only establishes a new\nbenchmark for streaming perception but also provides valuable engineering\ninsights for future work. More project information is available at\nhttps://tastevision.github.io/DyRoNet/\n","authors":["Xiang Huang","Zhi-Qi Cheng","Jun-Yan He","Chenyang Li","Wangmeng Xiang","Baigui Sun","Xiao Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05049v1","updated":"2024-03-08T04:52:22Z","published":"2024-03-08T04:52:22Z","title":"XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution","summary":"  Diffusion-based methods, endowed with a formidable generative prior, have\nreceived increasing attention in Image Super-Resolution (ISR) recently.\nHowever, as low-resolution (LR) images often undergo severe degradation, it is\nchallenging for ISR models to perceive the semantic and degradation\ninformation, resulting in restoration images with incorrect content or\nunrealistic artifacts. To address these issues, we propose a\n\\textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR,\nto acquire precise and comprehensive semantic conditions for the diffusion\nmodel, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To\nfacilitate better fusion of cross-modal priors, a \\textit{Semantic-Fusion\nAttention} is raised. To distill semantic-preserved information instead of\nundesired degradations, a \\textit{Degradation-Free Constraint} is attached\nbetween LR and its high-resolution (HR) counterpart. Quantitative and\nqualitative results show that XPSR is capable of generating high-fidelity and\nhigh-realism images across synthetic and real-world datasets. Codes will be\nreleased at \\url{https://github.com/qyp2000/XPSR}.\n","authors":["Yunpeng Qu","Kun Yuan","Kai Zhao","Qizhi Xie","Jinhua Hao","Ming Sun","Chao Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05049v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.05047v1","updated":"2024-03-08T04:48:56Z","published":"2024-03-08T04:48:56Z","title":"REPS: Reconstruction-based Point Cloud Sampling","summary":"  Sampling is widely used in various point cloud tasks as it can effectively\nreduce resource consumption. Recently, some methods have proposed utilizing\nneural networks to optimize the sampling process for various task requirements.\nCurrently, deep downsampling methods can be categorized into two main types:\ngenerative-based and score-based. Generative-based methods directly generate\nsampled point clouds using networks, whereas score-based methods assess the\nimportance of points according to specific rules and then select sampled point\nclouds based on their scores. However, these methods often result in noticeable\nclustering effects in high-intensity feature areas, compromising their ability\nto preserve small-scale features and leading to the loss of some structures,\nthereby affecting the performance of subsequent tasks. In this paper, we\npropose REPS, a reconstruction-based scoring strategy that evaluates the\nimportance of each vertex by removing and reconstructing them using surrounding\nvertices. Our reconstruction process comprises point reconstruction and shape\nreconstruction. The two aforementioned reconstruction methods effectively\nevaluate the importance of vertices by removing them at different scales for\nreconstruction. These reconstructions ensure that our method maintains the\noverall geometric features of the point cloud and avoids disturbing small-scale\nstructures during sampling. Additionally, we propose the Global-Local Fusion\nAttention (GLFA) module, which aggregates local and global attention features\nof point clouds, ensuring high-quality reconstruction and sampling effects. Our\nmethod outperforms previous approaches in preserving the structural features of\nthe sampled point clouds. Furthermore, abundant experimental results\ndemonstrate the superior performance of our method across various common tasks.\n","authors":["Guoqing Zhang","Wenbo Zhao","Jian Liu","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05047v1.pdf","comment":"project page: https://github.com/hitcslj/REPS"},{"id":"http://arxiv.org/abs/2402.18278v2","updated":"2024-03-08T04:33:06Z","published":"2024-02-28T12:16:42Z","title":"EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor\n  Neighborhoods","summary":"  High-definition (HD) map is crucial for autonomous driving systems. Most\nexisting works design map elements detection heads based on the DETR decoder.\nHowever, the initial queries lack explicit incorporation of physical positional\ninformation, and vanilla self-attention entails high computational complexity.\nTherefore, we propose EAN-MapNet for Efficiently constructing HD map using\nAnchor Neighborhoods. Firstly, we design query units based on the anchor\nneighborhoods, allowing non-neighborhood central anchors to effectively assist\nin fitting the neighborhood central anchors to the target points representing\nmap elements. Then, we propose grouped local self-attention (GL-SA) by\nleveraging the relative instance relationship among the queries. This\nfacilitates direct feature interaction among queries of the same instances,\nwhile innovatively employing local queries as intermediaries for interaction\namong queries from different instances. Consequently, GL-SA significantly\nreduces the computational complexity of self-attention while ensuring ample\nfeature interaction among queries. On the nuScenes dataset, EAN-MapNet achieves\na state-of-the-art performance with 63.0 mAP after training for 24 epochs,\nsurpassing MapTR by 12.7 mAP. Furthermore, it considerably reduces memory\nconsumption by 8198M compared to MapTRv2.\n","authors":["Huiyuan Xiong","Jun Shen","Taohong Zhu","Yuelong Pan"],"pdf_url":"https://arxiv.org/pdf/2402.18278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05034v1","updated":"2024-03-08T04:25:29Z","published":"2024-03-08T04:25:29Z","title":"CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction\n  Model","summary":"  Feed-forward 3D generative models like the Large Reconstruction Model (LRM)\nhave demonstrated exceptional generation speed. However, the transformer-based\nmethods do not leverage the geometric priors of the triplane component in their\narchitecture, often leading to sub-optimal quality given the limited size of 3D\ndata and slow training. In this work, we present the Convolutional\nReconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D\ngenerative model. Recognizing the limitations posed by sparse 3D data, we\nhighlight the necessity of integrating geometric priors into network design.\nCRM builds on the key observation that the visualization of triplane exhibits\nspatial correspondence of six orthographic images. First, it generates six\northographic view images from a single input image, then feeds these images\ninto a convolutional U-Net, leveraging its strong pixel-level alignment\ncapabilities and significant bandwidth to create a high-resolution triplane.\nCRM further employs Flexicubes as geometric representation, facilitating direct\nend-to-end optimization on textured meshes. Overall, our model delivers a\nhigh-fidelity textured mesh from an image in just 10 seconds, without any\ntest-time optimization.\n","authors":["Zhengyi Wang","Yikai Wang","Yifei Chen","Chendong Xiang","Shuo Chen","Dajiang Yu","Chongxuan Li","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.05034v1.pdf","comment":"Project page: https://ml.cs.tsinghua.edu.cn/~zhengyi/CRM/"},{"id":"http://arxiv.org/abs/2402.18152v2","updated":"2024-03-08T04:11:05Z","published":"2024-02-28T08:32:19Z","title":"Boosting Neural Representations for Videos with a Conditional Decoder","summary":"  Implicit neural representations (INRs) have emerged as a promising approach\nfor video storage and processing, showing remarkable versatility across various\nvideo tasks. However, existing methods often fail to fully leverage their\nrepresentation capabilities, primarily due to inadequate alignment of\nintermediate features during target frame decoding. This paper introduces a\nuniversal boosting framework for current implicit video representation\napproaches. Specifically, we utilize a conditional decoder with a\ntemporal-aware affine transform module, which uses the frame index as a prior\ncondition to effectively align intermediate features with target frames.\nBesides, we introduce a sinusoidal NeRV-like block to generate diverse\nintermediate features and achieve a more balanced parameter distribution,\nthereby enhancing the model's capacity. With a high-frequency\ninformation-preserving reconstruction loss, our approach successfully boosts\nmultiple baseline INRs in the reconstruction quality and convergence speed for\nvideo regression, and exhibits superior inpainting and interpolation results.\nFurther, we integrate a consistent entropy minimization technique and develop\nvideo codecs based on these boosted INRs. Experiments on the UVG dataset\nconfirm that our enhanced codecs significantly outperform baseline INRs and\noffer competitive rate-distortion performance compared to traditional and\nlearning-based codecs.\n","authors":["Xinjie Zhang","Ren Yang","Dailan He","Xingtong Ge","Tongda Xu","Yan Wang","Hongwei Qin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.18152v2.pdf","comment":"Accept by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05024v1","updated":"2024-03-08T04:02:34Z","published":"2024-03-08T04:02:34Z","title":"A Probabilistic Hadamard U-Net for MRI Bias Field Correction","summary":"  Magnetic field inhomogeneity correction remains a challenging task in MRI\nanalysis. Most established techniques are designed for brain MRI by supposing\nthat image intensities in the identical tissue follow a uniform distribution.\nSuch an assumption cannot be easily applied to other organs, especially those\nthat are small in size and heterogeneous in texture (large variations in\nintensity), such as the prostate. To address this problem, this paper proposes\na probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field\ncorrection. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the\nlow-frequency scalar field, multiplied by the original input to obtain the\nprototypical corrected image. HU-Net converts the input image from the time\ndomain into the frequency domain via Hadamard transform. In the frequency\ndomain, high-frequency components are eliminated using the trainable filter\n(scaling layer), hard-thresholding layer, and sparsity penalty. Next, a\nconditional variational autoencoder is used to encode possible bias\nfield-corrected variants into a low-dimensional latent space. Random samples\ndrawn from latent space are then incorporated with a prototypical corrected\nimage to generate multiple plausible images. Experimental results demonstrate\nthe effectiveness of PHU-Net in correcting bias-field in prostate MRI with a\nfast inference speed. It has also been shown that prostate MRI segmentation\naccuracy improves with the high-quality corrected images from PHU-Net. The code\nwill be available in the final version of this manuscript.\n","authors":["Xin Zhu","Hongyi Pan","Yury Velichko","Adam B. Murphy","Ashley Ross","Baris Turkbey","Ahmet Enis Cetin","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2403.05024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05023v1","updated":"2024-03-08T03:55:27Z","published":"2024-03-08T03:55:27Z","title":"Towards Multimodal Sentiment Analysis Debiasing via Bias Purification","summary":"  Multimodal Sentiment Analysis (MSA) aims to understand human intentions by\nintegrating emotion-related clues from diverse modalities, such as visual,\nlanguage, and audio. Unfortunately, the current MSA task invariably suffers\nfrom unplanned dataset biases, particularly multimodal utterance-level label\nbias and word-level context bias. These harmful biases potentially mislead\nmodels to focus on statistical shortcuts and spurious correlations, causing\nsevere performance bottlenecks. To alleviate these issues, we present a\nMultimodal Counterfactual Inference Sentiment (MCIS) analysis framework based\non causality rather than conventional likelihood. Concretely, we first\nformulate a causal graph to discover harmful biases from already-trained\nvanilla models. In the inference phase, given a factual multimodal input, MCIS\nimagines two counterfactual scenarios to purify and mitigate these biases.\nThen, MCIS can make unbiased decisions from biased observations by comparing\nfactual and counterfactual outcomes. We conduct extensive experiments on\nseveral standard MSA benchmarks. Qualitative and quantitative results show the\neffectiveness of the proposed framework.\n","authors":["Dingkang Yang","Mingcheng Li","Dongling Xiao","Yang Liu","Kun Yang","Zhaoyu Chen","Yuzheng Wang","Peng Zhai","Ke Li","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05023v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.05021v1","updated":"2024-03-08T03:54:22Z","published":"2024-03-08T03:54:22Z","title":"Beyond MOT: Semantic Multi-Object Tracking","summary":"  Current multi-object tracking (MOT) aims to predict trajectories of targets\n(i.e.,\"where\") in videos. Yet, knowing merely \"where\" is insufficient in many\ncrucial applications. In comparison, semantic understanding such as\nfine-grained behaviors, interactions, and overall summarized captions (i.e.,\n\"what\") from videos, associated with \"where\", is highly-desired for\ncomprehensive video analysis. Thus motivated, we introduce Semantic\nMulti-Object Tracking (SMOT), that aims to estimate object trajectories and\nmeanwhile understand semantic details of associated trajectories including\ninstance captions, instance interactions, and overall video captions,\nintegrating \"where\" and \"what\" for tracking. In order to foster the exploration\nof SMOT, we propose BenSMOT, a large-scale Benchmark for Semantic MOT.\nSpecifically, BenSMOT comprises 3,292 videos with 151K frames, covering various\nscenarios for semantic tracking of humans. BenSMOT provides annotations for the\ntrajectories of targets, along with associated instance captions in natural\nlanguage, instance interactions, and overall caption for each video sequence.\nTo our best knowledge, BenSMOT is the first publicly available benchmark for\nSMOT. Besides, to encourage future research, we present a novel tracker named\nSMOTer, which is specially designed and end-to-end trained for SMOT, showing\npromising performance. By releasing BenSMOT, we expect to go beyond\nconventional MOT by predicting \"where\" and \"what\" for SMOT, opening up a new\ndirection in tracking for video understanding. Our BenSMOT and SMOTer will be\nreleased.\n","authors":["Yunhao Li","Hao Wang","Qin Li","Xue Ma","Jiali Yao","Shaohua Dong","Heng Fan","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05019v1","updated":"2024-03-08T03:45:04Z","published":"2024-03-08T03:45:04Z","title":"ERASOR++: Height Coding Plus Egocentric Ratio Based Dynamic Object\n  Removal for Static Point Cloud Mapping","summary":"  Mapping plays a crucial role in location and navigation within automatic\nsystems. However, the presence of dynamic objects in 3D point cloud maps\ngenerated from scan sensors can introduce map distortion and long traces,\nthereby posing challenges for accurate mapping and navigation. To address this\nissue, we propose ERASOR++, an enhanced approach based on the Egocentric Ratio\nof Pseudo Occupancy for effective dynamic object removal. To begin, we\nintroduce the Height Coding Descriptor, which combines height difference and\nheight layer information to encode the point cloud. Subsequently, we propose\nthe Height Stack Test, Ground Layer Test, and Surrounding Point Test methods to\nprecisely and efficiently identify the dynamic bins within point cloud bins,\nthus overcoming the limitations of prior approaches. Through extensive\nevaluation on open-source datasets, our approach demonstrates superior\nperformance in terms of precision and efficiency compared to existing methods.\nFurthermore, the techniques described in our work hold promise for addressing\nvarious challenging tasks or aspects through subsequent migration.\n","authors":["Jiabao Zhang","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05019v1.pdf","comment":"7 pages, 6 figures, ICRA 2024"},{"id":"http://arxiv.org/abs/2403.05018v1","updated":"2024-03-08T03:43:04Z","published":"2024-03-08T03:43:04Z","title":"InstructGIE: Towards Generalizable Image Editing","summary":"  Recent advances in image editing have been driven by the development of\ndenoising diffusion models, marking a significant leap forward in this field.\nDespite these advances, the generalization capabilities of recent image editing\napproaches remain constrained. In response to this challenge, our study\nintroduces a novel image editing framework with enhanced generalization\nrobustness by boosting in-context learning capability and unifying language\ninstruction. This framework incorporates a module specifically optimized for\nimage editing tasks, leveraging the VMamba Block and an editing-shift matching\nstrategy to augment in-context learning. Furthermore, we unveil a selective\narea-matching technique specifically engineered to address and rectify\ncorrupted details in generated images, such as human facial features, to\nfurther improve the quality. Another key innovation of our approach is the\nintegration of a language unification technique, which aligns language\nembeddings with editing semantics to elevate the quality of image editing.\nMoreover, we compile the first dataset for image editing with visual prompts\nand editing instructions that could be used to enhance in-context capability.\nTrained on this dataset, our methodology not only achieves superior synthesis\nquality for trained tasks, but also demonstrates robust generalization\ncapability across unseen vision tasks through tailored prompts.\n","authors":["Zichong Meng","Changdi Yang","Jun Liu","Hao Tang","Pu Zhao","Yanzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05018v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.05016v1","updated":"2024-03-08T03:34:18Z","published":"2024-03-08T03:34:18Z","title":"DiffClass: Diffusion-Based Class Incremental Learning","summary":"  Class Incremental Learning (CIL) is challenging due to catastrophic\nforgetting. On top of that, Exemplar-free Class Incremental Learning is even\nmore challenging due to forbidden access to previous task data. Recent\nexemplar-free CIL methods attempt to mitigate catastrophic forgetting by\nsynthesizing previous task data. However, they fail to overcome the\ncatastrophic forgetting due to the inability to deal with the significant\ndomain gap between real and synthetic data. To overcome these issues, we\npropose a novel exemplar-free CIL method. Our method adopts multi-distribution\nmatching (MDM) diffusion models to unify quality and bridge domain gaps among\nall domains of training data. Moreover, our approach integrates selective\nsynthetic image augmentation (SSIA) to expand the distribution of the training\ndata, thereby improving the model's plasticity and reinforcing the performance\nof our method's ultimate component, multi-domain adaptation (MDA). With the\nproposed integrations, our method then reformulates exemplar-free CIL into a\nmulti-domain adaptation problem to implicitly address the domain gap problem to\nenhance model stability during incremental training. Extensive experiments on\nbenchmark class incremental datasets and settings demonstrate that our method\nexcels previous exemplar-free CIL methods and achieves state-of-the-art\nperformance.\n","authors":["Zichong Meng","Jie Zhang","Changdi Yang","Zheng Zhan","Pu Zhao","Yanzhi WAng"],"pdf_url":"https://arxiv.org/pdf/2403.05016v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2306.01669v2","updated":"2024-03-08T03:19:39Z","published":"2023-06-02T16:43:05Z","title":"Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label\n  Prompt Tuning","summary":"  Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is\noften necessary to optimize their performance. However, a major obstacle is the\nlimited availability of labeled data. We study the use of pseudolabels, i.e.,\nheuristic labels for unlabeled data, to enhance CLIP via prompt tuning.\nConventional pseudolabeling trains a model on labeled data and then generates\nlabels for unlabeled data. VLMs' zero-shot capabilities enable a \"second\ngeneration\" of pseudolabeling approaches that do not require task-specific\ntraining on labeled data. By using zero-shot pseudolabels as a source of\nsupervision, we observe that learning paradigms such as semi-supervised,\ntransductive zero-shot, and unsupervised learning can all be seen as optimizing\nthe same loss function. This unified view enables the development of versatile\ntraining strategies that are applicable across learning paradigms. We\ninvestigate them on image classification tasks where CLIP exhibits limitations,\nby varying prompt modalities, e.g., textual or visual prompts, and learning\nparadigms. We find that (1) unexplored prompt tuning strategies that\niteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5\npoints in semi-supervised learning, by 28.4 points in transductive zero-shot\nlearning, and by 15.2 points in unsupervised learning, and (2) unlike\nconventional semi-supervised pseudolabeling, which exacerbates model biases\ntoward classes with higher-quality pseudolabels, prompt tuning leads to a more\nequitable distribution of per-class accuracy. The code to reproduce the\nexperiments is at https://github.com/BatsResearch/menghini-neurips23-code.\n","authors":["Cristina Menghini","Andrew Delworth","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2306.01669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05005v1","updated":"2024-03-08T03:03:41Z","published":"2024-03-08T03:03:41Z","title":"DITTO: Dual and Integrated Latent Topologies for Implicit 3D\n  Reconstruction","summary":"  We propose a novel concept of dual and integrated latent topologies (DITTO in\nshort) for implicit 3D reconstruction from noisy and sparse point clouds. Most\nexisting methods predominantly focus on single latent type, such as point or\ngrid latents. In contrast, the proposed DITTO leverages both point and grid\nlatents (i.e., dual latent) to enhance their strengths, the stability of grid\nlatents and the detail-rich capability of point latents. Concretely, DITTO\nconsists of dual latent encoder and integrated implicit decoder. In the dual\nlatent encoder, a dual latent layer, which is the key module block composing\nthe encoder, refines both latents in parallel, maintaining their distinct\nshapes and enabling recursive interaction. Notably, a newly proposed dynamic\nsparse point transformer within the dual latent layer effectively refines point\nlatents. Then, the integrated implicit decoder systematically combines these\nrefined latents, achieving high-fidelity 3D reconstruction and surpassing\nprevious state-of-the-art methods on object- and scene-level datasets,\nespecially in thin and detailed structures.\n","authors":["Jaehyeok Shim","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2403.05005v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2310.08825v3","updated":"2024-03-08T02:49:12Z","published":"2023-10-13T02:41:55Z","title":"From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language\n  Models","summary":"  Multi-modal Large Language Models (MLLMs) have made significant strides in\nexpanding the capabilities of Large Language Models (LLMs) through the\nincorporation of visual perception interfaces. Despite the emergence of\nexciting applications and the availability of diverse instruction tuning data,\nexisting approaches often rely on CLIP or its variants as the visual branch,\nand merely extract features from the deep layers. However, these methods lack a\ncomprehensive analysis of the visual encoders in MLLMs. In this paper, we\nconduct an extensive investigation into the effectiveness of different vision\nencoders within MLLMs. Our findings reveal that the shallow layer features of\nCLIP offer particular advantages for fine-grained tasks such as grounding and\nregion understanding. Surprisingly, the vision-only model DINO, which is not\npretrained with text-image alignment, demonstrates promising performance as a\nvisual branch within MLLMs. By simply equipping it with an MLP layer for\nalignment, DINO surpasses CLIP in fine-grained related perception tasks.\nBuilding upon these observations, we propose a simple yet effective feature\nmerging strategy, named COMM, that integrates CLIP and DINO with Multi-level\nfeatures Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM\nthrough comprehensive experiments on a wide range of benchmarks, including\nimage captioning, visual question answering, visual grounding, and object\nhallucination. Experimental results demonstrate the superior performance of\nCOMM compared to existing methods, showcasing its enhanced visual capabilities\nwithin MLLMs.\n","authors":["Dongsheng Jiang","Yuchen Liu","Songlin Liu","Jin'e Zhao","Hao Zhang","Zhen Gao","Xiaopeng Zhang","Jin Li","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.08825v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03716v3","updated":"2024-03-08T02:47:44Z","published":"2023-05-05T17:57:04Z","title":"3D Small Object Detection with Dynamic Spatial Pruning","summary":"  In this paper, we propose an efficient feature pruning strategy for 3D small\nobject detection. Conventional 3D object detection methods struggle on small\nobjects due to the weak geometric information from a small number of points.\nAlthough increasing the spatial resolution of feature representations can\nimprove the detection performance on small objects, the additional\ncomputational overhead is unaffordable. With in-depth study, we observe the\ngrowth of computation mainly comes from the upsampling operation in the decoder\nof 3D detector. Motivated by this, we present a multi-level 3D detector named\nDSPDet3D which benefits from high spatial resolution to achieves high accuracy\non small object detection, while reducing redundant computation by only\nfocusing on small object areas. Specifically, we theoretically derive a dynamic\nspatial pruning (DSP) strategy to prune the redundant spatial representation of\n3D scene in a cascade manner according to the distribution of objects. Then we\ndesign DSP module following this strategy and construct DSPDet3D with this\nefficient module. On ScanNet and TO-SCENE dataset, our method achieves leading\nperformance on small object detection. Moreover, DSPDet3D trained with only\nScanNet rooms can generalize well to scenes in larger scale. It takes less than\n2s to directly process a whole building consisting of more than 4500k points\nwhile detecting out almost all objects, ranging from cups to beds, on a single\nRTX 3090 GPU. Project page: https://xuxw98.github.io/DSPDet3D/.\n","authors":["Xiuwei Xu","Zhihao Sun","Ziwei Wang","Hongmin Liu","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2305.03716v3.pdf","comment":"Code is available at: https://github.com/xuxw98/DSPDet3D"},{"id":"http://arxiv.org/abs/2402.11566v2","updated":"2024-03-08T02:46:23Z","published":"2024-02-18T12:27:59Z","title":"Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data\n  Augmentation and Consistency Training","summary":"  The 2D human pose estimation (HPE) is a basic visual problem. However, its\nsupervised learning requires massive keypoint labels, which is labor-intensive\nto collect. Thus, we aim at boosting a pose estimator by excavating extra\nunlabeled data with semi-supervised learning (SSL). Most previous SSHPE methods\nare consistency-based and strive to maintain consistent outputs for differently\naugmented inputs. Under this genre, we find that SSHPE can be boosted from two\ncores: advanced data augmentations and concise consistency training ways.\nSpecifically, for the first core, we discover the synergistic effects of\nexisting augmentations, and reveal novel paradigms for conveniently producing\nnew superior HPE-oriented augmentations which can more effectively add noise on\nunlabeled samples. We can therefore establish paired easy-hard augmentations\nwith larger difficulty gaps. For the second core, we propose to repeatedly\naugment unlabeled images with diverse hard augmentations, and generate\nmulti-path predictions sequentially for optimizing multi-losses in a single\nnetwork. This simple and compact design is interpretable, and easily benefits\nfrom newly found augmentations. Comparing to SOTA approaches, our method brings\nsubstantial improvements on public datasets. Code is in\n\\url{https://github.com/hnuzhy/MultiAugs}\n","authors":["Huayi Zhou","Mukun Luo","Fei Jiang","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2402.11566v2.pdf","comment":"14 pages. Semi-Supervised 2D Human Pose Estimation"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2302.11370v5","updated":"2024-03-08T17:55:25Z","published":"2023-02-22T13:39:54Z","title":"Recall, Robustness, and Lexicographic Evaluation","summary":"  Although originally developed to evaluate sets of items, recall is often used\nto evaluate rankings of items, including those produced by recommender,\nretrieval, and other machine learning systems. The application of recall\nwithout a formal evaluative motivation has led to criticism of recall as a\nvague or inappropriate measure. In light of this debate, we reflect on the\nmeasurement of recall in rankings from a formal perspective. Our analysis is\ncomposed of three tenets: recall, robustness, and lexicographic evaluation.\nFirst, we formally define `recall-orientation' as the sensitivity of a metric\nto a user interested in finding every relevant item. Second, we analyze\nrecall-orientation from the perspective of robustness with respect to possible\ncontent consumers and providers, connecting recall to recent conversations\nabout fair ranking. Finally, we extend this conceptual and theoretical\ntreatment of recall by developing a practical preference-based evaluation\nmethod based on lexicographic comparison. Through extensive empirical analysis\nacross three recommendation tasks and 17 information retrieval tasks, we\nestablish that our new evaluation method, lexirecall, has convergent validity\n(i.e., it is correlated with existing recall metrics) and exhibits\nsubstantially higher sensitivity in terms of discriminative power and stability\nin the presence of missing labels. Our conceptual, theoretical, and empirical\nanalysis substantially deepens our understanding of recall and motivates its\nadoption through connections to robustness and fairness.\n","authors":["Fernando Diaz","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2302.11370v5.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.05440v1","updated":"2024-03-08T16:48:20Z","published":"2024-03-08T16:48:20Z","title":"Is Cosine-Similarity of Embeddings Really About Similarity?","summary":"  Cosine-similarity is the cosine of the angle between two vectors, or\nequivalently the dot product between their normalizations. A popular\napplication is to quantify semantic similarity between high-dimensional objects\nby applying cosine-similarity to a learned low-dimensional feature embedding.\nThis can work better but sometimes also worse than the unnormalized dot-product\nbetween embedded vectors in practice. To gain insight into this empirical\nobservation, we study embeddings derived from regularized linear models, where\nclosed-form solutions facilitate analytical insights. We derive analytically\nhow cosine-similarity can yield arbitrary and therefore meaningless\n`similarities.' For some linear models the similarities are not even unique,\nwhile for others they are implicitly controlled by the regularization. We\ndiscuss implications beyond linear models: a combination of different\nregularizations are employed when learning deep models; these have implicit and\nunintended effects when taking cosine-similarities of the resulting embeddings,\nrendering results opaque and possibly arbitrary. Based on these insights, we\ncaution against blindly using cosine-similarity and outline alternatives.\n","authors":["Harald Steck","Chaitanya Ekanadham","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2403.05440v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2402.13787v3","updated":"2024-03-08T15:05:10Z","published":"2024-02-21T13:14:45Z","title":"Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks","summary":"  In this paper, we investigate the conditions under which link analysis\nalgorithms prevent minority groups from reaching high ranking slots. We find\nthat the most common link-based algorithms using centrality metrics, such as\nPageRank and HITS, can reproduce and even amplify bias against minority groups\nin networks. Yet, their behavior differs: one one hand, we empirically show\nthat PageRank mirrors the degree distribution for most of the ranking positions\nand it can equalize representation of minorities among the top ranked nodes; on\nthe other hand, we find that HITS amplifies pre-existing bias in homophilic\nnetworks through a novel theoretical analysis, supported by empirical results.\nWe find the root cause of bias amplification in HITS to be the level of\nhomophily present in the network, modeled through an evolving network model\nwith two communities. We illustrate our theoretical analysis on both synthetic\nand real datasets and we present directions for future work.\n","authors":["Ana-Andreea Stoica","Nelly Litvak","Augustin Chaintreau"],"pdf_url":"https://arxiv.org/pdf/2402.13787v3.pdf","comment":"Accepted for publication in Proceedings of The Web Conference, 2024"},{"id":"http://arxiv.org/abs/2206.02631v3","updated":"2024-03-08T12:44:46Z","published":"2022-05-31T01:54:29Z","title":"Contemporary Recommendation Systems on Big Data and Their Applications:\n  A Survey","summary":"  This survey paper conducts a comprehensive analysis of the evolution and\ncontemporary landscape of recommendation systems, which have been extensively\nincorporated across a myriad of web applications. It delves into the\nprogression of personalized recommendation methodologies tailored for online\nproducts or services, organizing the array of recommendation techniques into\nfour main categories: content-based, collaborative filtering, knowledge-based,\nand hybrid approaches, each designed to cater to specific contexts. The\ndocument provides an in-depth review of both the historical underpinnings and\nthe cutting-edge innovations in the domain of recommendation systems, with a\nspecial focus on implementations leveraging big data analytics. It further\noutlines and explores the predominant challenges encountered in the current\ngeneration of recommendation systems, including issues related to data\nsparsity, scalability, and the imperative for diversified recommendation\noutputs. The survey underscores these challenges as promising directions for\nsubsequent research endeavors within the discipline. Additionally, the paper\nexamines various real-life applications driven by recommendation systems,\naddressing the hurdles involved in seamlessly integrating these systems into\neveryday life. Ultimately, the survey underscores how the advancements in\nrecommendation systems, propelled by big data technologies, have the potential\nto significantly enhance real-world experiences.\n","authors":["Ziyuan Xia","Anchen Sun","Jingyi Xu","Yuanzhe Peng","Rui Ma","Minghui Cheng"],"pdf_url":"https://arxiv.org/pdf/2206.02631v3.pdf","comment":"29 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.05217v1","updated":"2024-03-08T11:09:13Z","published":"2024-03-08T11:09:13Z","title":"Harnessing Multi-Role Capabilities of Large Language Models for\n  Open-Domain Question Answering","summary":"  Open-domain question answering (ODQA) has emerged as a pivotal research\nspotlight in information systems. Existing methods follow two main paradigms to\ncollect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves\npertinent documents from an external corpus; and (2) the\n\\textit{generate-then-read} paradigm employs large language models (LLMs) to\ngenerate relevant documents. However, neither can fully address multifaceted\nrequirements for evidence. To this end, we propose LLMQA, a generalized\nframework that formulates the ODQA process into three basic steps: query\nexpansion, document selection, and answer generation, combining the superiority\nof both retrieval-based and generation-based evidence. Since LLMs exhibit their\nexcellent capabilities to accomplish various tasks, we instruct LLMs to play\nmultiple roles as generators, rerankers, and evaluators within our framework,\nintegrating them to collaborate in the ODQA process. Furthermore, we introduce\na novel prompt optimization algorithm to refine role-playing prompts and steer\nLLMs to produce higher-quality evidence and answers. Extensive experimental\nresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that\nLLMQA achieves the best performance in terms of both answer accuracy and\nevidence quality, showcasing its potential for advancing ODQA research and\napplications.\n","authors":["Hongda Sun","Yuxuan Liu","Chengwei Wu","Haiyu Yan","Cheng Tai","Xin Gao","Shuo Shang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.05217v1.pdf","comment":"TheWebConf 2024 (WWW 2024) oral, code repo:\n  https://github.com/EthanLeo-LYX/LLMQA"},{"id":"http://arxiv.org/abs/2403.05185v1","updated":"2024-03-08T09:53:07Z","published":"2024-03-08T09:53:07Z","title":"Personalized Audiobook Recommendations at Spotify Through Graph Neural\n  Networks","summary":"  In the ever-evolving digital audio landscape, Spotify, well-known for its\nmusic and talk content, has recently introduced audiobooks to its vast user\nbase. While promising, this move presents significant challenges for\npersonalized recommendations. Unlike music and podcasts, audiobooks, initially\navailable for a fee, cannot be easily skimmed before purchase, posing higher\nstakes for the relevance of recommendations. Furthermore, introducing a new\ncontent type into an existing platform confronts extreme data sparsity, as most\nusers are unfamiliar with this new content type. Lastly, recommending content\nto millions of users requires the model to react fast and be scalable. To\naddress these challenges, we leverage podcast and music user preferences and\nintroduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous\nGraph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach\nuncovers nuanced item relationships while ensuring low latency and complexity.\nWe decouple users from the HGNN graph and propose an innovative multi-link\nneighbor sampler. These choices, together with the 2T component, significantly\nreduce the complexity of the HGNN model. Empirical evaluations involving\nmillions of users show significant improvement in the quality of personalized\nrecommendations, resulting in a +46% increase in new audiobooks start rate and\na +23% boost in streaming rates. Intriguingly, our model's impact extends\nbeyond audiobooks, benefiting established products like podcasts.\n","authors":["Marco De Nadai","Francesco Fabbri","Paul Gigioli","Alice Wang","Ang Li","Fabrizio Silvestri","Laura Kim","Shawn Lin","Vladan Radosavljevic","Sandeep Ghael","David Nyhan","Hugues Bouchard","Mounia Lalmas-Roelleke","Andreas Damianou"],"pdf_url":"https://arxiv.org/pdf/2403.05185v1.pdf","comment":"To appear in The Web Conference 2024 proceedings"},{"id":"http://arxiv.org/abs/2403.05122v1","updated":"2024-03-08T07:36:14Z","published":"2024-03-08T07:36:14Z","title":"Multi-Tower Multi-Interest Recommendation with User Representation Repel","summary":"  In the era of information overload, the value of recommender systems has been\nprofoundly recognized in academia and industry alike. Multi-interest sequential\nrecommendation, in particular, is a subfield that has been receiving increasing\nattention in recent years. By generating multiple-user representations,\nmulti-interest learning models demonstrate superior expressiveness than\nsingle-user representation models, both theoretically and empirically. Despite\nmajor advancements in the field, three major issues continue to plague the\nperformance and adoptability of multi-interest learning methods, the difference\nbetween training and deployment objectives, the inability to access item\ninformation, and the difficulty of industrial adoption due to its single-tower\narchitecture. We address these challenges by proposing a novel multi-tower\nmulti-interest framework with user representation repel. Experimental results\nacross multiple large-scale industrial datasets proved the effectiveness and\ngeneralizability of our proposed framework.\n","authors":["Tianyu Xiong","Xiaohan Yu"],"pdf_url":"https://arxiv.org/pdf/2403.05122v1.pdf","comment":"9 pages, 5 figures, 4 tables, Submitted to ACM/SIGIR - The 47th\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval"},{"id":"http://arxiv.org/abs/2310.20091v4","updated":"2024-03-08T06:09:40Z","published":"2023-10-31T00:12:13Z","title":"Density-based User Representation using Gaussian Process Regression for\n  Multi-interest Personalized Retrieval","summary":"  Accurate modeling of the diverse and dynamic interests of users remains a\nsignificant challenge in the design of personalized recommender systems.\nExisting user modeling methods, like single-point and multi-point\nrepresentations, have limitations w.r.t. accuracy, diversity, computational\ncost, and adaptability. To overcome these deficiencies, we introduce\ndensity-based user representations (DURs), a novel model that leverages\nGaussian process regression for effective multi-interest recommendation and\nretrieval. Our approach, GPR4DUR, exploits DURs to capture user interest\nvariability without manual tuning, incorporates uncertainty-awareness, and\nscales well to large numbers of users. Experiments using real-world offline\ndatasets confirm the adaptability and efficiency of GPR4DUR, while online\nexperiments with simulated users demonstrate its ability to address the\nexploration-exploitation trade-off by effectively utilizing model uncertainty.\n","authors":["Haolun Wu","Ofer Meshi","Masrour Zoghi","Fernando Diaz","Xue Liu","Craig Boutilier","Maryam Karimzadehgan"],"pdf_url":"https://arxiv.org/pdf/2310.20091v4.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2403.05063v1","updated":"2024-03-08T05:23:27Z","published":"2024-03-08T05:23:27Z","title":"Aligning Large Language Models for Controllable Recommendations","summary":"  Inspired by the exceptional general intelligence of Large Language Models\n(LLMs), researchers have begun to explore their application in pioneering the\nnext generation of recommender systems - systems that are conversational,\nexplainable, and controllable. However, existing literature primarily\nconcentrates on integrating domain-specific knowledge into LLMs to enhance\naccuracy, often neglecting the ability to follow instructions. To address this\ngap, we initially introduce a collection of supervised learning tasks,\naugmented with labels derived from a conventional recommender model, aimed at\nexplicitly improving LLMs' proficiency in adhering to recommendation-specific\ninstructions. Subsequently, we develop a reinforcement learning-based alignment\nprocedure to further strengthen LLMs' aptitude in responding to users'\nintentions and mitigating formatting errors. Through extensive experiments on\ntwo real-world datasets, our method markedly advances the capability of LLMs to\ncomply with instructions within recommender systems, while sustaining a high\nlevel of accuracy performance.\n","authors":["Wensheng Lu","Jianxun Lian","Wei Zhang","Guanghua Li","Mingyang Zhou","Hao Liao","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2403.05063v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.05004v1","updated":"2024-03-08T03:03:20Z","published":"2024-03-08T03:03:20Z","title":"Can't Remember Details in Long Documents? You Need Some R&R","summary":"  Long-context large language models (LLMs) hold promise for tasks such as\nquestion-answering (QA) over long documents, but they tend to miss important\ninformation in the middle of context documents (arXiv:2307.03172v3). Here, we\nintroduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods\ncalled $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to\nalleviate this effect in document-based QA. In reprompting, we repeat the\nprompt instructions periodically throughout the context document to remind the\nLLM of its original task. In ICR, rather than instructing the LLM to answer the\nquestion directly, we instruct it to retrieve the top $k$ passage numbers most\nrelevant to the given question, which are then used as an abbreviated context\nin a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents\nup to 80k tokens in length and observe a 16-point boost in QA accuracy on\naverage. Our further analysis suggests that R&R improves performance on long\ndocument-based QA because it reduces the distance between relevant context and\nthe instructions. Finally, we show that compared to short-context chunkwise\nmethods, R&R enables the use of larger chunks that cost fewer LLM calls and\noutput tokens, while minimizing the drop in accuracy.\n","authors":["Devanshu Agrawal","Shang Gao","Martin Gajek"],"pdf_url":"https://arxiv.org/pdf/2403.05004v1.pdf","comment":"13 pages, 1 figure, 9 tables. For associated code repository see\n  https://github.com/casetext/r-and-r"},{"id":"http://arxiv.org/abs/2403.05668v1","updated":"2024-03-08T20:44:59Z","published":"2024-03-08T20:44:59Z","title":"CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model\n  Recommender System","summary":"  In the evolving landscape of recommender systems, the integration of Large\nLanguage Models (LLMs) such as ChatGPT marks a new era, introducing the concept\nof Recommendation via LLM (RecLLM). While these advancements promise\nunprecedented personalization and efficiency, they also bring to the fore\ncritical concerns regarding fairness, particularly in how recommendations might\ninadvertently perpetuate or amplify biases associated with sensitive user\nattributes. In order to address these concerns, our study introduces a\ncomprehensive evaluation framework, CFaiRLLM, aimed at evaluating (and thereby\nmitigating) biases on the consumer side within RecLLMs.\n  Our research methodically assesses the fairness of RecLLMs by examining how\nrecommendations might vary with the inclusion of sensitive attributes such as\ngender, age, and their intersections, through both similarity alignment and\ntrue preference alignment. By analyzing recommendations generated under\ndifferent conditions-including the use of sensitive attributes in user\nprompts-our framework identifies potential biases in the recommendations\nprovided. A key part of our study involves exploring how different detailed\nstrategies for constructing user profiles (random, top-rated, recent) impact\nthe alignment between recommendations made without consideration of sensitive\nattributes and those that are sensitive-attribute-aware, highlighting the bias\nmechanisms within RecLLMs.\n  The findings in our study highlight notable disparities in the fairness of\nrecommendations, particularly when sensitive attributes are integrated into the\nrecommendation process, either individually or in combination. The analysis\ndemonstrates that the choice of user profile sampling strategy plays a\nsignificant role in affecting fairness outcomes, highlighting the complexity of\nachieving fair recommendations in the era of LLMs.\n","authors":["Yashar Deldjoo","Tommaso di Noia"],"pdf_url":"https://arxiv.org/pdf/2403.05668v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.05532v1","updated":"2024-03-08T18:57:00Z","published":"2024-03-08T18:57:00Z","title":"Tune without Validation: Searching for Learning Rate and Weight Decay on\n  Training Sets","summary":"  We introduce Tune without Validation (Twin), a pipeline for tuning learning\nrate and weight decay without validation sets. We leverage a recent theoretical\nframework concerning learning phases in hypothesis space to devise a heuristic\nthat predicts what hyper-parameter (HP) combinations yield better\ngeneralization. Twin performs a grid search of trials according to an\nearly-/non-early-stopping scheduler and then segments the region that provides\nthe best results in terms of training loss. Among these trials, the weight norm\nstrongly correlates with predicting generalization. To assess the effectiveness\nof Twin, we run extensive experiments on 20 image classification datasets and\ntrain several families of deep networks, including convolutional, transformer,\nand feed-forward models. We demonstrate proper HP selection when training from\nscratch and fine-tuning, emphasizing small-sample scenarios.\n","authors":["Lorenzo Brigato","Stavroula Mougiakakou"],"pdf_url":"https://arxiv.org/pdf/2403.05532v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2403.05529v1","updated":"2024-03-08T18:50:19Z","published":"2024-03-08T18:50:19Z","title":"The Computational Complexity of Learning Gaussian Single-Index Models","summary":"  Single-Index Models are high-dimensional regression problems with planted\nstructure, whereby labels depend on an unknown one-dimensional projection of\nthe input via a generic, non-linear, and potentially non-deterministic\ntransformation. As such, they encompass a broad class of statistical inference\ntasks, and provide a rich template to study statistical and computational\ntrade-offs in the high-dimensional regime.\n  While the information-theoretic sample complexity to recover the hidden\ndirection is linear in the dimension $d$, we show that computationally\nefficient algorithms, both within the Statistical Query (SQ) and the Low-Degree\nPolynomial (LDP) framework, necessarily require $\\Omega(d^{k^\\star/2})$\nsamples, where $k^\\star$ is a \"generative\" exponent associated with the model\nthat we explicitly characterize. Moreover, we show that this sample complexity\nis also sufficient, by establishing matching upper bounds using a partial-trace\nalgorithm. Therefore, our results provide evidence of a sharp\ncomputational-to-statistical gap (under both the SQ and LDP class) whenever\n$k^\\star>2$. To complete the study, we provide examples of smooth and Lipschitz\ndeterministic target functions with arbitrarily large generative exponents\n$k^\\star$.\n","authors":["Alex Damian","Loucas Pillaud-Vivien","Jason D. Lee","Joan Bruna"],"pdf_url":"https://arxiv.org/pdf/2403.05529v1.pdf","comment":"57 pages"},{"id":"http://arxiv.org/abs/2403.05527v1","updated":"2024-03-08T18:48:30Z","published":"2024-03-08T18:48:30Z","title":"GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless\n  Generative Inference of LLM","summary":"  Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.\n","authors":["Hao Kang","Qingru Zhang","Souvik Kundu","Geonhwa Jeong","Zaoxing Liu","Tushar Krishna","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.05527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.05275v4","updated":"2024-03-08T18:40:01Z","published":"2022-04-11T17:26:19Z","title":"Settling the Sample Complexity of Model-Based Offline Reinforcement\n  Learning","summary":"  This paper is concerned with offline reinforcement learning (RL), which\nlearns using pre-collected data without further exploration. Effective offline\nRL would be able to accommodate distribution shift and limited data coverage.\nHowever, prior algorithms or analyses either suffer from suboptimal sample\ncomplexities or incur high burn-in cost to reach sample optimality, thus posing\nan impediment to efficient offline RL in sample-starved applications.\n  We demonstrate that the model-based (or \"plug-in\") approach achieves\nminimax-optimal sample complexity without burn-in cost for tabular Markov\ndecision processes (MDPs). Concretely, consider a finite-horizon (resp.\n$\\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$\n(resp. effective horizon $\\frac{1}{1-\\gamma}$), and suppose the distribution\nshift of data is reflected by some single-policy clipped concentrability\ncoefficient $C^{\\star}_{\\text{clipped}}$. We prove that model-based offline RL\nyields $\\varepsilon$-accuracy with a sample complexity of \\[ \\begin{cases}\n\\frac{H^{4}SC_{\\text{clipped}}^{\\star}}{\\varepsilon^{2}} &\n(\\text{finite-horizon MDPs})\n\\frac{SC_{\\text{clipped}}^{\\star}}{(1-\\gamma)^{3}\\varepsilon^{2}} &\n(\\text{infinite-horizon MDPs}) \\end{cases} \\] up to log factor, which is\nminimax optimal for the entire $\\varepsilon$-range. The proposed algorithms are\n\"pessimistic\" variants of value iteration with Bernstein-style penalties, and\ndo not require sophisticated variance reduction. Our analysis framework is\nestablished upon delicate leave-one-out decoupling arguments in conjunction\nwith careful self-bounding techniques tailored to MDPs.\n","authors":["Gen Li","Laixi Shi","Yuxin Chen","Yuejie Chi","Yuting Wei"],"pdf_url":"https://arxiv.org/pdf/2204.05275v4.pdf","comment":"accepted to the Annals of Statistics"},{"id":"http://arxiv.org/abs/2110.05365v3","updated":"2024-03-08T18:10:06Z","published":"2021-10-11T15:50:49Z","title":"Intriguing Properties of Input-dependent Randomized Smoothing","summary":"  Randomized smoothing is currently considered the state-of-the-art method to\nobtain certifiably robust classifiers. Despite its remarkable performance, the\nmethod is associated with various serious problems such as \"certified accuracy\nwaterfalls\", certification vs.\\ accuracy trade-off, or even fairness issues.\nInput-dependent smoothing approaches have been proposed with intention of\novercoming these flaws. However, we demonstrate that these methods lack formal\nguarantees and so the resulting certificates are not justified. We show that in\ngeneral, the input-dependent smoothing suffers from the curse of\ndimensionality, forcing the variance function to have low semi-elasticity. On\nthe other hand, we provide a theoretical and practical framework that enables\nthe usage of input-dependent smoothing even in the presence of the curse of\ndimensionality, under strict restrictions. We present one concrete design of\nthe smoothing variance function and test it on CIFAR10 and MNIST. Our design\nmitigates some of the problems of classical smoothing and is formally\nunderlined, yet further improvement of the design is still necessary.\n","authors":["Peter Súkeník","Aleksei Kuvshinov","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2110.05365v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08434v3","updated":"2024-03-08T18:01:00Z","published":"2023-02-16T17:18:03Z","title":"On marginal feature attributions of tree-based models","summary":"  Due to their power and ease of use, tree-based machine learning models, such\nas random forests and gradient-boosted tree ensembles, have become very\npopular. To interpret them, local feature attributions based on marginal\nexpectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values,\nmay be employed. Such methods are true to the model and implementation\ninvariant, i.e. dependent only on the input-output function of the model. We\ncontrast this with the popular TreeSHAP algorithm by presenting two\n(statistically similar) decision trees that compute the exact same function for\nwhich the \"path-dependent\" TreeSHAP yields different rankings of features,\nwhereas the marginal Shapley values coincide. Furthermore, we discuss how the\ninternal structure of tree-based models may be leveraged to help with computing\ntheir marginal feature attributions according to a linear game value. One\nimportant observation is that these are simple (piecewise-constant) functions\nwith respect to a certain grid partition of the input space determined by the\ntrained model. Another crucial observation, showcased by experiments with\nXGBoost, LightGBM and CatBoost libraries, is that only a portion of all\nfeatures appears in a tree from the ensemble. Thus, the complexity of computing\nmarginal Shapley (or Owen or Banzhaf) feature attributions may be reduced. This\nremains valid for a broader class of game values which we shall axiomatically\ncharacterize. A prime example is the case of CatBoost models where the trees\nare oblivious (symmetric) and the number of features in each of them is no\nlarger than the depth. We exploit the symmetry to derive an explicit formula,\nwith improved complexity and only in terms of the internal model parameters,\nfor marginal Shapley (and Banzhaf and Owen) values of CatBoost models. This\nresults in a fast, accurate algorithm for estimating these feature\nattributions.\n","authors":["Khashayar Filom","Alexey Miroshnikov","Konstandinos Kotsiopoulos","Arjun Ravi Kannan"],"pdf_url":"https://arxiv.org/pdf/2302.08434v3.pdf","comment":"Typos corrected. Section 4 is reorganized and new experiments on Owen\n  values are added. Minor changes in the Introduction. 30 pages+appendix (64\n  pages in total), 10 figures"},{"id":"http://arxiv.org/abs/2403.05490v1","updated":"2024-03-08T17:55:41Z","published":"2024-03-08T17:55:41Z","title":"Poly-View Contrastive Learning","summary":"  Contrastive learning typically matches pairs of related views among a number\nof unrelated negative views. Views can be generated (e.g. by augmentations) or\nbe observed. We investigate matching when there are more than two related views\nwhich we call poly-view tasks, and derive new representation learning\nobjectives using information maximization and sufficient statistics. We show\nthat with unlimited computation, one should maximize the number of related\nviews, and with a fixed compute budget, it is beneficial to decrease the number\nof unique samples whilst increasing the number of views of those samples. In\nparticular, poly-view contrastive models trained for 128 epochs with batch size\n256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,\nchallenging the belief that contrastive models require large batch sizes and\nmany training epochs.\n","authors":["Amitis Shidani","Devon Hjelm","Jason Ramapuram","Russ Webb","Eeshan Gunesh Dhekane","Dan Busbridge"],"pdf_url":"https://arxiv.org/pdf/2403.05490v1.pdf","comment":"Accepted to ICLR 2024. 42 pages, 7 figures, 3 tables, loss\n  pseudo-code included in appendix"},{"id":"http://arxiv.org/abs/2402.17987v2","updated":"2024-03-08T17:47:21Z","published":"2024-02-28T02:11:47Z","title":"Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A\n  Bayesian Fusion Approach","summary":"  Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs)\ninvolves transmitting Electromagnetic Waves (EMWs) and performing target type\nrecognition on the received radar echo, crucial for defense and aerospace\napplications. Previous studies highlighted the advantages of multistatic radar\nconfigurations over monostatic ones in RATR. However, fusion methods in\nmultistatic radar configurations often suboptimally combine classification\nvectors from individual radars probabilistically. To address this, we propose a\nfully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to\naggregate classification probability vectors from multiple radars. OBF, based\non expected 0-1 loss, updates a Recursive Bayesian Classification (RBC)\nposterior distribution for target UAV type, conditioned on historical\nobservations across multiple time steps. We evaluate the approach using\nsimulated random walk trajectories for seven drones, correlating target aspect\nangles to Radar Cross Section (RCS) measurements in an anechoic chamber.\nComparing against single radar Automated Target Recognition (ATR) systems and\nsuboptimal fusion methods, our empirical results demonstrate that the OBF\nmethod integrated with RBC significantly enhances classification accuracy\ncompared to other fusion methods and single radar configurations.\n","authors":["Michael Potter","Murat Akcakaya","Marius Necsoiu","Gunar Schirner","Deniz Erdogmus","Tales Imbiriba"],"pdf_url":"https://arxiv.org/pdf/2402.17987v2.pdf","comment":"To be submitted to IEEE Transactions on Aerospace and Electronic\n  Systems"},{"id":"http://arxiv.org/abs/2211.15856v3","updated":"2024-03-08T17:45:35Z","published":"2022-11-29T01:11:04Z","title":"Beyond Ensemble Averages: Leveraging Climate Model Ensembles for\n  Subseasonal Forecasting","summary":"  Producing high-quality forecasts of key climate variables, such as\ntemperature and precipitation, on subseasonal time scales has long been a gap\nin operational forecasting. This study explores an application of machine\nlearning (ML) models as post-processing tools for subseasonal forecasting.\nLagged numerical ensemble forecasts (i.e., an ensemble where the members have\ndifferent initial dates) and observational data, including relative humidity,\npressure at sea level, and geopotential height, are incorporated into various\nML methods to predict monthly average precipitation and two-meter temperature\ntwo weeks in advance for the continental United States. Regression, quantile\nregression, and tercile classification tasks using linear models, random\nforests, convolutional neural networks, and stacked models (a multi-model\napproach based on the prediction of the individual ML models) are considered.\nUnlike previous ML approaches that often use ensemble mean alone, we leverage\ninformation embedded in the ensemble forecasts to enhance prediction accuracy.\nAdditionally, we investigate extreme event predictions that are crucial for\nplanning and mitigation efforts. Considering ensemble members as a collection\nof spatial forecasts, we explore different approaches to address spatial\nvariability. Trade-offs between different approaches may be mitigated with\nmodel stacking. Our proposed models outperform standard baselines such as\nclimatological forecasts and ensemble means. This paper further includes an\ninvestigation of feature importance, trade-offs between using the full ensemble\nor only the ensemble mean, and different modes of accounting for spatial\nvariability.\n","authors":["Elena Orlova","Haokun Liu","Raphael Rossellini","Benjamin Cash","Rebecca Willett"],"pdf_url":"https://arxiv.org/pdf/2211.15856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05465v1","updated":"2024-03-08T17:28:49Z","published":"2024-03-08T17:28:49Z","title":"Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit\n  Encodings for Efficient DNN Inference","summary":"  Traditional Deep Neural Network (DNN) quantization methods using integer,\nfixed-point, or floating-point data types struggle to capture diverse DNN\nparameter distributions at low precision, and often require large silicon\noverhead and intensive quantization-aware training. In this study, we introduce\nLogarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by\nposits that dynamically adapts to DNN weight/activation distributions by\nparameterizing LP bit fields. We also develop a novel genetic-algorithm based\nframework, LP Quantization (LPQ), to find optimal layer-wise LP parameters\nwhile reducing representational divergence between quantized and full-precision\nmodels through a novel global-local contrastive objective. Additionally, we\ndesign a unified mixed-precision LP accelerator (LPA) architecture comprising\nof processing elements (PEs) incorporating LP in the computational datapath.\nOur algorithm-hardware co-design demonstrates on average <1% drop in top-1\naccuracy across various CNN and ViT models. It also achieves ~ 2x improvements\nin performance per unit area and 2.2x gains in energy efficiency compared to\nstate-of-the-art quantization accelerators using different data types.\n","authors":["Akshat Ramachandran","Zishen Wan","Geonhwa Jeong","John Gustafson","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.05465v1.pdf","comment":"2024 61st IEEE/ACM Design Automation Conference (DAC)"},{"id":"http://arxiv.org/abs/2306.04527v4","updated":"2024-03-08T17:28:47Z","published":"2023-06-07T15:36:26Z","title":"ContriMix: Scalable stain color augmentation for domain generalization\n  without domain labels in digital pathology","summary":"  Differences in staining and imaging procedures can cause significant color\nvariations in histopathology images, leading to poor generalization when\ndeploying deep-learning models trained from a different data source. Various\ncolor augmentation methods have been proposed to generate synthetic images\nduring training to make models more robust, eliminating the need for stain\nnormalization during test time. Many color augmentation methods leverage domain\nlabels to generate synthetic images. This approach causes three significant\nchallenges to scaling such a model. Firstly, incorporating data from a new\ndomain into deep-learning models trained on existing domain labels is not\nstraightforward. Secondly, dependency on domain labels prevents the use of\npathology images without domain labels to improve model performance. Finally,\nimplementation of these methods becomes complicated when multiple domain labels\n(e.g., patient identification, medical center, etc) are associated with a\nsingle image. We introduce ContriMix, a novel domain label free stain color\naugmentation method based on DRIT++, a style-transfer method. Contrimix\nleverages sample stain color variation within a training minibatch and random\nmixing to extract content and attribute information from pathology images. This\ninformation can be used by a trained ContriMix model to create synthetic images\nto improve the performance of existing classifiers. ContriMix outperforms\ncompeting methods on the Camelyon17-WILDS dataset. Its performance is\nconsistent across different slides in the test set while being robust to the\ncolor variation from rare substances in pathology images. We make our code and\ntrained ContriMix models available for research use. The code for ContriMix can\nbe found at https://gitlab.com/huutan86/contrimix\n","authors":["Tan H. Nguyen","Dinkar Juyal","Jin Li","Aaditya Prakash","Shima Nofallah","Chintan Shah","Sai Chowdary Gullapally","Limin Yu","Michael Griffin","Anand Sampat","John Abel","Justin Lee","Amaro Taylor-Weiner"],"pdf_url":"https://arxiv.org/pdf/2306.04527v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04235v3","updated":"2024-03-08T17:04:49Z","published":"2023-11-06T08:50:29Z","title":"Can LLMs Follow Simple Rules?","summary":"  As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Existing evaluations\nof adversarial attacks and defenses on LLMs generally require either expensive\nmanual review or unreliable heuristic checks. To address this issue, we propose\nRule-following Language Evaluation Scenarios (RuLES), a programmatic framework\nfor measuring rule-following ability in LLMs. RuLES consists of 14 simple text\nscenarios in which the model is instructed to obey various rules while\ninteracting with the user. Each scenario has a programmatic evaluation function\nto determine whether the model has broken any rules in a conversation. Our\nevaluations of proprietary and open models show that almost all current models\nstruggle to follow scenario rules, even on straightforward test cases. We also\ndemonstrate that simple optimization attacks suffice to significantly increase\nfailure rates on test cases. We conclude by exploring two potential avenues for\nimprovement: test-time steering and supervised fine-tuning.\n","authors":["Norman Mu","Sarah Chen","Zifan Wang","Sizhe Chen","David Karamardian","Lulwa Aljeraisy","Basel Alomair","Dan Hendrycks","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2311.04235v3.pdf","comment":"Project website: https://eecs.berkeley.edu/~normanmu/llm_rules;\n  revised content"},{"id":"http://arxiv.org/abs/2403.04493v2","updated":"2024-03-08T17:02:55Z","published":"2024-03-07T13:49:43Z","title":"What makes an image realistic?","summary":"  The last decade has seen tremendous progress in our ability to generate\nrealistic-looking data, be it images, text, audio, or video. Here, we discuss\nthe closely related problem of quantifying realism, that is, designing\nfunctions that can reliably tell realistic data from unrealistic data. This\nproblem turns out to be significantly harder to solve and remains poorly\nunderstood, despite its prevalence in machine learning and recent breakthroughs\nin generative AI. Drawing on insights from algorithmic information theory, we\ndiscuss why this problem is challenging, why a good generative model alone is\ninsufficient to solve it, and what a good solution would look like. In\nparticular, we introduce the notion of a universal critic, which unlike\nadversarial critics does not require adversarial training. While universal\ncritics are not immediately practical, they can serve both as a North Star for\nguiding practical implementations and as a tool for analyzing existing attempts\nto capture realism.\n","authors":["Lucas Theis"],"pdf_url":"https://arxiv.org/pdf/2403.04493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05452v1","updated":"2024-03-08T16:57:54Z","published":"2024-03-08T16:57:54Z","title":"The R2D2 deep neural network series paradigm for fast precision imaging\n  in radio astronomy","summary":"  Radio-interferometric (RI) imaging entails solving high-resolution\nhigh-dynamic range inverse problems from large data volumes. Recent image\nreconstruction techniques grounded in optimization theory have demonstrated\nremarkable capability for imaging precision, well beyond CLEAN's capability.\nThese range from advanced proximal algorithms propelled by handcrafted\nregularization operators, such as the SARA family, to hybrid plug-and-play\n(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.\nOptimization and PnP structures are however highly iterative, which hinders\ntheir ability to handle the extreme data sizes expected from future\ninstruments. To address this scalability challenge, we introduce a novel deep\nlearning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic\nrange imaging'. R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. It\nthus takes a hybrid structure between a PnP algorithm and a learned version of\nthe matching pursuit algorithm that underpins CLEAN. We present a comprehensive\nstudy of our approach, featuring its multiple incarnations distinguished by\ntheir DNN architectures. We provide a detailed description of its training\nprocess, targeting a telescope-specific approach. R2D2's capability to deliver\nhigh precision is demonstrated in simulation, across a variety of image and\nobservation settings using the Very Large Array (VLA). Its reconstruction speed\nis also demonstrated: with only few iterations required to clean data residuals\nat dynamic ranges up to 105, R2D2 opens the door to fast precision imaging.\nR2D2 codes are available in the BASPLib library on GitHub.\n","authors":["Amir Aghabiglou","Chung San Chu","Arwa Dabbech","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.05452v1.pdf","comment":"20 pages, 10 figures, submitted to APJ"},{"id":"http://arxiv.org/abs/2403.05446v1","updated":"2024-03-08T16:54:27Z","published":"2024-03-08T16:54:27Z","title":"An Improved Algorithm for Learning Drifting Discrete Distributions","summary":"  We present a new adaptive algorithm for learning discrete distributions under\ndistribution drift. In this setting, we observe a sequence of independent\nsamples from a discrete distribution that is changing over time, and the goal\nis to estimate the current distribution. Since we have access to only a single\nsample for each time step, a good estimation requires a careful choice of the\nnumber of past samples to use. To use more samples, we must resort to samples\nfurther in the past, and we incur a drift error due to the bias introduced by\nthe change in distribution. On the other hand, if we use a small number of past\nsamples, we incur a large statistical error as the estimation has a high\nvariance. We present a novel adaptive algorithm that can solve this trade-off\nwithout any prior knowledge of the drift. Unlike previous adaptive results, our\nalgorithm characterizes the statistical error using data-dependent bounds. This\ntechnicality enables us to overcome the limitations of the previous work that\nrequire a fixed finite support whose size is known in advance and that cannot\nchange over time. Additionally, we can obtain tighter bounds depending on the\ncomplexity of the drifting distribution, and also consider distributions with\ninfinite support.\n","authors":["Alessio Mazzetto"],"pdf_url":"https://arxiv.org/pdf/2403.05446v1.pdf","comment":"To be published in AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.05441v1","updated":"2024-03-08T16:51:27Z","published":"2024-03-08T16:51:27Z","title":"Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity\n  Prices","summary":"  We present a first study of Bayesian forecasting of electricity prices traded\non the German continuous intraday market which fully incorporates parameter\nuncertainty. Our target variable is the IDFull price index, forecasts are given\nin terms of posterior predictive distributions. For validation we use the\nexceedingly volatile electricity prices of 2022, which have hardly been the\nsubject of forecasting studies before. As a benchmark model, we use all\navailable intraday transactions at the time of forecast creation to compute a\ncurrent value for the IDFull. According to the weak-form efficiency hypothesis,\nit would not be possible to significantly improve this benchmark built from\nlast price information. We do, however, observe statistically significant\nimprovement in terms of both point measures and probability scores. Finally, we\nchallenge the declared gold standard of using LASSO for feature selection in\nelectricity price forecasting by presenting strong statistical evidence that\nOrthogonal Matching Pursuit (OMP) leads to better forecasting performance.\n","authors":["Daniel Nickelsen","Gernot Müller"],"pdf_url":"https://arxiv.org/pdf/2403.05441v1.pdf","comment":"22 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.05440v1","updated":"2024-03-08T16:48:20Z","published":"2024-03-08T16:48:20Z","title":"Is Cosine-Similarity of Embeddings Really About Similarity?","summary":"  Cosine-similarity is the cosine of the angle between two vectors, or\nequivalently the dot product between their normalizations. A popular\napplication is to quantify semantic similarity between high-dimensional objects\nby applying cosine-similarity to a learned low-dimensional feature embedding.\nThis can work better but sometimes also worse than the unnormalized dot-product\nbetween embedded vectors in practice. To gain insight into this empirical\nobservation, we study embeddings derived from regularized linear models, where\nclosed-form solutions facilitate analytical insights. We derive analytically\nhow cosine-similarity can yield arbitrary and therefore meaningless\n`similarities.' For some linear models the similarities are not even unique,\nwhile for others they are implicitly controlled by the regularization. We\ndiscuss implications beyond linear models: a combination of different\nregularizations are employed when learning deep models; these have implicit and\nunintended effects when taking cosine-similarities of the resulting embeddings,\nrendering results opaque and possibly arbitrary. Based on these insights, we\ncaution against blindly using cosine-similarity and outline alternatives.\n","authors":["Harald Steck","Chaitanya Ekanadham","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2403.05440v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2210.03859v3","updated":"2024-03-08T16:48:08Z","published":"2022-10-08T00:47:50Z","title":"Spectrally-Corrected and Regularized Linear Discriminant Analysis for\n  Spiked Covariance Model","summary":"  This paper proposes an improved linear discriminant analysis called\nspectrally-corrected and regularized LDA (SRLDA). This method integrates the\ndesign ideas of the sample spectrally-corrected covariance matrix and the\nregularized discriminant analysis. With the support of a large-dimensional\nrandom matrix analysis framework, it is proved that SRLDA has a linear\nclassification global optimal solution under the spiked model assumption.\nAccording to simulation data analysis, the SRLDA classifier performs better\nthan RLDA and ILDA and is closer to the theoretical classifier. Experiments on\ndifferent data sets show that the SRLDA algorithm performs better in\nclassification and dimensionality reduction than currently used tools.\n","authors":["Hua Li","Wenya Luo","Zhidong Bai","Huanchao Zhou","Zhangni Pu"],"pdf_url":"https://arxiv.org/pdf/2210.03859v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07138v3","updated":"2024-03-08T16:45:37Z","published":"2023-08-31T17:35:42Z","title":"Blind Source Separation of Single-Channel Mixtures via Multi-Encoder\n  Autoencoders","summary":"  The task of blind source separation (BSS) involves separating sources from a\nmixture without prior knowledge of the sources or the mixing system.\nSingle-channel mixtures and non-linear mixtures are a particularly challenging\nproblem in BSS. In this paper, we propose a novel method for addressing BSS\nwith single-channel non-linear mixtures by leveraging the natural feature\nsubspace specialization ability of multi-encoder autoencoders. During the\ntraining phase, our method unmixes the input into the separate encoding spaces\nof the multi-encoder network and then remixes these representations within the\ndecoder for a reconstruction of the input. Then to perform source inference, we\nintroduce a novel encoding masking technique whereby masking out all but one of\nthe encodings enables the decoder to estimate a source signal. To this end, we\nalso introduce a sparse mixing loss that encourages sparse remixing of source\nencodings throughout the decoder and a so-called zero reconstruction loss on\nthe decoder for coherent source estimations. To analyze and evaluate our\nmethod, we conduct experiments on a toy dataset, designed to demonstrate this\nproperty of feature subspace specialization, and with real-world biosignal\nrecordings from a polysomnography sleep study for extracting respiration from\nelectrocardiogram and photoplethysmography signals.\n","authors":["Matthew B. Webster","Joonnyong Lee"],"pdf_url":"https://arxiv.org/pdf/2309.07138v3.pdf","comment":"24 pages (with appendix), 12 figures(with appendix), resubmitted for\n  review"},{"id":"http://arxiv.org/abs/2211.12971v2","updated":"2024-03-08T16:40:10Z","published":"2022-11-23T14:27:25Z","title":"Cooperative data-driven modeling","summary":"  Data-driven modeling in mechanics is evolving rapidly based on recent machine\nlearning advances, especially on artificial neural networks. As the field\nmatures, new data and models created by different groups become available,\nopening possibilities for cooperative modeling. However, artificial neural\nnetworks suffer from catastrophic forgetting, i.e. they forget how to perform\nan old task when trained on a new one. This hinders cooperation because\nadapting an existing model for a new task affects the performance on a previous\ntask trained by someone else. The authors developed a continual learning method\nthat addresses this issue, applying it here for the first time to solid\nmechanics. In particular, the method is applied to recurrent neural networks to\npredict history-dependent plasticity behavior, although it can be used on any\nother architecture (feedforward, convolutional, etc.) and to predict other\nphenomena. This work intends to spawn future developments on continual learning\nthat will foster cooperative strategies among the mechanics community to solve\nincreasingly challenging problems. We show that the chosen continual learning\nstrategy can sequentially learn several constitutive laws without forgetting\nthem, using less data to achieve the same error as standard (non-cooperative)\ntraining of one law per model.\n","authors":["Aleksandr Dekhovich","O. Taylan Turan","Jiaxiang Yi","Miguel A. Bessa"],"pdf_url":"https://arxiv.org/pdf/2211.12971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00158v2","updated":"2024-03-08T16:26:03Z","published":"2024-02-29T22:19:46Z","title":"Automated Efficient Estimation using Monte Carlo Efficient Influence\n  Functions","summary":"  Many practical problems involve estimating low dimensional statistical\nquantities with high-dimensional models and datasets. Several approaches\naddress these estimation tasks based on the theory of influence functions, such\nas debiased/double ML or targeted minimum loss estimation. This paper\nintroduces \\textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully\nautomated technique for approximating efficient influence functions that\nintegrates seamlessly with existing differentiable probabilistic programming\nsystems. MC-EIF automates efficient statistical estimation for a broad class of\nmodels and target functionals that would previously require rigorous custom\nanalysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF\nachieve optimal $\\sqrt{N}$ convergence rates. We show empirically that\nestimators using MC-EIF are at parity with estimators using analytic EIFs.\nFinally, we demonstrate a novel capstone example using MC-EIF for optimal\nportfolio selection.\n","authors":["Raj Agrawal","Sam Witty","Andy Zane","Eli Bingham"],"pdf_url":"https://arxiv.org/pdf/2403.00158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05406v1","updated":"2024-03-08T16:04:36Z","published":"2024-03-08T16:04:36Z","title":"Considering Nonstationary within Multivariate Time Series with\n  Variational Hierarchical Transformer for Forecasting","summary":"  The forecasting of Multivariate Time Series (MTS) has long been an important\nbut challenging task. Due to the non-stationary problem across long-distance\ntime steps, previous studies primarily adopt stationarization method to\nattenuate the non-stationary problem of the original series for better\npredictability. However, existing methods always adopt the stationarized\nseries, which ignores the inherent non-stationarity, and has difficulty in\nmodeling MTS with complex distributions due to the lack of stochasticity. To\ntackle these problems, we first develop a powerful hierarchical probabilistic\ngenerative module to consider the non-stationarity and stochastic\ncharacteristics within MTS, and then combine it with transformer for a\nwell-defined variational generative dynamic model named Hierarchical Time\nseries Variational Transformer (HTV-Trans), which recovers the intrinsic\nnon-stationary information into temporal dependencies. Being a powerful\nprobabilistic model, HTV-Trans is utilized to learn expressive representations\nof MTS and applied to forecasting tasks. Extensive experiments on diverse\ndatasets show the efficiency of HTV-Trans on MTS forecasting tasks\n","authors":["Muyao Wang","Wenchao Chen","Bo Chen"],"pdf_url":"https://arxiv.org/pdf/2403.05406v1.pdf","comment":"accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.05396v1","updated":"2024-03-08T15:51:43Z","published":"2024-03-08T15:51:43Z","title":"HistGen: Histopathology Report Generation via Local-Global Feature\n  Encoding and Cross-modal Context Interaction","summary":"  Histopathology serves as the gold standard in cancer diagnosis, with clinical\nreports being vital in interpreting and understanding this process, guiding\ncancer treatment and patient care. The automation of histopathology report\ngeneration with deep learning stands to significantly enhance clinical\nefficiency and lessen the labor-intensive, time-consuming burden on\npathologists in report writing. In pursuit of this advancement, we introduce\nHistGen, a multiple instance learning-empowered framework for histopathology\nreport generation together with the first benchmark dataset for evaluation.\nInspired by diagnostic and report-writing workflows, HistGen features two\ndelicately designed modules, aiming to boost report generation by aligning\nwhole slide images (WSIs) and diagnostic reports from local and global\ngranularity. To achieve this, a local-global hierarchical encoder is developed\nfor efficient visual feature aggregation from a region-to-slide perspective.\nMeanwhile, a cross-modal context module is proposed to explicitly facilitate\nalignment and interaction between distinct modalities, effectively bridging the\ngap between the extensive visual sequences of WSIs and corresponding highly\nsummarized reports. Experimental results on WSI report generation show the\nproposed model outperforms state-of-the-art (SOTA) models by a large margin.\nMoreover, the results of fine-tuning our model on cancer subtyping and survival\nanalysis tasks further demonstrate superior performance compared to SOTA\nmethods, showcasing strong transfer learning capability. Dataset, model\nweights, and source code are available in\nhttps://github.com/dddavid4real/HistGen.\n","authors":["Zhengrui Guo","Jiabo Ma","Yingxue Xu","Yihui Wang","Liansheng Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.05396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05395v1","updated":"2024-03-08T15:45:13Z","published":"2024-03-08T15:45:13Z","title":"Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems\n  trained with Gradient Descent","summary":"  Advanced machine learning methods, and more prominently neural networks, have\nbecome standard to solve inverse problems over the last years. However, the\ntheoretical recovery guarantees of such methods are still scarce and difficult\nto achieve. Only recently did unsupervised methods such as Deep Image Prior\n(DIP) get equipped with convergence and recovery guarantees for generic loss\nfunctions when trained through gradient flow with an appropriate\ninitialization. In this paper, we extend these results by proving that these\nguarantees hold true when using gradient descent with an appropriately chosen\nstep-size/learning rate. We also show that the discretization only affects the\noverparametrization bound for a two-layer DIP network by a constant and thus\nthat the different guarantees found for the gradient flow will hold for\ngradient descent.\n","authors":["Nathan Buskulic","Jalal Fadili","Yvain Quéau"],"pdf_url":"https://arxiv.org/pdf/2403.05395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04056v2","updated":"2024-03-08T15:43:50Z","published":"2023-11-07T15:07:08Z","title":"Multi-View Causal Representation Learning with Partial Observability","summary":"  We present a unified framework for studying the identifiability of\nrepresentations learned from simultaneously observed views, such as different\ndata modalities. We allow a partially observed setting in which each view\nconstitutes a nonlinear mixture of a subset of underlying latent variables,\nwhich can be causally related. We prove that the information shared across all\nsubsets of any number of views can be learned up to a smooth bijection using\ncontrastive learning and a single encoder per view. We also provide graphical\ncriteria indicating which latent variables can be identified through a simple\nset of rules, which we refer to as identifiability algebra. Our general\nframework and theoretical results unify and extend several previous works on\nmulti-view nonlinear ICA, disentanglement, and causal representation learning.\nWe experimentally validate our claims on numerical, image, and multi-modal data\nsets. Further, we demonstrate that the performance of prior methods is\nrecovered in different special cases of our setup. Overall, we find that access\nto multiple partial views enables us to identify a more fine-grained\nrepresentation, under the generally milder assumption of partial observability.\n","authors":["Dingling Yao","Danru Xu","Sébastien Lachapelle","Sara Magliacane","Perouz Taslakian","Georg Martius","Julius von Kügelgen","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2311.04056v2.pdf","comment":"28 pages, 10 figures, 11 tables"},{"id":"http://arxiv.org/abs/2403.05385v1","updated":"2024-03-08T15:30:58Z","published":"2024-03-08T15:30:58Z","title":"Switching the Loss Reduces the Cost in Batch Reinforcement Learning","summary":"  We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch\nreinforcement learning (RL). We show that the number of samples needed to learn\na near-optimal policy with FQI-LOG scales with the accumulated cost of the\noptimal policy, which is zero in problems where acting optimally achieves the\ngoal and incurs no cost. In doing so, we provide a general framework for\nproving $\\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal\nachievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses\nfewer samples than FQI trained with squared loss on problems where the optimal\npolicy reliably achieves the goal.\n","authors":["Alex Ayoub","Kaiwen Wang","Vincent Liu","Samuel Robertson","James McInerney","Dawen Liang","Nathan Kallus","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2403.05385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12307v3","updated":"2024-03-08T15:26:38Z","published":"2023-09-21T17:59:11Z","title":"LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models","summary":"  We present LongLoRA, an efficient fine-tuning approach that extends the\ncontext sizes of pre-trained large language models (LLMs), with limited\ncomputation cost. Typically, training LLMs with long context sizes is\ncomputationally expensive, requiring extensive training hours and GPU\nresources. For example, training on the context length of 8192 needs 16x\ncomputational costs in self-attention layers as that of 2048. In this paper, we\nspeed up the context extension of LLMs in two aspects. On the one hand,\nalthough dense global attention is needed during inference, fine-tuning the\nmodel can be effectively and efficiently done by sparse local attention. The\nproposed shifted sparse attention effectively enables context extension,\nleading to non-trivial computation saving with similar performance to\nfine-tuning with vanilla attention. Particularly, it can be implemented with\nonly two lines of code in training, while being optional in inference. On the\nother hand, we revisit the parameter-efficient fine-tuning regime for context\nexpansion. Notably, we find that LoRA for context extension works well under\nthe premise of trainable embedding and normalization. LongLoRA combines this\nimproved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on\nvarious tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B\nfrom 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine.\nLongLoRA extends models' context while retaining their original architectures,\nand is compatible with most existing techniques, like Flash-Attention2. In\naddition, we further conduct supervised fine-tuning with LongLoRA and our long\ninstruction-following LongAlpaca dataset.\n","authors":["Yukang Chen","Shengju Qian","Haotian Tang","Xin Lai","Zhijian Liu","Song Han","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2309.12307v3.pdf","comment":"Code, models, dataset, and demo are available at\n  https://github.com/dvlab-research/LongLoRA"},{"id":"http://arxiv.org/abs/2403.01192v2","updated":"2024-03-08T15:18:19Z","published":"2024-03-02T12:12:04Z","title":"A Composite Decomposition Method for Large-Scale Global Optimization","summary":"  Cooperative co-evolution (CC) algorithms, based on the divide-and-conquer\nstrategy, have emerged as the predominant approach to solving large-scale\nglobal optimization (LSGO) problems. The efficiency and accuracy of the\ngrouping stage significantly impact the performance of the optimization\nprocess. While the general separability grouping (GSG) method has overcome the\nlimitation of previous differential grouping (DG) methods by enabling the\ndecomposition of non-additively separable functions, it suffers from high\ncomputational complexity. To address this challenge, this article proposes a\ncomposite separability grouping (CSG) method, seamlessly integrating DG and GSG\ninto a problem decomposition framework to utilize the strengths of both\napproaches. CSG introduces a step-by-step decomposition framework that\naccurately decomposes various problem types using fewer computational\nresources. By sequentially identifying additively, multiplicatively and\ngenerally separable variables, CSG progressively groups non-separable variables\nby recursively considering the interactions between each non-separable variable\nand the formed non-separable groups. Furthermore, to enhance the efficiency and\naccuracy of CSG, we introduce two innovative methods: a multiplicatively\nseparable variable detection method and a non-separable variable grouping\nmethod. These two methods are designed to effectively detect multiplicatively\nseparable variables and efficiently group non-separable variables,\nrespectively. Extensive experimental results demonstrate that CSG achieves more\naccurate variable grouping with lower computational complexity compared to GSG\nand state-of-the-art DG series designs.\n","authors":["Maojiang Tian","Minyang Chen","Wei Du","Yang Tang","Yaochu Jin","Gary G. Yen"],"pdf_url":"https://arxiv.org/pdf/2403.01192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14809v2","updated":"2024-03-08T15:15:47Z","published":"2024-02-22T18:59:02Z","title":"CriticBench: Benchmarking LLMs for Critique-Correct Reasoning","summary":"  The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.\n","authors":["Zicheng Lin","Zhibin Gou","Tian Liang","Ruilin Luo","Haowei Liu","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2402.14809v2.pdf","comment":"Corrected computation errors in Tables 1, 7-11; updated corresponding\n  figs"},{"id":"http://arxiv.org/abs/2309.06782v5","updated":"2024-03-08T15:01:11Z","published":"2023-09-13T08:16:15Z","title":"Improved particle-flow event reconstruction with scalable neural\n  networks for current and future particle detectors","summary":"  Efficient and accurate algorithms are necessary to reconstruct particles in\nthe highly granular detectors anticipated at the High-Luminosity Large Hadron\nCollider and the Future Circular Collider. We study scalable machine learning\nmodels for event reconstruction in electron-positron collisions based on a full\ndetector simulation. Particle-flow reconstruction can be formulated as a\nsupervised learning task using tracks and calorimeter clusters. We compare a\ngraph neural network and kernel-based transformer and demonstrate that we can\navoid quadratic operations while achieving realistic reconstruction. We show\nthat hyperparameter tuning significantly improves the performance of the\nmodels. The best graph neural network model shows improvement in the jet\ntransverse momentum resolution by up to 50% compared to the rule-based\nalgorithm. The resulting model is portable across Nvidia, AMD and Habana\nhardware. Accurate and fast machine-learning based reconstruction can\nsignificantly improve future measurements at colliders.\n","authors":["Joosep Pata","Eric Wulff","Farouk Mokhtar","David Southwick","Mengke Zhang","Maria Girone","Javier Duarte"],"pdf_url":"https://arxiv.org/pdf/2309.06782v5.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.05368v1","updated":"2024-03-08T14:59:15Z","published":"2024-03-08T14:59:15Z","title":"Exploring the Links between the Fundamental Lemma and Kernel Regression","summary":"  Generalizations and variations of the fundamental lemma by Willems et al. are\nan active topic of recent research. In this note, we explore and formalize the\nlinks between kernel regression and known nonlinear extensions of the\nfundamental lemma. Applying a transformation to the usual linear equation in\nHankel matrices, we arrive at an alternative implicit kernel representation of\nthe system trajectories while keeping the requirements on persistency of\nexcitation. We show that this representation is equivalent to the solution of a\nspecific kernel regression problem. We explore the possible structures of the\nunderlying kernel as well as the system classes to which they correspond.\n","authors":["Oleksii Molodchyk","Timm Faulwasser"],"pdf_url":"https://arxiv.org/pdf/2403.05368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05365v1","updated":"2024-03-08T14:55:05Z","published":"2024-03-08T14:55:05Z","title":"The Impact of Quantization on the Robustness of Transformer-based Text\n  Classifiers","summary":"  Transformer-based models have made remarkable advancements in various NLP\nareas. Nevertheless, these models often exhibit vulnerabilities when confronted\nwith adversarial attacks. In this paper, we explore the effect of quantization\non the robustness of Transformer-based models. Quantization usually involves\nmapping a high-precision real number to a lower-precision value, aiming at\nreducing the size of the model at hand. To the best of our knowledge, this work\nis the first application of quantization on the robustness of NLP models. In\nour experiments, we evaluate the impact of quantization on BERT and DistilBERT\nmodels in text classification using SST-2, Emotion, and MR datasets. We also\nevaluate the performance of these models against TextFooler, PWWS, and PSO\nadversarial attacks. Our findings show that quantization significantly improves\n(by an average of 18.68%) the adversarial accuracy of the models. Furthermore,\nwe compare the effect of quantization versus that of the adversarial training\napproach on robustness. Our experiments indicate that quantization increases\nthe robustness of the model by 18.80% on average compared to adversarial\ntraining without imposing any extra computational overhead during training.\nTherefore, our results highlight the effectiveness of quantization in improving\nthe robustness of NLP models.\n","authors":["Seyed Parsa Neshaei","Yasaman Boreshban","Gholamreza Ghassem-Sani","Seyed Abolghasem Mirroshandel"],"pdf_url":"https://arxiv.org/pdf/2403.05365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10139v2","updated":"2024-03-08T14:51:55Z","published":"2023-03-17T17:27:18Z","title":"Distill n' Explain: explaining graph neural networks using simple\n  surrogates","summary":"  Explaining node predictions in graph neural networks (GNNs) often boils down\nto finding graph substructures that preserve predictions. Finding these\nstructures usually implies back-propagating through the GNN, bonding the\ncomplexity (e.g., number of layers) of the GNN to the cost of explaining it.\nThis naturally begs the question: Can we break this bond by explaining a\nsimpler surrogate GNN? To answer the question, we propose Distill n' Explain\n(DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX\nextracts node or edge-level explanations by solving a simple convex program. We\nalso propose FastDnX, a faster version of DnX that leverages the linear\ndecomposition of our surrogate model. Experiments show that DnX and FastDnX\noften outperform state-of-the-art GNN explainers while being orders of\nmagnitude faster. Additionally, we support our empirical findings with\ntheoretical results linking the quality of the surrogate model (i.e.,\ndistillation error) to the faithfulness of explanations.\n","authors":["Tamara Pereira","Erik Nascimento","Lucas E. Resck","Diego Mesquita","Amauri Souza"],"pdf_url":"https://arxiv.org/pdf/2303.10139v2.pdf","comment":"To appear in AISTATS 2023"},{"id":"http://arxiv.org/abs/2403.03020v2","updated":"2024-03-08T14:51:29Z","published":"2024-03-05T14:57:04Z","title":"SplAgger: Split Aggregation for Meta-Reinforcement Learning","summary":"  A core ambition of reinforcement learning (RL) is the creation of agents\ncapable of rapid learning in novel tasks. Meta-RL aims to achieve this by\ndirectly learning such agents. Black box methods do so by training\noff-the-shelf sequence models end-to-end. By contrast, task inference methods\nexplicitly infer a posterior distribution over the unknown task, typically\nusing distinct objectives and sequence models designed to enable task\ninference. Recent work has shown that task inference methods are not necessary\nfor strong performance. However, it remains unclear whether task inference\nsequence models are beneficial even when task inference objectives are not. In\nthis paper, we present strong evidence that task inference sequence models are\nstill beneficial. In particular, we investigate sequence models with\npermutation invariant aggregation, which exploit the fact that, due to the\nMarkov property, the task posterior does not depend on the order of data. We\nempirically confirm the advantage of permutation invariant sequence models\nwithout the use of task inference objectives. However, we also find,\nsurprisingly, that there are multiple conditions under which permutation\nvariance remains useful. Therefore, we propose SplAgger, which uses both\npermutation variant and invariant components to achieve the best of both\nworlds, outperforming all baselines on continuous control and memory\nenvironments.\n","authors":["Jacob Beck","Matthew Jackson","Risto Vuorio","Zheng Xiong","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2403.03020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04605v2","updated":"2024-03-08T14:49:34Z","published":"2024-03-07T15:54:46Z","title":"In-n-Out: Calibrating Graph Neural Networks for Link Prediction","summary":"  Deep neural networks are notoriously miscalibrated, i.e., their outputs do\nnot reflect the true probability of the event we aim to predict. While networks\nfor tabular or image data are usually overconfident, recent works have shown\nthat graph neural networks (GNNs) show the opposite behavior for node-level\nclassification. But what happens when we are predicting links? We show that, in\nthis case, GNNs often exhibit a mixed behavior. More specifically, they may be\noverconfident in negative predictions while being underconfident in positive\nones. Based on this observation, we propose IN-N-OUT, the first-ever method to\ncalibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions:\ni) attributing true/false labels to an edge while respecting a GNNs prediction\nshould cause but small fluctuations in that edge's embedding; and, conversely,\nii) if we label that same edge contradicting our GNN, embeddings should change\nmore substantially. An extensive experimental campaign shows that IN-N-OUT\nsignificantly improves the calibration of GNNs in link prediction, consistently\noutperforming the baselines available -- which are not designed for this\nspecific task.\n","authors":["Erik Nascimento","Diego Mesquita","Samuel Kaski","Amauri H Souza"],"pdf_url":"https://arxiv.org/pdf/2403.04605v2.pdf","comment":"18 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.05358v1","updated":"2024-03-08T14:45:18Z","published":"2024-03-08T14:45:18Z","title":"Variational Inference of Parameters in Opinion Dynamics Models","summary":"  Despite the frequent use of agent-based models (ABMs) for studying social\nphenomena, parameter estimation remains a challenge, often relying on costly\nsimulation-based heuristics. This work uses variational inference to estimate\nthe parameters of an opinion dynamics ABM, by transforming the estimation\nproblem into an optimization task that can be solved directly.\n  Our proposal relies on probabilistic generative ABMs (PGABMs): we start by\nsynthesizing a probabilistic generative model from the ABM rules. Then, we\ntransform the inference process into an optimization problem suitable for\nautomatic differentiation. In particular, we use the Gumbel-Softmax\nreparameterization for categorical agent attributes and stochastic variational\ninference for parameter estimation. Furthermore, we explore the trade-offs of\nusing variational distributions with different complexity: normal distributions\nand normalizing flows.\n  We validate our method on a bounded confidence model with agent roles\n(leaders and followers). Our approach estimates both macroscopic (bounded\nconfidence intervals and backfire thresholds) and microscopic ($200$\ncategorical, agent-level roles) more accurately than simulation-based and MCMC\nmethods. Consequently, our technique enables experts to tune and validate their\nABMs against real-world observations, thus providing insights into human\nbehavior in social systems via data-driven analysis.\n","authors":["Jacopo Lenti","Fabrizio Silvestri","Gianmarco De Francisci Morales"],"pdf_url":"https://arxiv.org/pdf/2403.05358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05353v1","updated":"2024-03-08T14:34:32Z","published":"2024-03-08T14:34:32Z","title":"Hybridized Convolutional Neural Networks and Long Short-Term Memory for\n  Improved Alzheimer's Disease Diagnosis from MRI Scans","summary":"  Brain-related diseases are more sensitive than other diseases due to several\nfactors, including the complexity of surgical procedures, high costs, and other\nchallenges. Alzheimer's disease is a common brain disorder that causes memory\nloss and the shrinking of brain cells. Early detection is critical for\nproviding proper treatment to patients. However, identifying Alzheimer's at an\nearly stage using manual scanning of CT or MRI scans is challenging. Therefore,\nresearchers have delved into the exploration of computer-aided systems,\nemploying Machine Learning and Deep Learning methodologies, which entail the\ntraining of datasets to detect Alzheimer's disease. This study aims to present\na hybrid model that combines a CNN model's feature extraction capabilities with\nan LSTM model's detection capabilities. This study has applied the transfer\nlearning called VGG16 in the hybrid model to extract features from MRI images.\nThe LSTM detects features between the convolution layer and the fully connected\nlayer. The output layer of the fully connected layer uses the softmax function.\nThe training of the hybrid model involved utilizing the ADNI dataset. The trial\nfindings revealed that the model achieved a level of accuracy of 98.8%, a\nsensitivity rate of 100%, and a specificity rate of 76%. The proposed hybrid\nmodel outperforms its contemporary CNN counterparts, showcasing a superior\nperformance.\n","authors":["Maleka Khatun","Md Manowarul Islam","Habibur Rahman Rifat","Md. Shamim Bin Shahid","Md. Alamin Talukder","Md Ashraf Uddin"],"pdf_url":"https://arxiv.org/pdf/2403.05353v1.pdf","comment":"Accepted In The 26th International Conference on Computer and\n  Information Technology (ICCIT) On 13-15 December 2023"},{"id":"http://arxiv.org/abs/2403.04650v2","updated":"2024-03-08T14:29:41Z","published":"2024-03-07T16:50:25Z","title":"Context-Based Multimodal Fusion","summary":"  The fusion models, which effectively combine information from different\nsources, are widely used in solving multimodal tasks. However, they have\nsignificant limitations related to aligning data distributions across different\nmodalities. This challenge can lead to inconsistencies and difficulties in\nlearning robust representations. Alignment models, while specifically\naddressing this issue, often require training \"from scratch\" with large\ndatasets to achieve optimal results, which can be costly in terms of resources\nand time. To overcome these limitations, we propose an innovative model called\nContext-Based Multimodal Fusion (CBMF), which combines both modality fusion and\ndata distribution alignment. In CBMF, each modality is represented by a\nspecific context vector, fused with the embedding of each modality. This\nenables the use of large pre-trained models that can be frozen, reducing the\ncomputational and training data requirements. Additionally, the network learns\nto differentiate embeddings of different modalities through fusion with context\nand aligns data distributions using a contrastive approach for self-supervised\nlearning. Thus, CBMF offers an effective and economical solution for solving\ncomplex multimodal tasks.\n","authors":["Bilal Faye","Hanane Azzag","Mustapha Lebbah","Djamel Bouchaffra"],"pdf_url":"https://arxiv.org/pdf/2403.04650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05340v1","updated":"2024-03-08T14:17:07Z","published":"2024-03-08T14:17:07Z","title":"Embedded Deployment of Semantic Segmentation in Medicine through\n  Low-Resolution Inputs","summary":"  When deploying neural networks in real-life situations, the size and\ncomputational effort are often the limiting factors. This is especially true in\nenvironments where big, expensive hardware is not affordable, like in embedded\nmedical devices, where budgets are often tight. State-of-the-art proposed\nmultiple different lightweight solutions for such use cases, mostly by changing\nthe base model architecture, not taking the input and output resolution into\nconsideration. In this paper, we propose our architecture that takes advantage\nof the fact that in hardware-limited environments, we often refrain from using\nthe highest available input resolutions to guarantee a higher throughput.\nAlthough using lower-resolution input leads to a significant reduction in\ncomputing and memory requirements, it may also incur reduced prediction\nquality. Our architecture addresses this problem by exploiting the fact that we\ncan still utilize high-resolution ground-truths in training. The proposed model\ninputs lower-resolution images and high-resolution ground truths, which can\nimprove the prediction quality by 5.5% while adding less than 200 parameters to\nthe model. %reducing the frames per second only from 25 to 20. We conduct an\nextensive analysis to illustrate that our architecture enhances existing\nstate-of-the-art frameworks for lightweight semantic segmentation of cancer in\nMRI images. We also tested the deployment speed of state-of-the-art lightweight\nnetworks and our architecture on Nvidia's Jetson Nano to emulate deployment in\nresource-constrained embedded scenarios.\n","authors":["Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.05340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10486v2","updated":"2024-03-08T14:10:02Z","published":"2023-10-16T15:06:16Z","title":"ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse\n  Quadruped Robots","summary":"  Learning a locomotion policy for quadruped robots has traditionally been\nconstrained to a specific robot morphology, mass, and size. The learning\nprocess must usually be repeated for every new robot, where hyperparameters and\nreward function weights must be re-tuned to maximize performance for each new\nsystem. Alternatively, attempting to train a single policy to accommodate\ndifferent robot sizes, while maintaining the same degrees of freedom (DoF) and\nmorphology, requires either complex learning frameworks, or mass, inertia, and\ndimension randomization, which leads to prolonged training periods. In our\nstudy, we show that drawing inspiration from animal motor control allows us to\neffectively train a single locomotion policy capable of controlling a diverse\nrange of quadruped robots. The robot differences encompass: a variable number\nof DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass\nrange spanning from 2 kg to 200 kg, and nominal standing heights ranging from\n18 cm to 100 cm. Our policy modulates a representation of the Central Pattern\nGenerator (CPG) in the spinal cord, effectively coordinating both frequencies\nand amplitudes of the CPG to produce rhythmic output (Rhythm Generation), which\nis then mapped to a Pattern Formation (PF) layer. Across different robots, the\nonly varying component is the PF layer, which adjusts the scaling parameters\nfor the stride height and length. Subsequently, we evaluate the sim-to-real\ntransfer by testing the single policy on both the Unitree Go1 and A1 robots.\nRemarkably, we observe robust performance, even when adding a 15 kg load,\nequivalent to 125% of the A1 robot's nominal mass.\n","authors":["Milad Shafiee","Guillaume Bellegarda","Auke Ijspeert"],"pdf_url":"https://arxiv.org/pdf/2310.10486v2.pdf","comment":"Accepted for IEEE International Conference on Robotics and Automation\n  (ICRA) 2024, Webpage: https://miladshafiee.github.io/ManyQuadrupeds/"},{"id":"http://arxiv.org/abs/2403.05318v1","updated":"2024-03-08T13:49:21Z","published":"2024-03-08T13:49:21Z","title":"Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling\n  Salesman Problem","summary":"  Many real-world problems can be formulated as a constrained Traveling\nSalesman Problem (TSP). However, the constraints are always complex and\nnumerous, making the TSPs challenging to solve. When the number of complicated\nconstraints grows, it is time-consuming for traditional heuristic algorithms to\navoid illegitimate outcomes. Learning-based methods provide an alternative to\nsolve TSPs in a soft manner, which also supports GPU acceleration to generate\nsolutions quickly. Nevertheless, the soft manner inevitably results in\ndifficulty solving hard-constrained problems with learning algorithms, and the\nconflicts between legality and optimality may substantially affect the\noptimality of the solution. To overcome this problem and to have an effective\nsolution against hard constraints, we proposed a novel learning-based method\nthat uses looking-ahead information as the feature to improve the legality of\nTSP with Time Windows (TSPTW) solutions. Besides, we constructed TSPTW datasets\nwith hard constraints in order to accurately evaluate and benchmark the\nstatistical performance of various approaches, which can serve the community\nfor future research. With comprehensive experiments on diverse datasets, MUSLA\noutperforms existing baselines and shows generalizability potential.\n","authors":["Jingxiao Chen","Ziqin Gong","Minghuan Liu","Jun Wang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08977v2","updated":"2024-03-08T13:37:55Z","published":"2024-01-17T05:04:33Z","title":"FedLoGe: Joint Local and Generic Federated Learning under Long-tailed\n  Data","summary":"  Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected\nfrom decentralized local clients manifests a globally prevalent long-tailed\ndistribution, has garnered considerable attention in recent times. In the\ncontext of Fed-LT, existing works have predominantly centered on addressing the\ndata imbalance issue to enhance the efficacy of the generic global model while\nneglecting the performance at the local level. In contrast, conventional\nPersonalized Federated Learning (pFL) techniques are primarily devised to\noptimize personalized local models under the presumption of a balanced global\ndata distribution. This paper introduces an approach termed Federated Local and\nGeneric Model Training in Fed-LT (FedLoGe), which enhances both local and\ngeneric model performance through the integration of representation learning\nand classifier alignment within a neural collapse framework. Our investigation\nreveals the feasibility of employing a shared backbone as a foundational\nframework for capturing overarching global trends, while concurrently employing\nindividualized classifiers to encapsulate distinct refinements stemming from\neach client's local features. Building upon this discovery, we establish the\nStatic Sparse Equiangular Tight Frame Classifier (SSE-C), inspired by neural\ncollapse principles that naturally prune extraneous noisy features and foster\nthe acquisition of potent data representations. Furthermore, leveraging\ninsights from imbalance neural collapse's classifier norm patterns, we develop\nGlobal and Local Adaptive Feature Realignment (GLA-FR) via an auxiliary global\nclassifier and personalized Euclidean norm transfer to align global features\nwith client preferences. Extensive experimental results on CIFAR-10/100-LT,\nImageNet, and iNaturalist demonstrate the advantage of our method over\nstate-of-the-art pFL and Fed-LT approaches.\n","authors":["Zikai Xiao","Zihan Chen","Liyinglan Liu","Yang Feng","Jian Wu","Wanlu Liu","Joey Tianyi Zhou","Howard Hao Yang","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2401.08977v2.pdf","comment":"Accepted by ICLR 2024, code: https://github.com/ZackZikaiXiao/FedLoGe"},{"id":"http://arxiv.org/abs/2310.19812v2","updated":"2024-03-08T13:35:13Z","published":"2023-10-18T09:51:38Z","title":"Brain decoding: toward real-time reconstruction of visual perception","summary":"  In the past five years, the use of generative and foundational AI systems has\ngreatly improved the decoding of brain activity. Visual perception, in\nparticular, can now be decoded from functional Magnetic Resonance Imaging\n(fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers\nfrom a limited temporal resolution ($\\approx$0.5 Hz) and thus fundamentally\nconstrains its real-time usage. Here, we propose an alternative approach based\non magnetoencephalography (MEG), a neuroimaging device capable of measuring\nbrain activity with high temporal resolution ($\\approx$5,000 Hz). For this, we\ndevelop an MEG decoding model trained with both contrastive and regression\nobjectives and consisting of three modules: i) pretrained embeddings obtained\nfrom the image, ii) an MEG module trained end-to-end and iii) a pretrained\nimage generator. Our results are threefold: Firstly, our MEG decoder shows a 7X\nimprovement of image-retrieval over classic linear decoders. Second, late brain\nresponses to images are best decoded with DINOv2, a recent foundational image\nmodel. Third, image retrievals and generations both suggest that high-level\nvisual features can be decoded from MEG signals, although the same approach\napplied to 7T fMRI also recovers better low-level features. Overall, these\nresults, while preliminary, provide an important step towards the decoding --\nin real-time -- of the visual processes continuously unfolding within the human\nbrain.\n","authors":["Yohann Benchetrit","Hubert Banville","Jean-Rémi King"],"pdf_url":"https://arxiv.org/pdf/2310.19812v2.pdf","comment":"25 pages, 13 figures, updated version following acceptance at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2402.01943v2","updated":"2024-03-08T13:34:12Z","published":"2024-02-02T22:39:50Z","title":"Precedence-Constrained Winter Value for Effective Graph Data Valuation","summary":"  Data valuation is essential for quantifying data's worth, aiding in assessing\ndata quality and determining fair compensation. While existing data valuation\nmethods have proven effective in evaluating the value of Euclidean data, they\nface limitations when applied to the increasingly popular graph-structured\ndata. Particularly, graph data valuation introduces unique challenges,\nprimarily stemming from the intricate dependencies among nodes and the\nexponential growth in value estimation costs. To address the challenging\nproblem of graph data valuation, we put forth an innovative solution,\nPrecedence-Constrained Winter (PC-Winter) Value, to account for the complex\ngraph structure. Furthermore, we develop a variety of strategies to address the\ncomputational challenges and enable efficient approximation of PC-Winter.\nExtensive experiments demonstrate the effectiveness of PC-Winter across diverse\ndatasets and tasks.\n","authors":["Hongliang Chi","Wei Jin","Charu Aggarwal","Yao Ma"],"pdf_url":"https://arxiv.org/pdf/2402.01943v2.pdf","comment":"17 pages in total"},{"id":"http://arxiv.org/abs/2403.05300v1","updated":"2024-03-08T13:29:46Z","published":"2024-03-08T13:29:46Z","title":"Unity by Diversity: Improved Representation Learning in Multimodal VAEs","summary":"  Variational Autoencoders for multimodal data hold promise for many tasks in\ndata analysis, such as representation learning, conditional generation, and\nimputation. Current architectures either share the encoder output, decoder\ninput, or both across modalities to learn a shared representation. Such\narchitectures impose hard constraints on the model. In this work, we show that\na better latent representation can be obtained by replacing these hard\nconstraints with a soft constraint. We propose a new mixture-of-experts prior,\nsoftly guiding each modality's latent representation towards a shared aggregate\nposterior. This approach results in a superior latent representation and allows\neach encoding to preserve information from its uncompressed original features\nbetter. In extensive experiments on multiple benchmark datasets and a\nchallenging real-world neuroscience data set, we show improved learned latent\nrepresentations and imputation of missing data modalities compared to existing\nmethods.\n","authors":["Thomas M. Sutter","Yang Meng","Norbert Fortin","Julia E. Vogt","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2403.05300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13454v2","updated":"2024-03-08T13:29:29Z","published":"2023-12-20T22:13:45Z","title":"MixEHR-SurG: a joint proportional hazard and guided topic model for\n  inferring mortality-associated topics from electronic health records","summary":"  Existing survival models either do not scale to high dimensional and\nmulti-modal data or are difficult to interpret. In this study, we present a\nsupervised topic model called MixEHR-SurG to simultaneously integrate\nheterogeneous EHR data and model survival hazard. Our contributions are\nthree-folds: (1) integrating EHR topic inference with Cox proportional hazards\nlikelihood; (2) integrating patient-specific topic hyperparameters using the\nPheCode concepts such that each topic can be identified with exactly one\nPheCode-associated phenotype; (3) multi-modal survival topic inference. This\nleads to a highly interpretable survival topic model that can infer\nPheCode-specific phenotype topics associated with patient mortality. We\nevaluated MixEHR-SurG using a simulated dataset and two real-world EHR\ndatasets: the Quebec Congenital Heart Disease (CHD) data consisting of 8,211\nsubjects with 75,187 outpatient claim records of 1,767 unique ICD codes; the\nMIMIC-III consisting of 1,458 subjects with multi-modal EHR records. Compared\nto the baselines, MixEHR-SurG achieved a superior dynamic AUROC for mortality\nprediction, with a mean AUROC score of 0.89 in the simulation dataset and a\nmean AUROC of 0.645 on the CHD dataset. Qualitatively, MixEHR-SurG associates\nsevere cardiac conditions with high mortality risk among the CHD patients after\nthe first heart failure hospitalization and critical brain injuries with\nincreased mortality among the MIMIC- III patients after their ICU discharge.\nTogether, the integration of the Cox proportional hazards model and EHR topic\ninference in MixEHR-SurG not only leads to competitive mortality prediction but\nalso meaningful phenotype topics for in-depth survival analysis. The software\nis available at GitHub: https://github.com/li-lab-mcgill/MixEHR-SurG.\n","authors":["Yixuan Li","Ariane Marelli","Archer Y. Yang","Yue Li"],"pdf_url":"https://arxiv.org/pdf/2312.13454v2.pdf","comment":"45 pages total, 19 pages main text, 6 main figures, 10 supplementary\n  figrues"},{"id":"http://arxiv.org/abs/2403.05293v1","updated":"2024-03-08T13:21:07Z","published":"2024-03-08T13:21:07Z","title":"Leveraging Continuous Time to Understand Momentum When Training Diagonal\n  Linear Networks","summary":"  In this work, we investigate the effect of momentum on the optimisation\ntrajectory of gradient descent. We leverage a continuous-time approach in the\nanalysis of momentum gradient descent with step size $\\gamma$ and momentum\nparameter $\\beta$ that allows us to identify an intrinsic quantity $\\lambda =\n\\frac{ \\gamma }{ (1 - \\beta)^2 }$ which uniquely defines the optimisation path\nand provides a simple acceleration rule. When training a $2$-layer diagonal\nlinear network in an overparametrised regression setting, we characterise the\nrecovered solution through an implicit regularisation problem. We then prove\nthat small values of $\\lambda$ help to recover sparse solutions. Finally, we\ngive similar but weaker results for stochastic momentum gradient descent. We\nprovide numerical experiments which support our claims.\n","authors":["Hristo Papazov","Scott Pesme","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2403.05293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05290v1","updated":"2024-03-08T13:16:17Z","published":"2024-03-08T13:16:17Z","title":"Foundational propositions of hesitant fuzzy soft $β$-covering\n  approximation spaces","summary":"  Soft set theory serves as a mathematical framework for handling uncertain\ninformation, and hesitant fuzzy sets find extensive application in scenarios\ninvolving uncertainty and hesitation. Hesitant fuzzy sets exhibit diverse\nmembership degrees, giving rise to various forms of inclusion relationships\namong them. This article introduces the notions of hesitant fuzzy soft\n$\\beta$-coverings and hesitant fuzzy soft $\\beta$-neighborhoods, which are\nformulated based on distinct forms of inclusion relationships among hesitancy\nfuzzy sets. Subsequently, several associated properties are investigated.\nAdditionally, specific variations of hesitant fuzzy soft $\\beta$-coverings are\nintroduced by incorporating hesitant fuzzy rough sets, followed by an\nexploration of properties pertaining to hesitant fuzzy soft $\\beta$-covering\napproximation spaces.\n","authors":["Shizhan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.05290v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.05268v1","updated":"2024-03-08T12:45:53Z","published":"2024-03-08T12:45:53Z","title":"Deep Prompt Multi-task Network for Abuse Language Detection","summary":"  The detection of abusive language remains a long-standing challenge with the\nextensive use of social networks. The detection task of abusive language\nsuffers from limited accuracy. We argue that the existing detection methods\nutilize the fine-tuning technique of the pre-trained language models (PLMs) to\nhandle downstream tasks. Hence, these methods fail to stimulate the general\nknowledge of the PLMs. To address the problem, we propose a novel Deep Prompt\nMulti-task Network (DPMN) for abuse language detection. Specifically, DPMN\nfirst attempts to design two forms of deep prompt tuning and light prompt\ntuning for the PLMs. The effects of different prompt lengths, tuning\nstrategies, and prompt initialization methods on detecting abusive language are\nstudied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which\ncan be used as a short text classifier. Eventually, DPMN utilizes multi-task\nlearning to improve detection metrics further. The multi-task network has the\nfunction of transferring effective knowledge. The proposed DPMN is evaluated\nagainst eight typical methods on three public datasets: OLID, SOLID, and\nAbuseAnalyzer. The experimental results show that our DPMN outperforms the\nstate-of-the-art methods.\n","authors":["Jian Zhu","Yuping Ruan","Jingfei Chang","Cheng Luo"],"pdf_url":"https://arxiv.org/pdf/2403.05268v1.pdf","comment":"Submitted to the International Conference on Pattern Recognition\n  (ICPR) 2024"},{"id":"http://arxiv.org/abs/2403.05266v1","updated":"2024-03-08T12:42:36Z","published":"2024-03-08T12:42:36Z","title":"ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models","summary":"  Large language models (LLMs) have achieved unprecedented performance in\nvarious applications, yet their evaluation remains a critical issue. Existing\nhallucination benchmarks are either static or lack adjustable complexity for\nthorough analysis. We contend that utilizing existing relational databases is a\npromising approach for constructing benchmarks due to their accurate knowledge\ndescription via functional dependencies. We propose ERBench to automatically\nconvert any relational database into a benchmark based on the\nentity-relationship (ER) model. Our key idea is to construct questions using\nthe database schema, records, and functional dependencies such that they can be\nautomatically verified. In addition, we use foreign key constraints to join\nrelations and construct multihop questions, which can be arbitrarily complex\nand used to debug the intermediate answers of LLMs. Finally, ERBench supports\ncontinuous evaluation, multimodal questions, and various prompt engineering\ntechniques. In our experiments, we construct an LLM benchmark using databases\nof multiple domains and make an extensive comparison of contemporary LLMs. We\nobserve that better LLMs like GPT-4 can handle a larger variety of question\ntypes, but are by no means perfect. Also, correct answers do not necessarily\nimply correct rationales, which is an important evaluation that ERBench does\nbetter than other benchmarks for various question types. Code is available at\nhttps: //github.com/DILAB-KAIST/ERBench.\n","authors":["Jio Oh","Soyeon Kim","Junseok Seo","Jindong Wang","Ruochen Xu","Xing Xie","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2403.05266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03357v2","updated":"2024-03-08T12:38:27Z","published":"2023-02-07T10:02:05Z","title":"Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair\n  Mining Approach","summary":"  Not all positive pairs are beneficial to time series contrastive learning. In\nthis paper, we study two types of bad positive pairs that can impair the\nquality of time series representation learned through contrastive learning: the\nnoisy positive pair and the faulty positive pair. We observe that, with the\npresence of noisy positive pairs, the model tends to simply learn the pattern\nof noise (Noisy Alignment). Meanwhile, when faulty positive pairs arise, the\nmodel wastes considerable amount of effort aligning non-representative patterns\n(Faulty Alignment). To address this problem, we propose a Dynamic Bad Pair\nMining (DBPM) algorithm, which reliably identifies and suppresses bad positive\npairs in time series contrastive learning. Specifically, DBPM utilizes a memory\nmodule to dynamically track the training behavior of each positive pair along\ntraining process. This allows us to identify potential bad positive pairs at\neach epoch based on their historical training behaviors. The identified bad\npairs are subsequently down-weighted through a transformation module, thereby\nmitigating their negative impact on the representation learning process. DBPM\nis a simple algorithm designed as a lightweight plug-in without learnable\nparameters to enhance the performance of existing state-of-the-art methods.\nThrough extensive experiments conducted on four large-scale, real-world time\nseries datasets, we demonstrate DBPM's efficacy in mitigating the adverse\neffects of bad positive pairs.\n","authors":["Xiang Lan","Hanshu Yan","Shenda Hong","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2302.03357v2.pdf","comment":"ICLR 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2312.08852v2","updated":"2024-03-08T12:29:44Z","published":"2023-12-13T17:59:07Z","title":"ERASE: Error-Resilient Representation Learning on Graphs for Label Noise\n  Tolerance","summary":"  Deep learning has achieved remarkable success in graph-related tasks, yet\nthis accomplishment heavily relies on large-scale high-quality annotated\ndatasets. However, acquiring such datasets can be cost-prohibitive, leading to\nthe practical use of labels obtained from economically efficient sources such\nas web searches and user tags. Unfortunately, these labels often come with\nnoise, compromising the generalization performance of deep networks. To tackle\nthis challenge and enhance the robustness of deep learning models against label\nnoise in graph-based tasks, we propose a method called ERASE (Error-Resilient\nrepresentation learning on graphs for lAbel noiSe tolerancE). The core idea of\nERASE is to learn representations with error tolerance by maximizing coding\nrate reduction. Particularly, we introduce a decoupled label propagation method\nfor learning representations. Before training, noisy labels are pre-corrected\nthrough structural denoising. During training, ERASE combines prototype\npseudo-labels with propagated denoised labels and updates representations with\nerror resilience, which significantly improves the generalization performance\nin node classification. The proposed method allows us to more effectively\nwithstand errors caused by mislabeled nodes, thereby strengthening the\nrobustness of deep networks in handling noisy graph data. Extensive\nexperimental results show that our method can outperform multiple baselines\nwith clear margins in broad noise levels and enjoy great scalability. Codes are\nreleased at https://github.com/eraseai/erase.\n","authors":["Ling-Hao Chen","Yuanshuo Zhang","Taohua Huang","Liangcai Su","Zeyi Lin","Xi Xiao","Xiaobo Xia","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2312.08852v2.pdf","comment":"24 pages, 14 figures, 15 tables and a project page at\n  https://eraseai.github.io/ERASE-page"},{"id":"http://arxiv.org/abs/2403.05256v1","updated":"2024-03-08T12:26:48Z","published":"2024-03-08T12:26:48Z","title":"DuDoUniNeXt: Dual-domain unified hybrid model for single and\n  multi-contrast undersampled MRI reconstruction","summary":"  Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to\nincorporate a reference image of auxiliary modality to guide the reconstruction\nprocess of the target modality. Known MC reconstruction methods perform well\nwith a fully sampled reference image, but usually exhibit inferior performance,\ncompared to single-contrast (SC) methods, when the reference image is missing\nor of low quality. To address this issue, we propose DuDoUniNeXt, a unified\ndual-domain MRI reconstruction network that can accommodate to scenarios\ninvolving absent, low-quality, and high-quality reference images. DuDoUniNeXt\nadopts a hybrid backbone that combines CNN and ViT, enabling specific\nadjustment of image domain and k-space reconstruction. Specifically, an\nadaptive coarse-to-fine feature fusion module (AdaC2F) is devised to\ndynamically process the information from reference images of varying qualities.\nBesides, a partially shared shallow feature extractor (PaSS) is proposed, which\nuses shared and distinct parameters to handle consistent and discrepancy\ninformation among contrasts. Experimental results demonstrate that the proposed\nmodel surpasses state-of-the-art SC and MC models significantly. Ablation\nstudies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS,\nand the dual-domain unified learning scheme.\n","authors":["Ziqi Gao","Yue Zhang","Xinwen Liu","Kaiyan Li","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05256v1.pdf","comment":"11 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2402.03921v2","updated":"2024-03-08T12:23:56Z","published":"2024-02-06T11:44:06Z","title":"Large Language Models to Enhance Bayesian Optimization","summary":"  Bayesian optimization (BO) is a powerful approach for optimizing complex and\nexpensive-to-evaluate black-box functions. Its importance is underscored in\nmany applications, notably including hyperparameter tuning, but its efficacy\ndepends on efficiently balancing exploration and exploitation. While there has\nbeen substantial progress in BO methods, striking this balance remains a\ndelicate process. In this light, we present LLAMBO, a novel approach that\nintegrates the capabilities of Large Language Models (LLM) within BO. At a high\nlevel, we frame the BO problem in natural language, enabling LLMs to\niteratively propose and evaluate promising solutions conditioned on historical\nevaluations. More specifically, we explore how combining contextual\nunderstanding, few-shot learning proficiency, and domain knowledge of LLMs can\nimprove model-based BO. Our findings illustrate that LLAMBO is effective at\nzero-shot warmstarting, and enhances surrogate modeling and candidate sampling,\nespecially in the early stages of search when observations are sparse. Our\napproach is performed in context and does not require LLM finetuning.\nAdditionally, it is modular by design, allowing individual components to be\nintegrated into existing BO frameworks, or function cohesively as an end-to-end\nmethod. We empirically validate LLAMBO's efficacy on the problem of\nhyperparameter tuning, highlighting strong empirical performance across a range\nof diverse benchmarks, proprietary, and synthetic tasks.\n","authors":["Tennison Liu","Nicolás Astorga","Nabeel Seedat","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2402.03921v2.pdf","comment":"Accepted as Poster at ICLR2024"},{"id":"http://arxiv.org/abs/2403.05249v1","updated":"2024-03-08T12:13:11Z","published":"2024-03-08T12:13:11Z","title":"On Representing Electronic Wave Functions with Sign Equivariant Neural\n  Networks","summary":"  Recent neural networks demonstrated impressively accurate approximations of\nelectronic ground-state wave functions. Such neural networks typically consist\nof a permutation-equivariant neural network followed by a\npermutation-antisymmetric operation to enforce the electronic exchange\nsymmetry. While accurate, such neural networks are computationally expensive.\nIn this work, we explore the flipped approach, where we first compute\nantisymmetric quantities based on the electronic coordinates and then apply\nsign equivariant neural networks to preserve the antisymmetry. While this\napproach promises acceleration thanks to the lower-dimensional representation,\nwe demonstrate that it reduces to a Jastrow factor, a commonly used\npermutation-invariant multiplicative factor in the wave function. Our empirical\nresults support this further, finding little to no improvements over baselines.\nWe conclude with neither theoretical nor empirical advantages of sign\nequivariant functions for representing electronic wave functions within the\nevaluation of this work.\n","authors":["Nicholas Gao","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2403.05249v1.pdf","comment":"Published at Workshop on AI4DifferentialEquations in Science at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2309.05436v2","updated":"2024-03-08T12:12:38Z","published":"2023-09-11T13:18:19Z","title":"Quantized Fourier and Polynomial Features for more Expressive Tensor\n  Network Models","summary":"  In the context of kernel machines, polynomial and Fourier features are\ncommonly used to provide a nonlinear extension to linear models by mapping the\ndata to a higher-dimensional space. Unless one considers the dual formulation\nof the learning problem, which renders exact large-scale learning unfeasible,\nthe exponential increase of model parameters in the dimensionality of the data\ncaused by their tensor-product structure prohibits to tackle high-dimensional\nproblems. One of the possible approaches to circumvent this exponential scaling\nis to exploit the tensor structure present in the features by constraining the\nmodel weights to be an underparametrized tensor network. In this paper we\nquantize, i.e. further tensorize, polynomial and Fourier features. Based on\nthis feature quantization we propose to quantize the associated model weights,\nyielding quantized models. We show that, for the same number of model\nparameters, the resulting quantized models have a higher bound on the\nVC-dimension as opposed to their non-quantized counterparts, at no additional\ncomputational cost while learning from identical features. We verify\nexperimentally how this additional tensorization regularizes the learning\nproblem by prioritizing the most salient features in the data and how it\nprovides models with increased generalization capabilities. We finally\nbenchmark our approach on large regression task, achieving state-of-the-art\nresults on a laptop computer.\n","authors":["Frederiek Wesel","Kim Batselier"],"pdf_url":"https://arxiv.org/pdf/2309.05436v2.pdf","comment":"9 pages, 4 figures. Reviewed version after peer-review. To be\n  published in the proceedings of the 27th International Conference on\n  Artificial Intelligence and Statistics (AISTATS)"},{"id":"http://arxiv.org/abs/2306.10715v4","updated":"2024-03-08T12:07:10Z","published":"2023-06-19T06:22:02Z","title":"Maximum Entropy Heterogeneous-Agent Reinforcement Learning","summary":"  Multi-agent reinforcement learning (MARL) has been shown effective for\ncooperative games in recent years. However, existing state-of-the-art methods\nface challenges related to sample complexity, training instability, and the\nrisk of converging to a suboptimal Nash Equilibrium. In this paper, we propose\na unified framework for learning \\emph{stochastic} policies to resolve these\nissues. We embed cooperative MARL problems into probabilistic graphical models,\nfrom which we derive the maximum entropy (MaxEnt) objective for MARL. Based on\nthe MaxEnt framework, we propose Heterogeneous-Agent Soft Actor-Critic (HASAC)\nalgorithm. Theoretically, we prove the monotonic improvement and convergence to\nquantal response equilibrium (QRE) properties of HASAC. Furthermore, we\ngeneralize a unified template for MaxEnt algorithmic design named Maximum\nEntropy Heterogeneous-Agent Mirror Learning (MEHAML), which provides any\ninduced method with the same guarantees as HASAC. We evaluate HASAC on six\nbenchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge,\nGoogle Research Football, Multi-Agent Particle Environment, and Light Aircraft\nGame. Results show that HASAC consistently outperforms strong baselines,\nexhibiting better sample efficiency, robustness, and sufficient exploration.\n","authors":["Jiarong Liu","Yifan Zhong","Siyi Hu","Haobo Fu","Qiang Fu","Xiaojun Chang","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2306.10715v4.pdf","comment":"ICLR 2024 spotlight"},{"id":"http://arxiv.org/abs/2403.05239v1","updated":"2024-03-08T11:59:32Z","published":"2024-03-08T11:59:32Z","title":"Towards Effective Usage of Human-Centric Priors in Diffusion Models for\n  Text-based Human Image Generation","summary":"  Vanilla text-to-image diffusion models struggle with generating accurate\nhuman images, commonly resulting in imperfect anatomies such as unnatural\npostures or disproportionate limbs.Existing methods address this issue mostly\nby fine-tuning the model with extra images or adding additional controls --\nhuman-centric priors such as pose or depth maps -- during the image generation\nphase. This paper explores the integration of these human-centric priors\ndirectly into the model fine-tuning stage, essentially eliminating the need for\nextra conditions at the inference stage. We realize this idea by proposing a\nhuman-centric alignment loss to strengthen human-related information from the\ntextual prompts within the cross-attention maps. To ensure semantic detail\nrichness and human structural accuracy during fine-tuning, we introduce\nscale-aware and step-wise constraints within the diffusion process, according\nto an in-depth analysis of the cross-attention layer. Extensive experiments\nshow that our method largely improves over state-of-the-art text-to-image\nmodels to synthesize high-quality human images based on user-written prompts.\nProject page: \\url{https://hcplayercvpr2024.github.io}.\n","authors":["Junyan Wang","Zhenhong Sun","Zhiyu Tan","Xuanbai Chen","Weihua Chen","Hao Li","Cheng Zhang","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2403.05239v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05235v1","updated":"2024-03-08T11:51:00Z","published":"2024-03-08T11:51:00Z","title":"Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine\n  Learning in Healthcare","summary":"  The escalating integration of machine learning in high-stakes fields such as\nhealthcare raises substantial concerns about model fairness. We propose an\ninterpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to\nimprove model fairness without compromising performance, featuring an\ninteractive interface to identify a \"fairer\" model from a set of\nhigh-performing models and promoting the integration of data-driven evidence\nand clinical expertise to enhance contextualized fairness. We demonstrated\nFAIM's value in reducing sex and race biases by predicting hospital admission\nwith two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both\ndatasets, FAIM models not only exhibited satisfactory discriminatory\nperformance but also significantly mitigated biases as measured by\nwell-established fairness metrics, outperforming commonly used bias-mitigation\nmethods. Our approach demonstrates the feasibility of improving fairness\nwithout sacrificing performance and provides an a modeling mode that invites\ndomain experts to engage, fostering a multidisciplinary effort toward tailored\nAI fairness.\n","authors":["Mingxuan Liu","Yilin Ning","Yuhe Ke","Yuqing Shang","Bibhas Chakraborty","Marcus Eng Hock Ong","Roger Vaughan","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05446v5","updated":"2024-03-08T11:31:28Z","published":"2023-10-09T06:43:38Z","title":"RetSeg: Retention-based Colorectal Polyps Segmentation Network","summary":"  Vision Transformers (ViTs) have revolutionized medical imaging analysis,\nshowcasing superior efficacy compared to conventional Convolutional Neural\nNetworks (CNNs) in vital tasks such as polyp classification, detection, and\nsegmentation. Leveraging attention mechanisms to focus on specific image\nregions, ViTs exhibit contextual awareness in processing visual data,\nculminating in robust and precise predictions, even for intricate medical\nimages. Moreover, the inherent self-attention mechanism in Transformers\naccommodates varying input sizes and resolutions, granting an unprecedented\nflexibility absent in traditional CNNs. However, Transformers grapple with\nchallenges like excessive memory usage and limited training parallelism due to\nself-attention, rendering them impractical for real-time disease detection on\nresource-constrained devices. In this study, we address these hurdles by\ninvestigating the integration of the recently introduced retention mechanism\ninto polyp segmentation, introducing RetSeg, an encoder-decoder network\nfeaturing multi-head retention blocks. Drawing inspiration from Retentive\nNetworks (RetNet), RetSeg is designed to bridge the gap between precise polyp\nsegmentation and resource utilization, particularly tailored for colonoscopy\nimages. We train and validate RetSeg for polyp segmentation employing two\npublicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we\nshowcase RetSeg's promising performance across diverse public datasets,\nincluding CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While\nour work represents an early-stage exploration, further in-depth studies are\nimperative to advance these promising findings.\n","authors":["Khaled ELKarazle","Valliappan Raman","Caslon Chua","Patrick Then"],"pdf_url":"https://arxiv.org/pdf/2310.05446v5.pdf","comment":"Updated PDF"},{"id":"http://arxiv.org/abs/2403.05220v1","updated":"2024-03-08T11:18:26Z","published":"2024-03-08T11:18:26Z","title":"Synthetic Privileged Information Enhances Medical Image Representation\n  Learning","summary":"  Multimodal self-supervised representation learning has consistently proven to\nbe a highly effective method in medical image analysis, offering strong task\nperformance and producing biologically informed insights. However, these\nmethods heavily rely on large, paired datasets, which is prohibitive for their\nuse in scenarios where paired data does not exist, or there is only a small\namount available. In contrast, image generation methods can work well on very\nsmall datasets, and can find mappings between unpaired datasets, meaning an\neffectively unlimited amount of paired synthetic data can be generated. In this\nwork, we demonstrate that representation learning can be significantly improved\nby synthetically generating paired information, both compared to training on\neither single-modality (up to 4.4x error reduction) or authentic multi-modal\npaired datasets (up to 5.6x error reduction).\n","authors":["Lucas Farndale","Chris Walsh","Robert Insall","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.05220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06549v4","updated":"2024-03-08T11:07:51Z","published":"2023-10-10T11:51:12Z","title":"Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks","summary":"  Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06549v4.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2311.02455v2","updated":"2024-03-08T11:04:49Z","published":"2023-11-04T16:42:42Z","title":"Mixed Models with Multiple Instance Learning","summary":"  Predicting patient features from single-cell data can help identify cellular\nstates implicated in health and disease. Linear models and average cell type\nexpressions are typically favored for this task for their efficiency and\nrobustness, but they overlook the rich cell heterogeneity inherent in\nsingle-cell data. To address this gap, we introduce MixMIL, a framework\nintegrating Generalized Linear Mixed Models (GLMM) and Multiple Instance\nLearning (MIL), upholding the advantages of linear models while modeling cell\nstate heterogeneity. By leveraging predefined cell embeddings, MixMIL enhances\ncomputational efficiency and aligns with recent advancements in single-cell\nrepresentation learning. Our empirical results reveal that MixMIL outperforms\nexisting MIL models in single-cell datasets, uncovering new associations and\nelucidating biological mechanisms across different domains.\n","authors":["Jan P. Engelmann","Alessandro Palma","Jakub M. Tomczak","Fabian J. Theis","Francesco Paolo Casale"],"pdf_url":"https://arxiv.org/pdf/2311.02455v2.pdf","comment":"AISTATS 2024 Oral, Code: https://github.com/AIH-SGML/MixMIL"},{"id":"http://arxiv.org/abs/2401.10632v2","updated":"2024-03-08T10:51:42Z","published":"2024-01-19T11:20:31Z","title":"Interventional Fairness on Partially Known Causal Graphs: A Constrained\n  Optimization Approach","summary":"  Fair machine learning aims to prevent discrimination against individuals or\nsub-populations based on sensitive attributes such as gender and race. In\nrecent years, causal inference methods have been increasingly used in fair\nmachine learning to measure unfairness by causal effects. However, current\nmethods assume that the true causal graph is given, which is often not true in\nreal-world applications. To address this limitation, this paper proposes a\nframework for achieving causal fairness based on the notion of interventions\nwhen the true causal graph is partially known. The proposed approach involves\nmodeling fair prediction using a Partially Directed Acyclic Graph (PDAG),\nspecifically, a class of causal DAGs that can be learned from observational\ndata combined with domain knowledge. The PDAG is used to measure causal\nfairness, and a constrained optimization problem is formulated to balance\nbetween fairness and accuracy. Results on both simulated and real-world\ndatasets demonstrate the effectiveness of this method.\n","authors":["Aoqi Zuo","Yiqing Li","Susan Wei","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2401.10632v2.pdf","comment":"Accepted to ICLR24"},{"id":"http://arxiv.org/abs/2402.06295v2","updated":"2024-03-08T10:50:21Z","published":"2024-02-09T10:16:58Z","title":"Multimodal Interpretable Data-Driven Models for Early Prediction of\n  Antimicrobial Multidrug Resistance Using Multivariate Time-Series","summary":"  Electronic health records (EHR) is an inherently multimodal register of the\npatient's health status characterized by static data and multivariate time\nseries (MTS). While MTS are a valuable tool for clinical prediction, their\nfusion with other data modalities can possibly result in more thorough insights\nand more accurate results. Deep neural networks (DNNs) have emerged as\nfundamental tools for identifying and defining underlying patterns in the\nhealthcare domain. However, fundamental improvements in interpretability are\nneeded for DNN models to be widely used in the clinical setting. In this study,\nwe present an approach built on a collection of interpretable multimodal\ndata-driven models that may anticipate and understand the emergence of\nantimicrobial multidrug resistance (AMR) germs in the intensive care unit (ICU)\nof the University Hospital of Fuenlabrada (Madrid, Spain). The profile and\ninitial health status of the patient are modeled using static variables, while\nthe evolution of the patient's health status during the ICU stay is modeled\nusing several MTS, including mechanical ventilation and antibiotics intake. The\nmultimodal DNNs models proposed in this paper include interpretable principles\nin addition to being effective at predicting AMR and providing an explainable\nprediction support system for AMR in the ICU. Furthermore, our proposed\nmethodology based on multimodal models and interpretability schemes can be\nleveraged in additional clinical problems dealing with EHR data, broadening the\nimpact and applicability of our results.\n","authors":["Sergio Martínez-Agüero","Antonio G. Marques","Inmaculada Mora-Jiménez","Joaquín Alvárez-Rodríguez","Cristina Soguero-Ruiz"],"pdf_url":"https://arxiv.org/pdf/2402.06295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05209v1","updated":"2024-03-08T10:49:37Z","published":"2024-03-08T10:49:37Z","title":"Overcoming Data Inequality across Domains with Semi-Supervised Domain\n  Generalization","summary":"  While there have been considerable advancements in machine learning driven by\nextensive datasets, a significant disparity still persists in the availability\nof data across various sources and populations. This inequality across domains\nposes challenges in modeling for those with limited data, which can lead to\nprofound practical and ethical concerns. In this paper, we address a\nrepresentative case of data inequality problem across domains termed\nSemi-Supervised Domain Generalization (SSDG), in which only one domain is\nlabeled while the rest are unlabeled. We propose a novel algorithm, ProUD,\nwhich can effectively learn domain-invariant features via domain-aware\nprototypes along with progressive generalization via uncertainty-adaptive\nmixing of labeled and unlabeled domains. Our experiments on three different\nbenchmark datasets demonstrate the effectiveness of ProUD, outperforming all\nbaseline models including single domain generalization and semi-supervised\nlearning. Source code will be released upon acceptance of the paper.\n","authors":["Jinha Park","Wonguk Cho","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2403.05209v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.05196v1","updated":"2024-03-08T10:19:00Z","published":"2024-03-08T10:19:00Z","title":"Denoising Autoregressive Representation Learning","summary":"  In this paper, we explore a new generative approach for learning visual\nrepresentations. Our method, DARL, employs a decoder-only Transformer to\npredict image patches autoregressively. We find that training with Mean Squared\nError (MSE) alone leads to strong representations. To enhance the image\ngeneration ability, we replace the MSE loss with the diffusion objective by\nusing a denoising patch decoder. We show that the learned representation can be\nimproved by using tailored noise schedules and longer training in larger\nmodels. Notably, the optimal schedule differs significantly from the typical\nones used in standard image diffusion models. Overall, despite its simple\narchitecture, DARL delivers performance remarkably close to state-of-the-art\nmasked prediction models under the fine-tuning protocol. This marks an\nimportant step towards a unified model capable of both visual perception and\ngeneration, effectively combining the strengths of autoregressive and denoising\ndiffusion models.\n","authors":["Yazhe Li","Jorg Bornschein","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.05196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07323v2","updated":"2024-03-08T10:09:43Z","published":"2023-11-13T13:22:21Z","title":"A Voting Approach for Explainable Classification with Rule Learning","summary":"  State-of-the-art results in typical classification tasks are mostly achieved\nby unexplainable machine learning methods, like deep neural networks, for\ninstance. Contrarily, in this paper, we investigate the application of rule\nlearning methods in such a context. Thus, classifications become based on\ncomprehensible (first-order) rules, explaining the predictions made. In\ngeneral, however, rule-based classifications are less accurate than\nstate-of-the-art results (often significantly). As main contribution, we\nintroduce a voting approach combining both worlds, aiming to achieve comparable\nresults as (unexplainable) state-of-the-art methods, while still providing\nexplanations in the form of deterministic rules. Considering a variety of\nbenchmark data sets including a use case of significant interest to insurance\nindustries, we prove that our approach not only clearly outperforms ordinary\nrule learning methods, but also yields results on a par with state-of-the-art\noutcomes.\n","authors":["Albert Nössig","Tobias Hell","Georg Moser"],"pdf_url":"https://arxiv.org/pdf/2311.07323v2.pdf","comment":"35 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.08850v2","updated":"2024-03-08T10:06:53Z","published":"2024-01-16T21:47:23Z","title":"REValueD: Regularised Ensemble Value-Decomposition for Factorisable\n  Markov Decision Processes","summary":"  Discrete-action reinforcement learning algorithms often falter in tasks with\nhigh-dimensional discrete action spaces due to the vast number of possible\nactions. A recent advancement leverages value-decomposition, a concept from\nmulti-agent reinforcement learning, to tackle this challenge. This study delves\ndeep into the effects of this value-decomposition, revealing that whilst it\ncurtails the over-estimation bias inherent to Q-learning algorithms, it\namplifies target variance. To counteract this, we present an ensemble of\ncritics to mitigate target variance. Moreover, we introduce a regularisation\nloss that helps to mitigate the effects that exploratory actions in one\ndimension can have on the value of optimal actions in other dimensions. Our\nnovel algorithm, REValueD, tested on discretised versions of the DeepMind\nControl Suite tasks, showcases superior performance, especially in the\nchallenging humanoid and dog tasks. We further dissect the factors influencing\nREValueD's performance, evaluating the significance of the regularisation loss\nand the scalability of REValueD with increasing sub-actions per dimension.\n","authors":["David Ireland","Giovanni Montana"],"pdf_url":"https://arxiv.org/pdf/2401.08850v2.pdf","comment":"ICLR camera ready version"},{"id":"http://arxiv.org/abs/2211.03464v2","updated":"2024-03-08T10:06:43Z","published":"2022-11-07T11:25:47Z","title":"A Survey on Quantum Reinforcement Learning","summary":"  Quantum reinforcement learning is an emerging field at the intersection of\nquantum computing and machine learning. While we intend to provide a broad\noverview of the literature on quantum reinforcement learning - our\ninterpretation of this term will be clarified below - we put particular\nemphasis on recent developments. With a focus on already available noisy\nintermediate-scale quantum devices, these include variational quantum circuits\nacting as function approximators in an otherwise classical reinforcement\nlearning setting. In addition, we survey quantum reinforcement learning\nalgorithms based on future fault-tolerant hardware, some of which come with a\nprovable quantum advantage. We provide both a birds-eye-view of the field, as\nwell as summaries and reviews for selected parts of the literature.\n","authors":["Nico Meyer","Christian Ufrecht","Maniraman Periyasamy","Daniel D. Scherer","Axel Plinge","Christopher Mutschler"],"pdf_url":"https://arxiv.org/pdf/2211.03464v2.pdf","comment":"83 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.02746v2","updated":"2024-03-08T09:59:40Z","published":"2024-03-05T08:02:00Z","title":"Learning without Exact Guidance: Updating Large-scale High-resolution\n  Land Cover Maps from Low-resolution Historical Labels","summary":"  Large-scale high-resolution (HR) land-cover mapping is a vital task to survey\nthe Earth's surface and resolve many challenges facing humanity. However, it is\nstill a non-trivial task hindered by complex ground details, various landforms,\nand the scarcity of accurate training labels over a wide-span geographic area.\nIn this paper, we propose an efficient, weakly supervised framework\n(Paraformer) to guide large-scale HR land-cover mapping with easy-access\nhistorical land-cover data of low resolution (LR). Specifically, existing\nland-cover mapping approaches reveal the dominance of CNNs in preserving local\nground details but still suffer from insufficient global modeling in various\nlandforms. Therefore, we design a parallel CNN-Transformer feature extractor in\nParaformer, consisting of a downsampling-free CNN branch and a Transformer\nbranch, to jointly capture local and global contextual information. Besides,\nfacing the spatial mismatch of training data, a pseudo-label-assisted training\n(PLAT) module is adopted to reasonably refine LR labels for weakly supervised\nsemantic segmentation of HR images. Experiments on two large-scale datasets\ndemonstrate the superiority of Paraformer over other state-of-the-art methods\nfor automatically updating HR land-cover maps from LR historical labels.\n","authors":["Zhuohong Li","Wei He","Jiepan Li","Fangxiao Lu","Hongyan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.02746v2.pdf","comment":"11 pages, 9 figures, accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2310.05365v5","updated":"2024-03-08T09:54:25Z","published":"2023-10-09T02:51:01Z","title":"Molecular De Novo Design through Transformer-based Reinforcement\n  Learning","summary":"  In this work, we introduce a method to fine-tune a Transformer-based\ngenerative model for molecular de novo design. Leveraging the superior sequence\nlearning capacity of Transformers over Recurrent Neural Networks (RNNs), our\nmodel can generate molecular structures with desired properties effectively. In\ncontrast to the traditional RNN-based models, our proposed method exhibits\nsuperior performance in generating compounds predicted to be active against\nvarious biological targets, capturing long-term dependencies in the molecular\nstructure sequence. The model's efficacy is demonstrated across numerous tasks,\nincluding generating analogues to a query structure and producing compounds\nwith particular attributes, outperforming the baseline RNN-based methods. Our\napproach can be used for scaffold hopping, library expansion starting from a\nsingle molecule, and generating compounds with high predicted activity against\nbiological targets.\n","authors":["Pengcheng Xu","Tao Feng","Tianfan Fu","Siddhartha Laghuvarapu","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2310.05365v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05185v1","updated":"2024-03-08T09:53:07Z","published":"2024-03-08T09:53:07Z","title":"Personalized Audiobook Recommendations at Spotify Through Graph Neural\n  Networks","summary":"  In the ever-evolving digital audio landscape, Spotify, well-known for its\nmusic and talk content, has recently introduced audiobooks to its vast user\nbase. While promising, this move presents significant challenges for\npersonalized recommendations. Unlike music and podcasts, audiobooks, initially\navailable for a fee, cannot be easily skimmed before purchase, posing higher\nstakes for the relevance of recommendations. Furthermore, introducing a new\ncontent type into an existing platform confronts extreme data sparsity, as most\nusers are unfamiliar with this new content type. Lastly, recommending content\nto millions of users requires the model to react fast and be scalable. To\naddress these challenges, we leverage podcast and music user preferences and\nintroduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous\nGraph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach\nuncovers nuanced item relationships while ensuring low latency and complexity.\nWe decouple users from the HGNN graph and propose an innovative multi-link\nneighbor sampler. These choices, together with the 2T component, significantly\nreduce the complexity of the HGNN model. Empirical evaluations involving\nmillions of users show significant improvement in the quality of personalized\nrecommendations, resulting in a +46% increase in new audiobooks start rate and\na +23% boost in streaming rates. Intriguingly, our model's impact extends\nbeyond audiobooks, benefiting established products like podcasts.\n","authors":["Marco De Nadai","Francesco Fabbri","Paul Gigioli","Alice Wang","Ang Li","Fabrizio Silvestri","Laura Kim","Shawn Lin","Vladan Radosavljevic","Sandeep Ghael","David Nyhan","Hugues Bouchard","Mounia Lalmas-Roelleke","Andreas Damianou"],"pdf_url":"https://arxiv.org/pdf/2403.05185v1.pdf","comment":"To appear in The Web Conference 2024 proceedings"},{"id":"http://arxiv.org/abs/2403.05181v1","updated":"2024-03-08T09:43:27Z","published":"2024-03-08T09:43:27Z","title":"Adversarial Sparse Teacher: Defense Against Distillation-Based Model\n  Stealing Attacks Using Adversarial Examples","summary":"  Knowledge Distillation (KD) facilitates the transfer of discriminative\ncapabilities from an advanced teacher model to a simpler student model,\nensuring performance enhancement without compromising accuracy. It is also\nexploited for model stealing attacks, where adversaries use KD to mimic the\nfunctionality of a teacher model. Recent developments in this domain have been\ninfluenced by the Stingy Teacher model, which provided empirical analysis\nshowing that sparse outputs can significantly degrade the performance of\nstudent models. Addressing the risk of intellectual property leakage, our work\nintroduces an approach to train a teacher model that inherently protects its\nlogits, influenced by the Nasty Teacher concept. Differing from existing\nmethods, we incorporate sparse outputs of adversarial examples with standard\ntraining data to strengthen the teacher's defense against student distillation.\nOur approach carefully reduces the relative entropy between the original and\nadversarially perturbed outputs, allowing the model to produce adversarial\nlogits with minimal impact on overall performance. The source codes will be\nmade publicly available soon.\n","authors":["Eda Yilmaz","Hacer Yalim Keles"],"pdf_url":"https://arxiv.org/pdf/2403.05181v1.pdf","comment":"12 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.05175v1","updated":"2024-03-08T09:32:43Z","published":"2024-03-08T09:32:43Z","title":"Continual Learning and Catastrophic Forgetting","summary":"  This book chapter delves into the dynamics of continual learning, which is\nthe process of incrementally learning from a non-stationary stream of data.\nAlthough continual learning is a natural skill for the human brain, it is very\nchallenging for artificial neural networks. An important reason is that, when\nlearning something new, these networks tend to quickly and drastically forget\nwhat they had learned before, a phenomenon known as catastrophic forgetting.\nEspecially in the last decade, continual learning has become an extensively\nstudied topic in deep learning. This book chapter reviews the insights that\nthis field has generated.\n","authors":["Gido M. van de Ven","Nicholas Soures","Dhireesha Kudithipudi"],"pdf_url":"https://arxiv.org/pdf/2403.05175v1.pdf","comment":"Preprint of a book chapter; 21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.05174v1","updated":"2024-03-08T09:28:42Z","published":"2024-03-08T09:28:42Z","title":"VTruST: Controllable value function based subset selection for\n  Data-Centric Trustworthy AI","summary":"  Trustworthy AI is crucial to the widespread adoption of AI in high-stakes\napplications with fairness, robustness, and accuracy being some of the key\ntrustworthiness metrics. In this work, we propose a controllable framework for\ndata-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the\ntrade-offs between the different trustworthiness metrics of the constructed\ntraining datasets. A key challenge in implementing an efficient DCTAI framework\nis to design an online value-function-based training data subset selection\nalgorithm. We pose the training data valuation and subset selection problem as\nan online sparse approximation formulation. We propose a novel online version\nof the Orthogonal Matching Pursuit (OMP) algorithm for solving this problem.\nExperimental results show that VTruST outperforms the state-of-the-art\nbaselines on social, image, and scientific datasets. We also show that the data\nvalues generated by VTruST can provide effective data-centric explanations for\ndifferent trustworthiness metrics.\n","authors":["Soumi Das","Shubhadip Nag","Shreyyash Sharma","Suparna Bhattacharya","Sourangshu Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.05174v1.pdf","comment":"Accepted in ICLR 2024 DMLR workshop"},{"id":"http://arxiv.org/abs/2402.13005v3","updated":"2024-03-08T09:21:14Z","published":"2024-02-20T13:38:04Z","title":"SzCORE: A Seizure Community Open-source Research Evaluation framework\n  for the validation of EEG-based automated seizure detection algorithms","summary":"  The need for high-quality automated seizure detection algorithms based on\nelectroencephalography (EEG) becomes ever more pressing with the increasing use\nof ambulatory and long-term EEG monitoring. Heterogeneity in validation methods\nof these algorithms influences the reported results and makes comprehensive\nevaluation and comparison challenging. This heterogeneity concerns in\nparticular the choice of datasets, evaluation methodologies, and performance\nmetrics. In this paper, we propose a unified framework designed to establish\nstandardization in the validation of EEG-based seizure detection algorithms.\nBased on existing guidelines and recommendations, the framework introduces a\nset of recommendations and standards related to datasets, file formats, EEG\ndata input content, seizure annotation input and output, cross-validation\nstrategies, and performance metrics. We also propose the 10-20 seizure\ndetection benchmark, a machine-learning benchmark based on public datasets\nconverted to a standardized format. This benchmark defines the machine-learning\ntask as well as reporting metrics. We illustrate the use of the benchmark by\nevaluating a set of existing seizure detection algorithms. The SzCORE (Seizure\nCommunity Open-source Research Evaluation) framework and benchmark are made\npublicly available along with an open-source software library to facilitate\nresearch use, while enabling rigorous evaluation of the clinical significance\nof the algorithms, fostering a collective effort to more optimally detect\nseizures to improve the lives of people with epilepsy.\n","authors":["Jonathan Dan","Una Pale","Alireza Amirshahi","William Cappelletti","Thorir Mar Ingolfsson","Xiaying Wang","Andrea Cossettini","Adriano Bernini","Luca Benini","Sándor Beniczky","David Atienza","Philippe Ryvlin"],"pdf_url":"https://arxiv.org/pdf/2402.13005v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05171v1","updated":"2024-03-08T09:20:12Z","published":"2024-03-08T09:20:12Z","title":"Overcoming Reward Overoptimization via Adversarial Policy Optimization\n  with Lightweight Uncertainty Estimation","summary":"  We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the\npervasive issue of reward over-optimization in Reinforcement Learning from\nHuman Feedback (RLHF) for Large Language Models (LLMs). Over-optimization\noccurs when a reward model serves as an imperfect proxy for human preference,\nand RL-driven policy optimization erroneously exploits reward inaccuracies. In\nthis paper, we begin by introducing a lightweight way to quantify uncertainties\nin rewards, relying solely on the last layer embeddings of the reward model,\nwithout the need for computationally expensive reward ensembles. AdvPO then\naddresses a distributionally robust optimization problem centred around the\nconfidence interval of the reward model's predictions for policy improvement.\nThrough comprehensive experiments on the Anthropic HH and TL;DR summarization\ndatasets, we illustrate the efficacy of AdvPO in mitigating the\noveroptimization issue, consequently resulting in enhanced performance as\nevaluated through human-assisted evaluation.\n","authors":["Xiaoying Zhang","Jean-Francois Ton","Wei Shen","Hongning Wang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05164v1","updated":"2024-03-08T09:09:15Z","published":"2024-03-08T09:09:15Z","title":"Synthetic data generation for system identification: leveraging\n  knowledge transfer from similar systems","summary":"  This paper addresses the challenge of overfitting in the learning of\ndynamical systems by introducing a novel approach for the generation of\nsynthetic data, aimed at enhancing model generalization and robustness in\nscenarios characterized by data scarcity. Central to the proposed methodology\nis the concept of knowledge transfer from systems within the same class.\nSpecifically, synthetic data is generated through a pre-trained meta-model that\ndescribes a broad class of systems to which the system of interest is assumed\nto belong. Training data serves a dual purpose: firstly, as input to the\npre-trained meta model to discern the system's dynamics, enabling the\nprediction of its behavior and thereby generating synthetic output sequences\nfor new input sequences; secondly, in conjunction with synthetic data, to\ndefine the loss function used for model estimation. A validation dataset is\nused to tune a scalar hyper-parameter balancing the relative importance of\ntraining and synthetic data in the definition of the loss function. The same\nvalidation set can be also used for other purposes, such as early stopping\nduring the training, fundamental to avoid overfitting in case of small-size\ntraining datasets. The efficacy of the approach is shown through a numerical\nexample that highlights the advantages of integrating synthetic data into the\nsystem identification process.\n","authors":["Dario Piga","Matteo Rufolo","Gabriele Maroni","Manas Mejari","Marco Forgione"],"pdf_url":"https://arxiv.org/pdf/2403.05164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05158v1","updated":"2024-03-08T08:51:37Z","published":"2024-03-08T08:51:37Z","title":"Adaptive Split Learning over Energy-Constrained Wireless Edge Networks","summary":"  Split learning (SL) is a promising approach for training artificial\nintelligence (AI) models, in which devices collaborate with a server to train\nan AI model in a distributed manner, based on a same fixed split point.\nHowever, due to the device heterogeneity and variation of channel conditions,\nthis way is not optimal in training delay and energy consumption. In this\npaper, we design an adaptive split learning (ASL) scheme which can dynamically\nselect split points for devices and allocate computing resource for the server\nin wireless edge networks. We formulate an optimization problem to minimize the\naverage training latency subject to long-term energy consumption constraint.\nThe difficulties in solving this problem are the lack of future information and\nmixed integer programming (MIP). To solve it, we propose an online algorithm\nleveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP\nproblem only with the current information. Then, a two-layer optimization\nmethod is proposed to solve the MIP problem. Extensive simulation results\ndemonstrate that the ASL scheme can reduce the average training delay and\nenergy consumption by 53.7% and 22.1%, respectively, as compared to the\nexisting SL schemes.\n","authors":["Zuguang Li","Wen Wu","Shaohua Wu","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05158v1.pdf","comment":"6 pages, 5 figures, 20 conferences"},{"id":"http://arxiv.org/abs/2403.05138v1","updated":"2024-03-08T08:12:05Z","published":"2024-03-08T08:12:05Z","title":"Greedy feature selection: Classifier-dependent feature selection via\n  greedy methods","summary":"  The purpose of this study is to introduce a new approach to feature ranking\nfor classification tasks, called in what follows greedy feature selection. In\nstatistical learning, feature selection is usually realized by means of methods\nthat are independent of the classifier applied to perform the prediction using\nthat reduced number of features. Instead, greedy feature selection identifies\nthe most important feature at each step and according to the selected\nclassifier. In the paper, the benefits of such scheme are investigated\ntheoretically in terms of model capacity indicators, such as the\nVapnik-Chervonenkis (VC) dimension or the kernel alignment, and tested\nnumerically by considering its application to the problem of predicting\ngeo-effective manifestations of the active Sun.\n","authors":["Fabiana Camattari","Sabrina Guastavino","Francesco Marchetti","Michele Piana","Emma Perracchione"],"pdf_url":"https://arxiv.org/pdf/2403.05138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05134v1","updated":"2024-03-08T08:07:26Z","published":"2024-03-08T08:07:26Z","title":"Follow-the-Perturbed-Leader with Fréchet-type Tail Distributions:\n  Optimality in Adversarial Bandits and Best-of-Both-Worlds","summary":"  This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL)\npolicy in both adversarial and stochastic $K$-armed bandits. Despite the\nwidespread use of the Follow-the-Regularized-Leader (FTRL) framework with\nvarious choices of regularization, the FTPL framework, which relies on random\nperturbations, has not received much attention, despite its inherent\nsimplicity. In adversarial bandits, there has been conjecture that FTPL could\npotentially achieve $\\mathcal{O}(\\sqrt{KT})$ regrets if perturbations follow a\ndistribution with a Fr\\'{e}chet-type tail. Recent work by Honda et al. (2023)\nshowed that FTPL with Fr\\'{e}chet distribution with shape $\\alpha=2$ indeed\nattains this bound and, notably logarithmic regret in stochastic bandits,\nmeaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result\nonly partly resolves the above conjecture because their analysis heavily relies\non the specific form of the Fr\\'{e}chet distribution with this shape. In this\npaper, we establish a sufficient condition for perturbations to achieve\n$\\mathcal{O}(\\sqrt{KT})$ regrets in the adversarial setting, which covers,\ne.g., Fr\\'{e}chet, Pareto, and Student-$t$ distributions. We also demonstrate\nthe BOBW achievability of FTPL with certain Fr\\'{e}chet-type tail\ndistributions. Our results contribute not only to resolving existing\nconjectures through the lens of extreme value theory but also potentially offer\ninsights into the effect of the regularization functions in FTRL through the\nmapping from FTPL to FTRL.\n","authors":["Jongyeong Lee","Junya Honda","Shinji Ito","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2403.05134v1.pdf","comment":"54 pages"},{"id":"http://arxiv.org/abs/2403.05133v1","updated":"2024-03-08T08:05:50Z","published":"2024-03-08T08:05:50Z","title":"RIS-empowered Topology Control for Distributed Learning in Urban Air\n  Mobility","summary":"  Urban Air Mobility (UAM) expands vehicles from the ground to the near-ground\nspace, envisioned as a revolution for transportation systems. Comprehensive\nscene perception is the foundation for autonomous aerial driving. However, UAM\nencounters the intelligent perception challenge: high perception learning\nrequirements conflict with the limited sensors and computing chips of flying\ncars. To overcome the challenge, federated learning (FL) and other\ncollaborative learning have been proposed to enable resource-limited devices to\nconduct onboard deep learning (DL) collaboratively. But traditional\ncollaborative learning like FL relies on a central integrator for DL model\naggregation, which is difficult to deploy in dynamic environments. The fully\ndecentralized learning schemes may be the intuitive solution while the\nconvergence of distributed learning cannot be guaranteed. Accordingly, this\npaper explores reconfigurable intelligent surfaces (RIS) empowered distributed\nlearning, taking account of topological attributes to facilitate the learning\nperformance with convergence guarantee. We propose several FL topological\ncriteria for optimizing the transmission delay and convergence rate by\nexploiting the Laplacian matrix eigenvalues of the communication network.\nSubsequently, we innovatively leverage the RIS link modification ability to\nremold the current network according to the proposed topological criteria. This\npaper rethinks the functions of RIS from the perspective of the network layer.\nFurthermore, a deep deterministic policy gradient-based RIS phase shift control\nalgorithm is developed to construct or deconstruct the network links\nsimultaneously to reshape the communication network. Simulation experiments are\nconducted over MobileNet-based multi-view learning to verify the efficiency of\nthe distributed FL framework.\n","authors":["Kai Xiong","Rui Wang","Supeng Leng","Wenyang Che","Chongwen Huang","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2403.05133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04629v2","updated":"2024-03-08T07:52:32Z","published":"2024-03-07T16:13:32Z","title":"Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI\n  Collaboration","summary":"  Bayesian optimization (BO) with Gaussian processes (GP) has become an\nindispensable algorithm for black box optimization problems. Not without a dash\nof irony, BO is often considered a black box itself, lacking ways to provide\nreasons as to why certain parameters are proposed to be evaluated. This is\nparticularly relevant in human-in-the-loop applications of BO, such as in\nrobotics. We address this issue by proposing ShapleyBO, a framework for\ninterpreting BO's proposals by game-theoretic Shapley values.They quantify each\nparameter's contribution to BO's acquisition function. Exploiting the linearity\nof Shapley values, we are further able to identify how strongly each parameter\ndrives BO's exploration and exploitation for additive acquisition functions\nlike the confidence bound. We also show that ShapleyBO can disentangle the\ncontributions to exploration into those that explore aleatoric and epistemic\nuncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human\nmachine interface (HMI), allowing users to interfere with BO in case proposals\ndo not align with human reasoning. We demonstrate this HMI's benefits for the\nuse case of personalizing wearable robotic devices (assistive back exosuits) by\nhuman-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO\ncan achieve lower regret than teams without.\n","authors":["Julian Rodemann","Federico Croppi","Philipp Arens","Yusuf Sale","Julia Herbinger","Bernd Bischl","Eyke Hüllermeier","Thomas Augustin","Conor J. Walsh","Giuseppe Casalicchio"],"pdf_url":"https://arxiv.org/pdf/2403.04629v2.pdf","comment":"Preprint. Copyright by the authors. 19 pages, 24 figures"},{"id":"http://arxiv.org/abs/2307.00529v2","updated":"2024-03-08T07:42:28Z","published":"2023-07-02T09:46:05Z","title":"New intelligent defense systems to reduce the risks of Selfish Mining\n  and Double-Spending attacks using Learning Automata","summary":"  In this paper, we address the critical challenges of double-spending and\nselfish mining attacks in blockchain-based digital currencies. Double-spending\nis a problem where the same tender is spent multiple times during a digital\ncurrency transaction, while selfish mining is an intentional alteration of a\nblockchain to increase rewards to one miner or a group of miners. We introduce\na new attack that combines both these attacks and propose a machine\nlearning-based solution to mitigate the risks associated with them.\nSpecifically, we use the learning automaton, a powerful online learning method,\nto develop two models, namely the SDTLA and WVBM, which can effectively defend\nagainst selfish mining attacks. Our experimental results show that the SDTLA\nmethod increases the profitability threshold of selfish mining up to 47$\\%$,\nwhile the WVBM method performs even better and is very close to the ideal\nsituation where each miner's revenue is proportional to their shared hash\nprocessing power. Additionally, we demonstrate that both methods can\neffectively reduce the risks of double-spending by tuning the $Z$ Parameter.\nOur findings highlight the potential of SDTLA and WVBM as promising solutions\nfor enhancing the security and efficiency of blockchain networks.\n","authors":["Seyed Ardalan Ghoreishi","Mohammad Reza Meybodi"],"pdf_url":"https://arxiv.org/pdf/2307.00529v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05123v1","updated":"2024-03-08T07:36:46Z","published":"2024-03-08T07:36:46Z","title":"ECToNAS: Evolutionary Cross-Topology Neural Architecture Search","summary":"  We present ECToNAS, a cost-efficient evolutionary cross-topology neural\narchitecture search algorithm that does not require any pre-trained meta\ncontrollers. Our framework is able to select suitable network architectures for\ndifferent tasks and hyperparameter settings, independently performing\ncross-topology optimisation where required. It is a hybrid approach that fuses\ntraining and topology optimisation together into one lightweight,\nresource-friendly process. We demonstrate the validity and power of this\napproach with six standard data sets (CIFAR-10, CIFAR-100, EuroSAT, Fashion\nMNIST, MNIST, SVHN), showcasing the algorithm's ability to not only optimise\nthe topology within an architectural type, but also to dynamically add and\nremove convolutional cells when and where required, thus crossing boundaries\nbetween different network types. This enables researchers without a background\nin machine learning to make use of appropriate model types and topologies and\nto apply machine learning methods in their domains, with a computationally\ncheap, easy-to-use cross-topology neural architecture search framework that\nfully encapsulates the topology optimisation within the training process.\n","authors":["Elisabeth J. Schiessler","Roland C. Aydin","Christian J. Cyron"],"pdf_url":"https://arxiv.org/pdf/2403.05123v1.pdf","comment":"15 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.05122v1","updated":"2024-03-08T07:36:14Z","published":"2024-03-08T07:36:14Z","title":"Multi-Tower Multi-Interest Recommendation with User Representation Repel","summary":"  In the era of information overload, the value of recommender systems has been\nprofoundly recognized in academia and industry alike. Multi-interest sequential\nrecommendation, in particular, is a subfield that has been receiving increasing\nattention in recent years. By generating multiple-user representations,\nmulti-interest learning models demonstrate superior expressiveness than\nsingle-user representation models, both theoretically and empirically. Despite\nmajor advancements in the field, three major issues continue to plague the\nperformance and adoptability of multi-interest learning methods, the difference\nbetween training and deployment objectives, the inability to access item\ninformation, and the difficulty of industrial adoption due to its single-tower\narchitecture. We address these challenges by proposing a novel multi-tower\nmulti-interest framework with user representation repel. Experimental results\nacross multiple large-scale industrial datasets proved the effectiveness and\ngeneralizability of our proposed framework.\n","authors":["Tianyu Xiong","Xiaohan Yu"],"pdf_url":"https://arxiv.org/pdf/2403.05122v1.pdf","comment":"9 pages, 5 figures, 4 tables, Submitted to ACM/SIGIR - The 47th\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval"},{"id":"http://arxiv.org/abs/2403.05119v1","updated":"2024-03-08T07:32:28Z","published":"2024-03-08T07:32:28Z","title":"Estimation of Electronic Band Gap Energy From Material Properties Using\n  Machine Learning","summary":"  Machine learning techniques are utilized to estimate the electronic band gap\nenergy and forecast the band gap category of materials based on experimentally\nquantifiable properties. The determination of band gap energy is critical for\ndiscerning various material properties, such as its metallic nature, and\npotential applications in electronic and optoelectronic devices. While\nnumerical methods exist for computing band gap energy, they often entail high\ncomputational costs and have limitations in accuracy and scalability. A machine\nlearning-driven model capable of swiftly predicting material band gap energy\nusing easily obtainable experimental properties would offer a superior\nalternative to conventional density functional theory (DFT) methods. Our model\ndoes not require any preliminary DFT-based calculation or knowledge of the\nstructure of the material. We present a scheme for improving the performance of\nsimple regression and classification models by partitioning the dataset into\nmultiple clusters. A new evaluation scheme for comparing the performance of\nML-based models in material sciences involving both regression and\nclassification tasks is introduced based on traditional evaluation metrics. It\nis shown that on this new evaluation metric, our method of clustering the\ndataset results in better performance.\n","authors":["Sagar Prakash Barad","Sajag Kumar","Subhankar Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.05119v1.pdf","comment":"6 pages, IC-CGU 2024"},{"id":"http://arxiv.org/abs/2309.15278v2","updated":"2024-03-08T07:29:13Z","published":"2023-09-26T21:31:24Z","title":"Out of Sight, Still in Mind: Reasoning and Planning about Unobserved\n  Objects with Video Tracking Enabled Memory Models","summary":"  Robots need to have a memory of previously observed, but currently occluded\nobjects to work reliably in realistic environments. We investigate the problem\nof encoding object-oriented memory into a multi-object manipulation reasoning\nand planning framework. We propose DOOM and LOOM, which leverage transformer\nrelational dynamics to encode the history of trajectories given partial-view\npoint clouds and an object discovery and tracking engine. Our approaches can\nperform multiple challenging tasks including reasoning with occluded objects,\nnovel objects appearance, and object reappearance. Throughout our extensive\nsimulation and real-world experiments, we find that our approaches perform well\nin terms of different numbers of objects and different numbers of distractor\nactions. Furthermore, we show our approaches outperform an implicit memory\nbaseline.\n","authors":["Yixuan Huang","Jialin Yuan","Chanho Kim","Pupul Pradhan","Bryan Chen","Li Fuxin","Tucker Hermans"],"pdf_url":"https://arxiv.org/pdf/2309.15278v2.pdf","comment":"Accepted at IEEE Conference on Robotics and Automation (ICRA) 2024.\n  Website: https://sites.google.com/view/rdmemory"},{"id":"http://arxiv.org/abs/2403.05110v1","updated":"2024-03-08T07:15:38Z","published":"2024-03-08T07:15:38Z","title":"Efficient Data Collection for Robotic Manipulation via Compositional\n  Generalization","summary":"  Data collection has become an increasingly important problem in robotic\nmanipulation, yet there still lacks much understanding of how to effectively\ncollect data to facilitate broad generalization. Recent works on large-scale\nrobotic data collection typically vary a wide range of environmental factors\nduring data collection, such as object types and table textures. While these\nworks attempt to cover a diverse variety of scenarios, they do not explicitly\naccount for the possible compositional abilities of policies trained on the\ndata. If robot policies are able to compose different environmental factors of\nvariation (e.g., object types, table heights) from their training data to\nsucceed when encountering unseen factor combinations, then we can exploit this\nto avoid collecting data for situations that composition would address. To\ninvestigate this possibility, we conduct thorough empirical studies both in\nsimulation and on a real robot that compare data collection strategies and\nassess whether visual imitation learning policies can compose environmental\nfactors. We find that policies do exhibit composition, although leveraging\nprior robotic datasets is critical for this on a real robot. We use these\ninsights to provide better practices for in-domain data collection by proposing\ndata collection strategies that exploit composition, which can induce better\ngeneralization than naive approaches for the same amount of effort during data\ncollection. We further demonstrate that a real robot policy trained on data\nfrom such a strategy achieves a success rate of 77.5% when transferred to\nentirely new environments that encompass unseen combinations of environmental\nfactors, whereas policies trained using data collected without accounting for\nenvironmental variation fail to transfer effectively, with a success rate of\nonly 2.5%. We provide videos at http://iliad.stanford.edu/robot-data-comp/.\n","authors":["Jensen Gao","Annie Xie","Ted Xiao","Chelsea Finn","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2403.05110v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.05106v1","updated":"2024-03-08T07:09:56Z","published":"2024-03-08T07:09:56Z","title":"Simulating Battery-Powered TinyML Systems Optimised using Reinforcement\n  Learning in Image-Based Anomaly Detection","summary":"  Advances in Tiny Machine Learning (TinyML) have bolstered the creation of\nsmart industry solutions, including smart agriculture, healthcare and smart\ncities. Whilst related research contributes to enabling TinyML solutions on\nconstrained hardware, there is a need to amplify real-world applications by\noptimising energy consumption in battery-powered systems. The work presented\nextends and contributes to TinyML research by optimising battery-powered\nimage-based anomaly detection Internet of Things (IoT) systems. Whilst previous\nwork in this area has yielded the capabilities of on-device inferencing and\ntraining, there has yet to be an investigation into optimising the management\nof such capabilities using machine learning approaches, such as Reinforcement\nLearning (RL), to improve the deployment battery life of such systems. Using\nmodelled simulations, the battery life effects of an RL algorithm are\nbenchmarked against static and dynamic optimisation approaches, with the\nfoundation laid for a hardware benchmark to follow. It is shown that using RL\nwithin a TinyML-enabled IoT system to optimise the system operations, including\ncloud anomaly processing and on-device training, yields an improved battery\nlife of 22.86% and 10.86% compared to static and dynamic optimisation\napproaches respectively. The proposed solution can be deployed to\nresource-constrained hardware, given its low memory footprint of 800 B, which\ncould be further reduced. This further facilitates the real-world deployment of\nsuch systems, including key sectors such as smart agriculture.\n","authors":["Jared M. Ping","Ken J. Nixon"],"pdf_url":"https://arxiv.org/pdf/2403.05106v1.pdf","comment":"Accepted as a full paper by the tinyML Research Symposium 2024"},{"id":"http://arxiv.org/abs/2403.05100v1","updated":"2024-03-08T07:03:18Z","published":"2024-03-08T07:03:18Z","title":"Exploring the Adversarial Frontier: Quantifying Robustness via\n  Adversarial Hypervolume","summary":"  The escalating threat of adversarial attacks on deep learning models,\nparticularly in security-critical fields, has underscored the need for robust\ndeep learning systems. Conventional robustness evaluations have relied on\nadversarial accuracy, which measures a model's performance under a specific\nperturbation intensity. However, this singular metric does not fully\nencapsulate the overall resilience of a model against varying degrees of\nperturbation. To address this gap, we propose a new metric termed adversarial\nhypervolume, assessing the robustness of deep learning models comprehensively\nover a range of perturbation intensities from a multi-objective optimization\nstandpoint. This metric allows for an in-depth comparison of defense mechanisms\nand recognizes the trivial improvements in robustness afforded by less potent\ndefensive strategies. Additionally, we adopt a novel training algorithm that\nenhances adversarial robustness uniformly across various perturbation\nintensities, in contrast to methods narrowly focused on optimizing adversarial\naccuracy. Our extensive empirical studies validate the effectiveness of the\nadversarial hypervolume metric, demonstrating its ability to reveal subtle\ndifferences in robustness that adversarial accuracy overlooks. This research\ncontributes a new measure of robustness and establishes a standard for\nassessing and benchmarking the resilience of current and future defensive\nmodels against adversarial threats.\n","authors":["Ping Guo","Cheng Gong","Xi Lin","Zhiyuan Yang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05905v2","updated":"2024-03-08T06:39:39Z","published":"2023-10-09T17:49:50Z","title":"TAIL: Task-specific Adapters for Imitation Learning with Large\n  Pretrained Models","summary":"  The full potential of large pretrained models remains largely untapped in\ncontrol domains like robotics. This is mainly because of the scarcity of data\nand the computational challenges associated with training or fine-tuning these\nlarge models for such applications. Prior work mainly emphasizes either\neffective pretraining of large models for decision-making or single-task\nadaptation. But real-world problems will require data-efficient, continual\nadaptation for new control tasks. Recognizing these constraints, we introduce\nTAIL (Task-specific Adapters for Imitation Learning), a framework for efficient\nadaptation to new control tasks. Inspired by recent advancements in\nparameter-efficient fine-tuning in language domains, we explore efficient\nfine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank\nAdaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks\nwith limited demonstration data. Our extensive experiments in large-scale\nlanguage-conditioned manipulation tasks comparing prevalent parameter-efficient\nfine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can\nachieve the best post-adaptation performance with only 1\\% of the trainable\nparameters of full fine-tuning, while avoiding catastrophic forgetting and\npreserving adaptation plasticity in continual learning settings.\n","authors":["Zuxin Liu","Jesse Zhang","Kavosh Asadi","Yao Liu","Ding Zhao","Shoham Sabach","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2310.05905v2.pdf","comment":"Published on ICLR 2024"},{"id":"http://arxiv.org/abs/2109.03396v2","updated":"2024-03-08T06:31:18Z","published":"2021-09-08T02:05:40Z","title":"A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with\n  an Arbitrary Opponent","summary":"  In this paper, we propose Posterior Sampling Reinforcement Learning for\nZero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that\nachieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon\nzero-sum stochastic games with average-reward criterion. Here $H$ is an upper\nbound on the span of the bias function, $S$ is the number of states, $A$ is the\nnumber of joint actions and $T$ is the horizon. We consider the online setting\nwhere the opponent can not be controlled and can take any arbitrary\ntime-adaptive history-dependent strategy. Our regret bound improves on the best\nexisting regret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the\nsame assumption and matches the theoretical lower bound in $T$.\n","authors":["Mehdi Jafarnia-Jahromi","Rahul Jain","Ashutosh Nayyar"],"pdf_url":"https://arxiv.org/pdf/2109.03396v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14255v2","updated":"2024-03-08T06:25:50Z","published":"2023-11-24T02:42:42Z","title":"Out-of-Distribution Generalized Dynamic Graph Neural Network with\n  Disentangled Intervention and Invariance Promotion","summary":"  Dynamic graph neural networks (DyGNNs) have demonstrated powerful predictive\nabilities by exploiting graph structural and temporal dynamics. However, the\nexisting DyGNNs fail to handle distribution shifts, which naturally exist in\ndynamic graphs, mainly because the patterns exploited by DyGNNs may be variant\nwith respect to labels under distribution shifts. In this paper, we propose\nDisentangled Intervention-based Dynamic graph Attention networks with\nInvariance Promotion (I-DIDA) to handle spatio-temporal distribution shifts in\ndynamic graphs by discovering and utilizing invariant patterns, i.e.,\nstructures and features whose predictive abilities are stable across\ndistribution shifts. Specifically, we first propose a disentangled\nspatio-temporal attention network to capture the variant and invariant\npatterns. By utilizing the disentangled patterns, we design a spatio-temporal\nintervention mechanism to create multiple interventional distributions and an\nenvironment inference module to infer the latent spatio-temporal environments,\nand minimize the variance of predictions among these intervened distributions\nand environments, so that our model can make predictions based on invariant\npatterns with stable predictive abilities under distribution shifts. Extensive\nexperiments demonstrate the superiority of our method over state-of-the-art\nbaselines under distribution shifts. Our work is the first study of\nspatio-temporal distribution shifts in dynamic graphs, to the best of our\nknowledge.\n","authors":["Zeyang Zhang","Xin Wang","Ziwei Zhang","Haoyang Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.14255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17269v2","updated":"2024-03-08T06:00:12Z","published":"2024-02-27T07:28:05Z","title":"Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion\n  Recognition","summary":"  Emotion recognition in conversation (ERC) is a crucial task in natural\nlanguage processing and affective computing. This paper proposes MultiDAG+CL, a\nnovel approach for Multimodal Emotion Recognition in Conversation (ERC) that\nemploys Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual\nfeatures within a unified framework. The model is enhanced by Curriculum\nLearning (CL) to address challenges related to emotional shifts and data\nimbalance. Curriculum learning facilitates the learning process by gradually\npresenting training samples in a meaningful order, thereby improving the\nmodel's performance in handling emotional variations and data imbalance.\nExperimental results on the IEMOCAP and MELD datasets demonstrate that the\nMultiDAG+CL models outperform baseline models. We release the code for\nMultiDAG+CL and experiments: https://github.com/vanntc711/MultiDAG-CL\n","authors":["Cam-Van Thi Nguyen","Cao-Bach Nguyen","Quang-Thuy Ha","Duc-Trong Le"],"pdf_url":"https://arxiv.org/pdf/2402.17269v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.05075v1","updated":"2024-03-08T05:59:56Z","published":"2024-03-08T05:59:56Z","title":"Benchmarking Large Language Models for Molecule Prediction Tasks","summary":"  Large Language Models (LLMs) stand at the forefront of a number of Natural\nLanguage Processing (NLP) tasks. Despite the widespread adoption of LLMs in\nNLP, much of their potential in broader fields remains largely unexplored, and\nsignificant limitations persist in their design and implementation. Notably,\nLLMs struggle with structured data, such as graphs, and often falter when\ntasked with answering domain-specific questions requiring deep expertise, such\nas those in biology and chemistry. In this paper, we explore a fundamental\nquestion: Can LLMs effectively handle molecule prediction tasks? Rather than\npursuing top-tier performance, our goal is to assess how LLMs can contribute to\ndiverse molecule tasks. We identify several classification and regression\nprediction tasks across six standard molecule datasets. Subsequently, we\ncarefully design a set of prompts to query LLMs on these tasks and compare\ntheir performance with existing Machine Learning (ML) models, which include\ntext-based models and those specifically designed for analysing the geometric\nstructure of molecules. Our investigation reveals several key insights:\nFirstly, LLMs generally lag behind ML models in achieving competitive\nperformance on molecule tasks, particularly when compared to models adept at\ncapturing the geometric structure of molecules, highlighting the constrained\nability of LLMs to comprehend graph data. Secondly, LLMs show promise in\nenhancing the performance of ML models when used collaboratively. Lastly, we\nengage in a discourse regarding the challenges and promising avenues to harness\nLLMs for molecule prediction tasks. The code and models are available at\nhttps://github.com/zhiqiangzhongddu/LLMaMol.\n","authors":["Zhiqiang Zhong","Kuangyu Zhou","Davide Mottin"],"pdf_url":"https://arxiv.org/pdf/2403.05075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05069v1","updated":"2024-03-08T05:43:00Z","published":"2024-03-08T05:43:00Z","title":"Improving Diffusion-Based Generative Models via Approximated Optimal\n  Transport","summary":"  We introduce the Approximated Optimal Transport (AOT) technique, a novel\ntraining scheme for diffusion-based generative models. Our approach aims to\napproximate and integrate optimal transport into the training process,\nsignificantly enhancing the ability of diffusion models to estimate the\ndenoiser outputs accurately. This improvement leads to ODE trajectories of\ndiffusion models with lower curvature and reduced truncation errors during\nsampling. We achieve superior image quality and reduced sampling steps by\nemploying AOT in training. Specifically, we achieve FID scores of 1.88 with\njust 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional\ngenerations, respectively. Furthermore, when applying AOT to train the\ndiscriminator for guidance, we establish new state-of-the-art FID scores of\n1.68 and 1.58 for unconditional and conditional generations, respectively, each\nwith 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing\nthe performance of diffusion models.\n","authors":["Daegyu Kim","Jooyoung Choi","Chaehun Shin","Uiwon Hwang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.05069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05066v1","updated":"2024-03-08T05:37:59Z","published":"2024-03-08T05:37:59Z","title":"Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual\n  Reinforcement Learning","summary":"  We argue that one of the main obstacles for developing effective Continual\nReinforcement Learning (CRL) algorithms is the negative transfer issue\noccurring when the new task to learn arrives. Through comprehensive\nexperimental validation, we demonstrate that such issue frequently exists in\nCRL and cannot be effectively addressed by several recent work on mitigating\nplasticity loss of RL agents. To that end, we develop Reset & Distill (R&D), a\nsimple yet highly effective method, to overcome the negative transfer problem\nin CRL. R&D combines a strategy of resetting the agent's online actor and\ncritic networks to learn a new task and an offline learning step for distilling\nthe knowledge from the online actor and previous expert's action probabilities.\nWe carried out extensive experiments on long sequence of Meta-World tasks and\nshow that our method consistently outperforms recent baselines, achieving\nsignificantly higher success rates across a range of tasks. Our findings\nhighlight the importance of considering negative transfer in CRL and emphasize\nthe need for robust strategies like R&D to mitigate its detrimental effects.\n","authors":["Hongjoon Ahn","Jinu Hyeon","Youngmin Oh","Bosun Hwang","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2403.05066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19095v2","updated":"2024-03-08T05:30:10Z","published":"2024-02-29T12:24:20Z","title":"A Protein Structure Prediction Approach Leveraging Transformer and CNN\n  Integration","summary":"  Proteins are essential for life, and their structure determines their\nfunction. The protein secondary structure is formed by the folding of the\nprotein primary structure, and the protein tertiary structure is formed by the\nbending and folding of the secondary structure. Therefore, the study of protein\nsecondary structure is very helpful to the overall understanding of protein\nstructure. Although the accuracy of protein secondary structure prediction has\ncontinuously improved with the development of machine learning and deep\nlearning, progress in the field of protein structure prediction, unfortunately,\nremains insufficient to meet the large demand for protein information.\nTherefore, based on the advantages of deep learning-based methods in feature\nextraction and learning ability, this paper adopts a two-dimensional fusion\ndeep neural network model, DstruCCN, which uses Convolutional Neural Networks\n(CCN) and a supervised Transformer protein language model for single-sequence\nprotein structure prediction. The training features of the two are combined to\npredict the protein Transformer binding site matrix, and then the\nthree-dimensional structure is reconstructed using energy minimization.\n","authors":["Yanlin Zhou","Kai Tan","Xinyu Shen","Zheng He","Haotian Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.19095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05064v1","updated":"2024-03-08T05:23:55Z","published":"2024-03-08T05:23:55Z","title":"Unsupervised Graph Neural Architecture Search with Disentangled\n  Self-supervision","summary":"  The existing graph neural architecture search (GNAS) methods heavily rely on\nsupervised labels during the search process, failing to handle ubiquitous\nscenarios where supervisions are not available. In this paper, we study the\nproblem of unsupervised graph neural architecture search, which remains\nunexplored in the literature. The key problem is to discover the latent graph\nfactors that drive the formation of graph data as well as the underlying\nrelations between the factors and the optimal neural architectures. Handling\nthis problem is challenging given that the latent graph factors together with\narchitectures are highly entangled due to the nature of the graph and the\ncomplexity of the neural architecture search process. To address the challenge,\nwe propose a novel Disentangled Self-supervised Graph Neural Architecture\nSearch (DSGAS) model, which is able to discover the optimal architectures\ncapturing various latent graph factors in a self-supervised fashion based on\nunlabeled graph data. Specifically, we first design a disentangled graph\nsuper-network capable of incorporating multiple architectures with factor-wise\ndisentanglement, which are optimized simultaneously. Then, we estimate the\nperformance of architectures under different factors by our proposed\nself-supervised training with joint architecture-graph disentanglement.\nFinally, we propose a contrastive search with architecture augmentations to\ndiscover architectures with factor-specific expertise. Extensive experiments on\n11 real-world datasets demonstrate that the proposed model is able to achieve\nstate-of-the-art performance against several baseline methods in an\nunsupervised manner.\n","authors":["Zeyang Zhang","Xin Wang","Ziwei Zhang","Guangyao Shen","Shiqi Shen","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.05064v1.pdf","comment":"NeurIPS'23"},{"id":"http://arxiv.org/abs/2210.07290v4","updated":"2024-03-08T05:04:23Z","published":"2022-10-13T18:36:20Z","title":"Joint control variate for faster black-box variational inference","summary":"  Black-box variational inference performance is sometimes hindered by the use\nof gradient estimators with high variance. This variance comes from two sources\nof randomness: Data subsampling and Monte Carlo sampling. While existing\ncontrol variates only address Monte Carlo noise, and incremental gradient\nmethods typically only address data subsampling, we propose a new \"joint\"\ncontrol variate that jointly reduces variance from both sources of noise. This\nsignificantly reduces gradient variance, leading to faster optimization in\nseveral applications.\n","authors":["Xi Wang","Tomas Geffner","Justin Domke"],"pdf_url":"https://arxiv.org/pdf/2210.07290v4.pdf","comment":"Published in the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2024)"},{"id":"http://arxiv.org/abs/2403.05054v1","updated":"2024-03-08T05:01:43Z","published":"2024-03-08T05:01:43Z","title":"A Sinkhorn-type Algorithm for Constrained Optimal Transport","summary":"  Entropic optimal transport (OT) and the Sinkhorn algorithm have made it\npractical for machine learning practitioners to perform the fundamental task of\ncalculating transport distance between statistical distributions. In this work,\nwe focus on a general class of OT problems under a combination of equality and\ninequality constraints. We derive the corresponding entropy regularization\nformulation and introduce a Sinkhorn-type algorithm for such constrained OT\nproblems supported by theoretical guarantees. We first bound the approximation\nerror when solving the problem through entropic regularization, which reduces\nexponentially with the increase of the regularization parameter. Furthermore,\nwe prove a sublinear first-order convergence rate of the proposed Sinkhorn-type\nalgorithm in the dual space by characterizing the optimization procedure with a\nLyapunov function. To achieve fast and higher-order convergence under weak\nentropy regularization, we augment the Sinkhorn-type algorithm with dynamic\nregularization scheduling and second-order acceleration. Overall, this work\nsystematically combines recent theoretical and numerical advances in entropic\noptimal transport with the constrained case, allowing practitioners to derive\napproximate transport plans in complex scenarios.\n","authors":["Xun Tang","Holakou Rahmanian","Michael Shavlovsky","Kiran Koshy Thekumparampil","Tesi Xiao","Lexing Ying"],"pdf_url":"https://arxiv.org/pdf/2403.05054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.14545v3","updated":"2024-03-08T04:50:17Z","published":"2022-05-28T23:59:50Z","title":"Functional Linear Regression of Cumulative Distribution Functions","summary":"  The estimation of cumulative distribution functions (CDF) is an important\nlearning task with a great variety of downstream applications, such as risk\nassessments in predictions and decision making. In this paper, we study\nfunctional regression of contextual CDFs where each data point is sampled from\na linear combination of context dependent CDF basis functions. We propose\nfunctional ridge-regression-based estimation methods that estimate CDFs\naccurately everywhere. In particular, given $n$ samples with $d$ basis\nfunctions, we show estimation error upper bounds of $\\widetilde O(\\sqrt{d/n})$\nfor fixed design, random design, and adversarial context cases. We also derive\nmatching information theoretic lower bounds, establishing minimax optimality\nfor CDF functional regression. Furthermore, we remove the burn-in time in the\nrandom design setting using an alternative penalized estimator. Then, we\nconsider agnostic settings where there is a mismatch in the data generation\nprocess. We characterize the error of the proposed estimators in terms of the\nmismatched error, and show that the estimators are well-behaved under model\nmismatch. Moreover, to complete our study, we formalize infinite dimensional\nmodels where the parameter space is an infinite dimensional Hilbert space, and\nestablish a self-normalized estimation error upper bound for this setting.\nNotably, the upper bound reduces to the $\\widetilde O(\\sqrt{d/n})$ bound when\nthe parameter space is constrained to be $d$-dimensional. Our comprehensive\nnumerical experiments validate the efficacy of our estimation methods in both\nsynthetic and practical settings.\n","authors":["Qian Zhang","Anuran Makur","Kamyar Azizzadenesheli"],"pdf_url":"https://arxiv.org/pdf/2205.14545v3.pdf","comment":"56 pages, 7 figures, accepted by TMLR"},{"id":"http://arxiv.org/abs/2403.05045v1","updated":"2024-03-08T04:44:25Z","published":"2024-03-08T04:44:25Z","title":"Are Human Conversations Special? A Large Language Model Perspective","summary":"  This study analyzes changes in the attention mechanisms of large language\nmodels (LLMs) when used to understand natural conversations between humans\n(human-human). We analyze three use cases of LLMs: interactions over web\ncontent, code, and mathematical texts. By analyzing attention distance,\ndispersion, and interdependency across these domains, we highlight the unique\nchallenges posed by conversational data. Notably, conversations require nuanced\nhandling of long-term contextual relationships and exhibit higher complexity\nthrough their attention patterns. Our findings reveal that while language\nmodels exhibit domain-specific attention behaviors, there is a significant gap\nin their ability to specialize in human conversations. Through detailed\nattention entropy analysis and t-SNE visualizations, we demonstrate the need\nfor models trained with a diverse array of high-quality conversational data to\nenhance understanding and generation of human-like dialogue. This research\nhighlights the importance of domain specialization in language models and\nsuggests pathways for future advancement in modeling human conversational\nnuances.\n","authors":["Toshish Jawale","Chaitanya Animesh","Sekhar Vallath","Kartik Talamadupula","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.05045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05034v1","updated":"2024-03-08T04:25:29Z","published":"2024-03-08T04:25:29Z","title":"CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction\n  Model","summary":"  Feed-forward 3D generative models like the Large Reconstruction Model (LRM)\nhave demonstrated exceptional generation speed. However, the transformer-based\nmethods do not leverage the geometric priors of the triplane component in their\narchitecture, often leading to sub-optimal quality given the limited size of 3D\ndata and slow training. In this work, we present the Convolutional\nReconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D\ngenerative model. Recognizing the limitations posed by sparse 3D data, we\nhighlight the necessity of integrating geometric priors into network design.\nCRM builds on the key observation that the visualization of triplane exhibits\nspatial correspondence of six orthographic images. First, it generates six\northographic view images from a single input image, then feeds these images\ninto a convolutional U-Net, leveraging its strong pixel-level alignment\ncapabilities and significant bandwidth to create a high-resolution triplane.\nCRM further employs Flexicubes as geometric representation, facilitating direct\nend-to-end optimization on textured meshes. Overall, our model delivers a\nhigh-fidelity textured mesh from an image in just 10 seconds, without any\ntest-time optimization.\n","authors":["Zhengyi Wang","Yikai Wang","Yifei Chen","Chendong Xiang","Shuo Chen","Dajiang Yu","Chongxuan Li","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.05034v1.pdf","comment":"Project page: https://ml.cs.tsinghua.edu.cn/~zhengyi/CRM/"},{"id":"http://arxiv.org/abs/2403.05033v1","updated":"2024-03-08T04:23:50Z","published":"2024-03-08T04:23:50Z","title":"Quantifying Manifolds: Do the manifolds learned by Generative\n  Adversarial Networks converge to the real data manifold","summary":"  This paper presents our experiments to quantify the manifolds learned by ML\nmodels (in our experiment, we use a GAN model) as they train. We compare the\nmanifolds learned at each epoch to the real manifolds representing the real\ndata. To quantify a manifold, we study the intrinsic dimensions and topological\nfeatures of the manifold learned by the ML model, how these metrics change as\nwe continue to train the model, and whether these metrics convergence over the\ncourse of training to the metrics of the real data manifold.\n","authors":["Anupam Chaudhuri","Anj Simmons","Mohamed Abdelrazek"],"pdf_url":"https://arxiv.org/pdf/2403.05033v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.13102"},{"id":"http://arxiv.org/abs/2403.05030v1","updated":"2024-03-08T04:22:48Z","published":"2024-03-08T04:22:48Z","title":"Defending Against Unforeseen Failure Modes with Latent Adversarial\n  Training","summary":"  AI systems sometimes exhibit harmful unintended behaviors post-deployment.\nThis is often despite extensive diagnostics and debugging by developers.\nMinimizing risks from models is challenging because the attack surface is so\nlarge. It is not tractable to exhaustively search for inputs that may cause a\nmodel to fail. Red-teaming and adversarial training (AT) are commonly used to\nmake AI systems more robust. However, they have not been sufficient to avoid\nmany real-world failure modes that differ from the ones adversarially trained\non. In this work, we utilize latent adversarial training (LAT) to defend\nagainst vulnerabilities without generating inputs that elicit them. LAT\nleverages the compressed, abstract, and structured latent representations of\nconcepts that the network actually uses for prediction. We use LAT to remove\ntrojans and defend against held-out classes of adversarial attacks. We show in\nimage classification, text classification, and text generation tasks that LAT\nusually improves both robustness and performance on clean data relative to AT.\nThis suggests that LAT can be a promising tool for defending against failure\nmodes that are not explicitly identified by developers.\n","authors":["Stephen Casper","Lennart Schulze","Oam Patel","Dylan Hadfield-Menell"],"pdf_url":"https://arxiv.org/pdf/2403.05030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05026v1","updated":"2024-03-08T04:07:23Z","published":"2024-03-08T04:07:23Z","title":"Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts","summary":"  Dynamic graph neural networks (DyGNNs) currently struggle with handling\ndistribution shifts that are inherent in dynamic graphs. Existing work on\nDyGNNs with out-of-distribution settings only focuses on the time domain,\nfailing to handle cases involving distribution shifts in the spectral domain.\nIn this paper, we discover that there exist cases with distribution shifts\nunobservable in the time domain while observable in the spectral domain, and\npropose to study distribution shifts on dynamic graphs in the spectral domain\nfor the first time. However, this investigation poses two key challenges: i) it\nis non-trivial to capture different graph patterns that are driven by various\nfrequency components entangled in the spectral domain; and ii) it remains\nunclear how to handle distribution shifts with the discovered spectral\npatterns. To address these challenges, we propose Spectral Invariant Learning\nfor Dynamic Graphs under Distribution Shifts (SILD), which can handle\ndistribution shifts on dynamic graphs by capturing and utilizing invariant and\nvariant spectral patterns. Specifically, we first design a DyGNN with Fourier\ntransform to obtain the ego-graph trajectory spectrums, allowing the mixed\ndynamic graph patterns to be transformed into separate frequency components. We\nthen develop a disentangled spectrum mask to filter graph dynamics from various\nfrequency components and discover the invariant and variant spectral patterns.\nFinally, we propose invariant spectral filtering, which encourages the model to\nrely on invariant patterns for generalization under distribution shifts.\nExperimental results on synthetic and real-world dynamic graph datasets\ndemonstrate the superiority of our method for both node classification and link\nprediction tasks under distribution shifts.\n","authors":["Zeyang Zhang","Xin Wang","Ziwei Zhang","Zhou Qin","Weigao Wen","Hui Xue","Haoyang Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.05026v1.pdf","comment":"NeurIPS'23"},{"id":"http://arxiv.org/abs/2402.04437v2","updated":"2024-03-08T04:03:27Z","published":"2024-02-06T22:15:09Z","title":"Structured Entity Extraction Using Large Language Models","summary":"  Recent advances in machine learning have significantly impacted the field of\ninformation extraction, with Large Language Models (LLMs) playing a pivotal\nrole in extracting structured information from unstructured text. This paper\nexplores the challenges and limitations of current methodologies in structured\nentity extraction and introduces a novel approach to address these issues. We\ncontribute to the field by first introducing and formalizing the task of\nStructured Entity Extraction (SEE), followed by proposing Approximate Entity\nSet OverlaP (AESOP) Metric designed to appropriately assess model performance\non this task. Later, we propose a new model that harnesses the power of LLMs\nfor enhanced effectiveness and efficiency through decomposing the entire\nextraction task into multiple stages. Quantitative evaluation and human\nside-by-side evaluation confirm that our model outperforms baselines, offering\npromising directions for future advancements in structured entity extraction.\n","authors":["Haolun Wu","Ye Yuan","Liana Mikaelyan","Alexander Meulemans","Xue Liu","James Hensman","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.04437v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.05024v1","updated":"2024-03-08T04:02:34Z","published":"2024-03-08T04:02:34Z","title":"A Probabilistic Hadamard U-Net for MRI Bias Field Correction","summary":"  Magnetic field inhomogeneity correction remains a challenging task in MRI\nanalysis. Most established techniques are designed for brain MRI by supposing\nthat image intensities in the identical tissue follow a uniform distribution.\nSuch an assumption cannot be easily applied to other organs, especially those\nthat are small in size and heterogeneous in texture (large variations in\nintensity), such as the prostate. To address this problem, this paper proposes\na probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field\ncorrection. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the\nlow-frequency scalar field, multiplied by the original input to obtain the\nprototypical corrected image. HU-Net converts the input image from the time\ndomain into the frequency domain via Hadamard transform. In the frequency\ndomain, high-frequency components are eliminated using the trainable filter\n(scaling layer), hard-thresholding layer, and sparsity penalty. Next, a\nconditional variational autoencoder is used to encode possible bias\nfield-corrected variants into a low-dimensional latent space. Random samples\ndrawn from latent space are then incorporated with a prototypical corrected\nimage to generate multiple plausible images. Experimental results demonstrate\nthe effectiveness of PHU-Net in correcting bias-field in prostate MRI with a\nfast inference speed. It has also been shown that prostate MRI segmentation\naccuracy improves with the high-quality corrected images from PHU-Net. The code\nwill be available in the final version of this manuscript.\n","authors":["Xin Zhu","Hongyi Pan","Yury Velichko","Adam B. Murphy","Ashley Ross","Baris Turkbey","Ahmet Enis Cetin","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2403.05024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15545v2","updated":"2024-03-08T03:49:24Z","published":"2023-11-27T05:21:08Z","title":"Out-of-Distribution Generalized Dynamic Graph Neural Network for Human\n  Albumin Prediction","summary":"  Human albumin is essential for indicating the body's overall health.\nAccurately predicting plasma albumin levels and determining appropriate doses\nare urgent clinical challenges, particularly in critically ill patients, to\nmaintain optimal blood levels. However, human albumin prediction is non-trivial\nthat has to leverage the dynamics of biochemical markers as well as the\nexperience of treating patients. Moreover, the problem of distribution shift is\noften encountered in real clinical data, which may lead to a decline in the\nmodel prediction performance and reduce the reliability of the model's\napplication. In this paper, we propose a framework named Out-of-Distribution\nGeneralized Dynamic Graph Neural Network for Human Albumin Prediction\n(DyG-HAP), which is able to provide accurate albumin predictions for Intensity\nCare Unit (ICU) patients during hospitalization. We first model human albumin\nprediction as a dynamic graph regression problem to model the dynamics and\npatient relationship. Then, we propose a disentangled dynamic graph attention\nmechanism to capture and disentangle the patterns whose relationship to labels\nunder distribution shifts is invariant and variant respectively. Last, we\npropose an invariant dynamic graph regression method to encourage the model to\nrely on invariant patterns to make predictions. Moreover, we propose a dataset\nnamed Albumin level testing and nutritional dosing data for Intensive Care\n(ANIC) for evaluation. Extensive experiments demonstrate the superiority of our\nmethod compared to several baseline methods in human albumin prediction.\n","authors":["Zeyang Zhang","Xingwang Li","Fei Teng","Ning Lin","Xueling Zhu","Xin Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.15545v2.pdf","comment":"MedAI'23"},{"id":"http://arxiv.org/abs/2109.01051v2","updated":"2024-03-08T03:47:22Z","published":"2021-09-02T16:07:02Z","title":"Can Error Mitigation Improve Trainability of Noisy Variational Quantum\n  Algorithms?","summary":"  Variational Quantum Algorithms (VQAs) are often viewed as the best hope for\nnear-term quantum advantage. However, recent studies have shown that noise can\nseverely limit the trainability of VQAs, e.g., by exponentially flattening the\ncost landscape and suppressing the magnitudes of cost gradients. Error\nMitigation (EM) shows promise in reducing the impact of noise on near-term\ndevices. Thus, it is natural to ask whether EM can improve the trainability of\nVQAs. In this work, we first show that, for a broad class of EM strategies,\nexponential cost concentration cannot be resolved without committing\nexponential resources elsewhere. This class of strategies includes as special\ncases Zero Noise Extrapolation, Virtual Distillation, Probabilistic Error\nCancellation, and Clifford Data Regression. Second, we perform analytical and\nnumerical analysis of these EM protocols, and we find that some of them (e.g.,\nVirtual Distillation) can make it harder to resolve cost function values\ncompared to running no EM at all. As a positive result, we do find numerical\nevidence that Clifford Data Regression (CDR) can aid the training process in\ncertain settings where cost concentration is not too severe. Our results show\nthat care should be taken in applying EM protocols as they can either worsen or\nnot improve trainability. On the other hand, our positive results for CDR\nhighlight the possibility of engineering error mitigation methods to improve\ntrainability.\n","authors":["Samson Wang","Piotr Czarnik","Andrew Arrasmith","M. Cerezo","Lukasz Cincio","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2109.01051v2.pdf","comment":"24+29 pages, 6+4 figures"},{"id":"http://arxiv.org/abs/2403.02545v2","updated":"2024-03-08T03:39:59Z","published":"2024-03-04T23:40:20Z","title":"Wukong: Towards a Scaling Law for Large-Scale Recommendation","summary":"  Scaling laws play an instrumental role in the sustainable improvement in\nmodel quality. Unfortunately, recommendation models to date do not exhibit such\nlaws similar to those observed in the domain of large language models, due to\nthe inefficiencies of their upscaling mechanisms. This limitation poses\nsignificant challenges in adapting these models to increasingly more complex\nreal-world datasets. In this paper, we propose an effective network\narchitecture based purely on stacked factorization machines, and a synergistic\nupscaling strategy, collectively dubbed Wukong, to establish a scaling law in\nthe domain of recommendation. Wukong's unique design makes it possible to\ncapture diverse, any-order of interactions simply through taller and wider\nlayers. We conducted extensive evaluations on six public datasets, and our\nresults demonstrate that Wukong consistently outperforms state-of-the-art\nmodels quality-wise. Further, we assessed Wukong's scalability on an internal,\nlarge-scale dataset. The results show that Wukong retains its superiority in\nquality over state-of-the-art models, while holding the scaling law across two\norders of magnitude in model complexity, extending beyond 100 Gflop or\nequivalently up to Large Language Model (GPT-3) training compute scale, where\nprior arts fall short.\n","authors":["Buyun Zhang","Liang Luo","Yuxin Chen","Jade Nie","Xi Liu","Daifeng Guo","Yanli Zhao","Shen Li","Yuchen Hao","Yantao Yao","Guna Lakshminarayanan","Ellie Dingqiao Wen","Jongsoo Park","Maxim Naumov","Wenlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.02545v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2311.16496v2","updated":"2024-03-08T03:39:24Z","published":"2023-11-27T08:49:26Z","title":"DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection","summary":"  The spread of fake news using out-of-context images has become widespread and\nis a relevant problem in this era of information overload. Such out-of-context\nfake news may arise across different domains like politics, sports,\nentertainment, etc. In practical scenarios, an inherent problem of imbalance\nexists among news articles from such widely varying domains, resulting in a few\ndomains with abundant data, while the rest containing very limited data. Under\nsuch circumstances, it is imperative to develop methods which can work in such\nvarying amounts of data setting. In this work, we explore whether out-of-domain\ndata can help to improve out-of-context misinformation detection (termed here\nas multi-modal fake news detection) of a desired domain, to address this\nchallenging problem. Towards this goal, we propose a novel framework termed\nDPOD (Domain-specific Prompt-tuning using Out-of-Domain data). First, to\ncompute generalizable features, we modify the Vision-Language Model, CLIP to\nextract features that helps to align the representations of the images and\ncorresponding text captions of both the in-domain and out-of-domain data in a\nlabel-aware manner. Further, we propose a domain-specific prompt learning\ntechnique which leverages the training samples of all the available domains\nbased on the extent they can be useful to the desired domain. Extensive\nexperiments on a large-scale benchmark dataset, namely NewsCLIPpings\ndemonstrate that the proposed framework achieves state of-the-art performance,\nsignificantly surpassing the existing approaches for this challenging task.\nCode will be released on acceptance.\n","authors":["Debarshi Brahma","Amartya Bhattacharya","Suraj Nagaje Mahadev","Anmol Asati","Vikas Verma","Soma Biswas"],"pdf_url":"https://arxiv.org/pdf/2311.16496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05014v1","updated":"2024-03-08T03:27:58Z","published":"2024-03-08T03:27:58Z","title":"Simple Multigraph Convolution Networks","summary":"  Existing multigraph convolution methods either ignore the cross-view\ninteraction among multiple graphs, or induce extremely high computational cost\ndue to standard cross-view polynomial operators. To alleviate this problem,\nthis paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which\nfirst extracts consistent cross-view topology from multigraphs including\nedge-level and subgraph-level topology, then performs polynomial expansion\nbased on raw multigraphs and consistent topologies. In theory, SMGCN utilizes\nthe consistent topologies in polynomial expansion rather than standard\ncross-view polynomial expansion, which performs credible cross-view spatial\nmessage-passing, follows the spectral convolution paradigm, and effectively\nreduces the complexity of standard polynomial expansion. In the simulations,\nexperimental results demonstrate that SMGCN achieves state-of-the-art\nperformance on ACM and DBLP multigraph benchmark datasets. Our codes are\navailable at https://github.com/frinkleko/SMGCN.\n","authors":["Danyang Wu","Xinjie Shen","Jitao Lu","Jin Xu","Feiping Nie"],"pdf_url":"https://arxiv.org/pdf/2403.05014v1.pdf","comment":"Accepted by WWW 2024 Short"},{"id":"http://arxiv.org/abs/2403.03542v3","updated":"2024-03-08T03:24:00Z","published":"2024-03-06T08:38:34Z","title":"DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE\n  Pre-Training","summary":"  Pre-training has been investigated to improve the efficiency and performance\nof training neural operators in data-scarce settings. However, it is largely in\nits infancy due to the inherent complexity and diversity, such as long\ntrajectories, multiple scales and varying dimensions of partial differential\nequations (PDEs) data. In this paper, we present a new auto-regressive\ndenoising pre-training strategy, which allows for more stable and efficient\npre-training on PDE data and generalizes to various downstream tasks. Moreover,\nby designing a flexible and scalable model architecture based on Fourier\nattention, we can easily scale up the model for large-scale pre-training. We\ntrain our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets\nwith more than 100k trajectories. Extensive experiments show that we achieve\nSOTA on these benchmarks and validate the strong generalizability of our model\nto significantly enhance performance on diverse downstream PDE tasks like 3D\ndata. Code is available at \\url{https://github.com/thu-ml/DPOT}.\n","authors":["Zhongkai Hao","Chang Su","Songming Liu","Julius Berner","Chengyang Ying","Hang Su","Anima Anandkumar","Jian Song","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.03542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01669v2","updated":"2024-03-08T03:19:39Z","published":"2023-06-02T16:43:05Z","title":"Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label\n  Prompt Tuning","summary":"  Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is\noften necessary to optimize their performance. However, a major obstacle is the\nlimited availability of labeled data. We study the use of pseudolabels, i.e.,\nheuristic labels for unlabeled data, to enhance CLIP via prompt tuning.\nConventional pseudolabeling trains a model on labeled data and then generates\nlabels for unlabeled data. VLMs' zero-shot capabilities enable a \"second\ngeneration\" of pseudolabeling approaches that do not require task-specific\ntraining on labeled data. By using zero-shot pseudolabels as a source of\nsupervision, we observe that learning paradigms such as semi-supervised,\ntransductive zero-shot, and unsupervised learning can all be seen as optimizing\nthe same loss function. This unified view enables the development of versatile\ntraining strategies that are applicable across learning paradigms. We\ninvestigate them on image classification tasks where CLIP exhibits limitations,\nby varying prompt modalities, e.g., textual or visual prompts, and learning\nparadigms. We find that (1) unexplored prompt tuning strategies that\niteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5\npoints in semi-supervised learning, by 28.4 points in transductive zero-shot\nlearning, and by 15.2 points in unsupervised learning, and (2) unlike\nconventional semi-supervised pseudolabeling, which exacerbates model biases\ntoward classes with higher-quality pseudolabels, prompt tuning leads to a more\nequitable distribution of per-class accuracy. The code to reproduce the\nexperiments is at https://github.com/BatsResearch/menghini-neurips23-code.\n","authors":["Cristina Menghini","Andrew Delworth","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2306.01669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05006v1","updated":"2024-03-08T03:05:11Z","published":"2024-03-08T03:05:11Z","title":"Provable Multi-Party Reinforcement Learning with Diverse Human Feedback","summary":"  Reinforcement learning with human feedback (RLHF) is an emerging paradigm to\nalign models with human preferences. Typically, RLHF aggregates preferences\nfrom multiple individuals who have diverse viewpoints that may conflict with\neach other. Our work \\textit{initiates} the theoretical study of multi-party\nRLHF that explicitly models the diverse preferences of multiple individuals. We\nshow how traditional RLHF approaches can fail since learning a single reward\nfunction cannot capture and balance the preferences of multiple individuals. To\novercome such limitations, we incorporate meta-learning to learn multiple\npreferences and adopt different social welfare functions to aggregate the\npreferences across multiple parties. We focus on the offline learning setting\nand establish sample complexity bounds, along with efficiency and fairness\nguarantees, for optimizing diverse social welfare functions such as Nash,\nUtilitarian, and Leximin welfare functions. Our results show a separation\nbetween the sample complexities of multi-party RLHF and traditional\nsingle-party RLHF. Furthermore, we consider a reward-free setting, where each\nindividual's preference is no longer consistent with a reward model, and give\npessimistic variants of the von Neumann Winner based on offline preference\ndata. Taken together, our work showcases the advantage of multi-party RLHF but\nalso highlights its more demanding statistical complexity.\n","authors":["Huiying Zhong","Zhun Deng","Weijie J. Su","Zhiwei Steven Wu","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05004v1","updated":"2024-03-08T03:03:20Z","published":"2024-03-08T03:03:20Z","title":"Can't Remember Details in Long Documents? You Need Some R&R","summary":"  Long-context large language models (LLMs) hold promise for tasks such as\nquestion-answering (QA) over long documents, but they tend to miss important\ninformation in the middle of context documents (arXiv:2307.03172v3). Here, we\nintroduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods\ncalled $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to\nalleviate this effect in document-based QA. In reprompting, we repeat the\nprompt instructions periodically throughout the context document to remind the\nLLM of its original task. In ICR, rather than instructing the LLM to answer the\nquestion directly, we instruct it to retrieve the top $k$ passage numbers most\nrelevant to the given question, which are then used as an abbreviated context\nin a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents\nup to 80k tokens in length and observe a 16-point boost in QA accuracy on\naverage. Our further analysis suggests that R&R improves performance on long\ndocument-based QA because it reduces the distance between relevant context and\nthe instructions. Finally, we show that compared to short-context chunkwise\nmethods, R&R enables the use of larger chunks that cost fewer LLM calls and\noutput tokens, while minimizing the drop in accuracy.\n","authors":["Devanshu Agrawal","Shang Gao","Martin Gajek"],"pdf_url":"https://arxiv.org/pdf/2403.05004v1.pdf","comment":"13 pages, 1 figure, 9 tables. For associated code repository see\n  https://github.com/casetext/r-and-r"},{"id":"http://arxiv.org/abs/2310.17110v2","updated":"2024-03-08T03:03:10Z","published":"2023-10-26T02:37:43Z","title":"LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on\n  Dynamic Graphs?","summary":"  In an era marked by the increasing adoption of Large Language Models (LLMs)\nfor various tasks, there is a growing focus on exploring LLMs' capabilities in\nhandling web data, particularly graph data. Dynamic graphs, which capture\ntemporal network evolution patterns, are ubiquitous in real-world web data.\nEvaluating LLMs' competence in understanding spatial-temporal information on\ndynamic graphs is essential for their adoption in web applications, which\nremains unexplored in the literature. In this paper, we bridge the gap via\nproposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic\ngraphs, to the best of our knowledge, for the first time. Specifically, we\npropose the LLM4DyG benchmark, which includes nine specially designed tasks\nconsidering the capability evaluation of LLMs from both temporal and spatial\ndimensions. Then, we conduct extensive experiments to analyze the impacts of\ndifferent data generators, data statistics, prompting techniques, and LLMs on\nthe model performance. Finally, we propose Disentangled Spatial-Temporal\nThoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal\nunderstanding abilities. Our main observations are: 1) LLMs have preliminary\nspatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph\ntasks show increasing difficulties for LLMs as the graph size and density\nincrease, while not sensitive to the time span and data generation mechanism,\n3) the proposed DST2 prompting method can help to improve LLMs'\nspatial-temporal understanding abilities on dynamic graphs for most tasks. The\ndata and codes will be open-sourced at publication time.\n","authors":["Zeyang Zhang","Xin Wang","Ziwei Zhang","Haoyang Li","Yijian Qin","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.17110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16208v3","updated":"2024-03-08T02:50:45Z","published":"2023-06-28T13:43:46Z","title":"Continuous-time q-learning for mean-field control problems","summary":"  This paper studies the q-learning, recently coined as the continuous time\ncounterpart of Q-learning by Jia and Zhou (2023), for continuous time\nMckean-Vlasov control problems in the setting of entropy-regularized\nreinforcement learning. In contrast to the single agent's control problem in\nJia and Zhou (2023), the mean-field interaction of agents renders the\ndefinition of the q-function more subtle, for which we reveal that two distinct\nq-functions naturally arise: (i) the integrated q-function (denoted by $q$) as\nthe first-order approximation of the integrated Q-function introduced in Gu,\nGuo, Wei and Xu (2023), which can be learnt by a weak martingale condition\ninvolving test policies; and (ii) the essential q-function (denoted by $q_e$)\nthat is employed in the policy improvement iterations. We show that two\nq-functions are related via an integral representation under all test policies.\nBased on the weak martingale condition and our proposed searching method of\ntest policies, some model-free learning algorithms are devised. In two\nexamples, one in LQ control framework and one beyond LQ control framework, we\ncan obtain the exact parameterization of the optimal value function and\nq-functions and illustrate our algorithms with simulation experiments.\n","authors":["Xiaoli Wei","Xiang Yu"],"pdf_url":"https://arxiv.org/pdf/2306.16208v3.pdf","comment":"Keywords: Continuous-time reinforcement learning, continuous-time\n  q-function, Mckean-Vlasov control, weak martingale characterization, test\n  policies"},{"id":"http://arxiv.org/abs/2312.16083v2","updated":"2024-03-08T02:41:37Z","published":"2023-12-26T15:11:55Z","title":"A Variational Autoencoder for Neural Temporal Point Processes with\n  Dynamic Latent Graphs","summary":"  Continuously-observed event occurrences, often exhibit self- and\nmutually-exciting effects, which can be well modeled using temporal point\nprocesses. Beyond that, these event dynamics may also change over time, with\ncertain periodic trends. We propose a novel variational auto-encoder to capture\nsuch a mixture of temporal dynamics. More specifically, the whole time interval\nof the input sequence is partitioned into a set of sub-intervals. The event\ndynamics are assumed to be stationary within each sub-interval, but could be\nchanging across those sub-intervals. In particular, we use a sequential latent\nvariable model to learn a dependency graph between the observed dimensions, for\neach sub-interval. The model predicts the future event times, by using the\nlearned dependency graph to remove the noncontributing influences of past\nevents. By doing so, the proposed model demonstrates its higher accuracy in\npredicting inter-event times and event types for several real-world event\nsequences, compared with existing state of the art neural point processes.\n","authors":["Sikun Yang","Hongyuan Zha"],"pdf_url":"https://arxiv.org/pdf/2312.16083v2.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2312.07624v2","updated":"2024-03-08T02:37:16Z","published":"2023-12-12T06:35:56Z","title":"A dynamical clipping approach with task feedback for Proximal Policy\n  Optimization","summary":"  Proximal Policy Optimization (PPO) has been broadly applied to various\ndomains, including Large Language Model (LLM) optimization and Robotics\nlearning, etc. However, PPO is limited by a fixed setting for the clipping\nbound. Specifically, there is no theoretical proof that the optimal clipping\nbound remains consistent throughout the entire training process. Truncating the\nratio of the new and old policies with a unique clipping bound ensures stable\ntraining and can achieve the best training performance. Additionally, previous\nresearch suggests that a fixed clipping bound limits the agent's exploration.\nTherefore, researching a dynamical clipping bound to enhance PPO's performance\ncan be highly beneficial. Different from previous clipping approaches, we\nconsider increasing the maximum cumulative Return in reinforcement learning\n(RL) tasks as the preference of the RL task, and propose a bi-level proximal\npolicy optimization paradigm, which involves not only optimizing the policy but\nalso dynamically adjusting the clipping bound to reflect the preference of the\nRL tasks to further elevate the training outcomes and stability of PPO. Based\non this bi-level proximal policy optimization paradigm, we introduce a new\nalgorithm named Preference based Proximal Policy Optimization (Pb-PPO). This\nalgorithm utilizes a multi-armed bandit algorithm to reflect RL preferences (we\nalso validate that such approach can be utilized to reflect human preference),\nrecommending the optimal clipping bound for PPO in each epoch, thereby\nachieving more stable and better training outcomes.\n","authors":["Ziqi Zhang","Jingzehua Xu","Zifeng Zhuang","Jinxin Liu","Donglin wang","Shuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.07624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04202v2","updated":"2024-03-08T02:25:09Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07222v3","updated":"2024-03-08T02:14:15Z","published":"2023-11-13T10:40:17Z","title":"Neural General Circulation Models for Weather and Climate","summary":"  General circulation models (GCMs) are the foundation of weather and climate\nprediction. GCMs are physics-based simulators which combine a numerical solver\nfor large-scale dynamics with tuned representations for small-scale processes\nsuch as cloud formation. Recently, machine learning (ML) models trained on\nreanalysis data achieved comparable or better skill than GCMs for deterministic\nweather forecasting. However, these models have not demonstrated improved\nensemble forecasts, or shown sufficient stability for long-term weather and\nclimate simulations. Here we present the first GCM that combines a\ndifferentiable solver for atmospheric dynamics with ML components, and show\nthat it can generate forecasts of deterministic weather, ensemble weather and\nclimate on par with the best ML and physics-based methods. NeuralGCM is\ncompetitive with ML models for 1-10 day forecasts, and with the European Centre\nfor Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts.\nWith prescribed sea surface temperature, NeuralGCM can accurately track climate\nmetrics such as global mean temperature for multiple decades, and climate\nforecasts with 140 km resolution exhibit emergent phenomena such as realistic\nfrequency and trajectories of tropical cyclones. For both weather and climate,\nour approach offers orders of magnitude computational savings over conventional\nGCMs. Our results show that end-to-end deep learning is compatible with tasks\nperformed by conventional GCMs, and can enhance the large-scale physical\nsimulations that are essential for understanding and predicting the Earth\nsystem.\n","authors":["Dmitrii Kochkov","Janni Yuval","Ian Langmore","Peter Norgaard","Jamie Smith","Griffin Mooers","Milan Klöwer","James Lottes","Stephan Rasp","Peter Düben","Sam Hatfield","Peter Battaglia","Alvaro Sanchez-Gonzalez","Matthew Willson","Michael P. Brenner","Stephan Hoyer"],"pdf_url":"https://arxiv.org/pdf/2311.07222v3.pdf","comment":"92 pages, 54 figures"},{"id":"http://arxiv.org/abs/2310.18882v2","updated":"2024-03-08T02:13:00Z","published":"2023-10-29T03:07:30Z","title":"Differentiable Learning of Generalized Structured Matrices for Efficient\n  Deep Neural Networks","summary":"  This paper investigates efficient deep neural networks (DNNs) to replace\ndense unstructured weight matrices with structured ones that possess desired\nproperties. The challenge arises because the optimal weight matrix structure in\npopular neural network models is obscure in most cases and may vary from layer\nto layer even in the same network. Prior structured matrices proposed for\nefficient DNNs were mostly hand-crafted without a generalized framework to\nsystematically learn them. To address this issue, we propose a generalized and\ndifferentiable framework to learn efficient structures of weight matrices by\ngradient descent. We first define a new class of structured matrices that\ncovers a wide range of structured matrices in the literature by adjusting the\nstructural parameters. Then, the frequency-domain differentiable\nparameterization scheme based on the Gaussian-Dirichlet kernel is adopted to\nlearn the structural parameters by proximal gradient descent. On the image and\nlanguage tasks, our method learns efficient DNNs with structured matrices,\nachieving lower complexity and/or higher performance than prior approaches that\nemploy low-rank, block-sparse, or block-low-rank matrices.\n","authors":["Changwoo Lee","Hun-Seok Kim"],"pdf_url":"https://arxiv.org/pdf/2310.18882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18863v2","updated":"2024-03-08T02:03:38Z","published":"2024-02-29T05:25:23Z","title":"Probabilistic Lipschitzness and the Stable Rank for Comparing\n  Explanation Models","summary":"  Explainability models are now prevalent within machine learning to address\nthe black-box nature of neural networks. The question now is which\nexplainability model is most effective. Probabilistic Lipschitzness has\ndemonstrated that the smoothness of a neural network is fundamentally linked to\nthe quality of post hoc explanations. In this work, we prove theoretical lower\nbounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and\nSmoothGrad. We propose a novel metric using probabilistic Lipschitzness,\nnormalised astuteness, to compare the robustness of explainability models.\nFurther, we prove a link between the local Lipschitz constant of a neural\nnetwork and its stable rank. We then demonstrate that the stable rank of a\nneural network provides a heuristic for the robustness of explainability\nmodels.\n","authors":["Lachlan Simpson","Kyle Millar","Adriel Cheng","Cheng-Chew Lim","Hong Gunn Chew"],"pdf_url":"https://arxiv.org/pdf/2402.18863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04990v1","updated":"2024-03-08T02:02:23Z","published":"2024-03-08T02:02:23Z","title":"Jet Discrimination with Quantum Complete Graph Neural Network","summary":"  Machine learning, particularly deep neural networks, has been widely utilized\nin high energy physics and has shown remarkable results in various\napplications. Moreover, the concept of machine learning has been extended to\nquantum computers, giving rise to a new research area known as quantum machine\nlearning. In this paper, we propose a novel variational quantum circuit model,\nQuantum Complete Graph Neural Network (QCGNN), designed for learning complete\ngraphs. We argue that QCGNN has a polynomial speedup against its classical\ncounterpart, due to the property of quantum parallelism. In this paper, we\nstudy the application of QCGNN through the challenging jet discrimination,\nwhere the jets are represented with complete graphs. Subsequently, we conduct a\ncomparative analysis with classical graph neural networks to establish a\nbenchmark.\n","authors":["Yi-An Chen","Kai-Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.04990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04978v1","updated":"2024-03-08T01:23:25Z","published":"2024-03-08T01:23:25Z","title":"Stacking as Accelerated Gradient Descent","summary":"  Stacking, a heuristic technique for training deep residual networks by\nprogressively increasing the number of layers and initializing new layers by\ncopying parameters from older layers, has proven quite successful in improving\nthe efficiency of training deep neural networks. In this paper, we propose a\ntheoretical explanation for the efficacy of stacking: viz., stacking implements\na form of Nesterov's accelerated gradient descent. The theory also covers\nsimpler models such as the additive ensembles constructed in boosting methods,\nand provides an explanation for a similar widely-used practical heuristic for\ninitializing the new classifier in each round of boosting. We also prove that\nfor certain deep linear residual networks, stacking does provide accelerated\ntraining, via a new potential function analysis of the Nesterov's accelerated\ngradient method which allows errors in updates. We conduct proof-of-concept\nexperiments to validate our theory as well.\n","authors":["Naman Agarwal","Pranjal Awasthi","Satyen Kale","Eric Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.04978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16609v4","updated":"2024-03-08T01:20:29Z","published":"2023-11-28T08:54:29Z","title":"Eigenmatrix for unstructured sparse recovery","summary":"  This note considers the unstructured sparse recovery problems in a general\nform. Examples include rational approximation, spectral function estimation,\nFourier inversion, Laplace inversion, and sparse deconvolution. The main\nchallenges are the noise in the sample values and the unstructured nature of\nthe sample locations. This note proposes the eigenmatrix, a data-driven\nconstruction with desired approximate eigenvalues and eigenvectors. The\neigenmatrix offers a new way for these sparse recovery problems. Numerical\nresults are provided to demonstrate the efficiency of the proposed method.\n","authors":["Lexing Ying"],"pdf_url":"https://arxiv.org/pdf/2311.16609v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01707v3","updated":"2024-03-08T01:00:47Z","published":"2022-03-03T13:30:28Z","title":"Testing Stationarity and Change Point Detection in Reinforcement\n  Learning","summary":"  We consider offline reinforcement learning (RL) methods in possibly\nnonstationary environments. Many existing RL algorithms in the literature rely\non the stationarity assumption that requires the system transition and the\nreward function to be constant over time. However, the stationarity assumption\nis restrictive in practice and is likely to be violated in a number of\napplications, including traffic signal control, robotics and mobile health. In\nthis paper, we develop a consistent procedure to test the nonstationarity of\nthe optimal Q-function based on pre-collected historical data, without\nadditional online data collection. Based on the proposed test, we further\ndevelop a sequential change point detection method that can be naturally\ncoupled with existing state-of-the-art RL methods for policy optimization in\nnonstationary environments. The usefulness of our method is illustrated by\ntheoretical results, simulation studies, and a real data example from the 2018\nIntern Health Study. A Python implementation of the proposed procedure is\navailable at https://github.com/limengbinggz/CUSUM-RL.\n","authors":["Mengbing Li","Chengchun Shi","Zhenke Wu","Piotr Fryzlewicz"],"pdf_url":"https://arxiv.org/pdf/2203.01707v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04305v3","updated":"2024-03-08T00:32:13Z","published":"2024-01-09T01:41:36Z","title":"Advancing Deep Active Learning & Data Subset Selection: Unifying\n  Principles with Information-Theory Intuitions","summary":"  At its core, this thesis aims to enhance the practicality of deep learning by\nimproving the label and training efficiency of deep learning models. To this\nend, we investigate data subset selection techniques, specifically active\nlearning and active sampling, grounded in information-theoretic principles.\nActive learning improves label efficiency, while active sampling enhances\ntraining efficiency. Supervised deep learning models often require extensive\ntraining with labeled data. Label acquisition can be expensive and\ntime-consuming, and training large models is resource-intensive, hindering the\nadoption outside academic research and \"big tech.\" Existing methods for data\nsubset selection in deep learning often rely on heuristics or lack a principled\ninformation-theoretic foundation. In contrast, this thesis examines several\nobjectives for data subset selection and their applications within deep\nlearning, striving for a more principled approach inspired by information\ntheory. We begin by disentangling epistemic and aleatoric uncertainty in single\nforward-pass deep neural networks, which provides helpful intuitions and\ninsights into different forms of uncertainty and their relevance for data\nsubset selection. We then propose and investigate various approaches for active\nlearning and data subset selection in (Bayesian) deep learning. Finally, we\nrelate various existing and proposed approaches to approximations of\ninformation quantities in weight or prediction space. Underpinning this work is\na principled and practical notation for information-theoretic quantities that\nincludes both random variables and observed outcomes. This thesis demonstrates\nthe benefits of working from a unified perspective and highlights the potential\nimpact of our contributions to the practical application of deep learning.\n","authors":["Andreas Kirsch"],"pdf_url":"https://arxiv.org/pdf/2401.04305v3.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2310.20673v2","updated":"2024-03-08T00:22:38Z","published":"2023-10-31T17:37:35Z","title":"Balancing Act: Constraining Disparate Impact in Sparse Models","summary":"  Model pruning is a popular approach to enable the deployment of large deep\nlearning models on edge devices with restricted computational or storage\ncapacities. Although sparse models achieve performance comparable to that of\ntheir dense counterparts at the level of the entire dataset, they exhibit high\naccuracy drops for some data sub-groups. Existing methods to mitigate this\ndisparate impact induced by pruning (i) rely on surrogate metrics that address\nthe problem indirectly and have limited interpretability; or (ii) scale poorly\nwith the number of protected sub-groups in terms of computational cost. We\npropose a constrained optimization approach that directly addresses the\ndisparate impact of pruning: our formulation bounds the accuracy change between\nthe dense and sparse models, for each sub-group. This choice of constraints\nprovides an interpretable success criterion to determine if a pruned model\nachieves acceptable disparity levels. Experimental results demonstrate that our\ntechnique scales reliably to problems involving large models and hundreds of\nprotected sub-groups.\n","authors":["Meraj Hashemizadeh","Juan Ramirez","Rohan Sukumaran","Golnoosh Farnadi","Simon Lacoste-Julien","Jose Gallego-Posada"],"pdf_url":"https://arxiv.org/pdf/2310.20673v2.pdf","comment":"Published at ICLR 2024. Code available at\n  https://github.com/merajhashemi/balancing-act"},{"id":"http://arxiv.org/abs/2403.04962v1","updated":"2024-03-08T00:15:43Z","published":"2024-03-08T00:15:43Z","title":"C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer\n  Grading","summary":"  Graph-based learning approaches, due to their ability to encode tissue/organ\nstructure information, are increasingly favored for grading colorectal cancer\nhistology images. Recent graph-based techniques involve dividing whole slide\nimages (WSIs) into smaller or medium-sized patches, and then building graphs on\neach patch for direct use in training. This method, however, fails to capture\nthe tissue structure information present in an entire WSI and relies on\ntraining from a significantly large dataset of image patches. In this paper, we\npropose a novel cell-to-patch graph convolutional network (C2P-GCN), which is a\ntwo-stage graph formation-based approach. In the first stage, it forms a\npatch-level graph based on the cell organization on each patch of a WSI. In the\nsecond stage, it forms an image-level graph based on a similarity measure\nbetween patches of a WSI considering each patch as a node of a graph. This\ngraph representation is then fed into a multi-layer GCN-based classification\nnetwork. Our approach, through its dual-phase graph construction, effectively\ngathers local structural details from individual patches and establishes a\nmeaningful connection among all patches across a WSI. As C2P-GCN integrates the\nstructural data of an entire WSI into a single graph, it allows our model to\nwork with significantly fewer training data compared to the latest models for\ncolorectal cancer. Experimental validation of C2P-GCN on two distinct\ncolorectal cancer datasets demonstrates the effectiveness of our method.\n","authors":["Sudipta Paul","Bulent Yener","Amanda W. Lund"],"pdf_url":"https://arxiv.org/pdf/2403.04962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16281v3","updated":"2024-03-08T00:15:02Z","published":"2023-03-28T19:49:58Z","title":"A \"Perspectival\" Mirror of the Elephant: Investigating Language Bias on\n  Google, ChatGPT, YouTube, and Wikipedia","summary":"  Contrary to Google Search's mission of delivering information from \"many\nangles so you can form your own understanding of the world,\" we find that\nGoogle and its most prominent returned results - Wikipedia and YouTube - simply\nreflect a narrow set of culturally dominant views tied to the search language\nfor complex topics like \"Buddhism,\" \"Liberalism,\" \"colonization,\" \"Iran\" and\n\"America.\" Simply stated, they present, to varying degrees, distinct\ninformation across the same search in different languages, a phenomenon we call\nlanguage bias. This paper presents evidence and analysis of language bias and\ndiscusses its larger social implications. We find that our online searches and\nemerging tools like ChatGPT turn us into the proverbial blind person touching a\nsmall portion of an elephant, ignorant of the existence of other cultural\nperspectives. Language bias sets a strong yet invisible cultural barrier\nonline, where each language group thinks they can see other groups through\nsearches, but in fact, what they see is their own reflection.\n","authors":["Queenie Luo","Michael J. Puett","Michael D. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.16281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04960v1","updated":"2024-03-08T00:02:30Z","published":"2024-03-08T00:02:30Z","title":"SecGPT: An Execution Isolation Architecture for LLM-Based Systems","summary":"  Large language models (LLMs) extended as systems, such as ChatGPT, have begun\nsupporting third-party applications. These LLM apps leverage the de facto\nnatural language-based automated execution paradigm of LLMs: that is, apps and\ntheir interactions are defined in natural language, provided access to user\ndata, and allowed to freely interact with each other and the system. These LLM\napp ecosystems resemble the settings of earlier computing platforms, where\nthere was insufficient isolation between apps and the system. Because\nthird-party apps may not be trustworthy, and exacerbated by the imprecision of\nthe natural language interfaces, the current designs pose security and privacy\nrisks for users. In this paper, we propose SecGPT, an architecture for\nLLM-based systems that aims to mitigate the security and privacy issues that\narise with the execution of third-party apps. SecGPT's key idea is to isolate\nthe execution of apps and more precisely mediate their interactions outside of\ntheir isolated environments. We evaluate SecGPT against a number of case study\nattacks and demonstrate that it protects against many security, privacy, and\nsafety issues that exist in non-isolated LLM-based systems. The performance\noverhead incurred by SecGPT to improve security is under 0.3x for\nthree-quarters of the tested queries. To foster follow-up research, we release\nSecGPT's source code at https://github.com/llm-platform-security/SecGPT.\n","authors":["Yuhao Wu","Franziska Roesner","Tadayoshi Kohno","Ning Zhang","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2403.04960v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.05428v1","updated":"2024-03-08T16:30:39Z","published":"2024-03-08T16:30:39Z","title":"Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker\n  Recognition","summary":"  In real-world conversations, the diversity and ambiguity of stickers often\nlead to varied interpretations based on the context, necessitating the\nrequirement for comprehensively understanding stickers and supporting\nmulti-tagging. To address this challenge, we introduce StickerTAG, the first\nmulti-tag sticker dataset comprising a collected tag set with 461 tags and\n13,571 sticker-tag pairs, designed to provide a deeper understanding of\nstickers. Recognizing multiple tags for stickers becomes particularly\nchallenging due to sticker tags usually are fine-grained attribute aware.\nHence, we propose an Attentive Attribute-oriented Prompt Learning method, ie,\nAtt$^2$PL, to capture informative features of stickers in a fine-grained manner\nto better differentiate tags. Specifically, we first apply an\nAttribute-oriented Description Generation (ADG) module to obtain the\ndescription for stickers from four attributes. Then, a Local Re-attention (LoR)\nmodule is designed to perceive the importance of local information. Finally, we\nuse prompt learning to guide the recognition process and adopt confidence\npenalty optimization to penalize the confident output distribution. Extensive\nexperiments show that our method achieves encouraging results for all commonly\nused metrics.\n","authors":["Bingbing Wang","Bin Liang","Chun-Mei Feng","Wangmeng Zuo","Zhixin Bai","Shijue Huang","Kam-Fai Wong","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.05428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05427v1","updated":"2024-03-08T16:24:42Z","published":"2024-03-08T16:24:42Z","title":"Reply with Sticker: New Dataset and Model for Sticker Retrieval","summary":"  Using stickers in online chatting is very prevalent on social media\nplatforms, where the stickers used in the conversation can express someone's\nintention/emotion/attitude in a vivid, tactful, and intuitive way. Existing\nsticker retrieval research typically retrieves stickers based on context and\nthe current utterance delivered by the user. That is, the stickers serve as a\nsupplement to the current utterance. However, in the real-world scenario, using\nstickers to express what we want to say rather than as a supplement to our\nwords only is also important. Therefore, in this paper, we create a new dataset\nfor sticker retrieval in conversation, called StickerInt, where stickers are\nused to reply to previous conversations or supplement our words. Based on the\ncreated dataset, we present a simple yet effective framework for sticker\nretrieval in conversation based on the learning of intention and the\ncross-modal relationships between conversation context and stickers, coined as\n\\textbf{Int-RA}. Specifically, we first devise a knowledge-enhanced intention\npredictor to introduce the intention information into the conversation\nrepresentations. Subsequently, a relation-aware sticker selector is devised to\nretrieve the response sticker via cross-modal relationships. Extensive\nexperiments on the created dataset show that the proposed model achieves\nstate-of-the-art performance in sticker retrieval.\n","authors":["Bin Liang","Bingbing Wang","Zhixin Bai","Qiwei Lang","Mingwei Sun","Kaiheng Hou","Kam-Fai Wong","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.05427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05261v1","updated":"2024-03-08T12:32:14Z","published":"2024-03-08T12:32:14Z","title":"Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval","summary":"  Current image-text retrieval methods have demonstrated impressive performance\nin recent years. However, they still face two problems: the inter-modal\nmatching missing problem and the intra-modal semantic loss problem. These\nproblems can significantly affect the accuracy of image-text retrieval. To\naddress these challenges, we propose a novel method called Cross-modal and\nUni-modal Soft-label Alignment (CUSA). Our method leverages the power of\nuni-modal pre-trained models to provide soft-label supervision signals for the\nimage-text retrieval model. Additionally, we introduce two alignment\ntechniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label\nAlignment (USA), to overcome false negatives and enhance similarity recognition\nbetween uni-modal samples. Our method is designed to be plug-and-play, meaning\nit can be easily applied to existing image-text retrieval models without\nchanging their original architectures. Extensive experiments on various\nimage-text retrieval models and datasets, we demonstrate that our method can\nconsistently improve the performance of image-text retrieval and achieve new\nstate-of-the-art results. Furthermore, our method can also boost the uni-modal\nretrieval performance of image-text retrieval models, enabling it to achieve\nuniversal retrieval. The code and supplementary files can be found at\nhttps://github.com/lerogo/aaai24_itr_cusa.\n","authors":["Hailang Huang","Zhijie Nie","Ziqiao Wang","Ziyu Shang"],"pdf_url":"https://arxiv.org/pdf/2403.05261v1.pdf","comment":"9 pages, Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.05192v1","updated":"2024-03-08T10:14:32Z","published":"2024-03-08T10:14:32Z","title":"An End-to-End Pipeline Perspective on Video Streaming in Best-Effort\n  Networks: A Survey and Tutorial","summary":"  Video streaming continues to captivate attention of users and service\nproviders, dominate in Internet traffic, and form a vibrant research field.\nTaking a pragmatic approach to reviewing recent research in the field, this\npaper considers the most dominant streaming paradigm, the main aspects of which\ninclude transmission of two-dimensional videos over the best-effort Internet,\nsupport from content delivery networks, and client-side bitrate adaptation. To\nmake the survey more accessible, we incorporate extensive tutorial materials.\nIn contrast with the siloed approaches of existing surveys, our paper\nholistically covers the end-to-end streaming pipeline from video capture and\nupload for server processing to distribution for playback on diverse user\ndevices. Reflecting the practical interests of respective stakeholders, our\nsurvey presents a novel perspective on end-to-end streaming and sheds light on\nthe relationships and interactions between its ingestion, processing, and\ndistribution stages. At each stage, we classify streaming designs in regard to\ntheir methodology depending on whether intuition, theory, or machine learning\nserves as a methodological basis for their core contribution. In addition to\ntasks confined to a single stage, the survey also examines transversal topics\nsuch as coding, super resolution, and quality of experience. After surveying\nmore than 200 papers, we synthesize current trends and project future\ndirections in video streaming research.\n","authors":["Leonardo Peroni","Sergey Gorinsky"],"pdf_url":"https://arxiv.org/pdf/2403.05192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05105v1","updated":"2024-03-08T07:09:30Z","published":"2024-03-08T07:09:30Z","title":"Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval","summary":"  Collecting well-matched multimedia datasets is crucial for training\ncross-modal retrieval models. However, in real-world scenarios, massive\nmultimodal data are harvested from the Internet, which inevitably contains\nPartially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data\nwill remarkably harm the cross-modal retrieval performance. Previous efforts\ntend to mitigate this problem by estimating a soft correspondence to\ndown-weight the contribution of PMPs. In this paper, we aim to address this\nchallenge from a new perspective: the potential semantic similarity among\nunpaired samples makes it possible to excavate useful knowledge from mismatched\npairs. To achieve this, we propose L2RM, a general framework based on Optimal\nTransport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to\ngenerate refined alignments by seeking a minimal-cost transport plan across\ndifferent modalities. To formalize the rematching idea in OT, first, we propose\na self-supervised cost function that automatically learns from explicit\nsimilarity-cost mapping relation. Second, we present to model a partial OT\nproblem while restricting the transport among false positives to further boost\nrefined alignments. Extensive experiments on three benchmarks demonstrate our\nL2RM significantly improves the robustness against PMPs for existing models.\nThe code is available at https://github.com/hhc1997/L2RM.\n","authors":["Haochen Han","Qinghua Zheng","Guang Dai","Minnan Luo","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05105v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.05060v1","updated":"2024-03-08T05:15:05Z","published":"2024-03-08T05:15:05Z","title":"Multimodal Infusion Tuning for Large Models","summary":"  Recent advancements in large-scale models have showcased remarkable\ngeneralization capabilities in various tasks. However, integrating multimodal\nprocessing into these models presents a significant challenge, as it often\ncomes with a high computational burden. To address this challenge, we introduce\na new parameter-efficient multimodal tuning strategy for large models in this\npaper, referred to as Multimodal Infusion Tuning (MiT). MiT leverages decoupled\nself-attention mechanisms within large language models to effectively integrate\ninformation from diverse modalities such as images and acoustics. In MiT, we\nalso design a novel adaptive rescaling strategy at the head level, which\noptimizes the representation of infused multimodal features. Notably, all\nfoundation models are kept frozen during the tuning process to reduce the\ncomputational burden(only 2.5\\% parameters are tunable). We conduct experiments\nacross a range of multimodal tasks, including image-related tasks like\nreferring segmentation and non-image tasks such as sentiment analysis. Our\nresults showcase that MiT achieves state-of-the-art performance in multimodal\nunderstanding while significantly reducing computational overhead(10\\% of\nprevious methods). Moreover, our tuned model exhibits robust reasoning\nabilities even in complex scenarios.\n","authors":["Hao Sun","Yu Song","Jihong Hu","Xinyao Yu","Jiaqing Liu","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2403.05060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05050v1","updated":"2024-03-08T04:53:53Z","published":"2024-03-08T04:53:53Z","title":"DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for\n  Streaming Perception","summary":"  Autonomous driving systems demand real-time, accurate perception to navigate\ncomplex environments. Addressing this, we introduce the Dynamic Router Network\n(DyRoNet), a framework that innovates with low-rank dynamic routing for\nenhanced streaming perception. By integrating specialized pre-trained branch\nnetworks, fine-tuned for various environmental conditions, DyRoNet achieves a\nbalance between latency and precision. Its core feature, the speed router\nmodule, intelligently directs input data to the best-suited branch network,\noptimizing performance. The extensive evaluations reveal that DyRoNet adapts\neffectively to multiple branch selection strategies, setting a new benchmark in\nperformance across a range of scenarios. DyRoNet not only establishes a new\nbenchmark for streaming perception but also provides valuable engineering\ninsights for future work. More project information is available at\nhttps://tastevision.github.io/DyRoNet/\n","authors":["Xiang Huang","Zhi-Qi Cheng","Jun-Yan He","Chenyang Li","Wangmeng Xiang","Baigui Sun","Xiao Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11351v2","updated":"2024-03-08T03:07:18Z","published":"2023-08-22T11:00:09Z","title":"MMAPS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product\n  Summarization","summary":"  Given the long textual product information and the product image, Multi-modal\nProduct Summarization (MPS) aims to increase customers' desire to purchase by\nhighlighting product characteristics with a short textual summary. Existing MPS\nmethods can produce promising results. Nevertheless, they still 1) lack\nend-to-end product summarization, 2) lack multi-grained multi-modal modeling,\nand 3) lack multi-modal attribute modeling. To improve MPS, we propose an\nend-to-end multi-grained multi-modal attribute-aware product summarization\nmethod (MMAPS) for generating high-quality product summaries in e-commerce.\nMMAPS jointly models product attributes and generates product summaries. We\ndesign several multi-grained multi-modal tasks to better guide the multi-modal\nlearning of MMAPS. Furthermore, we model product attributes based on both text\nand image modalities so that multi-modal product characteristics can be\nmanifested in the generated summaries. Extensive experiments on a real\nlarge-scale Chinese e-commence dataset demonstrate that our model outperforms\nstate-of-the-art product summarization methods w.r.t. several summarization\nmetrics. Our code is publicly available at: https://github.com/KDEGroup/MMAPS.\n","authors":["Tao Chen","Ze Lin","Hui Li","Jiayi Ji","Yiyi Zhou","Guanbin Li","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2308.11351v2.pdf","comment":"LREC-COLING 2024.11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.05658v1","updated":"2024-03-08T20:16:00Z","published":"2024-03-08T20:16:00Z","title":"Feature CAM: Interpretable AI in Image Classification","summary":"  Deep Neural Networks have often been called the black box because of the\ncomplex, deep architecture and non-transparency presented by the inner layers.\nThere is a lack of trust to use Artificial Intelligence in critical and\nhigh-precision fields such as security, finance, health, and manufacturing\nindustries. A lot of focused work has been done to provide interpretable\nmodels, intending to deliver meaningful insights into the thoughts and behavior\nof neural networks. In our research, we compare the state-of-the-art methods in\nthe Activation-based methods (ABM) for interpreting predictions of CNN models,\nspecifically in the application of Image Classification. We then extend the\nsame for eight CNN-based architectures to compare the differences in\nvisualization and thus interpretability. We introduced a novel technique\nFeature CAM, which falls in the perturbation-activation combination, to create\nfine-grained, class-discriminative visualizations. The resulting saliency maps\nfrom our experiments proved to be 3-4 times better human interpretable than the\nstate-of-the-art in ABM. At the same time it reserves machine interpretability,\nwhich is the average confidence scores in classification.\n","authors":["Frincy Clement","Ji Yang","Irene Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.05658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05628v1","updated":"2024-03-08T19:02:21Z","published":"2024-03-08T19:02:21Z","title":"AMUSE: Adaptive Multi-Segment Encoding for Dataset Watermarking","summary":"  Curating high quality datasets that play a key role in the emergence of new\nAI applications requires considerable time, money, and computational resources.\nSo, effective ownership protection of datasets is becoming critical. Recently,\nto protect the ownership of an image dataset, imperceptible watermarking\ntechniques are used to store ownership information (i.e., watermark) into the\nindividual image samples. Embedding the entire watermark into all samples leads\nto significant redundancy in the embedded information which damages the\nwatermarked dataset quality and extraction accuracy. In this paper, a\nmulti-segment encoding-decoding method for dataset watermarking (called AMUSE)\nis proposed to adaptively map the original watermark into a set of shorter\nsub-messages and vice versa. Our message encoder is an adaptive method that\nadjusts the length of the sub-messages according to the protection requirements\nfor the target dataset. Existing image watermarking methods are then employed\nto embed the sub-messages into the original images in the dataset and also to\nextract them from the watermarked images. Our decoder is then used to\nreconstruct the original message from the extracted sub-messages. The proposed\nencoder and decoder are plug-and-play modules that can easily be added to any\nwatermarking method. To this end, extensive experiments are preformed with\nmultiple watermarking solutions which show that applying AMUSE improves the\noverall message extraction accuracy upto 28% for the same given dataset\nquality. Furthermore, the image dataset quality is enhanced by a PSNR of\n$\\approx$2 dB on average, while improving the extraction accuracy for one of\nthe tested image watermarking methods.\n","authors":["Saeed Ranjbar Alvar","Mohammad Akbari"," David"," Yue","Lingyang Chu","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07938v1","updated":"2024-03-08T22:27:38Z","published":"2024-03-08T22:27:38Z","title":"Text-to-Audio Generation Synchronized with Videos","summary":"  In recent times, the focus on text-to-audio (TTA) generation has intensified,\nas researchers strive to synthesize audio from textual descriptions. However,\nmost existing methods, though leveraging latent diffusion models to learn the\ncorrelation between audio and text embeddings, fall short when it comes to\nmaintaining a seamless synchronization between the produced audio and its\nvideo. This often results in discernible audio-visual mismatches. To bridge\nthis gap, we introduce a groundbreaking benchmark for Text-to-Audio generation\nthat aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself\nwith three novel metrics dedicated to evaluating visual alignment and temporal\nconsistency. To complement this, we also present a simple yet effective\nvideo-aligned TTA generation model, namely T2AV. Moving beyond traditional\nmethods, T2AV refines the latent diffusion approach by integrating\nvisual-aligned text embeddings as its conditional foundation. It employs a\ntemporal multi-head attention transformer to extract and understand temporal\nnuances from video data, a feat amplified by our Audio-Visual ControlNet that\nadeptly merges temporal visual representations with text embeddings. Further\nenhancing this integration, we weave in a contrastive learning objective,\ndesigned to ensure that the visual-aligned text embeddings resonate closely\nwith the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench\ndemonstrate that our T2AV sets a new standard for video-aligned TTA generation\nin ensuring visual alignment and temporal consistency.\n","authors":["Shentong Mo","Jing Shi","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.07938v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.12903"}]},"2024-03-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.06970v1","updated":"2024-03-11T17:54:33Z","published":"2024-03-11T17:54:33Z","title":"MRL Parsing Without Tears: The Case of Hebrew","summary":"  Syntactic parsing remains a critical tool for relation extraction and\ninformation extraction, especially in resource-scarce languages where LLMs are\nlacking. Yet in morphologically rich languages (MRLs), where parsers need to\nidentify multiple lexical units in each token, existing systems suffer in\nlatency and setup complexity. Some use a pipeline to peel away the layers:\nfirst segmentation, then morphology tagging, and then syntax parsing; however,\nerrors in earlier layers are then propagated forward. Others use a joint\narchitecture to evaluate all permutations at once; while this improves\naccuracy, it is notoriously slow. In contrast, and taking Hebrew as a test\ncase, we present a new \"flipped pipeline\": decisions are made directly on the\nwhole-token units by expert classifiers, each one dedicated to one specific\ntask. The classifiers are independent of one another, and only at the end do we\nsynthesize their predictions. This blazingly fast approach sets a new SOTA in\nHebrew POS tagging and dependency parsing, while also reaching near-SOTA\nperformance on other Hebrew NLP tasks. Because our architecture does not rely\non any language-specific resources, it can serve as a model to develop similar\nparsers for other MRLs.\n","authors":["Shaltiel Shmidman","Avi Shmidman","Moshe Koppel","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2403.06970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06965v1","updated":"2024-03-11T17:47:47Z","published":"2024-03-11T17:47:47Z","title":"Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare\n  Linguistic Phenomena","summary":"  Argument Structure Constructions (ASCs) are one of the most well-studied\nconstruction groups, providing a unique opportunity to demonstrate the\nusefulness of Construction Grammar (CxG). For example, the caused-motion\nconstruction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates\nthat constructions must carry meaning, otherwise the fact that ``sneeze'' in\nthis context causes movement cannot be explained. We form the hypothesis that\nthis remains challenging even for state-of-the-art Large Language Models\n(LLMs), for which we devise a test based on substituting the verb with a\nprototypical motion verb. To be able to perform this test at statistically\nsignificant scale, in the absence of adequate CxG corpora, we develop a novel\npipeline of NLP-assisted collection of linguistically annotated text. We show\nhow dependency parsing and GPT-3.5 can be used to significantly reduce\nannotation cost and thus enable the annotation of rare phenomena at scale. We\nthen evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of\nthe CMC using the newly collected corpus. We find that all models struggle with\nunderstanding the motion component that the CMC adds to a sentence.\n","authors":["Leonie Weissweiler","Abdullatif Köksal","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2403.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06963v1","updated":"2024-03-11T17:47:30Z","published":"2024-03-11T17:47:30Z","title":"The pitfalls of next-token prediction","summary":"  Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this intuitive concern, which is fragmented in the literature. As a\nstarting point, we argue that the two often-conflated phases of next-token\nprediction -- autoregressive inference and teacher-forced training -- must be\ntreated distinctly. The popular criticism that errors can compound during\nautoregressive inference, crucially assumes that teacher-forcing has learned an\naccurate next-token predictor. This assumption sidesteps a more deep-rooted\nproblem we expose: in certain classes of tasks, teacher-forcing can simply fail\nto learn an accurate next-token predictor in the first place. We describe a\ngeneral mechanism of how teacher-forcing can fail, and design a minimal\nplanning task where both the Transformer and the Mamba architecture empirically\nfail in that manner -- remarkably, despite the task being straightforward to\nlearn. We provide preliminary evidence that this failure can be resolved when\ntraining to predict multiple tokens in advance. We hope this finding can ground\nfuture debates and inspire explorations beyond the next-token prediction\nparadigm. We make our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures\n","authors":["Gregor Bachmann","Vaishnavh Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2403.06963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16363v4","updated":"2024-03-11T17:46:49Z","published":"2024-02-26T07:33:05Z","title":"LLM Inference Unveiled: Survey and Roofline Model Insights","summary":"  The field of efficient Large Language Model (LLM) inference is rapidly\nevolving, presenting a unique blend of opportunities and challenges. Although\nthe field has expanded and is vibrant, there hasn't been a concise framework\nthat analyzes the various methods of LLM Inference to provide a clear\nunderstanding of this domain. Our survey stands out from traditional literature\nreviews by not only summarizing the current state of research but also by\nintroducing a framework based on roofline model for systematic analysis of LLM\ninference techniques. This framework identifies the bottlenecks when deploying\nLLMs on hardware devices and provides a clear understanding of practical\nproblems, such as why LLMs are memory-bound, how much memory and computation\nthey need, and how to choose the right hardware. We systematically collate the\nlatest advancements in efficient LLM inference, covering crucial areas such as\nmodel compression (e.g., Knowledge Distillation and Quantization), algorithm\nimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware and\nsystem-level enhancements. Our survey stands out by analyzing these methods\nwith roofline model, helping us understand their impact on memory access and\ncomputation. This distinctive approach not only showcases the current research\nlandscape but also delivers valuable insights for practical implementation,\npositioning our work as an indispensable resource for researchers new to the\nfield as well as for those seeking to deepen their understanding of efficient\nLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.\n","authors":["Zhihang Yuan","Yuzhang Shang","Yang Zhou","Zhen Dong","Zhe Zhou","Chenhao Xue","Bingzhe Wu","Zhikai Li","Qingyi Gu","Yong Jae Lee","Yan Yan","Beidi Chen","Guangyu Sun","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2402.16363v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06952v1","updated":"2024-03-11T17:35:33Z","published":"2024-03-11T17:35:33Z","title":"SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with\n  Auto-Generated Data","summary":"  Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.\n","authors":["Jialu Li","Jaemin Cho","Yi-Lin Sung","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2403.06952v1.pdf","comment":"First two authors contributed equally; Project website:\n  https://selma-t2i.github.io/"},{"id":"http://arxiv.org/abs/2403.06949v1","updated":"2024-03-11T17:34:25Z","published":"2024-03-11T17:34:25Z","title":"Materials science in the era of large language models: a perspective","summary":"  Large Language Models (LLMs) have garnered considerable interest due to their\nimpressive natural language capabilities, which in conjunction with various\nemergent properties make them versatile tools in workflows ranging from complex\ncode generation to heuristic finding for combinatorial problems. In this paper\nwe offer a perspective on their applicability to materials science research,\narguing their ability to handle ambiguous requirements across a range of tasks\nand disciplines mean they could be a powerful tool to aid researchers. We\nqualitatively examine basic LLM theory, connecting it to relevant properties\nand techniques in the literature before providing two case studies that\ndemonstrate their use in task automation and knowledge extraction at-scale. At\ntheir current stage of development, we argue LLMs should be viewed less as\noracles of novel insight, and more as tireless workers that can accelerate and\nunify exploration across domains. It is our hope that this paper can\nfamiliarise material science researchers with the concepts needed to leverage\nthese tools in their own research.\n","authors":["Ge Lei","Ronan Docherty","Samuel J. Cooper"],"pdf_url":"https://arxiv.org/pdf/2403.06949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06936v1","updated":"2024-03-11T17:21:39Z","published":"2024-03-11T17:21:39Z","title":"Counterfactual Reasoning with Knowledge Graph Embeddings","summary":"  Knowledge graph embeddings (KGEs) were originally developed to infer true but\nmissing facts in incomplete knowledge repositories. In this paper, we link\nknowledge graph completion and counterfactual reasoning via our new task CFKGR.\nWe model the original world state as a knowledge graph, hypothetical scenarios\nas edges added to the graph, and plausible changes to the graph as inferences\nfrom logical rules. We create corresponding benchmark datasets, which contain\ndiverse hypothetical scenarios with plausible changes to the original knowledge\ngraph and facts that should be retained. We develop COULDD, a general method\nfor adapting existing knowledge graph embeddings given a hypothetical premise,\nand evaluate it on our benchmark. Our results indicate that KGEs learn patterns\nin the graph without explicit training. We further observe that KGEs adapted\nwith COULDD solidly detect plausible counterfactual changes to the graph that\nfollow these patterns. An evaluation on human-annotated data reveals that KGEs\nadapted with COULDD are mostly unable to recognize changes to the graph that do\nnot follow learned inference rules. In contrast, ChatGPT mostly outperforms\nKGEs in detecting plausible changes to the graph but has poor knowledge\nretention. In summary, CFKGR connects two previously distinct areas, namely KG\ncompletion and counterfactual reasoning.\n","authors":["Lena Zellinger","Andreas Stephan","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2403.06936v1.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2403.06935v1","updated":"2024-03-11T17:20:12Z","published":"2024-03-11T17:20:12Z","title":"Naming, Describing, and Quantifying Visual Objects in Humans and LLMs","summary":"  While human speakers use a variety of different expressions when describing\nthe same object in an image, giving rise to a distribution of plausible labels\ndriven by pragmatic constraints, the extent to which current Vision \\& Language\nLarge Language Models (VLLMs) can mimic this crucial feature of language use is\nan open question. This applies to common, everyday objects, but it is\nparticularly interesting for uncommon or novel objects for which a category\nlabel may be lacking or fuzzy. Furthermore, humans show clear production\npreferences for highly context-sensitive expressions, such as the quantifiers\n`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on\nthree categories (nouns, attributes, and quantifiers) where humans show great\nsubjective variability concerning the distribution over plausible labels, using\ndatasets and resources mostly under-explored in previous work. Our results\nreveal mixed evidence on the ability of VLLMs to capture human naming\npreferences, with all models failing in tasks that require high-level reasoning\nsuch as assigning quantifiers.\n","authors":["Alberto Testoni","Juell Sprott","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2403.06935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06932v1","updated":"2024-03-11T17:18:53Z","published":"2024-03-11T17:18:53Z","title":"ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis","summary":"  Large language models (LLMs) have achieved commendable accomplishments in\nvarious natural language processing tasks. However, LLMs still encounter\nsignificant challenges when dealing with complex scenarios involving multiple\nentities. These challenges arise from the presence of implicit relationships\nthat demand multi-step reasoning. In this paper, we propose a novel approach\nERA-CoT, which aids LLMs in understanding context by capturing relationships\nbetween entities and supports the reasoning of diverse tasks through\nChain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates\nthe superior performance of our proposed method compared to current CoT\nprompting methods, achieving a significant improvement of an average of 5.1\\%\non GPT3.5 compared to previous SOTA baselines. Our analysis indicates that\nERA-CoT increases the LLM's understanding of entity relationships,\nsignificantly improves the accuracy of question answering, and enhances the\nreasoning ability of LLMs.\n","authors":["Yanming Liu","Xinyue Peng","Tianyu Du","Jianwei Yin","Weihao Liu","Xuhong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06932v1.pdf","comment":"14 pages, first version of ERA-CoT"},{"id":"http://arxiv.org/abs/2403.06925v1","updated":"2024-03-11T17:12:09Z","published":"2024-03-11T17:12:09Z","title":"Simplicity Bias of Transformers to Learn Low Sensitivity Functions","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of the inductive biases that they have and how\nthose biases are different from other neural network architectures remains\nelusive. Various neural network architectures such as fully connected networks\nhave been found to have a simplicity bias towards simple functions of the data;\none version of this simplicity bias is a spectral bias to learn simple\nfunctions in the Fourier space. In this work, we identify the notion of\nsensitivity of the model to random changes in the input as a notion of\nsimplicity bias which provides a unified metric to explain the simplicity and\nspectral bias of transformers across different data modalities. We show that\ntransformers have lower sensitivity than alternative architectures, such as\nLSTMs, MLPs and CNNs, across both vision and language tasks. We also show that\nlow-sensitivity bias correlates with improved robustness; furthermore, it can\nalso be used as an efficient intervention to further improve the robustness of\ntransformers.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v1.pdf","comment":"24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.06914v1","updated":"2024-03-11T17:03:04Z","published":"2024-03-11T17:03:04Z","title":"MEND: Meta dEmonstratioN Distillation for Efficient and Effective\n  In-Context Learning","summary":"  Large Language models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities, where a LLM makes predictions for a given test input\ntogether with a few input-output pairs (demonstrations). Nevertheless, the\ninclusion of demonstrations leads to a quadratic increase in the computational\noverhead of the self-attention mechanism. Existing solutions attempt to distill\nlengthy demonstrations into compact vectors. However, they often require\ntask-specific retraining or compromise LLM's in-context learning performance.\nTo mitigate these challenges, we present Meta dEmonstratioN Distillation\n(MEND), where a language model learns to distill any lengthy demonstrations\ninto vectors without retraining for a new downstream task. We exploit the\nknowledge distillation to enhance alignment between MEND and LLM, achieving\nboth efficiency and effectiveness simultaneously. MEND is endowed with the\nmeta-knowledge of distilling demonstrations through a two-stage training\nprocess, which includes meta-distillation pretraining and fine-tuning.\nComprehensive evaluations across seven diverse ICL task partitions using\ndecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not\nonly matches but often outperforms the Vanilla ICL as well as other\nstate-of-the-art distillation models, while significantly reducing the\ncomputational demands. This innovation promises enhanced scalability and\nefficiency for the practical deployment of large language models\n","authors":["Yichuan Li","Xiyao Ma","Sixing Lu","Kyumin Lee","Xiaohu Liu","Chenlei Guo"],"pdf_url":"https://arxiv.org/pdf/2403.06914v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.06892v1","updated":"2024-03-11T16:48:25Z","published":"2024-03-11T16:48:25Z","title":"Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head","summary":"  End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}\n","authors":["Tiancheng Zhao","Peng Liu","Xuan He","Lu Zhang","Kyusong Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06892v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.06872v1","updated":"2024-03-11T16:24:08Z","published":"2024-03-11T16:24:08Z","title":"Exploring Large Language Models and Hierarchical Frameworks for\n  Classification of Large Unstructured Legal Documents","summary":"  Legal judgment prediction suffers from the problem of long case documents\nexceeding tens of thousands of words, in general, and having a non-uniform\nstructure. Predicting judgments from such documents becomes a challenging task,\nmore so on documents with no structural annotation. We explore the\nclassification of these large legal documents and their lack of structural\ninformation with a deep-learning-based hierarchical framework which we call\nMESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment\nprediction. Specifically, we divide a document into parts to extract their\nembeddings from the last four layers of a custom fine-tuned Large Language\nModel, and try to approximate their structure through unsupervised clustering.\nWhich we use in another set of transformer encoder layers to learn the\ninter-chunk representations. We analyze the adaptability of Large Language\nModels (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the\nhierarchical framework of MESc and compare them with their standalone\nperformance on legal texts. We also study their intra-domain(legal) transfer\nlearning capability and the impact of combining embeddings from their last\nlayers in MESc. We test these methods and their effectiveness with extensive\nexperiments and ablation studies on legal documents from India, the European\nUnion, and the United States with the ILDC dataset and a subset of the LexGLUE\ndataset. Our approach achieves a minimum total performance gain of\napproximately 2 points over previous state-of-the-art methods.\n","authors":["Nishchal Prasad","Mohand Boughanem","Taoufiq Dkaki"],"pdf_url":"https://arxiv.org/pdf/2403.06872v1.pdf","comment":"This paper was accepted as a long paper at ECIR 2024. arXiv admin\n  note: substantial text overlap with arXiv:2309.10563"},{"id":"http://arxiv.org/abs/2403.06869v1","updated":"2024-03-11T16:22:41Z","published":"2024-03-11T16:22:41Z","title":"Learning with Noisy Foundation Models","summary":"  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n","authors":["Hao Chen","Jindong Wang","Zihan Wang","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2403.06869v1.pdf","comment":"18 pages, 10 figures, 6 tables, preprint. arXiv admin note:\n  substantial text overlap with arXiv:2309.17002"},{"id":"http://arxiv.org/abs/2403.06857v1","updated":"2024-03-11T16:12:34Z","published":"2024-03-11T16:12:34Z","title":"Development of a Reliable and Accessible Caregiving Language Model\n  (CaLM)","summary":"  Unlike professional caregivers, family caregivers often assume this role\nwithout formal preparation or training. Because of this, there is an urgent\nneed to enhance the capacity of family caregivers to provide quality care.\nLarge language models can potentially be used as a foundation technology for\nsupporting caregivers as educational tools or as adjunct to care. This study\naimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a\ncaregiving knowledge base, develop an accessible CaLM using a small FM that\nrequires fewer computing resources, and evaluate the performance of the model\ncompared to a large FM. We developed CaLM using the Retrieval Augmented\nGeneration (RAG) framework combined with FM fine-tuning for improving the\nquality of FM answers by grounding the model on a caregiving knowledge base. We\nused two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B\nparameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving\nknowledge base by gathering various types of documents from the Internet. In\nthis study, we focused on caregivers of individuals with Alzheimer's Disease\nRelated Dementias. We evaluated the models' performance using the benchmark\nmetrics commonly used in evaluating language models and their reliability to\nprovide accurate references with the answers. The RAG framework improved the\nperformance of all FMs used in this study across all measures. As expected, the\nlarge FM performed better than small FMs across all metrics. The most\ninteresting result is that small fine-tuned FMs with RAG performed\nsignificantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2\nsmall FM performed better than GPT 3.5 (even with RAG) in returning references\nwith the answers. The study shows that reliable and accessible CaLM can be\ndeveloped by using small FMs with a knowledge base specific to the caregiving\ndomain.\n","authors":["Bambang Parmanto","Bayu Aryoyudanta","Wilbert Soekinto","I Made Agus Setiawan","Yuhan Wang","Haomin Hu","Andi Saptono","Yong K. Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06840v1","updated":"2024-03-11T16:01:05Z","published":"2024-03-11T16:01:05Z","title":"RA-ISF: Learning to Answer and Understand from Retrieval Augmentation\n  via Iterative Self-Feedback","summary":"  Large language models (LLMs) demonstrate exceptional performance in numerous\ntasks but still heavily rely on knowledge stored in their parameters. Moreover,\nupdating this knowledge incurs high training costs. Retrieval-augmented\ngeneration (RAG) methods address this issue by integrating external knowledge.\nThe model can answer questions it couldn't previously by retrieving knowledge\nrelevant to the query. This approach improves performance in certain scenarios\nfor specific tasks. However, if irrelevant texts are retrieved, it may impair\nmodel performance. In this paper, we propose Retrieval Augmented Iterative\nSelf-Feedback (RA-ISF), a framework that iteratively decomposes tasks and\nprocesses them in three submodules to enhance the model's problem-solving\ncapabilities. Experiments show that our method outperforms existing benchmarks,\nperforming well on models like GPT3.5, Llama2, significantly enhancing factual\nreasoning capabilities and reducing hallucinations.\n","authors":["Yanming Liu","Xinyue Peng","Xuhong Zhang","Weihao Liu","Jianwei Yin","Jiannan Cao","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2403.06840v1.pdf","comment":"15 pages, 4 figures. Providing first version RA-ISF"},{"id":"http://arxiv.org/abs/2403.06835v1","updated":"2024-03-11T15:56:17Z","published":"2024-03-11T15:56:17Z","title":"Medical Image Synthesis via Fine-Grained Image-Text Alignment and\n  Anatomy-Pathology Prompting","summary":"  Data scarcity and privacy concerns limit the availability of high-quality\nmedical images for public use, which can be mitigated through medical image\nsynthesis. However, current medical image synthesis methods often struggle to\naccurately capture the complexity of detailed anatomical structures and\npathological conditions. To address these challenges, we propose a novel\nmedical image synthesis model that leverages fine-grained image-text alignment\nand anatomy-pathology prompts to generate highly detailed and accurate\nsynthetic medical images. Our method integrates advanced natural language\nprocessing techniques with image generative modeling, enabling precise\nalignment between descriptive text prompts and the synthesized images'\nanatomical and pathological details. The proposed approach consists of two key\ncomponents: an anatomy-pathology prompting module and a fine-grained\nalignment-based synthesis module. The anatomy-pathology prompting module\nautomatically generates descriptive prompts for high-quality medical images. To\nfurther synthesize high-quality medical images from the generated prompts, the\nfine-grained alignment-based synthesis module pre-defines a visual codebook for\nthe radiology dataset and performs fine-grained alignment between the codebook\nand generated prompts to obtain key patches as visual clues, facilitating\naccurate image synthesis. We validate the superiority of our method through\nexperiments on public chest X-ray datasets and demonstrate that our synthetic\nimages preserve accurate semantic information, making them valuable for various\nmedical applications.\n","authors":["Wenting Chen","Pengyu Wang","Hui Ren","Lichao Sun","Quanzheng Li","Yixuan Yuan","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.06835v1.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2403.06833v1","updated":"2024-03-11T15:48:56Z","published":"2024-03-11T15:48:56Z","title":"Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?","summary":"  Instruction-tuned Large Language Models (LLMs) have achieved breakthrough\nresults, opening countless new possibilities for many practical applications.\nHowever, LLMs lack elementary safety features that are established norms in\nother areas of computer science, such as the separation between instructions\nand data, causing them to malfunction or rendering them vulnerable to\nmanipulation and interference by third parties e.g., via indirect\nprompt/command injection. Even worse, so far, there is not even an established\ndefinition of what precisely such a separation would mean and how its violation\ncould be tested. In this work, we aim to close this gap. We introduce a formal\nmeasure to quantify the phenomenon of instruction-data separation as well as an\nempirical variant of the measure that can be computed from a model`s black-box\noutputs. We also introduce a new dataset, SEP (Should it be Executed or\nProcessed?), which allows estimating the measure, and we report results on\nseveral state-of-the-art open-source and closed LLMs. Finally, we\nquantitatively demonstrate that all evaluated LLMs fail to achieve a high\namount of separation, according to our measure. The source code and SEP dataset\nare openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.\n","authors":["Egor Zverev","Sahar Abdelnabi","Mario Fritz","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2403.06833v1.pdf","comment":"Accepted for ICLR 2024 Workshop on Secure and Trustworthy Large\n  Language Models, GitHub:\n  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 5 pages main\n  text, 17 pages in total"},{"id":"http://arxiv.org/abs/2403.06832v1","updated":"2024-03-11T15:48:43Z","published":"2024-03-11T15:48:43Z","title":"The Power of Noise: Toward a Unified Multi-modal Knowledge Graph\n  Representation Framework","summary":"  The advancement of Multi-modal Pre-training highlights the necessity for a\nrobust Multi-Modal Knowledge Graph (MMKG) representation learning framework.\nThis framework is crucial for integrating structured knowledge into multi-modal\nLarge Language Models (LLMs) at scale, aiming to alleviate issues like\nknowledge misconceptions and multi-modal hallucinations. In this work, to\nevaluate models' ability to accurately embed entities within MMKGs, we focus on\ntwo widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and\nMulti-modal Entity Alignment (MMEA). Building on this foundation, we propose a\nnovel SNAG method that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking for the robust integration of multi-modal entity\nfeatures in KGs. By incorporating specific training objectives for both MKGC\nand MMEA, our approach achieves SOTA performance across a total of ten datasets\n(three for MKGC and seven for MEMA), demonstrating its robustness and\nversatility. Besides, SNAG can not only function as a standalone model but also\nenhance other existing methods, providing stable performance improvements. Our\ncode and data are available at: https://github.com/zjukg/SNAG.\n","authors":["Zhuo Chen","Yin Fang","Yichi Zhang","Lingbing Guo","Jiaoyan Chen","Huajun Chen","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06832v1.pdf","comment":"Ongoing work; 10 pages, 6 Tables, 2 Figures; Repo is available at\n  https://github.com/zjukg/SNAG"},{"id":"http://arxiv.org/abs/2402.19088v2","updated":"2024-03-11T15:21:57Z","published":"2024-02-29T12:13:50Z","title":"Survey in Characterization of Semantic Change","summary":"  Live languages continuously evolve to integrate the cultural change of human\nsocieties. This evolution manifests through neologisms (new words) or\n\\textbf{semantic changes} of words (new meaning to existing words).\nUnderstanding the meaning of words is vital for interpreting texts coming from\ndifferent cultures (regionalism or slang), domains (e.g., technical terms), or\nperiods. In computer science, these words are relevant to computational\nlinguistics algorithms such as translation, information retrieval, question\nanswering, etc. Semantic changes can potentially impact the quality of the\noutcomes of these algorithms. Therefore, it is important to understand and\ncharacterize these changes formally. The study of this impact is a recent\nproblem that has attracted the attention of the computational linguistics\ncommunity. Several approaches propose methods to detect semantic changes with\ngood precision, but more effort is needed to characterize how the meaning of\nwords changes and to reason about how to reduce the impact of semantic change.\nThis survey provides an understandable overview of existing approaches to the\n\\textit{characterization of semantic changes} and also formally defines three\nclasses of characterizations: if the meaning of a word becomes more general or\nnarrow (change in dimension) if the word is used in a more pejorative or\npositive/ameliorated sense (change in orientation), and if there is a trend to\nuse the word in a, for instance, metaphoric or metonymic context (change in\nrelation). We summarized the main aspects of the selected publications in a\ntable and discussed the needs and trends in the research activities on semantic\nchange characterization.\n","authors":["Jader Martins Camboim de Sá","Marcos Da Silveira","Cédric Pruski"],"pdf_url":"https://arxiv.org/pdf/2402.19088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06789v1","updated":"2024-03-11T15:04:55Z","published":"2024-03-11T15:04:55Z","title":"SPLADE-v3: New baselines for SPLADE","summary":"  A companion to the release of the latest version of the SPLADE library. We\ndescribe changes to the training structure and present our latest series of\nmodels -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as\nre-rankers, and showcase its effectiveness via a meta-analysis over more than\n40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is\nstatistically significantly more effective than both BM25 and SPLADE++, while\ncomparing well to cross-encoder re-rankers. Specifically, it gets more than 40\nMRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on\nthe BEIR benchmark.\n","authors":["Carlos Lassance","Hervé Déjean","Thibault Formal","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2403.06789v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2307.00920v2","updated":"2024-03-11T14:56:47Z","published":"2023-07-03T10:44:07Z","title":"Node-weighted Graph Convolutional Network for Depression Detection in\n  Transcribed Clinical Interviews","summary":"  We propose a simple approach for weighting self-connecting edges in a Graph\nConvolutional Network (GCN) and show its impact on depression detection from\ntranscribed clinical interviews. To this end, we use a GCN for modeling\nnon-consecutive and long-distance semantics to classify the transcriptions into\ndepressed or control subjects. The proposed method aims to mitigate the\nlimiting assumptions of locality and the equal importance of self-connections\nvs. edges to neighboring nodes in GCNs, while preserving attractive features\nsuch as low computational cost, data agnostic, and interpretability\ncapabilities. We perform an exhaustive evaluation in two benchmark datasets.\nResults show that our approach consistently outperforms the vanilla GCN model\nas well as previously reported results, achieving an F1=0.84 on both datasets.\nFinally, a qualitative analysis illustrates the interpretability capabilities\nof the proposed approach and its alignment with previous findings in\npsychology.\n","authors":["Sergio Burdisso","Esaú Villatoro-Tello","Srikanth Madikeri","Petr Motlicek"],"pdf_url":"https://arxiv.org/pdf/2307.00920v2.pdf","comment":"Paper Accepted to Interspeech 2023"},{"id":"http://arxiv.org/abs/2308.13139v2","updated":"2024-03-11T14:50:03Z","published":"2023-08-25T02:32:36Z","title":"MatchXML: An Efficient Text-label Matching Framework for Extreme\n  Multi-label Text Classification","summary":"  The eXtreme Multi-label text Classification(XMC) refers to training a\nclassifier that assigns a text sample with relevant labels from an extremely\nlarge-scale label set (e.g., millions of labels). We propose MatchXML, an\nefficient text-label matching framework for XMC. We observe that the label\nembeddings generated from the sparse Term Frequency-Inverse Document\nFrequency(TF-IDF) features have several limitations. We thus propose label2vec\nto effectively train the semantic dense label embeddings by the Skip-gram\nmodel. The dense label embeddings are then used to build a Hierarchical Label\nTree by clustering. In fine-tuning the pre-trained encoder Transformer, we\nformulate the multi-label text classification as a text-label matching problem\nin a bipartite graph. We then extract the dense text representations from the\nfine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also\nextract the static dense sentence embeddings from a pre-trained Sentence\nTransformer. Finally, a linear ranker is trained by utilizing the sparse TF-IDF\nfeatures, the fine-tuned dense text representations and static dense sentence\nfeatures. Experimental results demonstrate that MatchXML achieves\nstate-of-the-art accuracy on five out of six datasets. As for the speed,\nMatchXML outperforms the competing methods on all the six datasets. Our source\ncode is publicly available at https://github.com/huiyegit/MatchXML.\n","authors":["Hui Ye","Rajshekhar Sunderraman","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2308.13139v2.pdf","comment":"Accepted to TKDE 2024"},{"id":"http://arxiv.org/abs/2312.12021v3","updated":"2024-03-11T14:42:28Z","published":"2023-12-19T10:16:24Z","title":"Synergistic Anchored Contrastive Pre-training for Few-Shot Relation\n  Extraction","summary":"  Few-shot Relation Extraction (FSRE) aims to extract relational facts from a\nsparse set of labeled corpora. Recent studies have shown promising results in\nFSRE by employing Pre-trained Language Models (PLMs) within the framework of\nsupervised contrastive learning, which considers both instances and label\nfacts. However, how to effectively harness massive instance-label pairs to\nencompass the learned representation with semantic richness in this learning\nparadigm is not fully explored. To address this gap, we introduce a novel\nsynergistic anchored contrastive pre-training framework. This framework is\nmotivated by the insight that the diverse viewpoints conveyed through\ninstance-label pairs capture incomplete yet complementary intrinsic textual\nsemantics. Specifically, our framework involves a symmetrical contrastive\nobjective that encompasses both sentence-anchored and label-anchored\ncontrastive losses. By combining these two losses, the model establishes a\nrobust and uniform representation space. This space effectively captures the\nreciprocal alignment of feature distributions among instances and relational\nfacts, simultaneously enhancing the maximization of mutual information across\ndiverse perspectives within the same relation. Experimental results demonstrate\nthat our framework achieves significant performance enhancements compared to\nbaseline models in downstream FSRE tasks. Furthermore, our approach exhibits\nsuperior adaptability to handle the challenges of domain shift and zero-shot\nrelation extraction. Our code is available online at\nhttps://github.com/AONE-NLP/FSRE-SaCon.\n","authors":["Da Luo","Yanglei Gan","Rui Hou","Run Lin","Qiao Liu","Yuxiang Cai","Wannian Gao"],"pdf_url":"https://arxiv.org/pdf/2312.12021v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06769v1","updated":"2024-03-11T14:38:16Z","published":"2024-03-11T14:38:16Z","title":"Strength Lies in Differences! Towards Effective Non-collaborative\n  Dialogues via Tailored Strategy Planning","summary":"  We investigate non-collaborative dialogue agents that must engage in tailored\nstrategic planning for diverse users to secure a favorable agreement. This\nposes challenges for existing dialogue agents due to two main reasons: their\ninability to integrate user-specific characteristics into their strategic\nplanning and their training paradigm's failure to produce strategic planners\nthat can generalize to diverse users. To address these challenges, we propose\nTRIP to enhance the capability in tailored strategic planning, incorporating a\nuser-aware strategic planning module and a population-based training paradigm.\nThrough experiments on benchmark non-collaborative dialogue tasks, we\ndemonstrate the effectiveness of TRIP in catering to diverse users.\n","authors":["Tong Zhang","Chen Huang","Yang Deng","Hongru Liang","Jia Liu","Zujie Wen","Wenqiang Lei","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2403.06769v1.pdf","comment":"A draft version and appendix will be coming soon!"},{"id":"http://arxiv.org/abs/2403.06765v1","updated":"2024-03-11T14:35:45Z","published":"2024-03-11T14:35:45Z","title":"ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large\n  Language Model","summary":"  The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/.\n","authors":["Zhiwei Liu","Boyang Liu","Paul Thompson","Kailai Yang","Raghav Jain","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2403.06765v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.06764v1","updated":"2024-03-11T14:35:32Z","published":"2024-03-11T14:35:32Z","title":"An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models","summary":"  In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.\n","authors":["Liang Chen","Haozhe Zhao","Tianyu Liu","Shuai Bai","Junyang Lin","Chang Zhou","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2403.06764v1.pdf","comment":"21 papes, 8 figures, code is released at\n  https://github.com/pkunlp-icler/FastV"},{"id":"http://arxiv.org/abs/2403.06754v1","updated":"2024-03-11T14:28:40Z","published":"2024-03-11T14:28:40Z","title":"ALaRM: Align Language Models via Hierarchical Rewards Modeling","summary":"  We introduce ALaRM, the first framework modeling hierarchical rewards in\nreinforcement learning from human feedback (RLHF), which is designed to enhance\nthe alignment of large language models (LLMs) with human preferences. The\nframework addresses the limitations of current alignment approaches, which\noften struggle with the inconsistency and sparsity of human supervision\nsignals, by integrating holistic rewards with aspect-specific rewards. This\nintegration enables more precise and consistent guidance of language models\ntowards desired outcomes, particularly in complex and open text generation\ntasks. By employing a methodology that filters and combines multiple rewards\nbased on their consistency, the framework provides a reliable mechanism for\nimproving model alignment. We validate our approach through applications in\nlong-form question answering and machine translation tasks, employing\ngpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over\nexisting baselines. Our work underscores the effectiveness of hierarchical\nrewards modeling in refining LLM training processes for better human preference\nalignment. We release our code at https://ALaRM-fdu.github.io.\n","authors":["Yuhang Lai","Siyuan Wang","Shujun Liu","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2403.06754v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.06745v1","updated":"2024-03-11T14:10:57Z","published":"2024-03-11T14:10:57Z","title":"ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine\n  Translation","summary":"  Large language model (LLM) has achieved promising performance in multilingual\nmachine translation tasks through zero/few-shot prompts or prompt-tuning.\nHowever, due to the mixture of multilingual data during the pre-training of\nLLM, the LLM-based translation models face the off-target issue in both\nprompt-based methods, including a series of phenomena, namely instruction\nmisunderstanding, translation with wrong language and over-generation. For this\nissue, this paper introduces an\n\\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction\n\\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual\n\\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine\n\\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised\nfine-tuning mechanism and orthogonal to the traditional prompt-based methods.\nIn this method, \\model automatically constructs a constrained template in the\ntarget side by adding trigger tokens ahead of the ground truth. Furthermore,\ntrigger tokens can be arranged and combined freely to represent different task\nsemantics, and they can be iteratively updated to maximize the label\nlikelihood. Experiments are performed on WMT test sets with multiple metrics,\nand the experimental results demonstrate that \\model achieves substantially\nimproved performance across multiple translation directions and reduce the\noff-target phenomena in the translation.\n","authors":["Shaojie Dai","Xin Liu","Ping Luo","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2403.06745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06734v1","updated":"2024-03-11T13:56:57Z","published":"2024-03-11T13:56:57Z","title":"Real-Time Multimodal Cognitive Assistant for Emergency Medical Services","summary":"  Emergency Medical Services (EMS) responders often operate under\ntime-sensitive conditions, facing cognitive overload and inherent risks,\nrequiring essential skills in critical thinking and rapid decision-making. This\npaper presents CognitiveEMS, an end-to-end wearable cognitive assistant system\nthat can act as a collaborative virtual partner engaging in the real-time\nacquisition and analysis of multimodal data from an emergency scene and\ninteracting with EMS responders through Augmented Reality (AR) smart glasses.\nCognitiveEMS processes the continuous streams of data in real-time and\nleverages edge computing to provide assistance in EMS protocol selection and\nintervention recognition. We address key technical challenges in real-time\ncognitive assistance by introducing three novel components: (i) a Speech\nRecognition model that is fine-tuned for real-world medical emergency\nconversations using simulated EMS audio recordings, augmented with synthetic\ndata generated by large language models (LLMs); (ii) an EMS Protocol Prediction\nmodel that combines state-of-the-art (SOTA) tiny language models with EMS\ndomain knowledge using graph-based attention mechanisms; (iii) an EMS Action\nRecognition module which leverages multimodal audio and video data and protocol\npredictions to infer the intervention/treatment actions taken by the responders\nat the incident scene. Our results show that for speech recognition we achieve\nsuperior performance compared to SOTA (WER of 0.290 vs. 0.618) on\nconversational data. Our protocol prediction component also significantly\noutperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition\nachieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s\nfor protocol prediction on the edge and 0.31s on the server.\n","authors":["Keshara Weerasinghe","Saahith Janapati","Xueren Ge","Sion Kim","Sneha Iyer","John A. Stankovic","Homa Alemzadeh"],"pdf_url":"https://arxiv.org/pdf/2403.06734v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.06725v1","updated":"2024-03-11T13:44:43Z","published":"2024-03-11T13:44:43Z","title":"Improving Low-Resource Knowledge Tracing Tasks by Supervised\n  Pre-training and Importance Mechanism Fine-tuning","summary":"  Knowledge tracing (KT) aims to estimate student's knowledge mastery based on\ntheir historical interactions. Recently, the deep learning based KT (DLKT)\napproaches have achieved impressive performance in the KT task. These DLKT\nmodels heavily rely on the large number of available student interactions.\nHowever, due to various reasons such as budget constraints and privacy\nconcerns, observed interactions are very limited in many real-world scenarios,\na.k.a, low-resource KT datasets. Directly training a DLKT model on a\nlow-resource KT dataset may lead to overfitting and it is difficult to choose\nthe appropriate deep neural architecture. Therefore, in this paper, we propose\na low-resource KT framework called LoReKT to address above challenges. Inspired\nby the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn\ntransferable parameters and representations from rich-resource KT datasets\nduring the pre-training stage and subsequently facilitate effective adaptation\nto low-resource KT datasets. Specifically, we simplify existing sophisticated\nDLKT model architectures with purely a stack of transformer decoders. We design\nan encoding mechanism to incorporate student interactions from multiple KT data\nsources and develop an importance mechanism to prioritize updating parameters\nwith high importance while constraining less important ones during the\nfine-tuning stage. We evaluate LoReKT on six public KT datasets and\nexperimental results demonstrate the superiority of our approach in terms of\nAUC and Accuracy. To encourage reproducible research, we make our data and code\npublicly available at https://anonymous.4open.science/r/LoReKT-C619.\n","authors":["Hengyuan Zhang","Zitao Liu","Shuyan Huang","Chenming Shang","Bojun Zhan","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.06725v1.pdf","comment":"29 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.06682v1","updated":"2024-03-11T12:57:28Z","published":"2024-03-11T12:57:28Z","title":"Restoring Ancient Ideograph: A Multimodal Multitask Neural Network\n  Approach","summary":"  Cultural heritage serves as the enduring record of human thought and history.\nDespite significant efforts dedicated to the preservation of cultural relics,\nmany ancient artefacts have been ravaged irreversibly by natural deterioration\nand human actions. Deep learning technology has emerged as a valuable tool for\nrestoring various kinds of cultural heritages, including ancient text\nrestoration. Previous research has approached ancient text restoration from\neither visual or textual perspectives, often overlooking the potential of\nsynergizing multimodal information. This paper proposes a novel Multimodal\nMultitask Restoring Model (MMRM) to restore ancient texts, particularly\nemphasising the ideograph. This model combines context understanding with\nresidual visual information from damaged ancient artefacts, enabling it to\npredict damaged characters and generate restored images simultaneously. We\ntested the MMRM model through experiments conducted on both simulated datasets\nand authentic ancient inscriptions. The results show that the proposed method\ngives insightful restoration suggestions in both simulation experiments and\nreal-world scenarios. To the best of our knowledge, this work represents the\npioneering application of multimodal deep learning in ancient text restoration,\nwhich will contribute to the understanding of ancient society and culture in\ndigital humanities fields.\n","authors":["Siyu Duan","Jun Wang","Qi Su"],"pdf_url":"https://arxiv.org/pdf/2403.06682v1.pdf","comment":"Accept by Lrec-Coling 2024"},{"id":"http://arxiv.org/abs/2403.06644v1","updated":"2024-03-11T12:07:13Z","published":"2024-03-11T12:07:13Z","title":"Elephants Never Forget: Testing Language Models for Memorization of\n  Tabular Data","summary":"  While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Starting with simple qualitative tests for whether an LLM knows\nthe names and values of features, we introduce a variety of different\ntechniques to assess the degrees of contamination, including statistical tests\nfor conditional distribution modeling and four tests that identify\nmemorization. Our investigation reveals that LLMs are pre-trained on many\npopular tabular datasets. This exposure can lead to invalid performance\nevaluation on downstream tasks because the LLMs have, in effect, been fit to\nthe test set. Interestingly, we also identify a regime where the language model\nreproduces important statistics of the data, but fails to reproduce the dataset\nverbatim. On these datasets, although seen during training, good performance on\ndownstream tasks might not be due to overfitting. Our findings underscore the\nneed for ensuring data integrity in machine learning tasks with LLMs. To\nfacilitate future research, we release an open-source tool that can perform\nvarious tests for memorization\n\\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.\n","authors":["Sebastian Bordt","Harsha Nori","Rich Caruana"],"pdf_url":"https://arxiv.org/pdf/2403.06644v1.pdf","comment":"Table Representation Learning Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.06642v1","updated":"2024-03-11T12:04:20Z","published":"2024-03-11T12:04:20Z","title":"KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation","summary":"  The utilization of semantic information is an important research problem in\nthe field of recommender systems, which aims to complement the missing parts of\nmainstream ID-based approaches. With the rise of LLM, its ability to act as a\nknowledge base and its reasoning capability have opened up new possibilities\nfor this research area, making LLM-based recommendation an emerging research\ndirection. However, directly using LLM to process semantic information for\nrecommendation scenarios is unreliable and sub-optimal due to several problems\nsuch as hallucination. A promising way to cope with this is to use external\nknowledge to aid LLM in generating truthful and usable text. Inspired by the\nabove motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to\nusing external knowledge in prompts, the proposed method also includes a\nknowledge-based contrastive learning scheme for training. Experiments on public\ndatasets and in-enterprise datasets validate the effectiveness of the proposed\nmethod.\n","authors":["Weiqing Luo","Chonggang Song","Lingling Yi","Gong Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.06642v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2308.08742v6","updated":"2024-03-11T11:35:48Z","published":"2023-08-17T02:33:43Z","title":"PMET: Precise Model Editing in a Transformer","summary":"  Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.\n","authors":["Xiaopeng Li","Shasha Li","Shezheng Song","Jing Yang","Jun Ma","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2308.08742v6.pdf","comment":"AAAI24"},{"id":"http://arxiv.org/abs/2403.03102v2","updated":"2024-03-11T11:25:48Z","published":"2024-03-05T16:43:03Z","title":"\"In Dialogues We Learn\": Towards Personalized Dialogue Without\n  Pre-defined Profiles through In-Dialogue Learning","summary":"  Personalized dialogue systems have gained significant attention in recent\nyears for their ability to generate responses in alignment with different\npersonas. However, most existing approaches rely on pre-defined personal\nprofiles, which are not only time-consuming and labor-intensive to create but\nalso lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning\nframework that enhances the ability of pre-trained large language models to\nleverage dialogue history to characterize persona for completing personalized\ndialogue generation tasks without pre-defined profiles. Our experiments on\nthree datasets demonstrate that IDL brings substantial improvements, with BLEU\nand ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,\nthe results of human evaluations further validate the efficacy of our proposed\nmethod.\n","authors":["Chuanqi Cheng","Quan Tu","Wei Wu","Shuo Shang","Cunli Mao","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.03102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09889v5","updated":"2024-03-11T11:05:21Z","published":"2023-11-16T13:37:21Z","title":"Language Generation from Brain Recordings","summary":"  Generating human language through non-invasive brain-computer interfaces\n(BCIs) has the potential to unlock many applications, such as serving disabled\npatients and improving communication. Currently, however, generating language\nvia BCIs has been previously successful only within a classification setup for\nselecting pre-generated sentence continuation candidates with the most likely\ncortical semantic representation. Inspired by recent research that revealed\nassociations between the brain and the large computational language models, we\npropose a generative language BCI that utilizes the capacity of a large\nlanguage model (LLM) jointly with a semantic brain decoder to directly generate\nlanguage from functional magnetic resonance imaging (fMRI) input. The proposed\nmodel can generate coherent language sequences aligned with the semantic\ncontent of visual or auditory language stimuli perceived, without prior\nknowledge of any pre-generated candidates. We compare the language generated\nfrom the presented model with a random control, pre-generated language\nselection approach, and a standard LLM, which generates common coherent text\nsolely based on the next word likelihood according to statistical language\ntraining data. The proposed model is found to generate language that is more\naligned with semantic stimulus in response to which brain input is sampled. Our\nfindings demonstrate the potential and feasibility of employing BCIs in direct\nlanguage generation.\n","authors":["Ziyi Ye","Qingyao Ai","Yiqun Liu","Maarten de Rijke","Min Zhang","Christina Lioma","Tuukka Ruotsalo"],"pdf_url":"https://arxiv.org/pdf/2311.09889v5.pdf","comment":"Preprint. Under Submission"},{"id":"http://arxiv.org/abs/2403.06611v1","updated":"2024-03-11T10:57:45Z","published":"2024-03-11T10:57:45Z","title":"MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway\n  Encoding","summary":"  With appropriate data selection and training techniques, Large Language\nModels (LLMs) have demonstrated exceptional success in various medical\nexaminations and multiple-choice questions. However, the application of LLMs in\nmedical dialogue generation-a task more closely aligned with actual medical\npractice-has been less explored. This gap is attributed to the insufficient\nmedical knowledge of LLMs, which leads to inaccuracies and hallucinated\ninformation in the generated medical responses. In this work, we introduce the\nMedical dialogue with Knowledge enhancement and clinical Pathway encoding\n(MedKP) framework, which integrates an external knowledge enhancement module\nthrough a medical knowledge graph and an internal clinical pathway encoding via\nmedical entities and physician actions. Evaluated with comprehensive metrics,\nour experiments on two large-scale, real-world online medical consultation\ndatasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines\nand mitigates the incidence of hallucinations, achieving a new\nstate-of-the-art. Extensive ablation studies further reveal the effectiveness\nof each component of MedKP. This enhancement advances the development of\nreliable, automated medical consultation responses using LLMs, thereby\nbroadening the potential accessibility of precise and real-time medical\nassistance.\n","authors":["Jiageng Wu","Xian Wu","Yefeng Zheng","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.06611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06609v1","updated":"2024-03-11T10:53:20Z","published":"2024-03-11T10:53:20Z","title":"Guiding Clinical Reasoning with Large Language Models via Knowledge\n  Seeds","summary":"  Clinical reasoning refers to the cognitive process that physicians employ in\nevaluating and managing patients. This process typically involves suggesting\nnecessary examinations, diagnosing patients' diseases, and deciding on\nappropriate therapies, etc. Accurate clinical reasoning requires extensive\nmedical knowledge and rich clinical experience, setting a high bar for\nphysicians. This is particularly challenging in developing countries due to the\noverwhelming number of patients and limited physician resources, contributing\nsignificantly to global health inequity and necessitating automated clinical\nreasoning approaches. Recently, the emergence of large language models (LLMs)\nsuch as ChatGPT and GPT-4 have demonstrated their potential in clinical\nreasoning. However, these LLMs are prone to hallucination problems, and the\nreasoning process of LLMs may not align with the clinical decision path of\nphysicians. In this study, we introduce a novel framework, In-Context Padding\n(ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer\ncritical clinical reasoning elements (referred to as knowledge seeds) and use\nthese as anchors to guide the generation process of LLMs. Experiments on two\nclinical question datasets demonstrate that ICP significantly improves the\nclinical reasoning ability of LLMs.\n","authors":["Jiageng WU","Xian Wu","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.06609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06591v1","updated":"2024-03-11T10:35:53Z","published":"2024-03-11T10:35:53Z","title":"Academically intelligent LLMs are not necessarily socially intelligent","summary":"  The academic intelligence of large language models (LLMs) has made remarkable\nprogress in recent times, but their social intelligence performance remains\nunclear. Inspired by established human social intelligence frameworks,\nparticularly Daniel Goleman's social intelligence theory, we have developed a\nstandardized social intelligence test based on real-world social scenarios to\ncomprehensively assess the social intelligence of LLMs, termed as the\nSituational Evaluation of Social Intelligence (SESI). We conducted an extensive\nevaluation with 13 recent popular and state-of-art LLM agents on SESI. The\nresults indicate the social intelligence of LLMs still has significant room for\nimprovement, with superficially friendliness as a primary reason for errors.\nMoreover, there exists a relatively low correlation between the social\nintelligence and academic intelligence exhibited by LLMs, suggesting that\nsocial intelligence is distinct from academic intelligence for LLMs.\nAdditionally, while it is observed that LLMs can't ``understand'' what social\nintelligence is, their social intelligence, similar to that of humans, is\ninfluenced by social factors.\n","authors":["Ruoxi Xu","Hongyu Lin","Xianpei Han","Le Sun","Yingfei Sun"],"pdf_url":"https://arxiv.org/pdf/2403.06591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06586v1","updated":"2024-03-11T10:32:23Z","published":"2024-03-11T10:32:23Z","title":"ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity\n  Recognition Models","summary":"  Context-aware Human Activity Recognition (HAR) is a hot research area in\nmobile computing, and the most effective solutions in the literature are based\non supervised deep learning models. However, the actual deployment of these\nsystems is limited by the scarcity of labeled data that is required for\ntraining. Neuro-Symbolic AI (NeSy) provides an interesting research direction\nto mitigate this issue, by infusing common-sense knowledge about human\nactivities and the contexts in which they can be performed into HAR deep\nlearning classifiers. Existing NeSy methods for context-aware HAR rely on\nknowledge encoded in logic-based models (e.g., ontologies) whose design,\nimplementation, and maintenance to capture new activities and contexts require\nsignificant human engineering efforts, technical knowledge, and domain\nexpertise. Recent works show that pre-trained Large Language Models (LLMs)\neffectively encode common-sense knowledge about human activities. In this work,\nwe propose ContextGPT: a novel prompt engineering approach to retrieve from\nLLMs common-sense knowledge about the relationship between human activities and\nthe context in which they are performed. Unlike ontologies, ContextGPT requires\nlimited human effort and expertise. An extensive evaluation carried out on two\npublic datasets shows how a NeSy model obtained by infusing common-sense\nknowledge from ContextGPT is effective in data scarcity scenarios, leading to\nsimilar (and sometimes better) recognition rates than logic-based approaches\nwith a fraction of the effort.\n","authors":["Luca Arrotta","Claudio Bettini","Gabriele Civitarese","Michele Fiori"],"pdf_url":"https://arxiv.org/pdf/2403.06586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06574v1","updated":"2024-03-11T10:24:37Z","published":"2024-03-11T10:24:37Z","title":"AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large\n  Language Models","summary":"  Given the importance of ancient Chinese in capturing the essence of rich\nhistorical and cultural heritage, the rapid advancements in Large Language\nModels (LLMs) necessitate benchmarks that can effectively evaluate their\nunderstanding of ancient contexts. To meet this need, we present AC-EVAL, an\ninnovative benchmark designed to assess the advanced knowledge and reasoning\ncapabilities of LLMs within the context of ancient Chinese. AC-EVAL is\nstructured across three levels of difficulty reflecting different facets of\nlanguage comprehension: general historical knowledge, short text understanding,\nand long text comprehension. The benchmark comprises 13 tasks, spanning\nhistorical facts, geography, social customs, art, philosophy, classical poetry\nand prose, providing a comprehensive assessment framework. Our extensive\nevaluation of top-performing LLMs, tailored for both English and Chinese,\nreveals a substantial potential for enhancing ancient text comprehension. By\nhighlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote\ntheir development and application forward in the realms of ancient Chinese\nlanguage education and scholarly research. The AC-EVAL data and evaluation code\nare available at https://github.com/yuting-wei/AC-EVAL.\n","authors":["Yuting Wei","Yuanxing Xu","Xinru Wei","Simin Yang","Yangfu Zhu","Yuqing Li","Di Liu","Bin Wu"],"pdf_url":"https://arxiv.org/pdf/2403.06574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06570v1","updated":"2024-03-11T10:11:29Z","published":"2024-03-11T10:11:29Z","title":"Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting\n  Applications","summary":"  Past studies on end-to-end meeting transcription have focused on model\narchitecture and have mostly been evaluated on simulated meeting data. We\npresent a novel study aiming to optimize the use of a Speaker-Attributed ASR\n(SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for\nimproved speaker assignment of speech segments. First, we propose a pipeline\ntailored to real-life applications involving Voice Activity Detection (VAD),\nSpeaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output\nsegments to fine-tune the SA-ASR model, considering that it is also applied to\nVAD segments during test, and show that this results in a relative reduction of\nSpeaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance\nthe extraction of the speaker embedding templates used as inputs by the SA-ASR\nsystem. We show that extracting them from SD output rather than annotated\nspeaker segments results in a relative SER reduction up to 20%.\n","authors":["Can Cui","Imran Ahamad Sheikh","Mostafa Sadeghi","Emmanuel Vincent"],"pdf_url":"https://arxiv.org/pdf/2403.06570v1.pdf","comment":"Submitted to Odyssey 2024"},{"id":"http://arxiv.org/abs/2403.06563v1","updated":"2024-03-11T10:05:29Z","published":"2024-03-11T10:05:29Z","title":"Unraveling the Mystery of Scaling Laws: Part I","summary":"  Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.\n","authors":["Hui Su","Zhi Tian","Xiaoyu Shen","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.06563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05783v3","updated":"2024-03-11T09:49:04Z","published":"2023-06-09T09:52:05Z","title":"Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge\n  Evaluation","summary":"  New Natural Langauge Process~(NLP) benchmarks are urgently needed to align\nwith the rapid development of large language models (LLMs). We present Xiezhi,\nthe most comprehensive evaluation suite designed to assess holistic domain\nknowledge. Xiezhi comprises multiple-choice questions across 516 diverse\ndisciplines ranging from 13 different subjects with 249,587 questions and\naccompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k\nquestions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results\nindicate that LLMs exceed average performance of humans in science,\nengineering, agronomy, medicine, and art, but fall short in economics,\njurisprudence, pedagogy, literature, history, and management. We anticipate\nXiezhi will help analyze important strengths and shortcomings of LLMs, and the\nbenchmark is released in~\\url{https://github.com/MikeGu721/XiezhiBenchmark}.\n","authors":["Zhouhong Gu","Xiaoxuan Zhu","Haoning Ye","Lin Zhang","Jianchen Wang","Yixin Zhu","Sihang Jiang","Zhuozhi Xiong","Zihan Li","Weijie Wu","Qianyu He","Rui Xu","Wenhao Huang","Jingping Liu","Zili Wang","Shusen Wang","Weiguo Zheng","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2306.05783v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2403.06537v1","updated":"2024-03-11T09:24:06Z","published":"2024-03-11T09:24:06Z","title":"On the Consideration of AI Openness: Can Good Intent Be Abused?","summary":"  Openness is critical for the advancement of science. In particular, recent\nrapid progress in AI has been made possible only by various open-source models,\ndatasets, and libraries. However, this openness also means that technologies\ncan be freely used for socially harmful purposes. Can open-source models or\ndatasets be used for malicious purposes? If so, how easy is it to adapt\ntechnology for such goals? Here, we conduct a case study in the legal domain, a\nrealm where individual decisions can have profound social consequences. To this\nend, we build EVE, a dataset consisting of 200 examples of questions and\ncorresponding answers about criminal activities based on 200 Korean precedents.\nWe found that a widely accepted open-source LLM, which initially refuses to\nanswer unethical questions, can be easily tuned with EVE to provide unethical\nand informative answers about criminal activities. This implies that although\nopen-source technologies contribute to scientific progress, some care must be\ntaken to mitigate possible malicious use cases. Warning: This paper contains\ncontents that some may find unethical.\n","authors":["Yeeun Kim","Eunkyung Choi","Hyunjun Kim","Hongseok Oh","Hyunseo Shin","Wonseok Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.06537v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.06520v1","updated":"2024-03-11T08:52:52Z","published":"2024-03-11T08:52:52Z","title":"How to Understand Named Entities: Using Common Sense for News Captioning","summary":"  News captioning aims to describe an image with its news article body as\ninput. It greatly relies on a set of detected named entities, including\nreal-world people, organizations, and places. This paper exploits commonsense\nknowledge to understand named entities for news captioning. By ``understand'',\nwe mean correlating the news content with common sense in the wild, which helps\nan agent to 1) distinguish semantically similar named entities and 2) describe\nnamed entities using words outside of training corpora. Our approach consists\nof three modules: (a) Filter Module aims to clarify the common sense concerning\na named entity from two aspects: what does it mean? and what is it related to?,\nwhich divide the common sense into explanatory knowledge and relevant\nknowledge, respectively. (b) Distinguish Module aggregates explanatory\nknowledge from node-degree, dependency, and distinguish three aspects to\ndistinguish semantically similar named entities. (c) Enrich Module attaches\nrelevant knowledge to named entities to enrich the entity description by\ncommonsense information (e.g., identity and social position). Finally, the\nprobability distributions from both modules are integrated to generate the news\ncaptions. Extensive experiments on two challenging datasets (i.e., GoodNews and\nNYTimes) demonstrate the superiority of our method. Ablation studies and\nvisualization further validate its effectiveness in understanding named\nentities.\n","authors":["Ning Xu","Yanhui Wang","Tingting Zhang","Hongshuo Tian","Mohan Kankanhalli","An-An Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00262v2","updated":"2024-03-11T08:30:31Z","published":"2023-11-01T03:20:16Z","title":"Plug-and-Play Policy Planner for Large Language Model Powered Dialogue\n  Agents","summary":"  Proactive dialogues serve as a practical yet challenging dialogue problem in\nthe era of large language models (LLMs), where the dialogue policy planning is\nthe key to improving the proactivity of LLMs. Most existing studies enable the\ndialogue policy planning of LLMs using various prompting schemes or iteratively\nenhance this capability in handling the given case with verbal AI feedback.\nHowever, these approaches are either bounded by the policy planning capability\nof the frozen LLMs or hard to be transferred to new cases. In this work, we\nintroduce a new dialogue policy planning paradigm to strategize LLMs for\nproactive dialogue problems with a tunable language model plug-in as a\nplug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a\nnovel training framework to facilitate supervised fine-tuning over available\nhuman-annotated data as well as reinforcement learning from goal-oriented AI\nfeedback with dynamic interaction data collected by the LLM-based self-play\nsimulation. In this manner, the LLM-powered dialogue agent can not only be\ngeneralized to different cases after the training, but also be applicable to\ndifferent applications by just substituting the learned plug-in. In addition,\nwe propose to evaluate the policy planning capability of dialogue systems under\nthe interactive setting. Experimental results demonstrate that PPDPP\nconsistently and substantially outperforms existing approaches on three\ndifferent proactive dialogue applications, including negotiation, emotional\nsupport, and tutoring dialogues.\n","authors":["Yang Deng","Wenxuan Zhang","Wai Lam","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.00262v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.06503v1","updated":"2024-03-11T08:25:52Z","published":"2024-03-11T08:25:52Z","title":"Automatic Generation of Python Programs Using Context-Free Grammars","summary":"  In recent years, data has emerged as the new gold, serving as a powerful tool\nfor creating intelligent systems. However, procuring high-quality data remains\nchallenging, especially for code. To address this, we developed TinyPy\nGenerator, a tool that generates random Python programs using a context-free\ngrammar. The generated programs are guaranteed to be correct by construction.\nOur system uses custom production rules (in the Backus-Naur Form (BNF) format)\nto recursively generate code. This allows us to generate code with different\nlevels of complexity, ranging from code containing only assignments to more\ncomplex code containing conditionals and loops. Our proposed tool enables\neffortless large-scale Python code generation, beneficial for a wide range of\napplications. TinyPy Generator is particularly useful in the field of machine\nlearning, where it can generate substantial amounts of Python code for training\nPython language models. Additionally, researchers who are studying programming\nlanguages can utilize this tool to create datasets for their experiments, which\ncan help validate the robustness of code interpreters or compilers. Unlike\nexisting research, we have open-sourced our implementation. This allows\ncustomization according to user needs and extends potential usage to other\nlanguages.\n","authors":["Kamel Yamani","Marwa Naïr","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.06503v1.pdf","comment":"This work was presented at the 2nd Languages, Architectures, and\n  Tools for Heterogeneous Computing (LATHC) Workshop 2024, organized in\n  conjunction with the IEEE/ACM International Symposium on Code Generation and\n  Optimization (CGO)"},{"id":"http://arxiv.org/abs/2402.16159v2","updated":"2024-03-11T08:11:08Z","published":"2024-02-25T17:40:49Z","title":"DistALANER: Distantly Supervised Active Learning Augmented Named Entity\n  Recognition in the Open Source Software Ecosystem","summary":"  With the AI revolution in place, the trend for building automated systems to\nsupport professionals in different domains such as the open source software\nsystems, healthcare systems, banking systems, transportation systems and many\nothers have become increasingly prominent. A crucial requirement in the\nautomation of support tools for such systems is the early identification of\nnamed entities, which serves as a foundation for developing specialized\nfunctionalities. However, due to the specific nature of each domain, different\ntechnical terminologies and specialized languages, expert annotation of\navailable data becomes expensive and challenging. In light of these challenges,\nthis paper proposes a novel named entity recognition (NER) technique\nspecifically tailored for the open-source software systems. Our approach aims\nto address the scarcity of annotated software data by employing a comprehensive\ntwo-step distantly supervised annotation process. This process strategically\nleverages language heuristics, unique lookup tables, external knowledge\nsources, and an active learning approach. By harnessing these powerful\ntechniques, we not only enhance model performance but also effectively mitigate\nthe limitations associated with cost and the scarcity of expert annotators. It\nis noteworthy that our model significantly outperforms the state-of-the-art\nLLMs by a substantial margin. We also show the effectiveness of NER in the\ndownstream task of relation extraction.\n","authors":["Somnath Banerjee","Avik Dutta","Aaditya Agrawal","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.16159v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2309.01940v4","updated":"2024-03-11T08:07:28Z","published":"2023-09-05T04:12:01Z","title":"CodeApex: A Bilingual Programming Evaluation Benchmark for Large\n  Language Models","summary":"  With the emergence of Large Language Models (LLMs), there has been a\nsignificant improvement in the programming capabilities of models, attracting\ngrowing attention from researchers. Evaluating the programming capabilities of\nLLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has\nnumerous downstream applications. In this paper, we propose CodeApex, a\nbilingual benchmark dataset focusing on the programming comprehension, code\ngeneration, and code correction abilities of LLMs. Programming comprehension\ntask tests LLMs on multiple-choice exam questions covering conceptual\nunderstanding, commonsense reasoning, and multi-hop reasoning. The code\ngeneration task evaluates LLMs through completing C++ functions based on\nprovided descriptions and prototypes. The code correction task asks LLMs to fix\nreal-world erroneous code segments with different error messages. We evaluate\n12 widely used LLMs, including both general-purpose and specialized models.\nGPT-4 exhibits the best programming capabilities, achieving approximate\naccuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to\nhuman performance, there is still significant room for improvement in LLM\nprogramming. We hope that CodeApex can serve as a reference for evaluating the\ncoding capabilities of LLMs, further promoting their development and growth.\n","authors":["Lingyue Fu","Huacan Chai","Shuang Luo","Kounianhua Du","Weiming Zhang","Longteng Fan","Jiayi Lei","Renting Rui","Jianghao Lin","Yuchen Fang","Yifan Liu","Jingkuan Wang","Siyuan Qi","Kangning Zhang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2309.01940v4.pdf","comment":"33pages"},{"id":"http://arxiv.org/abs/2401.05507v3","updated":"2024-03-11T07:57:59Z","published":"2024-01-10T19:04:00Z","title":"InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks","summary":"  In this paper, we introduce InfiAgent-DABench, the first benchmark\nspecifically designed to evaluate LLM-based agents on data analysis tasks.\nThese tasks require agents to end-to-end solving complex tasks by interacting\nwith an execution environment. This benchmark contains DAEval, a dataset\nconsisting of 257 data analysis questions derived from 52 CSV files, and an\nagent framework which incorporates LLMs to serve as data analysis agents for\nboth serving and evaluation. Since data analysis questions are often open-ended\nand hard to evaluate without human supervision, we adopt a format-prompting\ntechnique to convert each question into a closed-form format so that they can\nbe automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the\ncurrent challenges encountered in data analysis tasks. In addition, building on\ntop of our agent framework, we develop a specialized agent, DAAgent, which\nsurpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for\nInfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent .\n","authors":["Xueyu Hu","Ziyu Zhao","Shuang Wei","Ziwei Chai","Qianli Ma","Guoyin Wang","Xuwu Wang","Jing Su","Jingjing Xu","Ming Zhu","Yao Cheng","Jianbo Yuan","Jiwei Li","Kun Kuang","Yang Yang","Hongxia Yang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2401.05507v3.pdf","comment":"27 pages, 7 figures, work in progress"},{"id":"http://arxiv.org/abs/2403.06487v1","updated":"2024-03-11T07:50:29Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v1.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2312.15710v2","updated":"2024-03-11T07:50:05Z","published":"2023-12-25T12:32:49Z","title":"Alleviating Hallucinations of Large Language Models through Induced\n  Hallucinations","summary":"  Despite their impressive capabilities, large language models (LLMs) have been\nobserved to generate responses that include inaccurate or fabricated\ninformation, a phenomenon commonly known as ``hallucination''. In this work, we\npropose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to\nalleviate hallucinations. We first construct a factually weak LLM by inducing\nhallucinations from the original LLMs. Then, we penalize these induced\nhallucinations during decoding to enhance the factuality of the generated\ncontent. Concretely, we determine the final next-token predictions by\namplifying the predictions from the original model and downplaying the induced\nuntruthful predictions via contrastive decoding. Experimental results on both\ndiscrimination-based and generation-based hallucination evaluation benchmarks,\nsuch as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD\nmethods can effectively enhance the factuality of LLMs across various model\nsizes and families. For example, when equipped with ICD, Llama2-7B-Chat and\nMistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on\nTruthfulQA, respectively.\n","authors":["Yue Zhang","Leyang Cui","Wei Bi","Shuming Shi"],"pdf_url":"https://arxiv.org/pdf/2312.15710v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.06485v1","updated":"2024-03-11T07:48:35Z","published":"2024-03-11T07:48:35Z","title":"Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid\n  Approach","summary":"  Due to the scale and complexity of cloud systems, a system failure would\ntrigger an \"alert storm\", i.e., massive correlated alerts. Although these\nalerts can be traced back to a few root causes, the overwhelming number makes\nit infeasible for manual handling. Alert aggregation is thus critical to help\nengineers concentrate on the root cause and facilitate failure resolution.\nExisting methods typically utilize semantic similarity-based methods or\nstatistical methods to aggregate alerts. However, semantic similarity-based\nmethods overlook the causal rationale of alerts, while statistical methods can\nhardly handle infrequent alerts.\n  To tackle these limitations, we introduce leveraging external knowledge,\ni.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose\nCOLA, a novel hybrid approach based on correlation mining and LLM (Large\nLanguage Model) reasoning for online alert aggregation. The correlation mining\nmodule effectively captures the temporal and spatial relations between alerts,\nmeasuring their correlations in an efficient manner. Subsequently, only\nuncertain pairs with low confidence are forwarded to the LLM reasoning module\nfor detailed analysis. This hybrid design harnesses both statistical evidence\nfor frequent alerts and the reasoning capabilities of computationally intensive\nLLMs, ensuring the overall efficiency of COLA in handling large volumes of\nalerts in practical scenarios. We evaluate COLA on three datasets collected\nfrom the production environment of a large-scale cloud platform. The\nexperimental results show COLA achieves F1-scores from 0.901 to 0.930,\noutperforming state-of-the-art methods and achieving comparable efficiency. We\nalso share our experience in deploying COLA in our real-world cloud system,\nCloud X.\n","authors":["Jinxi Kuang","Jinyang Liu","Junjie Huang","Renyi Zhong","Jiazhen Gu","Lan Yu","Rui Tan","Zengyin Yang","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.06485v1.pdf","comment":"Accepted by Proceedings of the 46th International Conference on\n  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)"},{"id":"http://arxiv.org/abs/2403.04224v2","updated":"2024-03-11T07:04:42Z","published":"2024-03-07T04:54:56Z","title":"Aligners: Decoupling LLMs and Alignment","summary":"  Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We illustrate our method by training an \"ethical\" aligner and verify\nits efficacy empirically.\n","authors":["Lilian Ngweta","Mayank Agarwal","Subha Maity","Alex Gittens","Yuekai Sun","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2403.04224v2.pdf","comment":"Tiny Papers Track at the International Conference on Learning\n  Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2403.06448v1","updated":"2024-03-11T05:51:03Z","published":"2024-03-11T05:51:03Z","title":"Unsupervised Real-Time Hallucination Detection based on the Internal\n  States of Large Language Models","summary":"  Hallucinations in large language models (LLMs) refer to the phenomenon of\nLLMs producing responses that are coherent yet factually inaccurate. This issue\nundermines the effectiveness of LLMs in practical applications, necessitating\nresearch into detecting and mitigating hallucinations of LLMs. Previous studies\nhave mainly concentrated on post-processing techniques for hallucination\ndetection, which tend to be computationally intensive and limited in\neffectiveness due to their separation from the LLM's inference process. To\novercome these limitations, we introduce MIND, an unsupervised training\nframework that leverages the internal states of LLMs for real-time\nhallucination detection without requiring manual annotations. Additionally, we\npresent HELM, a new benchmark for evaluating hallucination detection across\nmultiple LLMs, featuring diverse LLM outputs and the internal states of LLMs\nduring their inference process. Our experiments demonstrate that MIND\noutperforms existing state-of-the-art methods in hallucination detection.\n","authors":["Weihang Su","Changyue Wang","Qingyao Ai","Yiran HU","Zhijing Wu","Yujia Zhou","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09533v2","updated":"2024-03-11T05:36:36Z","published":"2023-11-16T03:22:25Z","title":"Effective Large Language Model Adaptation for Improved Grounding and\n  Citation Generation","summary":"  Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage understanding and generation. However, one major issue towards their\nwidespread deployment in the real world is that they can generate\n\"hallucinated\" answers that are not factual. Towards this end, this paper\nfocuses on improving LLMs by grounding their responses in retrieved passages\nand by providing citations. We propose a new framework, AGREE, Adaptation for\nGRounding EnhancEment, that improves the grounding from a holistic perspective.\nOur framework tunes LLMs to selfground the claims in their responses and\nprovide accurate citations to retrieved documents. This tuning on top of the\npre-trained LLMs requires well-grounded responses (with citations) for paired\nqueries, for which we introduce a method that can automatically construct such\ndata from unlabeled queries. The selfgrounding capability of tuned LLMs further\ngrants them a test-time adaptation (TTA) capability that can actively retrieve\npassages to support the claims that have not been grounded, which iteratively\nimproves the responses of LLMs. Across five datasets and two LLMs, our results\nshow that the proposed tuningbased AGREE framework generates superior grounded\nresponses with more accurate citations compared to prompting-based approaches\nand post-hoc citing-based approaches\n","authors":["Xi Ye","Ruoxi Sun","Sercan Ö. Arik","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2311.09533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05101v2","updated":"2024-03-11T04:59:51Z","published":"2024-03-08T07:06:43Z","title":"Rule-driven News Captioning","summary":"  News captioning task aims to generate sentences by describing named entities\nor concrete events for an image with its news article. Existing methods have\nachieved remarkable results by relying on the large-scale pre-trained models,\nwhich primarily focus on the correlations between the input news content and\nthe output predictions. However, the news captioning requires adhering to some\nfundamental rules of news reporting, such as accurately describing the\nindividuals and actions associated with the event. In this paper, we propose\nthe rule-driven news captioning method, which can generate image descriptions\nfollowing designated rule signal. Specifically, we first design the news-aware\nsemantic rule for the descriptions. This rule incorporates the primary action\ndepicted in the image (e.g., \"performing\") and the roles played by named\nentities involved in the action (e.g., \"Agent\" and \"Place\"). Second, we inject\nthis semantic rule into the large-scale pre-trained model, BART, with the\nprefix-tuning strategy, where multiple encoder layers are embedded with\nnews-aware semantic rule. Finally, we can effectively guide BART to generate\nnews sentences that comply with the designated rule. Extensive experiments on\ntwo widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the\neffectiveness of our method.\n","authors":["Ning Xu","Tingting Zhang","Hongshuo Tian","An-An Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09476v2","updated":"2024-03-11T04:18:10Z","published":"2023-07-18T17:56:50Z","title":"Overthinking the Truth: Understanding how Language Models Process False\n  Demonstrations","summary":"  Modern language models can imitate complex patterns through few-shot\nlearning, enabling them to complete challenging tasks without fine-tuning.\nHowever, imitation can also lead models to reproduce inaccuracies or harmful\ncontent if present in the context. We study harmful imitation through the lens\nof a model's internal representations, and identify two related phenomena:\n\"overthinking\" and \"false induction heads\". The first phenomenon, overthinking,\nappears when we decode predictions from intermediate layers, given correct vs.\nincorrect few-shot demonstrations. At early layers, both demonstrations induce\nsimilar model behavior, but the behavior diverges sharply at some \"critical\nlayer\", after which the accuracy given incorrect demonstrations progressively\ndecreases. The second phenomenon, false induction heads, are a possible\nmechanistic cause of overthinking: these are heads in late layers that attend\nto and copy false information from previous demonstrations, and whose ablation\nreduces overthinking. Beyond scientific understanding, our results suggest that\nstudying intermediate model computations could be a promising avenue for\nunderstanding and guarding against harmful model behaviors.\n","authors":["Danny Halawi","Jean-Stanislas Denain","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2307.09476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14505v4","updated":"2024-03-11T04:13:50Z","published":"2023-10-23T02:32:30Z","title":"Sentiment analysis with adaptive multi-head attention in Transformer","summary":"  We propose a novel framework based on the attention mechanism to identify the\nsentiment of a movie review document. Previous efforts on deep neural networks\nwith attention mechanisms focus on encoder and decoder with fixed numbers of\nmulti-head attention. Therefore, we need a mechanism to stop the attention\nprocess automatically if no more useful information can be read from the\nmemory.In this paper, we propose an adaptive multi-head attention architecture\n(AdaptAttn) which varies the number of attention heads based on length of\nsentences. AdaptAttn has a data preprocessing step where each document is\nclassified into any one of the three bins small, medium or large based on\nlength of the sentence. The document classified as small goes through two heads\nin each layer, the medium group passes four heads and the large group is\nprocessed by eight heads. We examine the merit of our model on the Stanford\nlarge movie review dataset. The experimental results show that the F1 score\nfrom our model is on par with the baseline model.\n","authors":["Fanfei Meng","Chen-Ao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14505v4.pdf","comment":"Accepted by the 4th International Conference on Signal Processing and\n  Machine Learning"},{"id":"http://arxiv.org/abs/2402.12749v2","updated":"2024-03-11T04:04:59Z","published":"2024-02-20T06:37:31Z","title":"Me LLaMA: Foundation Large Language Models for Medical Applications","summary":"  Recent large language models (LLMs) such as ChatGPT and LLaMA have shown\ngreat promise in many AI applications. However, their performance on medical\ntasks is suboptimal and can be improved by training on extensive\ndomain-specific datasets. This study introduces Me LLaMA, a medical LLM family\nthat includes foundation models - Me LLaMA 13/70B, along with their\nchat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual\npre-training and instruction tuning of LLaMA2 using large medical datasets. Our\ndomain-specific data suite for training and evaluation includes a large-scale,\ncontinual pre-training dataset with 129B tokens, an instruction tuning dataset\nwith 214k samples, and a new medical evaluation benchmark (MIBE) across six\ntasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me\nLLaMA models achieve overall better performance than existing open-source\nmedical LLMs in zero-shot, few-shot and supervised learning abilities. Their\nzero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,\nwith a slight variance of within 3%, and yet falls short when compared to\nGPT-4. In addition, we investigated the catastrophic forgetting problem, and\nour results show that Me LLaMA models outperform other open-source medical LLMs\nin mitigating this issue. Me LLaMA is one of the largest open-source medical\nfoundation LLMs that use both biomedical and clinical data. It exhibits\nsuperior performance across both general and medical tasks compared to other\nopen-source medical LLMs, rendering it an attractive choice for medical AI\napplications. We release our models, datasets, and evaluation scripts at:\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\n","authors":["Qianqian Xie","Qingyu Chen","Aokun Chen","Cheng Peng","Yan Hu","Fongci Lin","Xueqing Peng","Jimin Huang","Jeffrey Zhang","Vipina Keloth","Xingyu Zhou","Huan He","Lucila Ohno-Machido","Yonghui Wu","Hua Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2402.12749v2.pdf","comment":"21 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.11791v2","updated":"2024-03-11T04:01:50Z","published":"2024-01-22T09:41:05Z","title":"SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic\n  Segmentation","summary":"  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each target object\ncategory. In this way, SemPLeS can perform better semantic alignment between\nobject regions and the associated class labels, resulting in desired pseudo\nmasks for training the segmentation model. The proposed SemPLeS framework\nachieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS\nCOCO, and shows compatibility with other WSSS methods. The source codes are\nprovided in the supplementary.\n","authors":["Ci-Siang Lin","Chien-Yi Wang","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06414v1","updated":"2024-03-11T03:55:24Z","published":"2024-03-11T03:55:24Z","title":"Evolving Knowledge Distillation with Large Language Models and Active\n  Learning","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks. However, their computational costs are prohibitively high.\nTo address this issue, previous research has attempted to distill the knowledge\nof LLMs into smaller models by generating annotated data. Nonetheless, these\nworks have mainly focused on the direct use of LLMs for text generation and\nlabeling, without fully exploring their potential to comprehend the target task\nand acquire valuable knowledge. In this paper, we propose EvoKD: Evolving\nKnowledge Distillation, which leverages the concept of active learning to\ninteractively enhance the process of data generation using large language\nmodels, simultaneously improving the task capabilities of small domain model\n(student model). Different from previous work, we actively analyze the student\nmodel's weaknesses, and then synthesize labeled samples based on the analysis.\nIn addition, we provide iterative feedback to the LLMs regarding the student\nmodel's performance to continuously construct diversified and challenging\nsamples. Experiments and analysis on different NLP tasks, namely, text\nclassification and named entity recognition show the effectiveness of EvoKD.\n","authors":["Chengyuan Liu","Yangyang Kang","Fubang Zhao","Kun Kuang","Zhuoren Jiang","Changlong Sun","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2403.06414v1.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2403.06412v1","updated":"2024-03-11T03:54:33Z","published":"2024-03-11T03:54:33Z","title":"CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in\n  Korean","summary":"  Despite the rapid development of large language models (LLMs) for the Korean\nlanguage, there remains an obvious lack of benchmark datasets that test the\nrequisite Korean cultural and linguistic knowledge. Because many existing\nKorean benchmark datasets are derived from the English counterparts through\ntranslation, they often overlook the different cultural contexts. For the few\nbenchmark datasets that are sourced from Korean data capturing cultural\nknowledge, only narrow tasks such as bias and hate speech detection are\noffered. To address this gap, we introduce a benchmark of Cultural and\nLinguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.\nCLIcK sources its data from official Korean exams and textbooks, partitioning\nthe questions into eleven categories under the two main categories of language\nand culture. For each instance in CLIcK, we provide fine-grained annotation of\nwhich cultural and linguistic knowledge is required to answer the question\ncorrectly. Using CLIcK, we test 13 language models to assess their performance.\nOur evaluation uncovers insights into their performances across the categories,\nas well as the diverse factors affecting their comprehension. CLIcK offers the\nfirst large-scale comprehensive Korean-centric analysis of LLMs' proficiency in\nKorean culture and language.\n","authors":["Eunsu Kim","Juyoung Suk","Philhoon Oh","Haneul Yoo","James Thorne","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2403.06412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06410v1","updated":"2024-03-11T03:45:09Z","published":"2024-03-11T03:45:09Z","title":"A Logical Pattern Memory Pre-trained Model for Entailment Tree\n  Generation","summary":"  Generating coherent and credible explanations remains a significant challenge\nin the field of AI. In recent years, researchers have delved into the\nutilization of entailment trees to depict explanations, which exhibit a\nreasoning process of how a hypothesis is deduced from the supporting facts.\nHowever, existing models often overlook the importance of generating\nintermediate conclusions with logical consistency from the given facts, leading\nto inaccurate conclusions and undermining the overall credibility of entailment\ntrees. To address this limitation, we propose the logical pattern memory\npre-trained model (LMPM). LMPM incorporates an external memory structure to\nlearn and store the latent representations of logical patterns, which aids in\ngenerating logically consistent conclusions. Furthermore, to mitigate the\ninfluence of logically irrelevant domain knowledge in the Wikipedia-based data,\nwe introduce an entity abstraction approach to construct the dataset for\npre-training LMPM. The experimental results highlight the effectiveness of our\napproach in improving the quality of entailment tree generation. By leveraging\nlogical entailment patterns, our model produces more coherent and reasonable\nconclusions that closely align with the underlying premises. Code and Data are\nreleased at https://github.com/YuanLi95/T5-LMPM\n","authors":["Li Yuan","Yi Cai","Haopeng Ren","Jiexin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06410v1.pdf","comment":"Accepted By Coling 2024"},{"id":"http://arxiv.org/abs/2403.06402v1","updated":"2024-03-11T03:28:13Z","published":"2024-03-11T03:28:13Z","title":"'One size doesn't fit all': Learning how many Examples to use for\n  In-Context Learning for Improved Text Classification","summary":"  Predictive models in natural language processing (NLP) have evolved from\ntraining models from scratch to fine-tuning pre-trained models with labelled\ndata. An extreme form of this fine-tuning involves in-context learning (ICL),\nwhere the output of a pre-trained generative model (frozen decoder parameters)\nis controlled only with variations in the input strings (called instructions or\nprompts). An important component of ICL is the use of a small number of\nlabelled data instances as examples in the prompt. While existing work uses a\nstatic number of examples during inference for each data instance, in this\npaper we propose a novel methodology of dynamically adapting the number of\nexamples as per the data. This is analogous to the use of a variable-sized\nneighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow\nof adaptive ICL (AICL), the number of demonstrations to employ during the\ninference on a particular data instance is predicted by the Softmax posteriors\nof a classifier. The parameters of this classifier are fitted on the optimal\nnumber of examples in ICL required to correctly infer the label of each\ninstance in the training set with the hypothesis that a test instance that is\nsimilar to a training instance should use the same (or a closely matching)\nnumber of few-shot examples. Our experiments show that our AICL method results\nin improvement in text classification task on several standard datasets.\n","authors":["Manish Chandra","Debasis Ganguly","Yiwen Li","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2403.06402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06399v1","updated":"2024-03-11T03:21:15Z","published":"2024-03-11T03:21:15Z","title":"GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing","summary":"  A key aspect of language documentation is the creation of annotated text in a\nformat such as interlinear glossed text (IGT), which captures fine-grained\nmorphosyntactic analyses in a morpheme-by-morpheme format. Prior work has\nexplored methods to automatically generate IGT in order to reduce the time cost\nof language analysis. However, many languages (particularly those requiring\npreservation) lack sufficient IGT data to train effective models, and\ncrosslingual transfer has been proposed as a method to overcome this\nlimitation.\n  We compile the largest existing corpus of IGT data from a variety of sources,\ncovering over 450k examples across 1.8k languages, to enable research on\ncrosslingual transfer and IGT generation. Then, we pretrain a large\nmultilingual model on a portion of this corpus, and further finetune it to\nspecific languages. Our model is competitive with state-of-the-art methods for\nsegmented data and large monolingual datasets. Meanwhile, our model outperforms\nSOTA models on unsegmented text and small corpora by up to 6.6% morpheme\naccuracy, demonstrating the effectiveness of crosslingual transfer for\nlow-resource languages.\n","authors":["Michael Ginn","Lindia Tjuatja","Taiqi He","Enora Rice","Graham Neubig","Alexis Palmer","Lori Levin"],"pdf_url":"https://arxiv.org/pdf/2403.06399v1.pdf","comment":"18 pages, 3 figures Submitted to ACL ARR Feb 2024 First two authors\n  are equal contribution"},{"id":"http://arxiv.org/abs/2401.14151v2","updated":"2024-03-11T03:15:58Z","published":"2024-01-25T13:03:20Z","title":"True Knowledge Comes from Practice: Aligning LLMs with Embodied\n  Environments via Reinforcement Learning","summary":"  Despite the impressive performance across numerous tasks, large language\nmodels (LLMs) often fail in solving simple decision-making tasks due to the\nmisalignment of the knowledge in LLMs with environments. On the contrary,\nreinforcement learning (RL) agents learn policies from scratch, which makes\nthem always align with environments but difficult to incorporate prior\nknowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a\nnovel general online framework that deploys LLMs as decision-making agents to\nefficiently interact and align with embodied environments via RL without\nrequiring any prepared datasets or prior knowledge of the environments.\nFirstly, we query the joint probabilities of each valid action with LLMs to\nform behavior policies. Then, to enhance the stability and robustness of the\npolicies, we propose two normalization methods and summarize four prompt design\nprinciples. Finally, we design a novel parameter-efficient training\narchitecture where the actor and critic share one frozen LLM equipped with\nlow-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to\nevaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency\nand performance compared to the conventional RL method, PPO, and prompt tuning\nmethod, SayCan, in both classical decision-making environment, Overcooked, and\nsimulated household environment, VirtualHome. ii) Benefiting from LLMs'\nopen-vocabulary feature, TWOSOME shows superior generalization ability to\nunseen tasks. iii) Under our framework, there is no significant loss of the\nLLMs' original ability during online PPO finetuning.\n","authors":["Weihao Tan","Wentao Zhang","Shanqi Liu","Longtao Zheng","Xinrun Wang","Bo An"],"pdf_url":"https://arxiv.org/pdf/2401.14151v2.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2309.03883v2","updated":"2024-03-11T02:01:09Z","published":"2023-09-07T17:45:31Z","title":"DoLa: Decoding by Contrasting Layers Improves Factuality in Large\n  Language Models","summary":"  Despite their impressive capabilities, large language models (LLMs) are prone\nto hallucinations, i.e., generating content that deviates from facts seen\nduring pretraining. We propose a simple decoding strategy for reducing\nhallucinations with pretrained LLMs that does not require conditioning on\nretrieved external knowledge nor additional fine-tuning. Our approach obtains\nthe next-token distribution by contrasting the differences in logits obtained\nfrom projecting the later layers versus earlier layers to the vocabulary space,\nexploiting the fact that factual knowledge in an LLMs has generally been shown\nto be localized to particular transformer layers. We find that this Decoding by\nContrasting Layers (DoLa) approach is able to better surface factual knowledge\nand reduce the generation of incorrect facts. DoLa consistently improves the\ntruthfulness across multiple choices tasks and open-ended generation tasks, for\nexample improving the performance of LLaMA family models on TruthfulQA by\n12-17% absolute points, demonstrating its potential in making LLMs reliably\ngenerate truthful facts.\n","authors":["Yung-Sung Chuang","Yujia Xie","Hongyin Luo","Yoon Kim","James Glass","Pengcheng He"],"pdf_url":"https://arxiv.org/pdf/2309.03883v2.pdf","comment":"ICLR 2024 main conference paper. The source code is available at\n  https://github.com/voidism/DoLa"},{"id":"http://arxiv.org/abs/2402.19467v3","updated":"2024-03-11T01:31:58Z","published":"2024-02-29T18:57:01Z","title":"TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning","summary":"  It is challenging to perform question-answering over complex, multimodal\ncontent such as television clips. This is in part because current\nvideo-language models rely on single-modality reasoning, have lowered\nperformance on long inputs, and lack interpetability. We propose TV-TREES, the\nfirst multimodal entailment tree generator. TV-TREES serves as an approach to\nvideo understanding that promotes interpretable joint-modality reasoning by\nproducing trees of entailment relationships between simple premises directly\nentailed by the videos and higher-level conclusions. We then introduce the task\nof multimodal entailment tree generation to evaluate the reasoning quality of\nsuch methods. Our method's experimental results on the challenging TVQA dataset\ndemonstrate intepretable, state-of-the-art zero-shot performance on full video\nclips, illustrating a best-of-both-worlds contrast to black-box methods.\n","authors":["Kate Sanders","Nathaniel Weir","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2402.19467v3.pdf","comment":"9 pages, preprint"},{"id":"http://arxiv.org/abs/2401.10286v3","updated":"2024-03-11T01:23:47Z","published":"2024-01-16T02:11:35Z","title":"Code-Based English Models Surprising Performance on Chinese QA Pair\n  Extraction Task","summary":"  In previous studies, code-based models have consistently outperformed\ntext-based models in reasoning-intensive scenarios. When generating our\nknowledge base for Retrieval-Augmented Generation (RAG), we observed that\ncode-based models also perform exceptionally well in Chinese QA Pair Extraction\ntask. Further, our experiments and the metrics we designed discovered that\ncode-based models containing a certain amount of Chinese data achieve even\nbetter performance. Additionally, the capabilities of code-based English models\nin specified Chinese tasks offer a distinct perspective for discussion on the\nphilosophical \"Chinese Room\" thought experiment.\n","authors":["Linghan Zheng","Hui Liu","Xiaojun Lin","Jiayuan Dong","Yue Sheng","Gang Shi","Zhiwei Liu","Hongwei Chen"],"pdf_url":"https://arxiv.org/pdf/2401.10286v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04769v2","updated":"2024-03-11T01:21:32Z","published":"2024-02-16T17:02:53Z","title":"Using Hallucinations to Bypass GPT4's Filter","summary":"  Large language models (LLMs) are initially trained on vast amounts of data,\nthen fine-tuned using reinforcement learning from human feedback (RLHF); this\nalso serves to teach the LLM to provide appropriate and safe responses. In this\npaper, we present a novel method to manipulate the fine-tuned version into\nreverting to its pre-RLHF behavior, effectively erasing the model's filters;\nthe exploit currently works for GPT4, Claude Sonnet, and (to some extent) for\nInflection-2.5. Unlike other jailbreaks (for example, the popular \"Do Anything\nNow\" (DAN) ), our method does not rely on instructing the LLM to override its\nRLHF policy; hence, simply modifying the RLHF process is unlikely to address\nit. Instead, we induce a hallucination involving reversed text during which the\nmodel reverts to a word bucket, effectively pausing the model's filter. We\nbelieve that our exploit presents a fundamental vulnerability in LLMs currently\nunaddressed, as well as an opportunity to better understand the inner workings\nof LLMs during hallucinations.\n","authors":["Benjamin Lemkin"],"pdf_url":"https://arxiv.org/pdf/2403.04769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06360v1","updated":"2024-03-11T01:18:00Z","published":"2024-03-11T01:18:00Z","title":"Human and Automatic Interpretation of Romanian Noun Compounds","summary":"  Determining the intended, context-dependent meanings of noun compounds like\n\"shoe sale\" and \"fire sale\" remains a challenge for NLP. Previous work has\nrelied on inventories of semantic relations that capture the different meanings\nbetween compound members. Focusing on Romanian compounds, whose morphosyntax\ndiffers from that of their English counterparts, we propose a new set of\nrelations and test it with human annotators and a neural net classifier.\nResults show an alignment of the network's predictions and human judgments,\neven where the human agreement rate is low. Agreement tracks with the frequency\nof the selected relations, regardless of structural differences. However, the\nmost frequently selected relation was none of the sixteen labeled semantic\nrelations, indicating the need for a better relation inventory.\n","authors":["Ioana Marinescu","Christiane Fellbaum"],"pdf_url":"https://arxiv.org/pdf/2403.06360v1.pdf","comment":"6 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2305.17126v2","updated":"2024-03-11T01:15:09Z","published":"2023-05-26T17:50:11Z","title":"Large Language Models as Tool Makers","summary":"  Recent research has highlighted the potential of large language models (LLMs)\nto improve their problem-solving capabilities with the aid of suitable external\ntools. In our work, we further advance this concept by introducing a\nclosed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs\ncreate their own reusable tools for problem-solving. Our approach consists of\ntwo phases: 1) tool making: an LLM acts as the tool maker that crafts tools for\na set of tasks. 2) tool using: another LLM acts as the tool user, which applies\nthe tool built by the tool maker for problem-solving. On the problem-solving\nserver side, tool-making enables continual tool generation and caching as new\nrequests emerge. This framework enables subsequent requests to access cached\ntools via their corresponding APIs, enhancing the efficiency of task\nresolution. Recognizing that tool-making requires more sophisticated\ncapabilities, we assign this task to a powerful, albeit resource-intensive,\nmodel. Conversely, the simpler tool-using phase is delegated to a lightweight\nmodel. This strategic division of labor allows the once-off cost of tool-making\nto be spread over multiple instances of tool-using, significantly reducing\naverage costs while maintaining strong performance. Furthermore, our method\noffers a functional cache through the caching and reuse of tools, which stores\nthe functionality of a class of requests instead of the natural language\nresponses from LLMs, thus extending the applicability of the conventional cache\nmechanism. We evaluate our approach across various complex reasoning tasks,\nincluding Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool\nuser, LATM demonstrates performance equivalent to using GPT-4 for both roles,\nbut with a significantly reduced inference cost.\n","authors":["Tianle Cai","Xuezhi Wang","Tengyu Ma","Xinyun Chen","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.17126v2.pdf","comment":"Code available at https://github.com/ctlllll/LLM-ToolMaker"},{"id":"http://arxiv.org/abs/2403.06355v1","updated":"2024-03-11T01:07:36Z","published":"2024-03-11T01:07:36Z","title":"Multi-modal Semantic Understanding with Contrastive Cross-modal Feature\n  Alignment","summary":"  Multi-modal semantic understanding requires integrating information from\ndifferent modalities to extract users' real intention behind words. Most\nprevious work applies a dual-encoder structure to separately encode image and\ntext, but fails to learn cross-modal feature alignment, making it hard to\nachieve cross-modal deep information interaction. This paper proposes a novel\nCLIP-guided contrastive-learning-based architecture to perform multi-modal\nfeature alignment, which projects the features derived from different\nmodalities into a unified deep space. On multi-modal sarcasm detection (MMSD)\nand multi-modal sentiment analysis (MMSA) tasks, the experimental results show\nthat our proposed model significantly outperforms several baselines, and our\nfeature alignment strategy brings obvious performance gain over models with\ndifferent aggregating methods and models even enriched with knowledge. More\nimportantly, our model is simple to implement without using task-specific\nexternal knowledge, and thus can easily migrate to other multi-modal tasks. Our\nsource codes are available at https://github.com/ChangKe123/CLFA.\n","authors":["Ming Zhang","Ke Chang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2403.06355v1.pdf","comment":"10 pages, 4 figures, accepted by LREC-COLING 2024(main conference,\n  long paper)"},{"id":"http://arxiv.org/abs/2403.06354v1","updated":"2024-03-11T01:04:36Z","published":"2024-03-11T01:04:36Z","title":"Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages","summary":"  Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible\nproficiency at natural language processing tasks and have even begun to excel\nat tasks across other modalities such as vision and audio. Despite their\nsuccess, LLMs often struggle to perform well on low-resource languages because\nthere is so little training data available. This shortcoming is especially\nprevalent with open source models. In this work, we explore training LLaMA-2 to\nspeak Amharic, a language which is spoken by over 50 million people world wide,\nbut has orders of magnitude less data available than languages like English. We\nemploy methods previously used for training LLMs on other languages with data\nscarcity, and use open source translation models to perform data augmentation\nand grow our dataset from millions of tokens to billions. We further enhance\nthe capabilities of our model by connecting an image encoder and training on a\ntranslated visual instruction tuning dataset in the same manner as LLaVA,\nresulting in a multimodal Amharic LLM that can understand images along with\ntext. We introduce an Amharic version of a popular benchmarking dataset to\nevaluate our work. Our models and dataset are open sourced and available on\nGitHub.\n","authors":["Michael Andersland"],"pdf_url":"https://arxiv.org/pdf/2403.06354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04789v2","updated":"2024-03-11T01:04:28Z","published":"2024-03-04T08:38:53Z","title":"TopicDiff: A Topic-enriched Diffusion Approach for Multimodal\n  Conversational Emotion Detection","summary":"  Multimodal Conversational Emotion (MCE) detection, generally spanning across\nthe acoustic, vision and language modalities, has attracted increasing interest\nin the multimedia community. Previous studies predominantly focus on learning\ncontextual information in conversations with only a few considering the topic\ninformation in single language modality, while always neglecting the acoustic\nand vision topic information. On this basis, we propose a model-agnostic\nTopic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic\ninformation in MCE tasks. Particularly, we integrate the diffusion model into\nneural topic model to alleviate the diversity deficiency problem of neural\ntopic model in capturing topic information. Detailed evaluations demonstrate\nthe significant improvements of TopicDiff over the state-of-the-art MCE\nbaselines, justifying the importance of multimodal topic information to MCE and\nthe effectiveness of TopicDiff in capturing such information. Furthermore, we\nobserve an interesting finding that the topic information in acoustic and\nvision is more discriminative and robust compared to the language.\n","authors":["Jiamin Luo","Jingjing Wang","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.04789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06350v1","updated":"2024-03-11T00:46:56Z","published":"2024-03-11T00:46:56Z","title":"IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning\n  Datasets for Indian Languages","summary":"  Despite the considerable advancements in English LLMs, the progress in\nbuilding comparable models for other languages has been hindered due to the\nscarcity of tailored resources. Our work aims to bridge this divide by\nintroducing an expansive suite of resources specifically designed for the\ndevelopment of Indic LLMs, covering 22 languages, containing a total of 251B\ntokens and 74.8M instruction-response pairs. Recognizing the importance of both\ndata quality and quantity, our approach combines highly curated manually\nverified data, unverified yet valuable data, and synthetic data. We build a\nclean, open-source pipeline for curating pre-training data from diverse\nsources, including websites, PDFs, and videos, incorporating best practices for\ncrawling, cleaning, flagging, and deduplication. For instruction-fine tuning,\nwe amalgamate existing Indic datasets, translate/transliterate English datasets\ninto Indian languages, and utilize LLaMa2 and Mixtral models to create\nconversations grounded in articles from Indian Wikipedia and Wikihow.\nAdditionally, we address toxicity alignment by generating toxic prompts for\nmultiple scenarios and then generate non-toxic responses by feeding these toxic\nprompts to an aligned LLaMa2 model. We hope that the datasets, tools, and\nresources released as a part of this work will not only propel the research and\ndevelopment of Indic LLMs but also establish an open-source blueprint for\nextending such efforts to other languages. The data and other artifacts created\nas part of this work are released with permissive licenses.\n","authors":["Mohammed Safi Ur Rahman Khan","Priyam Mehta","Ananth Sankar","Umashankar Kumaravelan","Sumanth Doddapaneni","Suriyaprasaad G","Varun Balan G","Sparsh Jain","Anoop Kunchukuttan","Pratyush Kumar","Raj Dabre","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2403.06350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15479v2","updated":"2024-03-11T23:37:36Z","published":"2024-01-27T19:04:30Z","title":"Navigating the Post-API Dilemma Search Engine Results Pages Present a\n  Biased View of Social Media Data","summary":"  Recent decisions to discontinue access to social media APIs are having\ndetrimental effects on Internet research and the field of computational social\nscience as a whole. This lack of access to data has been dubbed the Post-API\nera of Internet research. Fortunately, popular search engines have the means to\ncrawl, capture, and surface social media data on their Search Engine Results\nPages (SERP) if provided the proper search query, and may provide a solution to\nthis dilemma. In the present work we ask: does SERP provide a complete and\nunbiased sample of social media data? Is SERP a viable alternative to direct\nAPI-access? To answer these questions, we perform a comparative analysis\nbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. We\nfind that SERP results are highly biased in favor of popular posts; against\npolitical, pornographic, and vulgar posts; are more positive in their\nsentiment; and have large topical gaps. Overall, we conclude that SERP is not a\nviable alternative to social media API access.\n","authors":["Amrit Poudel","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2401.15479v2.pdf","comment":"Proceedings of the ACM Web Conference 2024 (WWW '24)"},{"id":"http://arxiv.org/abs/2402.05359v2","updated":"2024-03-11T23:15:10Z","published":"2024-02-08T02:37:30Z","title":"Guiding Large Language Models with Divide-and-Conquer Program for\n  Discerning Problem Solving","summary":"  Foundation models, such as Large language Models (LLMs), have attracted\nsignificant amount of interest due to their large number of applications.\nExisting works show that appropriate prompt design, such as Chain-of-Thoughts,\ncan unlock LLM's powerful capacity in diverse areas. However, when handling\ntasks involving repetitive sub-tasks and/or deceptive contents, such as\narithmetic calculation and article-level fake news detection, existing\nprompting strategies either suffers from insufficient expressive power or\nintermediate errors triggered by hallucination. To make LLM more discerning to\nsuch intermediate errors, we propose to guide LLM with a Divide-and-Conquer\nprogram that simultaneously ensures superior expressive power and disentangles\ntask decomposition, sub-task resolution, and resolution assembly process.\nTheoretic analysis reveals that our strategy can guide LLM to extend the\nexpressive power of fixed-depth Transformer. Experiments indicate that our\nproposed method can achieve better performance than typical prompting\nstrategies in tasks bothered by intermediate errors and deceptive contents,\nsuch as large integer multiplication, hallucination detection and\nmisinformation detection.\n","authors":["Yizhou Zhang","Lun Du","Defu Cao","Qiang Fu","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2402.05359v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.07202v1","updated":"2024-03-11T22:58:58Z","published":"2024-03-11T22:58:58Z","title":"SPAWNing Structural Priming Predictions from a Cognitively Motivated\n  Parser","summary":"  Structural priming is a widely used psycholinguistic paradigm to study human\nsentence representations. In this work we propose a framework for using\nempirical priming patterns to build a theory characterizing the structural\nrepresentations humans construct when processing sentences. This framework uses\na new cognitively motivated parser, SPAWN, to generate quantitative priming\npredictions from theoretical syntax and evaluate these predictions with\nempirical human behavior. As a case study, we apply this framework to study\nreduced relative clause representations in English. We use SPAWN to generate\npriming predictions from two theoretical accounts which make different\nassumptions about the structure of relative clauses. We find that the\npredictions from only one of these theories (Participial-Phase) align with\nempirical priming patterns, thus highlighting which assumptions about relative\nclause better capture human sentence representations.\n","authors":["Grusha Prasad","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2403.07202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07193v1","updated":"2024-03-11T22:27:16Z","published":"2024-03-11T22:27:16Z","title":"CuentosIE: can a chatbot about \"tales with a message\" help to teach\n  emotional intelligence?","summary":"  In this article, we present CuentosIE (TalesEI: chatbot of tales with a\nmessage to develop Emotional Intelligence), an educational chatbot on emotions\nthat also provides teachers and psychologists with a tool to monitor their\nstudents/patients through indicators and data compiled by CuentosIE. The use of\n\"tales with a message\" is justified by their simplicity and easy understanding,\nthanks to their moral or associated metaphors. The main contributions of\nCuentosIE are the selection, collection, and classification of a set of highly\nspecialized tales, as well as the provision of tools (searching, reading\ncomprehension, chatting, recommending, and classifying) that are useful for\nboth educating users about emotions and monitoring their emotional development.\nThe preliminary evaluation of the tool has obtained encouraging results, which\nprovides an affirmative answer to the question posed in the title of the\narticle.\n","authors":["Antonio Ferrández","Rocío Lavigne-Cerván","Jesús Peral","Ignasi Navarro-Soria","Ángel Lloret","David Gil","Carmen Rocamora"],"pdf_url":"https://arxiv.org/pdf/2403.07193v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2403.07191v1","updated":"2024-03-11T22:24:14Z","published":"2024-03-11T22:24:14Z","title":"$\\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking\n  Reinforcement Learning Algorithms in Generative Language Model","summary":"  Recent advances in reinforcement learning (RL) algorithms aim to enhance the\nperformance of language models at scale. Yet, there is a noticeable absence of\na cost-effective and standardized testbed tailored to evaluating and comparing\nthese algorithms. To bridge this gap, we present a generalized version of the\n24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a\ntarget value $K$ with $N$ integers. We evaluate the effectiveness of\nestablished RL algorithms such as Proximal Policy Optimization (PPO), alongside\nnovel approaches like Identity Policy Optimization (IPO) and Direct Policy\nOptimization (DPO).\n","authors":["Yufeng Zhang","Liyu Chen","Boyi Liu","Yingxiang Yang","Qiwen Cui","Yunzhe Tao","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2403.07191v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.07183v1","updated":"2024-03-11T21:51:39Z","published":"2024-03-11T21:51:39Z","title":"Monitoring AI-Modified Content at Scale: A Case Study on the Impact of\n  ChatGPT on AI Conference Peer Reviews","summary":"  We present an approach for estimating the fraction of text in a large corpus\nwhich is likely to be substantially modified or produced by a large language\nmodel (LLM). Our maximum likelihood model leverages expert-written and\nAI-generated reference texts to accurately and efficiently examine real-world\nLLM-use at the corpus level. We apply this approach to a case study of\nscientific peer review in AI conferences that took place after the release of\nChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest\nthat between 6.5% and 16.9% of text submitted as peer reviews to these\nconferences could have been substantially modified by LLMs, i.e. beyond\nspell-checking or minor writing updates. The circumstances in which generated\ntext occurs offer insight into user behavior: the estimated fraction of\nLLM-generated text is higher in reviews which report lower confidence, were\nsubmitted close to the deadline, and from reviewers who are less likely to\nrespond to author rebuttals. We also observe corpus-level trends in generated\ntext which may be too subtle to detect at the individual level, and discuss the\nimplications of such trends on peer review. We call for future\ninterdisciplinary work to examine how LLM use is changing our information and\nknowledge practices.\n","authors":["Weixin Liang","Zachary Izzo","Yaohui Zhang","Haley Lepp","Hancheng Cao","Xuandong Zhao","Lingjiao Chen","Haotian Ye","Sheng Liu","Zhi Huang","Daniel A. McFarland","James Y. Zou"],"pdf_url":"https://arxiv.org/pdf/2403.07183v1.pdf","comment":"42 pages, 30 figures"},{"id":"http://arxiv.org/abs/2403.07179v1","updated":"2024-03-11T21:44:54Z","published":"2024-03-11T21:44:54Z","title":"3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of\n  Molecular Graphs","summary":"  Generating molecules with desired properties is a critical task with broad\napplications in drug discovery and materials design. Inspired by recent\nadvances in large language models, there is a growing interest in using natural\nlanguage descriptions of molecules to generate molecules with the desired\nproperties. Most existing methods focus on generating molecules that precisely\nmatch the text description. However, practical applications call for methods\nthat generate diverse, and ideally novel, molecules with the desired\nproperties. We propose 3M-Diffusion, a novel multi-modal molecular graph\ngeneration method, to address this challenge. 3M-Diffusion first encodes\nmolecular graphs into a graph latent space aligned with text descriptions. It\nthen reconstructs the molecular structure and atomic attributes based on the\ngiven text descriptions using the molecule decoder. It then learns a\nprobabilistic mapping from the text space to the latent molecular graph space\nusing a diffusion model. The results of our extensive experiments on several\ndatasets demonstrate that 3M-Diffusion can generate high-quality, novel and\ndiverse molecular graphs that semantically match the textual description\nprovided.\n","authors":["Huaisheng Zhu","Teng Xiao","Vasant G Honavar"],"pdf_url":"https://arxiv.org/pdf/2403.07179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07175v1","updated":"2024-03-11T21:33:05Z","published":"2024-03-11T21:33:05Z","title":"Rebuilding ROME : Resolving Model Collapse during Sequential Model\n  Editing","summary":"  Recent work on model editing using Rank-One Model Editing (ROME), a popular\nmodel editing method, has shown that there are certain facts that the algorithm\nis unable to edit without breaking the model. Such edits have previously been\ncalled disabling edits. These disabling edits cause immediate model collapse\nand limits the use of ROME for sequential editing. In this paper, we make two\nmain contributions. Firstly, we show that model collapse with ROME only happens\nwhen making edits using the CounterFact dataset and does not happen when using\nthe zsRE dataset. Secondly, we find that disabling edits are an artifact of the\noriginal implementation of ROME. With this paper, we provide a more stable\nimplementation ROME, which we call r-ROME and show that we no longer observe\nmodel collapse when making large scale sequential edits with ROME.\n","authors":["Akshat Gupta","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.07175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07144v1","updated":"2024-03-11T20:28:27Z","published":"2024-03-11T20:28:27Z","title":"Thought Graph: Generating Thought Process for Biological Reasoning","summary":"  We present the Thought Graph as a novel framework to support complex\nreasoning and use gene set analysis as an example to uncover semantic\nrelationships between biological processes. Our framework stands out for its\nability to provide a deeper understanding of gene sets, significantly\nsurpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity\nto human annotations. Our analysis further provides insights into future\ndirections of biological processes naming, and implications for bioinformatics\nand precision medicine.\n","authors":["Chi-Yang Hsu","Kyle Cox","Jiawei Xu","Zhen Tan","Tianhua Zhai","Mengzhou Hu","Dexter Pratt","Tianlong Chen","Ziniu Hu","Ying Ding"],"pdf_url":"https://arxiv.org/pdf/2403.07144v1.pdf","comment":"4 pages. Accepted by Web Conf 2024"},{"id":"http://arxiv.org/abs/2403.07142v1","updated":"2024-03-11T20:23:59Z","published":"2024-03-11T20:23:59Z","title":"One Category One Prompt: Dataset Distillation using Diffusion Models","summary":"  The extensive amounts of data required for training deep neural networks pose\nsignificant challenges on storage and transmission fronts. Dataset distillation\nhas emerged as a promising technique to condense the information of massive\ndatasets into a much smaller yet representative set of synthetic samples.\nHowever, traditional dataset distillation approaches often struggle to scale\neffectively with high-resolution images and more complex architectures due to\nthe limitations in bi-level optimization. Recently, several works have proposed\nexploiting knowledge distillation with decoupled optimization schemes to scale\nup dataset distillation. Although these methods effectively address the\nscalability issue, they rely on extensive image augmentations requiring the\nstorage of soft labels for augmented images. In this paper, we introduce\nDataset Distillation using Diffusion Models (D3M) as a novel paradigm for\ndataset distillation, leveraging recent advancements in generative\ntext-to-image foundation models. Our approach utilizes textual inversion, a\ntechnique for fine-tuning text-to-image generative models, to create concise\nand informative representations for large datasets. By employing these learned\ntext prompts, we can efficiently store and infer new samples for introducing\ndata variability within a fixed memory budget. We show the effectiveness of our\nmethod through extensive experiments across various computer vision benchmark\ndatasets with different memory budgets.\n","authors":["Ali Abbasi","Ashkan Shahbazi","Hamed Pirsiavash","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2403.07142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07118v1","updated":"2024-03-11T19:19:59Z","published":"2024-03-11T19:19:59Z","title":"Narrating Causal Graphs with Large Language Models","summary":"  The use of generative AI to create text descriptions from graphs has mostly\nfocused on knowledge graphs, which connect concepts using facts. In this work\nwe explore the capability of large pretrained language models to generate text\nfrom causal graphs, where salient concepts are represented as nodes and\ncausality is represented via directed, typed edges. The causal reasoning\nencoded in these graphs can support applications as diverse as healthcare or\nmarketing. Using two publicly available causal graph datasets, we empirically\ninvestigate the performance of four GPT-3 models under various settings. Our\nresults indicate that while causal text descriptions improve with training\ndata, compared to fact-based graphs, they are harder to generate under\nzero-shot settings. Results further suggest that users of generative AI can\ndeploy future applications faster since similar performances are obtained when\ntraining a model with only a few examples as compared to fine-tuning via a\nlarge curated dataset.\n","authors":["Atharva Phatak","Vijay K. Mago","Ameeta Agrawal","Aravind Inbasekaran","Philippe J. Giabbanelli"],"pdf_url":"https://arxiv.org/pdf/2403.07118v1.pdf","comment":"HICSS '24"},{"id":"http://arxiv.org/abs/2403.05527v2","updated":"2024-03-11T18:55:40Z","published":"2024-03-08T18:48:30Z","title":"GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM","summary":"  Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.\n","authors":["Hao Kang","Qingru Zhang","Souvik Kundu","Geonhwa Jeong","Zaoxing Liu","Tushar Krishna","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.05527v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16315v2","updated":"2024-03-11T18:49:40Z","published":"2024-02-26T05:43:51Z","title":"Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models","summary":"  Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.\n","authors":["Jeonghwan Kim","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2402.16315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04964v2","updated":"2024-03-11T18:41:29Z","published":"2024-03-08T00:27:57Z","title":"Tell me the truth: A system to measure the trustworthiness of Large\n  Language Models","summary":"  Large Language Models (LLM) have taken the front seat in most of the news\nsince November 2022, when ChatGPT was introduced. After more than one year, one\nof the major reasons companies are resistant to adopting them is the limited\nconfidence they have in the trustworthiness of those systems. In a study by\n(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in\nidentifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics\nfound that ChatGPT has an accuracy rate of 17% percent when diagnosing\npediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust\nis a relative, subject condition that can change based on culture, domain,\nindividuals. And then, given a domain, how can the trustworthiness of a system\nbe measured? In this paper, I present a systematic approach to measure\ntrustworthiness based on a predefined ground truth, represented as a knowledge\ngraph of the domain. The approach is a process with humans in the loop to\nvalidate the representation of the domain and to fine-tune the system.\n  Measuring the trustworthiness would be essential for all the entities\noperating in critical environments, such as healthcare, defense, finance, but\nit would be very relevant for all the users of LLMs.\n","authors":["Carlo Lipizzi"],"pdf_url":"https://arxiv.org/pdf/2403.04964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07088v1","updated":"2024-03-11T18:26:02Z","published":"2024-03-11T18:26:02Z","title":"SPA: Towards A Computational Friendly Cloud-Base and On-Devices\n  Collaboration Seq2seq Personalized Generation","summary":"  Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require high computation cost and\nlarge memory cost. At the same time, LLMs may cause privacy leakage when\ntraining or prediction procedure contains sensitive information. In this paper,\nwe propose SPA(Side Plugin Adaption), a lightweight architecture for fast\non-devices inference and privacy retaining on the constraints of strict\non-devices computation and memory constraints. Compared with other on-devices\nseq2seq generation, SPA could make a fast and stable inference on low-resource\nconstraints, allowing it to obtain cost effiency. Our method establish an\ninteraction between a pretrained LLMs on-cloud and additive parameters\non-devices, which could provide the knowledge on both pretrained LLMs and\nprivate personal feature.Further more, SPA provides a framework to keep\nfeature-base parameters on private guaranteed but low computational devices\nwhile leave the parameters containing general information on the high\ncomputational devices.\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Le Dai","Xingzu Liu","Weihao Liu","Mingbang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07088v1.pdf","comment":"11 pages, first version of SPA(Side Plugin Adaption)"},{"id":"http://arxiv.org/abs/2403.07087v1","updated":"2024-03-11T18:25:01Z","published":"2024-03-11T18:25:01Z","title":"LSTM-Based Text Generation: A Study on Historical Datasets","summary":"  This paper presents an exploration of Long Short-Term Memory (LSTM) networks\nin the realm of text generation, focusing on the utilization of historical\ndatasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in\nhandling sequential data, are applied here to model complex language patterns\nand structures inherent in historical texts. The study demonstrates that\nLSTM-based models, when trained on historical datasets, can not only generate\ntext that is linguistically rich and contextually relevant but also provide\ninsights into the evolution of language patterns over time. The finding\npresents models that are highly accurate and efficient in predicting text from\nworks of Nietzsche, with low loss values and a training time of 100 iterations.\nThe accuracy of the model is 0.9521, indicating high accuracy. The loss of the\nmodel is 0.2518, indicating its effectiveness. The accuracy of the model in\npredicting text from the work of Shakespeare is 0.9125, indicating a low error\nrate. The training time of the model is 100, mirroring the efficiency of the\nNietzsche dataset. This efficiency demonstrates the effectiveness of the model\ndesign and training methodology, especially when handling complex literary\ntexts. This research contributes to the field of natural language processing by\nshowcasing the versatility of LSTM networks in text generation and offering a\npathway for future explorations in historical linguistics and beyond.\n","authors":["Mustafa Abbas Hussein Hussein","Serkan Savaş"],"pdf_url":"https://arxiv.org/pdf/2403.07087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02832v2","updated":"2024-03-11T18:18:41Z","published":"2023-10-04T13:59:45Z","title":"Out-of-Distribution Detection by Leveraging Between-Layer Transformation\n  Smoothness","summary":"  Effective out-of-distribution (OOD) detection is crucial for reliable machine\nlearning models, yet most current methods are limited in practical use due to\nrequirements like access to training data or intervention in training. We\npresent a novel method for detecting OOD data in Transformers based on\ntransformation smoothness between intermediate layers of a network (BLOOD),\nwhich is applicable to pre-trained models without access to training data.\nBLOOD utilizes the tendency of between-layer representation transformations of\nin-distribution (ID) data to be smoother than the corresponding transformations\nof OOD data, a property that we also demonstrate empirically. We evaluate BLOOD\non several text classification tasks with Transformer networks and demonstrate\nthat it outperforms methods with comparable resource requirements. Our analysis\nalso suggests that when learning simpler tasks, OOD data transformations\nmaintain their original sharpness, whereas sharpness increases with more\ncomplex tasks.\n","authors":["Fran Jelenić","Josip Jukić","Martin Tutek","Mate Puljiz","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2310.02832v2.pdf","comment":"International Conference on Learning Representations: ICLR 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.06978v1","updated":"2024-03-11T17:59:41Z","published":"2024-03-11T17:59:41Z","title":"Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained\n  Models for Spatiotemporal Modeling","summary":"  In this paper, we introduce Attention Prompt Tuning (APT) - a computationally\nefficient variant of prompt tuning for video-based applications such as action\nrecognition. Prompt tuning approaches involve injecting a set of learnable\nprompts along with data tokens during fine-tuning while keeping the backbone\nfrozen. This approach greatly reduces the number of learnable parameters\ncompared to full tuning. For image-based downstream tasks, normally a couple of\nlearnable prompts achieve results close to those of full tuning. However,\nvideos, which contain more complex spatiotemporal information, require hundreds\nof tunable prompts to achieve reasonably good results. This reduces the\nparameter efficiency observed in images and significantly increases latency and\nthe number of floating-point operations (FLOPs) during inference. To tackle\nthese issues, we directly inject the prompts into the keys and values of the\nnon-local attention mechanism within the transformer block. Additionally, we\nintroduce a novel prompt reparameterization technique to make APT more robust\nagainst hyperparameter selection. The proposed APT approach greatly reduces the\nnumber of FLOPs and latency while achieving a significant performance boost\nover the existing parameter-efficient tuning methods on UCF101, HMDB51, and\nSSv2 datasets for action recognition. The code and pre-trained models are\navailable at https://github.com/wgcban/apt\n","authors":["Wele Gedara Chaminda Bandara","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2403.06978v1.pdf","comment":"Accepted at 18th IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG'24) Code available at: https://github.com/wgcban/apt\n  12 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.06977v1","updated":"2024-03-11T17:59:34Z","published":"2024-03-11T17:59:34Z","title":"VideoMamba: State Space Model for Efficient Video Understanding","summary":"  Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba.\n","authors":["Kunchang Li","Xinhao Li","Yi Wang","Yinan He","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2403.06977v1.pdf","comment":"19 Pages, 7 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2403.06976v1","updated":"2024-03-11T17:59:31Z","published":"2024-03-11T17:59:31Z","title":"BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed\n  Dual-Branch Diffusion","summary":"  Image inpainting, the process of restoring corrupted images, has seen\nsignificant advancements with the advent of diffusion models (DMs). Despite\nthese advancements, current DM adaptations for inpainting, which involve\nmodifications to the sampling strategy or the development of\ninpainting-specific DMs, frequently suffer from semantic inconsistencies and\nreduced image quality. Addressing these challenges, our work introduces a novel\nparadigm: the division of masked image features and noisy latent into separate\nbranches. This division dramatically diminishes the model's learning load,\nfacilitating a nuanced incorporation of essential masked image information in a\nhierarchical fashion. Herein, we present BrushNet, a novel plug-and-play\ndual-branch model engineered to embed pixel-level masked image features into\nany pre-trained DM, guaranteeing coherent and enhanced image inpainting\noutcomes. Additionally, we introduce BrushData and BrushBench to facilitate\nsegmentation-based inpainting training and performance assessment. Our\nextensive experimental analysis demonstrates BrushNet's superior performance\nover existing models across seven key metrics, including image quality, mask\nregion preservation, and textual coherence.\n","authors":["Xuan Ju","Xian Liu","Xintao Wang","Yuxuan Bian","Ying Shan","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.06976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06974v1","updated":"2024-03-11T17:57:41Z","published":"2024-03-11T17:57:41Z","title":"Memory-based Adapters for Online 3D Scene Perception","summary":"  In this paper, we propose a new framework for online 3D scene perception.\nConventional 3D scene perception methods are offline, i.e., take an already\nreconstructed 3D scene geometry as input, which is not applicable in robotic\napplications where the input data is streaming RGB-D videos rather than a\ncomplete 3D scene reconstructed from pre-collected RGB-D videos. To deal with\nonline 3D scene perception tasks where data collection and perception should be\nperformed simultaneously, the model should be able to process 3D scenes frame\nby frame and make use of the temporal information. To this end, we propose an\nadapter-based plug-and-play module for the backbone of 3D scene perception\nmodel, which constructs memory to cache and aggregate the extracted RGB-D\nfeatures to empower offline models with temporal learning ability.\nSpecifically, we propose a queued memory mechanism to cache the supporting\npoint cloud and image features. Then we devise aggregation modules which\ndirectly perform on the memory and pass temporal information to current frame.\nWe further propose 3D-to-2D adapter to enhance image features with strong\nglobal context. Our adapters can be easily inserted into mainstream offline\narchitectures of different tasks and significantly boost their performance on\nonline tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate\nour approach achieves leading performance on three 3D scene perception tasks\ncompared with state-of-the-art online methods by simply finetuning existing\noffline models, without any model and task-specific designs.\n\\href{https://xuxw98.github.io/Online3D/}{Project page}.\n","authors":["Xiuwei Xu","Chong Xia","Ziwei Wang","Linqing Zhao","Yueqi Duan","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2403.06974v1.pdf","comment":"Accepted to CVPR24. Link: https://xuxw98.github.io/Online3D/"},{"id":"http://arxiv.org/abs/2403.06973v1","updated":"2024-03-11T17:55:53Z","published":"2024-03-11T17:55:53Z","title":"Bayesian Diffusion Models for 3D Shape Reconstruction","summary":"  We present Bayesian Diffusion Models (BDM), a prediction algorithm that\nperforms effective Bayesian inference by tightly coupling the top-down (prior)\ninformation with the bottom-up (data-driven) procedure via joint diffusion\nprocesses. We show the effectiveness of BDM on the 3D shape reconstruction\ntask. Compared to prototypical deep learning data-driven approaches trained on\npaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM\nbrings in rich prior information from standalone labels (e.g. point clouds) to\nimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesian\nframeworks where explicit prior and likelihood are required for the inference,\nBDM performs seamless information fusion via coupled diffusion processes with\nlearned gradient computation networks. The specialty of our BDM lies in its\ncapability to engage the active and effective information exchange and fusion\nof the top-down and bottom-up processes where each itself is a diffusion\nprocess. We demonstrate state-of-the-art results on both synthetic and\nreal-world benchmarks for 3D shape reconstruction.\n","authors":["Haiyang Xu","Yu Lei","Zeyuan Chen","Xiang Zhang","Yue Zhao","Yilin Wang","Zhuowen Tu"],"pdf_url":"https://arxiv.org/pdf/2403.06973v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.05210v3","updated":"2024-03-11T17:50:31Z","published":"2024-02-07T19:35:09Z","title":"Anatomically-Controllable Medical Image Generation with\n  Segmentation-Guided Diffusion Models","summary":"  Diffusion models have enabled remarkably high-quality medical image\ngeneration, yet it is challenging to enforce anatomical constraints in\ngenerated images. This hampers many useful applications, including\npre-registered image generation, counterfactual scenarios, and others. To this\nend, we propose a diffusion model-based method that supports\nanatomically-controllable medical image generation, by following a multi-class\nanatomical segmentation mask at each sampling step. We additionally introduce a\nrandom mask ablation training algorithm to enable conditioning on a selected\ncombination of anatomical constraints while allowing flexibility in other\nanatomical areas. We compare our model (\"Seg-Diff\") to existing methods on\nbreast MRI and abdominal/neck-to-pelvis CT datasets with a wide range of\nanatomical objects. Results show that it reaches a new state-of-the-art in the\nfaithfulness of generated images to input anatomical masks on both datasets,\nand is on par for general anatomical realism. Finally, our model also enjoys\nthe extra benefit of being able to adjust the anatomical similarity of\ngenerated images to real images of choice through interpolation in its latent\nspace.\n","authors":["Nicholas Konz","Yuwen Chen","Haoyu Dong","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2402.05210v3.pdf","comment":"Code and synthetic dataset:\n  https://github.com/mazurowski-lab/segmentation-guided-diffusion"},{"id":"http://arxiv.org/abs/2403.06961v1","updated":"2024-03-11T17:46:21Z","published":"2024-03-11T17:46:21Z","title":"Explainable Transformer Prototypes for Medical Diagnoses","summary":"  Deployments of artificial intelligence in medical diagnostics mandate not\njust accuracy and efficacy but also trust, emphasizing the need for\nexplainability in machine decisions. The recent trend in automated medical\nimage diagnostics leans towards the deployment of Transformer-based\narchitectures, credited to their impressive capabilities. Since the\nself-attention feature of transformers contributes towards identifying crucial\nregions during the classification process, they enhance the trustability of the\nmethods. However, the complex intricacies of these attention mechanisms may\nfall short of effectively pinpointing the regions of interest directly\ninfluencing AI decisions. Our research endeavors to innovate a unique attention\nblock that underscores the correlation between 'regions' rather than 'pixels'.\nTo address this challenge, we introduce an innovative system grounded in\nprototype learning, featuring an advanced self-attention mechanism that goes\nbeyond conventional ad-hoc visual explanation techniques by offering\ncomprehensible visual insights. A combined quantitative and qualitative\nmethodological approach was used to demonstrate the effectiveness of the\nproposed method on the large-scale NIH chest X-ray dataset. Experimental\nresults showed that our proposed method offers a promising direction for\nexplainability, which can lead to the development of more trustable systems,\nwhich can facilitate easier and rapid adoption of such technology into routine\nclinics. The code is available at www.github.com/NUBagcilab/r2r_proto.\n","authors":["Ugur Demir","Debesh Jha","Zheyuan Zhang","Elif Keles","Bradley Allen","Aggelos K. Katsaggelos","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2403.06961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06953v1","updated":"2024-03-11T17:36:11Z","published":"2024-03-11T17:36:11Z","title":"Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot\n  Domain Transfer","summary":"  Purpose: Advances in deep learning have resulted in effective models for\nsurgical video analysis; however, these models often fail to generalize across\nmedical centers due to domain shift caused by variations in surgical workflow,\ncamera setups, and patient demographics. Recently, object-centric learning has\nemerged as a promising approach for improved surgical scene understanding,\ncapturing and disentangling visual and semantic properties of surgical tools\nand anatomy to improve downstream task performance. In this work, we conduct a\nmulti-centric performance benchmark of object-centric approaches, focusing on\nCritical View of Safety assessment in laparoscopic cholecystectomy, then\npropose an improved approach for unseen domain generalization.\n  Methods: We evaluate four object-centric approaches for domain\ngeneralization, establishing baseline performance. Next, leveraging the\ndisentangled nature of object-centric representations, we dissect one of these\nmethods through a series of ablations (e.g. ignoring either visual or semantic\nfeatures for downstream classification). Finally, based on the results of these\nablations, we develop an optimized method specifically tailored for domain\ngeneralization, LG-DG, that includes a novel disentanglement loss function.\n  Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over\nthe best baseline approach. More broadly, we show that object-centric\napproaches are highly effective for domain generalization thanks to their\nmodular approach to representation learning.\n  Conclusion: We investigate the use of object-centric methods for unseen\ndomain generalization, identify method-agnostic factors critical for\nperformance, and present an optimized approach that substantially outperforms\nexisting methods.\n","authors":["Siddhant Satyanaik","Aditya Murali","Deepak Alapatt","Xin Wang","Pietro Mascagni","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2403.06953v1.pdf","comment":"7 pages, 3 figures, Accepted to IPCAI 2024"},{"id":"http://arxiv.org/abs/2403.06952v1","updated":"2024-03-11T17:35:33Z","published":"2024-03-11T17:35:33Z","title":"SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with\n  Auto-Generated Data","summary":"  Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.\n","authors":["Jialu Li","Jaemin Cho","Yi-Lin Sung","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2403.06952v1.pdf","comment":"First two authors contributed equally; Project website:\n  https://selma-t2i.github.io/"},{"id":"http://arxiv.org/abs/2403.06951v1","updated":"2024-03-11T17:35:23Z","published":"2024-03-11T17:35:23Z","title":"DEADiff: An Efficient Stylization Diffusion Model with Disentangled\n  Representations","summary":"  The diffusion-based text-to-image model harbors immense potential in\ntransferring reference style. However, current encoder-based approaches\nsignificantly impair the text controllability of text-to-image models while\ntransferring styles. In this paper, we introduce \\textit{DEADiff} to address\nthis issue using the following two strategies: 1) a mechanism to decouple the\nstyle and semantics of reference images. The decoupled feature representations\nare first extracted by Q-Formers which are instructed by different text\ndescriptions. Then they are injected into mutually exclusive subsets of\ncross-attention layers for better disentanglement. 2) A non-reconstructive\nlearning method. The Q-Formers are trained using paired images rather than the\nidentical target, in which the reference image and the ground-truth image are\nwith the same style or semantics. We show that DEADiff attains the best visual\nstylization results and optimal balance between the text controllability\ninherent in the text-to-image model and style similarity to the reference\nimage, as demonstrated both quantitatively and qualitatively. Our project page\nis~\\href{https://tianhao-qi.github.io/DEADiff/}{https://tianhao-qi.github.io/DEADiff/}.\n","authors":["Tianhao Qi","Shancheng Fang","Yanze Wu","Hongtao Xie","Jiawei Liu","Lang Chen","Qian He","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06951v1.pdf","comment":"Accepted by CVPR 2024!"},{"id":"http://arxiv.org/abs/2403.06950v1","updated":"2024-03-11T17:34:51Z","published":"2024-03-11T17:34:51Z","title":"Applicability of oculomics for individual risk prediction: Repeatability\n  and robustness of retinal Fractal Dimension using DART and AutoMorph","summary":"  Purpose: To investigate whether Fractal Dimension (FD)-based oculomics could\nbe used for individual risk prediction by evaluating repeatability and\nrobustness. Methods: We used two datasets: Caledonia, healthy adults imaged\nmultiple times in quick succession for research (26 subjects, 39 eyes, 377\ncolour fundus images), and GRAPE, glaucoma patients with baseline and follow-up\nvisits (106 subjects, 196 eyes, 392 images). Mean follow-up time was 18.3\nmonths in GRAPE, thus it provides a pessimistic lower-bound as vasculature\ncould change. FD was computed with DART and AutoMorph. Image quality was\nassessed with QuickQual, but no images were initially excluded. Pearson,\nSpearman, and Intraclass Correlation (ICC) were used for population-level\nrepeatability. For individual-level repeatability, we introduce measurement\nnoise parameter {\\lambda} which is within-eye Standard Deviation (SD) of FD\nmeasurements in units of between-eyes SD. Results: In Caledonia, ICC was 0.8153\nfor DART and 0.5779 for AutoMorph, Pearson/Spearman correlation (first and last\nimage) 0.7857/0.7824 for DART, and 0.3933/0.6253 for AutoMorph. In GRAPE,\nPearson/Spearman correlation (first and next visit) was 0.7479/0.7474 for DART,\nand 0.7109/0.7208 for AutoMorph (all p<0.0001). Median {\\lambda} in Caledonia\nwithout exclusions was 3.55\\% for DART and 12.65\\% for AutoMorph, and improved\nto up to 1.67\\% and 6.64\\% with quality-based exclusions, respectively. Quality\nexclusions primarily mitigated large outliers. Worst quality in an eye\ncorrelated strongly with {\\lambda} (Pearson 0.5350-0.7550, depending on dataset\nand method, all p<0.0001). Conclusions: Repeatability was sufficient for\nindividual-level predictions in heterogeneous populations. DART performed\nbetter on all metrics and might be able to detect small, longitudinal changes,\nhighlighting the potential of robust methods.\n","authors":["Justin Engelmann","Diana Moukaddem","Lucas Gago","Niall Strang","Miguel O. Bernabeu"],"pdf_url":"https://arxiv.org/pdf/2403.06950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06947v1","updated":"2024-03-11T17:33:25Z","published":"2024-03-11T17:33:25Z","title":"Advancing Generalizable Remote Physiological Measurement through the\n  Integration of Explicit and Implicit Prior Knowledge","summary":"  Remote photoplethysmography (rPPG) is a promising technology that captures\nphysiological signals from face videos, with potential applications in medical\nhealth, emotional computing, and biosecurity recognition. The demand for rPPG\ntasks has expanded from demonstrating good performance on intra-dataset testing\nto cross-dataset testing (i.e., domain generalization). However, most existing\nmethods have overlooked the prior knowledge of rPPG, resulting in poor\ngeneralization ability. In this paper, we propose a novel framework that\nsimultaneously utilizes explicit and implicit prior knowledge in the rPPG task.\nSpecifically, we systematically analyze the causes of noise sources (e.g.,\ndifferent camera, lighting, skin types, and movement) across different domains\nand incorporate these prior knowledge into the network. Additionally, we\nleverage a two-branch network to disentangle the physiological feature\ndistribution from noises through implicit label correlation. Our extensive\nexperiments demonstrate that the proposed method not only outperforms\nstate-of-the-art methods on RGB cross-dataset evaluation but also generalizes\nwell from RGB datasets to NIR datasets. The code is available at\nhttps://github.com/keke-nice/Greip.\n","authors":["Yuting Zhang","Hao Lu","Xin Liu","Yingcong Chen","Kaishun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.06947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06946v1","updated":"2024-03-11T17:33:12Z","published":"2024-03-11T17:33:12Z","title":"Split to Merge: Unifying Separated Modalities for Unsupervised Domain\n  Adaptation","summary":"  Large vision-language models (VLMs) like CLIP have demonstrated good\nzero-shot learning performance in the unsupervised domain adaptation task. Yet,\nmost transfer approaches for VLMs focus on either the language or visual\nbranches, overlooking the nuanced interplay between both modalities. In this\nwork, we introduce a Unified Modality Separation (UniMoS) framework for\nunsupervised domain adaptation. Leveraging insights from modality gap studies,\nwe craft a nimble modality separation network that distinctly disentangles\nCLIP's features into language-associated and vision-associated components. Our\nproposed Modality-Ensemble Training (MET) method fosters the exchange of\nmodality-agnostic information while maintaining modality-specific nuances. We\nalign features across domains using a modality discriminator. Comprehensive\nevaluations on three benchmarks reveal our approach sets a new state-of-the-art\nwith minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS\n","authors":["Xinyao Li","Yuke Li","Zhekai Du","Fengling Li","Ke Lu","Jingjing Li"],"pdf_url":"https://arxiv.org/pdf/2403.06946v1.pdf","comment":"CVPR 2024 camera ready"},{"id":"http://arxiv.org/abs/2311.15100v2","updated":"2024-03-11T17:23:24Z","published":"2023-11-25T18:58:15Z","title":"Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation","summary":"  In optimal transport (OT), a Monge map is known as a mapping that transports\na source distribution to a target distribution in the most cost-efficient way.\nRecently, multiple neural estimators for Monge maps have been developed and\napplied in diverse unpaired domain translation tasks, e.g. in single-cell\nbiology and computer vision. However, the classic OT framework enforces mass\nconservation, which makes it prone to outliers and limits its applicability in\nreal-world scenarios. The latter can be particularly harmful in OT domain\ntranslation tasks, where the relative position of a sample within a\ndistribution is explicitly taken into account. While unbalanced OT tackles this\nchallenge in the discrete setting, its integration into neural Monge map\nestimators has received limited attention. We propose a theoretically grounded\nmethod to incorporate unbalancedness into any Monge map estimator. We improve\nexisting estimators to model cell trajectories over time and to predict\ncellular responses to perturbations. Moreover, our approach seamlessly\nintegrates with the OT flow matching (OT-FM) framework. While we show that\nOT-FM performs competitively in image translation, we further improve\nperformance by incorporating unbalancedness (UOT-FM), which better preserves\nrelevant features. We hence establish UOT-FM as a principled method for\nunpaired image translation.\n","authors":["Luca Eyring","Dominik Klein","Théo Uscidda","Giovanni Palla","Niki Kilbertus","Zeynep Akata","Fabian Theis"],"pdf_url":"https://arxiv.org/pdf/2311.15100v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2304.10985v2","updated":"2024-03-11T17:14:40Z","published":"2023-04-21T14:35:47Z","title":"RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained\n  Scenarios","summary":"  Learning-based systems have been demonstrated to be vulnerable to backdoor\nattacks, wherein malicious users manipulate model performance by injecting\nbackdoors into the target model and activating them with specific triggers.\nPrevious backdoor attack methods primarily focused on two key metrics: attack\nsuccess rate and stealthiness. However, these methods often necessitate\nsignificant privileges over the target model, such as control over the training\nprocess, making them challenging to implement in real-world scenarios.\nMoreover, the robustness of existing backdoor attacks is not guaranteed, as\nthey prove sensitive to defenses such as image augmentations and model\ndistillation. In this paper, we address these two limitations and introduce\nRSBA (Robust Statistical Backdoor Attack under Privilege-constrained\nScenarios). The key insight of RSBA is that statistical features can naturally\ndivide images into different groups, offering a potential implementation of\ntriggers. This type of trigger is more robust than manually designed ones, as\nit is widely distributed in normal images. By leveraging these statistical\ntriggers, RSBA enables attackers to conduct black-box attacks by solely\npoisoning the labels or the images. We empirically and theoretically\ndemonstrate the robustness of RSBA against image augmentations and model\ndistillation. Experimental results show that RSBA achieves a 99.83\\% attack\nsuccess rate in black-box scenarios. Remarkably, it maintains a high success\nrate even after model distillation, where attackers lack access to the training\ndataset of the student model (1.39\\% success rate for baseline methods on\naverage).\n","authors":["Xiaolei Liu","Ming Yi","Kangyi Ding","Bangzhou Xin","Yixiao Xu","Li Yan","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2304.10985v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.06912v1","updated":"2024-03-11T17:02:11Z","published":"2024-03-11T17:02:11Z","title":"DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with\n  Global-Local Depth Normalization","summary":"  Radiance fields have demonstrated impressive performance in synthesizing\nnovel views from sparse input views, yet prevailing methods suffer from high\ntraining costs and slow inference speed. This paper introduces DNGaussian, a\ndepth-regularized framework based on 3D Gaussian radiance fields, offering\nreal-time and high-quality few-shot novel view synthesis at low costs. Our\nmotivation stems from the highly efficient representation and surprising\nquality of the recent 3D Gaussian Splatting, despite it will encounter a\ngeometry degradation when input views decrease. In the Gaussian radiance\nfields, we find this degradation in scene geometry primarily lined to the\npositioning of Gaussian primitives and can be mitigated by depth constraint.\nConsequently, we propose a Hard and Soft Depth Regularization to restore\naccurate scene geometry under coarse monocular depth supervision while\nmaintaining a fine-grained color appearance. To further refine detailed\ngeometry reshaping, we introduce Global-Local Depth Normalization, enhancing\nthe focus on small local depth changes. Extensive experiments on LLFF, DTU, and\nBlender datasets demonstrate that DNGaussian outperforms state-of-the-art\nmethods, achieving comparable or better results with significantly reduced\nmemory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$\nfaster rendering speed.\n","authors":["Jiahe Li","Jiawei Zhang","Xiao Bai","Jin Zheng","Xin Ning","Jun Zhou","Lin Gu"],"pdf_url":"https://arxiv.org/pdf/2403.06912v1.pdf","comment":"Accepted at CVPR 2024. Project page:\n  https://fictionarry.github.io/DNGaussian/"},{"id":"http://arxiv.org/abs/2403.06908v1","updated":"2024-03-11T17:00:27Z","published":"2024-03-11T17:00:27Z","title":"FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization","summary":"  3D Gaussian splatting has achieved very impressive performance in real-time\nnovel view synthesis. However, it often suffers from over-reconstruction during\nGaussian densification where high-variance image regions are covered by a few\nlarge Gaussians only, leading to blur and artifacts in the rendered images. We\ndesign a progressive frequency regularization (FreGS) technique to tackle the\nover-reconstruction issue within the frequency space. Specifically, FreGS\nperforms coarse-to-fine Gaussian densification by exploiting low-to-high\nfrequency components that can be easily extracted with low-pass and high-pass\nfilters in the Fourier space. By minimizing the discrepancy between the\nfrequency spectrum of the rendered image and the corresponding ground truth, it\nachieves high-quality Gaussian densification and alleviates the\nover-reconstruction of Gaussian splatting effectively. Experiments over\nmultiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and\nDeep Blending) show that FreGS achieves superior novel view synthesis and\noutperforms the state-of-the-art consistently.\n","authors":["Jiahui Zhang","Fangneng Zhan","Muyu Xu","Shijian Lu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2403.06908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06904v1","updated":"2024-03-11T16:56:37Z","published":"2024-03-11T16:56:37Z","title":"FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in\n  Human-Centric Tasks","summary":"  We propose FocusCLIP, integrating subject-level guidance--a specialized\nmechanism for target-specific supervision--into the CLIP framework for improved\nzero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP\non both the vision and text sides. On the vision side, we incorporate ROI\nheatmaps emulating human visual attention mechanisms to emphasize\nsubject-relevant image regions. On the text side, we introduce human pose\ndescriptions to provide rich contextual information. For human-centric tasks,\nFocusCLIP is trained with images from the MPII Human Pose dataset. The proposed\napproach surpassed CLIP by an average of 8.61% across five previously unseen\ndatasets covering three human-centric tasks. FocusCLIP achieved an average\naccuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement\nin activity recognition, a 14.78% improvement in age classification, and a\n7.06% improvement in emotion recognition. Moreover, using our proposed\nsingle-shot LLM prompting strategy, we release a high-quality MPII Pose\nDescriptions dataset to encourage further research in multimodal learning for\nhuman-centric tasks. Furthermore, we also demonstrate the effectiveness of our\nsubject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%\nimprovement over CLIP in zero-shot bird classification using the CUB dataset.\nOur findings emphasize the potential of integrating subject-level guidance with\ngeneral pretraining methods for enhanced downstream performance.\n","authors":["Muhammad Saif Ullah Khan","Muhammad Ferjad Naeem","Federico Tombari","Luc Van Gool","Didier Stricker","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2403.06904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06902v1","updated":"2024-03-11T16:55:19Z","published":"2024-03-11T16:55:19Z","title":"Deep adaptative spectral zoom for improved remote heart rate estimation","summary":"  Recent advances in remote heart rate measurement, motivated by data-driven\napproaches, have notably enhanced accuracy. However, these improvements\nprimarily focus on recovering the rPPG signal, overlooking the implicit\nchallenges of estimating the heart rate (HR) from the derived signal. While\nmany methods employ the Fast Fourier Transform (FFT) for HR estimation, the\nperformance of the FFT is inherently affected by a limited frequency\nresolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of\nFFT, can refine the spectrum to the narrow-band range of interest for heart\nrate, providing improved frequential resolution and, consequently, more\naccurate estimation. This paper presents the advantages of employing the CZT\nfor remote HR estimation and introduces a novel data-driven adaptive CZT\nestimator. The objective of our proposed model is to tailor the CZT to match\nthe characteristics of each specific dataset sensor, facilitating a more\noptimal and accurate estimation of HR from the rPPG signal without compromising\ngeneralization across diverse datasets. This is achieved through a Sparse\nMatrix Optimization (SMO). We validate the effectiveness of our model through\nexhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE,\nand UBFC-rPPG employing both intra- and cross-database performance metrics. The\nresults reveal outstanding heart rate estimation capabilities, establishing the\nproposed approach as a robust and versatile estimator for any rPPG method.\n","authors":["Joaquim Comas","Adria Ruiz","Federico Sukno"],"pdf_url":"https://arxiv.org/pdf/2403.06902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06895v1","updated":"2024-03-11T16:49:59Z","published":"2024-03-11T16:49:59Z","title":"GRITv2: Efficient and Light-weight Social Relation Recognition","summary":"  Our research focuses on the analysis and improvement of the Graph-based\nRelation Inference Transformer (GRIT), which serves as an important benchmark\nin the field. We conduct a comprehensive ablation study using the PISC-fine\ndataset, to find and explore improvement in efficiency and performance of\nGRITv2. Our research has provided a new state-of-the-art relation recognition\nmodel on the PISC relation dataset. We introduce several features in the GRIT\nmodel and analyse our new benchmarks in two versions: GRITv2-L (large) and\nGRITv2-S (small). Our proposed GRITv2-L surpasses existing methods on relation\nrecognition and the GRITv2-S is within 2% performance gap of GRITv2-L, which\nhas only 0.0625x the model size and parameters of GRITv2-L. Furthermore, we\nalso address the need for model compression, an area crucial for deploying\nefficient models on resource-constrained platforms. By applying quantization\ntechniques, we efficiently reduced the GRITv2-S size to 22MB and deployed it on\nthe flagship OnePlus 12 mobile which still surpasses the PISC-fine benchmarks\nin performance, highlighting the practical viability and improved efficiency of\nour model on mobile devices.\n","authors":["N K Sagar Reddy","Neeraj Kasera","Avinash Thakur"],"pdf_url":"https://arxiv.org/pdf/2403.06895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06892v1","updated":"2024-03-11T16:48:25Z","published":"2024-03-11T16:48:25Z","title":"Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head","summary":"  End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}\n","authors":["Tiancheng Zhao","Peng Liu","Xuan He","Lu Zhang","Kyusong Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06892v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.06884v1","updated":"2024-03-11T16:42:29Z","published":"2024-03-11T16:42:29Z","title":"A Holistic Framework Towards Vision-based Traffic Signal Control with\n  Microscopic Simulation","summary":"  Traffic signal control (TSC) is crucial for reducing traffic congestion that\nleads to smoother traffic flow, reduced idling time, and mitigated CO2\nemissions. In this study, we explore the computer vision approach for TSC that\nmodulates on-road traffic flows through visual observation. Unlike traditional\nfeature-based approaches, vision-based methods depend much less on heuristics\nand predefined features, bringing promising potentials for end-to-end learning\nand optimization of traffic signals. Thus, we introduce a holistic traffic\nsimulation framework called TrafficDojo towards vision-based TSC and its\nbenchmarking by integrating the microscopic traffic flow provided in SUMO into\nthe driving simulator MetaDrive. This proposed framework offers a versatile\ntraffic environment for in-depth analysis and comprehensive evaluation of\ntraffic signal controllers across diverse traffic conditions and scenarios. We\nestablish and compare baseline algorithms including both traditional and\nReinforecment Learning (RL) approaches. This work sheds insights into the\ndesign and development of vision-based TSC approaches and open up new research\nopportunities. All the code and baselines will be made publicly available.\n","authors":["Pan He","Quanyi Li","Xiaoyong Yuan","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.06884v1.pdf","comment":"Under review for IEEE publications"},{"id":"http://arxiv.org/abs/2402.16338v4","updated":"2024-03-11T16:40:27Z","published":"2024-02-26T06:36:32Z","title":"BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning\n  of SAM","summary":"  The Segment Anything Model (SAM), a foundation model pretrained on millions\nof images and segmentation masks, has significantly advanced semantic\nsegmentation, a fundamental task in computer vision. Despite its strengths, SAM\nencounters two major challenges. Firstly, it struggles with segmenting specific\nobjects autonomously, as it relies on users to manually input prompts like\npoints or bounding boxes to identify targeted objects. Secondly, SAM faces\nchallenges in excelling at specific downstream tasks, like medical imaging, due\nto a disparity between the distribution of its pretraining data, which\npredominantly consists of general-domain images, and the data used in\ndownstream tasks. Current solutions to these problems, which involve finetuning\nSAM, often lead to overfitting, a notable issue in scenarios with very limited\ndata, like in medical imaging. To overcome these limitations, we introduce\nBLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach\nallows for automatic image segmentation without the need for manual prompts, by\noptimizing a learnable prompt embedding. Furthermore, it significantly reduces\nthe risk of overfitting by training the model's weight parameters and the\nprompt embedding on two separate subsets of the training dataset, each at a\ndifferent level of optimization. We apply BLO-SAM to diverse semantic\nsegmentation tasks in general and medical domains. The results demonstrate\nBLO-SAM's superior performance over various state-of-the-art image semantic\nsegmentation methods.\n","authors":["Li Zhang","Youwei Liang","Ruiyi Zhang","Amirhosein Javadi","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2402.16338v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06877v1","updated":"2024-03-11T16:31:25Z","published":"2024-03-11T16:31:25Z","title":"SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields\n  for Robotic Inspection","summary":"  We present a neural-field-based large-scale reconstruction system that fuses\nlidar and vision data to generate high-quality reconstructions that are\ngeometrically accurate and capture photo-realistic textures. This system adapts\nthe state-of-the-art neural radiance field (NeRF) representation to also\nincorporate lidar data which adds strong geometric constraints on the depth and\nsurface normals. We exploit the trajectory from a real-time lidar SLAM system\nto bootstrap a Structure-from-Motion (SfM) procedure to both significantly\nreduce the computation time and to provide metric scale which is crucial for\nlidar depth loss. We use submapping to scale the system to large-scale\nenvironments captured over long trajectories. We demonstrate the reconstruction\nsystem with data from a multi-camera, lidar sensor suite onboard a legged\nrobot, hand-held while scanning building scenes for 600 metres, and onboard an\naerial robot surveying a multi-storey mock disaster site-building. Website:\nhttps://ori-drs.github.io/projects/silvr/\n","authors":["Yifu Tao","Yash Bhalgat","Lanke Frank Tarimo Fu","Matias Mattamala","Nived Chebrolu","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.06877v1.pdf","comment":"Accepted at ICRA 2024; Website:\n  https://ori-drs.github.io/projects/silvr/"},{"id":"http://arxiv.org/abs/2403.06874v1","updated":"2024-03-11T16:26:35Z","published":"2024-03-11T16:26:35Z","title":"COOD: Combined out-of-distribution detection using multiple measures for\n  anomaly & novel class detection in large-scale hierarchical classification","summary":"  High-performing out-of-distribution (OOD) detection, both anomaly and novel\nclass, is an important prerequisite for the practical use of classification\nmodels. In this paper, we focus on the species recognition task in images\nconcerned with large databases, a large number of fine-grained hierarchical\nclasses, severe class imbalance, and varying image quality. We propose a\nframework for combining individual OOD measures into one combined OOD (COOD)\nmeasure using a supervised model. The individual measures are several existing\nstate-of-the-art measures and several novel OOD measures developed with novel\nclass detection and hierarchical class structure in mind. COOD was extensively\nevaluated on three large-scale (500k+ images) biodiversity datasets in the\ncontext of anomaly and novel class detection. We show that COOD outperforms\nindividual, including state-of-the-art, OOD measures by a large margin in terms\nof TPR@1% FPR in the majority of experiments, e.g., improving detecting\nImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.\nSHAP (feature contribution) analysis shows that different individual OOD\nmeasures are essential for various tasks, indicating that multiple OOD measures\nand combinations are needed to generalize. Additionally, we show that\nexplicitly considering ID images that are incorrectly classified for the\noriginal (species) recognition task is important for constructing\nhigh-performing OOD detection methods and for practical applicability. The\nframework can easily be extended or adapted to other tasks and media\nmodalities.\n","authors":["L. E. Hogeweg","R. Gangireddy","D. Brunink","V. J. Kalkman","L. Cornelissen","J. W. Kamminga"],"pdf_url":"https://arxiv.org/pdf/2403.06874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06869v1","updated":"2024-03-11T16:22:41Z","published":"2024-03-11T16:22:41Z","title":"Learning with Noisy Foundation Models","summary":"  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n","authors":["Hao Chen","Jindong Wang","Zihan Wang","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2403.06869v1.pdf","comment":"18 pages, 10 figures, 6 tables, preprint. arXiv admin note:\n  substantial text overlap with arXiv:2309.17002"},{"id":"http://arxiv.org/abs/2403.06866v1","updated":"2024-03-11T16:21:50Z","published":"2024-03-11T16:21:50Z","title":"QUASAR: QUality and Aesthetics Scoring with Advanced Representations","summary":"  This paper introduces a new data-driven, non-parametric method for image\nquality and aesthetics assessment, surpassing existing approaches and requiring\nno prompt engineering or fine-tuning. We eliminate the need for expressive\ntextual embeddings by proposing efficient image anchors in the data. Through\nextensive evaluations of 7 state-of-the-art self-supervised models, our method\ndemonstrates superior performance and robustness across various datasets and\nbenchmarks. Notably, it achieves high agreement with human assessments even\nwith limited data and shows high robustness to the nature of data and their\npre-processing pipeline. Our contributions offer a streamlined solution for\nassessment of images while providing insights into the perception of visual\ninformation.\n","authors":["Sergey Kastryulin","Denis Prokopenko","Artem Babenko","Dmitry V. Dylov"],"pdf_url":"https://arxiv.org/pdf/2403.06866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06862v1","updated":"2024-03-11T16:15:51Z","published":"2024-03-11T16:15:51Z","title":"Real-Time Simulated Avatar from Head-Mounted Sensors","summary":"  We present SimXR, a method for controlling a simulated avatar from\ninformation (headset pose and cameras) obtained from AR / VR headsets. Due to\nthe challenging viewpoint of head-mounted cameras, the human body is often\nclipped out of view, making traditional image-based egocentric pose estimation\nchallenging. On the other hand, headset poses provide valuable information\nabout overall body motion, but lack fine-grained details about the hands and\nfeet. To synergize headset poses with cameras, we control a humanoid to track\nheadset movement while analyzing input images to decide body movement. When\nbody parts are seen, the movements of hands and feet will be guided by the\nimages; when unseen, the laws of physics guide the controller to generate\nplausible motion. We design an end-to-end method that does not rely on any\nintermediate representations and learns to directly map from images and headset\nposes to humanoid control signals. To train our method, we also propose a\nlarge-scale synthetic dataset created using camera configurations compatible\nwith a commercially available VR headset (Quest 2) and show promising results\non real-world captures. To demonstrate the applicability of our framework, we\nalso test it on an AR headset with a forward-facing camera.\n","authors":["Zhengyi Luo","Jinkun Cao","Rawal Khirodkar","Alexander Winkler","Kris Kitani","Weipeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.06862v1.pdf","comment":"CVPR 2024. Website: https://www.zhengyiluo.com/SimXR/"},{"id":"http://arxiv.org/abs/2403.06860v1","updated":"2024-03-11T16:13:58Z","published":"2024-03-11T16:13:58Z","title":"A Geospatial Approach to Predicting Desert Locust Breeding Grounds in\n  Africa","summary":"  Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.\n","authors":["Ibrahim Salihu Yusuf","Mukhtar Opeyemi Yusuf","Kobby Panford-Quainoo","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2403.06860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06846v1","updated":"2024-03-11T16:03:43Z","published":"2024-03-11T16:03:43Z","title":"DiaLoc: An Iterative Approach to Embodied Dialog Localization","summary":"  Multimodal learning has advanced the performance for many vision-language\ntasks. However, most existing works in embodied dialog research focus on\nnavigation and leave the localization task understudied. The few existing\ndialog-based localization approaches assume the availability of entire dialog\nprior to localizaiton, which is impractical for deployed dialog-based\nlocalization. In this paper, we propose DiaLoc, a new dialog-based localization\nframework which aligns with a real human operator behavior. Specifically, we\nproduce an iterative refinement of location predictions which can visualize\ncurrent pose believes after each dialog turn. DiaLoc effectively utilizes the\nmultimodal data for multi-shot localization, where a fusion encoder fuses\nvision and dialog information iteratively. We achieve state-of-the-art results\non embodied dialog-based localization task, in single-shot (+7.08% in\nAcc5@valUnseen) and multi- shot settings (+10.85% in Acc5@valUnseen). DiaLoc\nnarrows the gap between simulation and real-world applications, opening doors\nfor future research on collaborative localization and navigation.\n","authors":["Chao Zhang","Mohan Li","Ignas Budvytis","Stephan Liwicki"],"pdf_url":"https://arxiv.org/pdf/2403.06846v1.pdf","comment":"12 pages, 10 figures, to appear in CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06845v1","updated":"2024-03-11T16:03:35Z","published":"2024-03-11T16:03:35Z","title":"DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video\n  Generation","summary":"  World models have demonstrated superiority in autonomous driving,\nparticularly in the generation of multi-view driving videos. However,\nsignificant challenges still exist in generating customized driving videos. In\nthis paper, we propose DriveDreamer-2, which builds upon the framework of\nDriveDreamer and incorporates a Large Language Model (LLM) to generate\nuser-defined driving videos. Specifically, an LLM interface is initially\nincorporated to convert a user's query into agent trajectories. Subsequently, a\nHDMap, adhering to traffic regulations, is generated based on the trajectories.\nUltimately, we propose the Unified Multi-View Model to enhance temporal and\nspatial coherence in the generated driving videos. DriveDreamer-2 is the first\nworld model to generate customized driving videos, it can generate uncommon\ndriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.\nBesides, experimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and tracking).\nFurthermore, video generation quality of DriveDreamer-2 surpasses other\nstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,\nrepresenting relative improvements of 30% and 50%.\n","authors":["Guosheng Zhao","Xiaofeng Wang","Zheng Zhu","Xinze Chen","Guan Huang","Xiaoyi Bao","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06845v1.pdf","comment":"Project Page: https://drivedreamer2.github.io"},{"id":"http://arxiv.org/abs/2403.06837v1","updated":"2024-03-11T15:59:35Z","published":"2024-03-11T15:59:35Z","title":"Stochastic Cortical Self-Reconstruction","summary":"  Magnetic resonance imaging (MRI) is critical for diagnosing neurodegenerative\ndiseases, yet accurately assessing mild cortical atrophy remains a challenge\ndue to its subtlety. Automated cortex reconstruction, paired with healthy\nreference ranges, aids in pinpointing pathological atrophy, yet their\ngeneralization is limited by biases from image acquisition and processing. We\nintroduce the concept of stochastic cortical self-reconstruction (SCSR) that\ncreates a subject-specific healthy reference by taking MRI-derived thicknesses\nas input and, therefore, implicitly accounting for potential confounders. SCSR\nrandomly corrupts parts of the cortex and self-reconstructs them from the\nremaining information. Trained exclusively on healthy individuals, repeated\nself-reconstruction generates a stochastic reference cortex for assessing\ndeviations from the norm. We present three implementations of this concept:\nXGBoost applied on parcels, and two autoencoders on vertex level -- one based\non a multilayer perceptron and the other using a spherical U-Net. These models\nwere trained on healthy subjects from the UK Biobank and subsequently evaluated\nacross four public Alzheimer's datasets. Finally, we deploy the model on\nclinical in-house data, where deviation maps' high spatial resolution aids in\ndiscriminating between four types of dementia.\n","authors":["Christian Wachinger","Dennis Hedderich","Fabian Bongratz"],"pdf_url":"https://arxiv.org/pdf/2403.06837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17002v2","updated":"2024-03-11T15:59:28Z","published":"2023-09-29T06:18:15Z","title":"Understanding and Mitigating the Label Noise in Pre-training on\n  Downstream Tasks","summary":"  Pre-training on large-scale datasets and then fine-tuning on downstream tasks\nhave become a standard practice in deep learning. However, pre-training data\noften contain label noise that may adversely affect the generalization of the\nmodel. This paper aims to understand the nature of noise in pre-training\ndatasets and to mitigate its impact on downstream tasks. More specifically,\nthrough extensive experiments of supervised pre-training models on synthetic\nnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\nin pre-training can benefit in-domain (ID) transfer performance, where the\ntraining and testing data share the same distribution, it always deteriorates\nout-of-domain (OOD) performance, where training and testing data distribution\nare different. We empirically verify that the reason behind is noise in\npre-training shapes the feature space differently. We then propose a\nlight-weight black-box tuning method (NMTune) to affine the feature space to\nmitigate the malignant effect of noise and improve generalization on both ID\nand OOD tasks, considering one may not be able to fully fine-tune or even\naccess the pre-trained models. We conduct practical experiments on popular\nvision and language models that are pre-trained on noisy data for evaluation of\nour approach. Our analysis and results show the importance of this interesting\nand novel research direction, which we term Noisy Model Learning.\n","authors":["Hao Chen","Jindong Wang","Ankit Shah","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2309.17002v2.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.06835v1","updated":"2024-03-11T15:56:17Z","published":"2024-03-11T15:56:17Z","title":"Medical Image Synthesis via Fine-Grained Image-Text Alignment and\n  Anatomy-Pathology Prompting","summary":"  Data scarcity and privacy concerns limit the availability of high-quality\nmedical images for public use, which can be mitigated through medical image\nsynthesis. However, current medical image synthesis methods often struggle to\naccurately capture the complexity of detailed anatomical structures and\npathological conditions. To address these challenges, we propose a novel\nmedical image synthesis model that leverages fine-grained image-text alignment\nand anatomy-pathology prompts to generate highly detailed and accurate\nsynthetic medical images. Our method integrates advanced natural language\nprocessing techniques with image generative modeling, enabling precise\nalignment between descriptive text prompts and the synthesized images'\nanatomical and pathological details. The proposed approach consists of two key\ncomponents: an anatomy-pathology prompting module and a fine-grained\nalignment-based synthesis module. The anatomy-pathology prompting module\nautomatically generates descriptive prompts for high-quality medical images. To\nfurther synthesize high-quality medical images from the generated prompts, the\nfine-grained alignment-based synthesis module pre-defines a visual codebook for\nthe radiology dataset and performs fine-grained alignment between the codebook\nand generated prompts to obtain key patches as visual clues, facilitating\naccurate image synthesis. We validate the superiority of our method through\nexperiments on public chest X-ray datasets and demonstrate that our synthetic\nimages preserve accurate semantic information, making them valuable for various\nmedical applications.\n","authors":["Wenting Chen","Pengyu Wang","Hui Ren","Lichao Sun","Quanzheng Li","Yixuan Yuan","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.06835v1.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2310.16717v3","updated":"2024-03-11T15:54:38Z","published":"2023-10-25T15:44:50Z","title":"Prompt-Driven Building Footprint Extraction in Aerial Images with\n  Offset-Building Model","summary":"  More accurate extraction of invisible building footprints from\nvery-high-resolution (VHR) aerial images relies on roof segmentation and\nroof-to-footprint offset extraction. Existing state-of-the-art methods based on\ninstance segmentation suffer from poor generalization when extended to\nlarge-scale data production and fail to achieve low-cost human interactive\nannotation. The latest prompt paradigms inspire us to design a promptable\nframework for roof and offset extraction, which transforms end-to-end\nalgorithms into promptable methods. Within this framework, we propose a novel\nOffset-Building Model (OBM). To rigorously evaluate the algorithm's\ncapabilities, we introduce a prompt-based evaluation method, where our model\nreduces offset errors by 16.6% and improves roof Intersection over Union (IoU)\nby 10.8% compared to other models. Leveraging the common patterns in predicting\noffsets, we propose Distance-NMS (DNMS) algorithms, enabling the model to\nfurther reduce offset vector loss by 6.5%. To further validate the\ngeneralization of models, we tested them using a new dataset with over 7,000\nmanually annotated instance samples. Our algorithms and dataset are available\nat https://anonymous.4open.science/r/OBM-B3EC.\n","authors":["Kai Li","Yupeng Deng","Yunlong Kong","Diyou Liu","Jingbo Chen","Yu Meng","Junxian Ma"],"pdf_url":"https://arxiv.org/pdf/2310.16717v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06831v1","updated":"2024-03-11T15:48:17Z","published":"2024-03-11T15:48:17Z","title":"HDRTransDC: High Dynamic Range Image Reconstruction with Transformer\n  Deformation Convolution","summary":"  High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image\nwith realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.\nCaused by large motion and severe under-/over-exposure among input LDR images,\nHDR imaging suffers from ghosting artifacts and fusion distortions. To address\nthese critical issues, we propose an HDR Transformer Deformation Convolution\n(HDRTransDC) network to generate high-quality HDR images, which consists of the\nTransformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic\nWeight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM\nextracts long-distance content similar to the reference feature in the entire\nnon-reference features, which can accurately remove misalignment and fill the\ncontent occluded by moving objects. For the purpose of eliminating fusion\ndistortions, we propose DWFB to spatially adaptively select useful information\nacross frames to effectively fuse multi-exposed features. Extensive experiments\nshow that our method quantitatively and qualitatively achieves state-of-the-art\nperformance.\n","authors":["Shuaikang Shang","Xuejing Kang","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2403.06831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06813v1","updated":"2024-03-11T15:33:32Z","published":"2024-03-11T15:33:32Z","title":"LeOCLR: Leveraging Original Images for Contrastive Learning of Visual\n  Representations","summary":"  Contrastive instance discrimination outperforms supervised learning in\ndownstream tasks like image classification and object detection. However, this\napproach heavily relies on data augmentation during representation learning,\nwhich may result in inferior results if not properly implemented. Random\ncropping followed by resizing is a common form of data augmentation used in\ncontrastive learning, but it can lead to degraded representation learning if\nthe two random crops contain distinct semantic content. To address this issue,\nthis paper introduces LeOCLR (Leveraging Original Images for Contrastive\nLearning of Visual Representations), a framework that employs a new instance\ndiscrimination approach and an adapted loss function that ensures the shared\nregion between positive pairs is semantically correct. The experimental results\nshow that our approach consistently improves representation learning across\ndifferent datasets compared to baseline models. For example, our approach\noutperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several\nother methods on transfer learning tasks.\n","authors":["Mohammad Alkhalefi","Georgios Leontidis","Mingjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2403.06813v1.pdf","comment":"16 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.06810v1","updated":"2024-03-11T15:31:25Z","published":"2024-03-11T15:31:25Z","title":"Deep Learning Approaches for Human Action Recognition in Video Data","summary":"  Human action recognition in videos is a critical task with significant\nimplications for numerous applications, including surveillance, sports\nanalytics, and healthcare. The challenge lies in creating models that are both\nprecise in their recognition capabilities and efficient enough for practical\nuse. This study conducts an in-depth analysis of various deep learning models\nto address this challenge. Utilizing a subset of the UCF101 Videos dataset, we\nfocus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks\n(RNNs), and Two-Stream ConvNets. The research reveals that while CNNs\neffectively capture spatial features and RNNs encode temporal sequences,\nTwo-Stream ConvNets exhibit superior performance by integrating spatial and\ntemporal dimensions. These insights are distilled from the evaluation metrics\nof accuracy, precision, recall, and F1-score. The results of this study\nunderscore the potential of composite models in achieving robust human action\nrecognition and suggest avenues for future research in optimizing these models\nfor real-world deployment.\n","authors":["Yufei Xie"],"pdf_url":"https://arxiv.org/pdf/2403.06810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06807v1","updated":"2024-03-11T15:26:34Z","published":"2024-03-11T15:26:34Z","title":"Multistep Consistency Models","summary":"  Diffusion models are relatively easy to train but require many steps to\ngenerate samples. Consistency models are far more difficult to train, but\ngenerate samples in a single step.\n  In this paper we propose Multistep Consistency Models: A unification between\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\ncan interpolate between a consistency model and a diffusion model: a trade-off\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\nmodel is a conventional consistency model whereas we show that a $\\infty$-step\nconsistency model is a diffusion model.\n  Multistep Consistency Models work really well in practice. By increasing the\nsample budget from a single step to 2-8 steps, we can train models more easily\nthat generate higher quality samples, while retaining much of the sampling\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\nFID on Imagenet128 in 8 steps with consistency distillation. We also show that\nour method scales to a text-to-image diffusion model, generating samples that\nare very close to the quality of the original model.\n","authors":["Jonathan Heek","Emiel Hoogeboom","Tim Salimans"],"pdf_url":"https://arxiv.org/pdf/2403.06807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06804v1","updated":"2024-03-11T15:23:11Z","published":"2024-03-11T15:23:11Z","title":"Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape\n  Matching via Unsupervised Functional Map Regularized Reconstruction","summary":"  We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for\nnon-rigid shape matching that eliminates the need for extensive training or\nground truth data. SNK operates on a single pair of shapes, and employs a\nreconstruction-based strategy using an encoder-decoder architecture, which\ndeforms the source shape to closely match the target shape. During the process,\nan unsupervised functional map is predicted and converted into a point-to-point\nmap, serving as a supervisory mechanism for the reconstruction. To aid in\ntraining, we have designed a new decoder architecture that generates smooth,\nrealistic deformations. SNK demonstrates competitive results on traditional\nbenchmarks, simplifying the shape-matching process without compromising\naccuracy. Our code can be found online: https://github.com/pvnieo/SNK\n","authors":["Souhaib Attaiki","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2403.06804v1.pdf","comment":"NeurIPS 2023, 10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.06803v1","updated":"2024-03-11T15:22:28Z","published":"2024-03-11T15:22:28Z","title":"Data-Independent Operator: A Training-Free Artifact Representation\n  Extractor for Generalizable Deepfake Detection","summary":"  Recently, the proliferation of increasingly realistic synthetic images\ngenerated by various generative adversarial networks has increased the risk of\nmisuse. Consequently, there is a pressing need to develop a generalizable\ndetector for accurately recognizing fake images. The conventional methods rely\non generating diverse training sources or large pretrained models. In this\nwork, we show that, on the contrary, the small and training-free filter is\nsufficient to capture more general artifact representations. Due to its unbias\ntowards both the training and test sources, we define it as Data-Independent\nOperator (DIO) to achieve appealing improvements on unseen sources. In our\nframework, handcrafted filters and the randomly-initialized convolutional layer\ncan be used as the training-free artifact representations extractor with\nexcellent results. With the data-independent operator of a popular classifier,\nsuch as Resnet50, one could already reach a new state-of-the-art without bells\nand whistles. We evaluate the effectiveness of the DIO on 33 generation models,\neven DALLE and Midjourney. Our detector achieves a remarkable improvement of\n$13.3\\%$, establishing a new state-of-the-art performance. The DIO and its\nextension can serve as strong baselines for future methods. The code is\navailable at\n\\url{https://github.com/chuangchuangtan/Data-Independent-Operator}.\n","authors":["Chuangchuang Tan","Ping Liu","RenShuai Tao","Huan Liu","Yao Zhao","Baoyuan Wu","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2403.06803v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.06801v1","updated":"2024-03-11T15:17:45Z","published":"2024-03-11T15:17:45Z","title":"CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging","summary":"  Medical imaging plays a crucial role in diagnosis, with radiology reports\nserving as vital documentation. Automating report generation has emerged as a\ncritical need to alleviate the workload of radiologists. While machine learning\nhas facilitated report generation for 2D medical imaging, extending this to 3D\nhas been unexplored due to computational complexity and data scarcity. We\nintroduce the first method to generate radiology reports for 3D medical\nimaging, specifically targeting chest CT volumes. Given the absence of\ncomparable methods, we establish a baseline using an advanced 3D vision encoder\nin medical imaging to demonstrate our method's effectiveness, which leverages a\nnovel auto-regressive causal transformer. Furthermore, recognizing the benefits\nof leveraging information from previous visits, we augment CT2Rep with a\ncross-attention-based multi-modal fusion module and hierarchical memory,\nenabling the incorporation of longitudinal multimodal data. Access our code at:\nhttps://github.com/ibrahimethemhamamci/CT2Rep\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2403.06801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06800v1","updated":"2024-03-11T15:17:25Z","published":"2024-03-11T15:17:25Z","title":"MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in\n  Computational Pathology","summary":"  Multiple Instance Learning (MIL) has emerged as a dominant paradigm to\nextract discriminative feature representations within Whole Slide Images (WSIs)\nin computational pathology. Despite driving notable progress, existing MIL\napproaches suffer from limitations in facilitating comprehensive and efficient\ninteractions among instances, as well as challenges related to time-consuming\ncomputations and overfitting. In this paper, we incorporate the Selective Scan\nSpace State Sequential Model (Mamba) in Multiple Instance Learning (MIL) for\nlong sequence modeling with linear complexity, termed as MambaMIL. By\ninheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability\nto comprehensively understand and perceive long sequences of instances.\nFurthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the\norder and distribution of instances, which exploits the inherent valuable\ninformation embedded within the long sequences. With the SR-Mamba as the core\ncomponent, MambaMIL can effectively capture more discriminative features and\nmitigate the challenges associated with overfitting and high computational\noverhead. Extensive experiments on two public challenging tasks across nine\ndiverse datasets demonstrate that our proposed framework performs favorably\nagainst state-of-the-art MIL methods. The code is released at\nhttps://github.com/isyangshu/MambaMIL.\n","authors":["Shu Yang","Yihui Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.06800v1.pdf","comment":"Submitted to MICCAI-2024"},{"id":"http://arxiv.org/abs/2403.06798v1","updated":"2024-03-11T15:16:20Z","published":"2024-03-11T15:16:20Z","title":"Dynamic Perturbation-Adaptive Adversarial Training on Medical Image\n  Classification","summary":"  Remarkable successes were made in Medical Image Classification (MIC)\nrecently, mainly due to wide applications of convolutional neural networks\n(CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity\nwith raw data, raising serious concerns on network robustness. Although\nadversarial training (AT), in responding to malevolent AEs, was recognized as\nan effective approach to improve robustness, it was challenging to overcome\ngeneralization decline of networks caused by the AT. In this paper, in order to\nreserve high generalization while improving robustness, we proposed a dynamic\nperturbation-adaptive adversarial training (DPAAT) method, which placed AT in a\ndynamic learning environment to generate adaptive data-level perturbations and\nprovided a dynamically updated criterion by loss information collections to\nhandle the disadvantage of fixed perturbation sizes in conventional AT methods\nand the dependence on external transference. Comprehensive testing on\ndermatology HAM10000 dataset showed that the DPAAT not only achieved better\nrobustness improvement and generalization preservation but also significantly\nenhanced mean average precision and interpretability on various CNNs,\nindicating its great potential as a generic adversarial training method on the\nMIC.\n","authors":["Shuai Li","Xiaoguang Ma","Shancheng Jiang","Lu Meng"],"pdf_url":"https://arxiv.org/pdf/2403.06798v1.pdf","comment":"9 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.06797v1","updated":"2024-03-11T15:15:50Z","published":"2024-03-11T15:15:50Z","title":"Leveraging Internal Representations of Model for Magnetic Image\n  Classification","summary":"  Data generated by edge devices has the potential to train intelligent\nautonomous systems across various domains. Despite the emergence of diverse\nmachine learning approaches addressing privacy concerns and utilizing\ndistributed data, security issues persist due to the sensitive storage of data\nshards in disparate locations. This paper introduces a potentially\ngroundbreaking paradigm for machine learning model training, specifically\ndesigned for scenarios with only a single magnetic image and its corresponding\nlabel image available. We harness the capabilities of Deep Learning to generate\nconcise yet informative samples, aiming to overcome data scarcity. Through the\nutilization of deep learning's internal representations, our objective is to\nefficiently address data scarcity issues and produce meaningful results. This\nmethodology presents a promising avenue for training machine learning models\nwith minimal data.\n","authors":["Adarsh N L","Arun P V","Alok Porwal","Malcolm Aranha"],"pdf_url":"https://arxiv.org/pdf/2403.06797v1.pdf","comment":"5 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2310.16047v2","updated":"2024-03-11T15:14:51Z","published":"2023-10-24T17:58:54Z","title":"From Posterior Sampling to Meaningful Diversity in Image Restoration","summary":"  Image restoration problems are typically ill-posed in the sense that each\ndegraded image can be restored in infinitely many valid ways. To accommodate\nthis, many works generate a diverse set of outputs by attempting to randomly\nsample from the posterior distribution of natural images given the degraded\ninput. Here we argue that this strategy is commonly of limited practical value\nbecause of the heavy tail of the posterior distribution. Consider for example\ninpainting a missing region of the sky in an image. Since there is a high\nprobability that the missing region contains no object but clouds, any set of\nsamples from the posterior would be entirely dominated by (practically\nidentical) completions of sky. However, arguably, presenting users with only\none clear sky completion, along with several alternative solutions such as\nairships, birds, and balloons, would better outline the set of possibilities.\nIn this paper, we initiate the study of meaningfully diverse image restoration.\nWe explore several post-processing approaches that can be combined with any\ndiverse image restoration method to yield semantically meaningful diversity.\nMoreover, we propose a practical approach for allowing diffusion based image\nrestoration methods to generate meaningfully diverse outputs, while incurring\nonly negligent computational overhead. We conduct extensive user studies to\nanalyze the proposed techniques, and find the strategy of reducing similarity\nbetween outputs to be significantly favorable over posterior sampling. Code and\nexamples are available at https://noa-cohen.github.io/MeaningfulDiversityInIR.\n","authors":["Noa Cohen","Hila Manor","Yuval Bahat","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2310.16047v2.pdf","comment":"Accepted for ICLR 2024. Code and examples are available at\n  https://noa-cohen.github.io/MeaningfulDiversityInIR"},{"id":"http://arxiv.org/abs/2403.06793v1","updated":"2024-03-11T15:11:57Z","published":"2024-03-11T15:11:57Z","title":"Boosting Image Restoration via Priors from Pre-trained Models","summary":"  Pre-trained models with large-scale training data, such as CLIP and Stable\nDiffusion, have demonstrated remarkable performance in various high-level\ncomputer vision tasks such as image understanding and generation from language\ndescriptions. Yet, their potential for low-level tasks such as image\nrestoration remains relatively unexplored. In this paper, we explore such\nmodels to enhance image restoration. As off-the-shelf features (OSF) from\npre-trained models do not directly serve image restoration, we propose to learn\nan additional lightweight module called Pre-Train-Guided Refinement Module\n(PTG-RM) to refine restoration results of a target restoration network with\nOSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying\nEnhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention\n(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,\nwhile PTG-CSA enhances spatial-channel attention for restoration-related\nlearning. Extensive experiments demonstrate that PTG-RM, with its compact size\n($<$1M parameters), effectively enhances restoration performance of various\nmodels across different tasks, including low-light enhancement, deraining,\ndeblurring, and denoising.\n","authors":["Xiaogang Xu","Shu Kong","Tao Hu","Zhe Liu","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2403.06793v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2309.17164v2","updated":"2024-03-11T15:11:11Z","published":"2023-09-29T11:58:26Z","title":"Retail-786k: a Large-Scale Dataset for Visual Entity Matching","summary":"  Entity Matching (EM) defines the task of learning to group objects by\ntransferring semantic concepts from example groups (=entities) to unseen data.\nDespite the general availability of image data in the context of many\nEM-problems, most currently available EM-algorithms solely rely on (textual)\nmeta data. In this paper, we introduce the first publicly available large-scale\ndataset for \"visual entity matching\", based on a production level use case in\nthe retail domain. Using scanned advertisement leaflets, collected over several\nyears from different European retailers, we provide a total of ~786k manually\nannotated, high resolution product images containing ~18k different individual\nretail products which are grouped into ~3k entities. The annotation of these\nproduct entities is based on a price comparison task, where each entity forms\nan equivalence class of comparable products. Following on a first baseline\nevaluation, we show that the proposed \"visual entity matching\" constitutes a\nnovel learning problem which can not sufficiently be solved using standard\nimage based classification and retrieval algorithms. Instead, novel approaches\nwhich allow to transfer example based visual equivalent classes to new data are\nneeded to address the proposed problem. The aim of this paper is to provide a\nbenchmark for such algorithms.\n  Information about the dataset, evaluation code and download instructions are\nprovided under https://www.retail-786k.org/.\n","authors":["Bianca Lamm","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2309.17164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03077v2","updated":"2024-03-11T15:02:20Z","published":"2024-03-05T16:01:55Z","title":"MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual\n  Grounding","summary":"  3D visual grounding involves matching natural language descriptions with\ntheir corresponding objects in 3D spaces. Existing methods often face\nchallenges with accuracy in object recognition and struggle in interpreting\ncomplex linguistic queries, particularly with descriptions that involve\nmultiple anchors or are view-dependent. In response, we present the MiKASA\n(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model\nintegrates a self-attention-based scene-aware object encoder and an original\nmulti-key-anchor technique, enhancing object recognition accuracy and the\nunderstanding of spatial relationships. Furthermore, MiKASA improves the\nexplainability of decision-making, facilitating error diagnosis. Our model\nachieves the highest overall accuracy in the Referit3D challenge for both the\nSr3D and Nr3D datasets, particularly excelling by a large margin in categories\nthat require viewpoint-dependent descriptions.\n  The source code and additional resources for this project are available on\nGitHub: https://github.com/birdy666/MiKASA-3DVG\n","authors":["Chun-Peng Chang","Shaoxiang Wang","Alain Pagani","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2403.03077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06786v1","updated":"2024-03-11T15:00:56Z","published":"2024-03-11T15:00:56Z","title":"Genetic Learning for Designing Sim-to-Real Data Augmentations","summary":"  Data augmentations are useful in closing the sim-to-real domain gap when\ntraining on synthetic data. This is because they widen the training data\ndistribution, thus encouraging the model to generalize better to other domains.\nMany image augmentation techniques exist, parametrized by different settings,\nsuch as strength and probability. This leads to a large space of different\npossible augmentation policies. Some policies work better than others for\novercoming the sim-to-real gap for specific datasets, and it is unclear why.\nThis paper presents two different interpretable metrics that can be combined to\npredict how well a certain augmentation policy will work for a specific\nsim-to-real setting, focusing on object detection. We validate our metrics by\ntraining many models with different augmentation policies and showing a strong\ncorrelation with performance on real data. Additionally, we introduce\nGeneticAugment, a genetic programming method that can leverage these metrics to\nautomatically design an augmentation policy for a specific dataset without\nneeding to train a model.\n","authors":["Bram Vanherle","Nick Michiels","Frank Van Reeth"],"pdf_url":"https://arxiv.org/pdf/2403.06786v1.pdf","comment":"21 pages; accepted at DMLR Workshop @ ICRL 2024"},{"id":"http://arxiv.org/abs/2403.06775v1","updated":"2024-03-11T14:43:40Z","published":"2024-03-11T14:43:40Z","title":"FaceChain-SuDe: Building Derived Class to Inherit Category Attributes\n  for One-shot Subject-Driven Generation","summary":"  Subject-driven generation has garnered significant interest recently due to\nits ability to personalize text-to-image generation. Typical works focus on\nlearning the new subject's private attributes. However, an important fact has\nnot been taken seriously that a subject is not an isolated new concept but\nshould be a specialization of a certain category in the pre-trained model. This\nresults in the subject failing to comprehensively inherit the attributes in its\ncategory, causing poor attribute-related generations. In this paper, motivated\nby object-oriented programming, we model the subject as a derived class whose\nbase class is its semantic category. This modeling enables the subject to\ninherit public attributes from its category while learning its private\nattributes from the user-provided example. Specifically, we propose a\nplug-and-play method, Subject-Derived regularization (SuDe). It constructs the\nbase-derived class modeling by constraining the subject-driven generated images\nto semantically belong to the subject's category. Extensive experiments under\nthree baselines and two backbones on various subjects show that our SuDe\nenables imaginative attribute-related generations while maintaining subject\nfidelity. Codes will be open sourced soon at FaceChain\n(https://github.com/modelscope/facechain).\n","authors":["Pengchong Qiao","Lei Shang","Chang Liu","Baigui Sun","Xiangyang Ji","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2403.06775v1.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2305.16037v4","updated":"2024-03-11T14:37:26Z","published":"2023-05-25T13:16:39Z","title":"GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes","summary":"  GenerateCT, the first approach to generating 3D medical imaging conditioned\non free-form medical text prompts, incorporates a text encoder and three key\ncomponents: a novel causal vision transformer for encoding 3D CT volumes, a\ntext-image transformer for aligning CT and text tokens, and a text-conditional\nsuper-resolution diffusion model. Given the absence of directly comparable\nmethods in 3D medical imaging, we established baselines with cutting-edge\nmethods to demonstrate our method's effectiveness. GenerateCT significantly\noutperforms these methods across all key metrics. Importantly, we explored\nGenerateCT's clinical applications by evaluating its utility in a\nmulti-abnormality classification task. First, we established a baseline by\ntraining a multi-abnormality classifier on our real dataset. To further assess\nthe model's generalization to external datasets and its performance with unseen\nprompts in a zero-shot scenario, we employed an external dataset to train the\nclassifier, setting an additional benchmark. We conducted two experiments in\nwhich we doubled the training datasets by synthesizing an equal number of\nvolumes for each set using GenerateCT. The first experiment demonstrated an 11%\nimprovement in the AP score when training the classifier jointly on real and\ngenerated volumes. The second experiment showed a 7% improvement when training\non both real and generated volumes based on unseen prompts. Moreover,\nGenerateCT enables the scaling of synthetic training datasets to arbitrary\nsizes. As an example, we generated 100,000 3D CT volumes, fivefold the number\nin our real dataset, and trained the classifier exclusively on these synthetic\nvolumes. Impressively, this classifier surpassed the performance of the one\ntrained on all available real data by a margin of 8%. Lastly, domain experts\nevaluated the generated volumes, confirming a high degree of alignment with the\ntext prompt.\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Anjany Sekuboyina","Enis Simsar","Alperen Tezcan","Ayse Gulnihan Simsek","Sevval Nil Esirgun","Furkan Almas","Irem Dogan","Muhammed Furkan Dasdelen","Chinmay Prabhakar","Hadrien Reynaud","Sarthak Pati","Christian Bluethgen","Mehmet Kemal Ozdemir","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2305.16037v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02585v2","updated":"2024-03-11T14:36:22Z","published":"2023-06-05T04:24:11Z","title":"MotionTrack: Learning Motion Predictor for Multiple Object Tracking","summary":"  Significant progress has been achieved in multi-object tracking (MOT) through\nthe evolution of detection and re-identification (ReID) techniques. Despite\nthese advancements, accurately tracking objects in scenarios with homogeneous\nappearance and heterogeneous motion remains a challenge. This challenge arises\nfrom two main factors: the insufficient discriminability of ReID features and\nthe predominant utilization of linear motion models in MOT. In this context, we\nintroduce a novel motion-based tracker, MotionTrack, centered around a\nlearnable motion predictor that relies solely on object trajectory information.\nThis predictor comprehensively integrates two levels of granularity in motion\nfeatures to enhance the modeling of temporal dynamics and facilitate precise\nfuture motion prediction for individual objects. Specifically, the proposed\napproach adopts a self-attention mechanism to capture token-level information\nand a Dynamic MLP layer to model channel-level features. MotionTrack is a\nsimple, online tracking approach. Our experimental results demonstrate that\nMotionTrack yields state-of-the-art performance on datasets such as Dancetrack\nand SportsMOT, characterized by highly complex object motion.\n","authors":["Changcheng Xiao","Qiong Cao","Yujie Zhong","Long Lan","Xiang Zhang","Zhigang Luo","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2306.02585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06764v1","updated":"2024-03-11T14:35:32Z","published":"2024-03-11T14:35:32Z","title":"An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models","summary":"  In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.\n","authors":["Liang Chen","Haozhe Zhao","Tianyu Liu","Shuai Bai","Junyang Lin","Chang Zhou","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2403.06764v1.pdf","comment":"21 papes, 8 figures, code is released at\n  https://github.com/pkunlp-icler/FastV"},{"id":"http://arxiv.org/abs/2403.06759v1","updated":"2024-03-11T14:31:03Z","published":"2024-03-11T14:31:03Z","title":"Average Calibration Error: A Differentiable Loss for Improved\n  Reliability in Image Segmentation","summary":"  Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS\n","authors":["Theodore Barfoot","Luis Garcia-Peraza-Herrera","Ben Glocker","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06758v1","updated":"2024-03-11T14:30:51Z","published":"2024-03-11T14:30:51Z","title":"EarthLoc: Astronaut Photography Localization by Indexing Earth from\n  Space","summary":"  Astronaut photography, spanning six decades of human spaceflight, presents a\nunique Earth observations dataset with immense value for both scientific\nresearch and disaster response. Despite its significance, accurately localizing\nthe geographical extent of these images, crucial for effective utilization,\nposes substantial challenges. Current manual localization efforts are\ntime-consuming, motivating the need for automated solutions. We propose a novel\napproach - leveraging image retrieval - to address this challenge efficiently.\nWe introduce innovative training techniques, including Year-Wise Data\nAugmentation and a Neutral-Aware Multi-Similarity Loss, which contribute to the\ndevelopment of a high-performance model, EarthLoc. We develop six evaluation\ndatasets and perform a comprehensive benchmark comparing EarthLoc to existing\nmethods, showcasing its superior efficiency and accuracy. Our approach marks a\nsignificant advancement in automating the localization of astronaut\nphotography, which will help bridge a critical gap in Earth observations data.\nCode and datasets are available at https://github.com/gmberton/EarthLoc\n","authors":["Gabriele Berton","Alex Stoken","Barbara Caputo","Carlo Masone"],"pdf_url":"https://arxiv.org/pdf/2403.06758v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06748v1","updated":"2024-03-11T14:14:52Z","published":"2024-03-11T14:14:52Z","title":"Shortcut Learning in Medical Image Segmentation","summary":"  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation.\n","authors":["Manxi Lin","Nina Weng","Kamil Mikolaj","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark Christensen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.06748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08182v2","updated":"2024-03-11T14:08:04Z","published":"2023-07-17T00:56:21Z","title":"Zero-Shot Image Harmonization with Generative Model Prior","summary":"  We propose a zero-shot approach to image harmonization, aiming to overcome\nthe reliance on large amounts of synthetic composite images in existing\nmethods. These methods, while showing promising results, involve significant\ntraining expenses and often struggle with generalization to unseen images. To\nthis end, we introduce a fully modularized framework inspired by human\nbehavior. Leveraging the reasoning capabilities of recent foundation models in\nlanguage and vision, our approach comprises three main stages. Initially, we\nemploy a pretrained vision-language model (VLM) to generate descriptions for\nthe composite image. Subsequently, these descriptions guide the foreground\nharmonization direction of a text-to-image generative model (T2I). We refine\ntext embeddings for enhanced representation of imaging conditions and employ\nself-attention and edge maps for structure preservation. Following each\nharmonization iteration, an evaluator determines whether to conclude or modify\nthe harmonization direction. The resulting framework, mirroring human behavior,\nachieves harmonious results without the need for extensive training. We present\ncompelling visual results across diverse scenes and objects, along with a user\nstudy validating the effectiveness of our approach.\n","authors":["Jianqi Chen","Yilan Zhang","Zhengxia Zou","Keyan Chen","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2307.08182v2.pdf","comment":"Code Page: https://github.com/WindVChen/Diff-Harmonization. In\n  paper-v2, we introduce multiple new designs for solving previous limitations"},{"id":"http://arxiv.org/abs/2403.06741v1","updated":"2024-03-11T14:07:53Z","published":"2024-03-11T14:07:53Z","title":"Distribution-Aware Data Expansion with Diffusion Models","summary":"  The scale and quality of a dataset significantly impact the performance of\ndeep models. However, acquiring large-scale annotated datasets is both a costly\nand time-consuming endeavor. To address this challenge, dataset expansion\ntechnologies aim to automatically augment datasets, unlocking the full\npotential of deep models. Current data expansion methods encompass image\ntransformation-based and synthesis-based methods. The transformation-based\nmethods introduce only local variations, resulting in poor diversity. While\nimage synthesis-based methods can create entirely new content, significantly\nenhancing informativeness. However, existing synthesis methods carry the risk\nof distribution deviations, potentially degrading model performance with\nout-of-distribution samples. In this paper, we propose DistDiff, an effective\ndata expansion framework based on the distribution-aware diffusion model.\nDistDiff constructs hierarchical prototypes to approximate the real data\ndistribution, optimizing latent data points within diffusion models with\nhierarchical energy guidance. We demonstrate its ability to generate\ndistribution-consistent samples, achieving substantial improvements in data\nexpansion tasks. Specifically, without additional training, DistDiff achieves a\n30.7% improvement in accuracy across six image datasets compared to the model\ntrained on original datasets and a 9.8% improvement compared to the\nstate-of-the-art diffusion-based method. Our code is available at\nhttps://github.com/haoweiz23/DistDiff\n","authors":["Haowei Zhu","Ling Yang","Jun-Hai Yong","Wentao Zhang","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06741v1.pdf","comment":"Project: https://github.com/haoweiz23/DistDiff"},{"id":"http://arxiv.org/abs/2403.06738v1","updated":"2024-03-11T14:03:36Z","published":"2024-03-11T14:03:36Z","title":"V3D: Video Diffusion Models are Effective 3D Generators","summary":"  Automatic 3D generation has recently attracted widespread attention. Recent\nmethods have greatly accelerated the generation speed, but usually produce\nless-detailed objects due to limited model capacity or 3D data. Motivated by\nrecent advancements in video diffusion models, we introduce V3D, which\nleverages the world simulation capacity of pre-trained video diffusion models\nto facilitate 3D generation. To fully unleash the potential of video diffusion\nto perceive the 3D world, we further introduce geometrical consistency prior\nand extend the video diffusion model to a multi-view consistent 3D generator.\nBenefiting from this, the state-of-the-art video diffusion model could be\nfine-tuned to generate 360degree orbit frames surrounding an object given a\nsingle image. With our tailored reconstruction pipelines, we can generate\nhigh-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method\ncan be extended to scene-level novel view synthesis, achieving precise control\nover the camera path with sparse input views. Extensive experiments demonstrate\nthe superior performance of the proposed approach, especially in terms of\ngeneration quality and multi-view consistency. Our code is available at\nhttps://github.com/heheyas/V3D\n","authors":["Zilong Chen","Yikai Wang","Feng Wang","Zhengyi Wang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06738v1.pdf","comment":"Code available at https://github.com/heheyas/V3D Project page:\n  https://heheyas.github.io/V3D/"},{"id":"http://arxiv.org/abs/2403.06735v1","updated":"2024-03-11T13:57:05Z","published":"2024-03-11T13:57:05Z","title":"Enhancing Image Caption Generation Using Reinforcement Learning with\n  Human Feedback","summary":"  Research on generative models to produce human-aligned / human-preferred\noutputs has seen significant recent contributions. Between text and\nimage-generative models, we narrowed our focus to text-based generative models,\nparticularly to produce captions for images that align with human preferences.\nIn this research, we explored a potential method to amplify the performance of\nthe Deep Neural Network Model to generate captions that are preferred by\nhumans. This was achieved by integrating Supervised Learning and Reinforcement\nLearning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel\nloss function that is capable of optimizing the model based on human feedback\nis introduced. In this paper, we provide a concise sketch of our approach and\nresults, hoping to contribute to the ongoing advances in the field of\nhuman-aligned generative AI models.\n","authors":["Adarsh N L","Arun P V","Aravindh N L"],"pdf_url":"https://arxiv.org/pdf/2403.06735v1.pdf","comment":"6 Pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.06734v1","updated":"2024-03-11T13:56:57Z","published":"2024-03-11T13:56:57Z","title":"Real-Time Multimodal Cognitive Assistant for Emergency Medical Services","summary":"  Emergency Medical Services (EMS) responders often operate under\ntime-sensitive conditions, facing cognitive overload and inherent risks,\nrequiring essential skills in critical thinking and rapid decision-making. This\npaper presents CognitiveEMS, an end-to-end wearable cognitive assistant system\nthat can act as a collaborative virtual partner engaging in the real-time\nacquisition and analysis of multimodal data from an emergency scene and\ninteracting with EMS responders through Augmented Reality (AR) smart glasses.\nCognitiveEMS processes the continuous streams of data in real-time and\nleverages edge computing to provide assistance in EMS protocol selection and\nintervention recognition. We address key technical challenges in real-time\ncognitive assistance by introducing three novel components: (i) a Speech\nRecognition model that is fine-tuned for real-world medical emergency\nconversations using simulated EMS audio recordings, augmented with synthetic\ndata generated by large language models (LLMs); (ii) an EMS Protocol Prediction\nmodel that combines state-of-the-art (SOTA) tiny language models with EMS\ndomain knowledge using graph-based attention mechanisms; (iii) an EMS Action\nRecognition module which leverages multimodal audio and video data and protocol\npredictions to infer the intervention/treatment actions taken by the responders\nat the incident scene. Our results show that for speech recognition we achieve\nsuperior performance compared to SOTA (WER of 0.290 vs. 0.618) on\nconversational data. Our protocol prediction component also significantly\noutperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition\nachieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s\nfor protocol prediction on the edge and 0.31s on the server.\n","authors":["Keshara Weerasinghe","Saahith Janapati","Xueren Ge","Sion Kim","Sneha Iyer","John A. Stankovic","Homa Alemzadeh"],"pdf_url":"https://arxiv.org/pdf/2403.06734v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.06728v1","updated":"2024-03-11T13:47:11Z","published":"2024-03-11T13:47:11Z","title":"Large Model driven Radiology Report Generation with Clinical Quality\n  Reinforcement Learning","summary":"  Radiology report generation (RRG) has attracted significant attention due to\nits potential to reduce the workload of radiologists. Current RRG approaches\nare still unsatisfactory against clinical standards. This paper introduces a\nnovel RRG method, \\textbf{LM-RRG}, that integrates large models (LMs) with\nclinical quality reinforcement learning to generate accurate and comprehensive\nchest X-ray radiology reports. Our method first designs a large language model\ndriven feature extractor to analyze and interpret different regions of the\nchest X-ray image, emphasizing specific regions with medical significance.\nNext, based on the large model's decoder, we develop a multimodal report\ngenerator that leverages multimodal prompts from visual features and textual\ninstruction to produce the radiology report in an auto-regressive way. Finally,\nto better reflect the clinical significant and insignificant errors that\nradiologists would normally assign in the report, we introduce a novel clinical\nquality reinforcement learning strategy. It utilizes the radiology report\nclinical quality (RadCliQ) metric as a reward function in the learning process.\nExtensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the\nsuperiority of our method over the state of the art.\n","authors":["Zijian Zhou","Miaojing Shi","Meng Wei","Oluwatosin Alabi","Zijie Yue","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05746v2","updated":"2024-03-11T13:45:48Z","published":"2024-02-08T15:26:28Z","title":"Editable Scene Simulation for Autonomous Driving via Collaborative\n  LLM-Agents","summary":"  Scene simulation in autonomous driving has gained significant attention\nbecause of its huge potential for generating customized data. However, existing\neditable scene simulation approaches face limitations in terms of user\ninteraction efficiency, multi-camera photo-realistic rendering and external\ndigital assets integration. To address these challenges, this paper introduces\nChatSim, the first system that enables editable photo-realistic 3D driving\nscene simulations via natural language commands with external digital assets.\nTo enable editing with high command flexibility,~ChatSim leverages a large\nlanguage model (LLM) agent collaboration framework. To generate photo-realistic\noutcomes, ChatSim employs a novel multi-camera neural radiance field method.\nFurthermore, to unleash the potential of extensive high-quality digital assets,\nChatSim employs a novel multi-camera lighting estimation method to achieve\nscene-consistent assets' rendering. Our experiments on Waymo Open Dataset\ndemonstrate that ChatSim can handle complex language commands and generate\ncorresponding photo-realistic scene videos.\n","authors":["Yuxi Wei","Zi Wang","Yifan Lu","Chenxin Xu","Changxing Liu","Hao Zhao","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2402.05746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06726v1","updated":"2024-03-11T13:44:49Z","published":"2024-03-11T13:44:49Z","title":"Probabilistic Contrastive Learning for Long-Tailed Visual Recognition","summary":"  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n","authors":["Chaoqun Du","Yulin Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.06726v1.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)"},{"id":"http://arxiv.org/abs/2402.03917v2","updated":"2024-03-11T13:35:29Z","published":"2024-02-06T11:35:02Z","title":"Elastic Feature Consolidation for Cold Start Exemplar-free Incremental\n  Learning","summary":"  Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a\nsequence of tasks without having access to previous task data. In this paper,\nwe consider the challenging Cold Start scenario in which insufficient data is\navailable in the first task to learn a high-quality backbone. This is\nespecially challenging for EFCIL since it requires high plasticity, which\nresults in feature drift which is difficult to compensate for in the\nexemplar-free setting. To address this problem, we propose a simple and\neffective approach that consolidates feature representations by regularizing\ndrift in directions highly relevant to previous tasks and employs prototypes to\nreduce task-recency bias. Our method, called Elastic Feature Consolidation\n(EFC), exploits a tractable second-order approximation of feature drift based\non an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in\nfeature space which we use to regularize feature drift in important directions\nand to update Gaussian prototypes used in a novel asymmetric cross entropy loss\nwhich effectively balances prototype rehearsal with data from new tasks.\nExperimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and\nImageNet-1K demonstrate that Elastic Feature Consolidation is better able to\nlearn new tasks by maintaining model plasticity and significantly outperform\nthe state-of-the-art.\n","authors":["Simone Magistri","Tomaso Trinci","Albin Soutif-Cormerais","Joost van de Weijer","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2402.03917v2.pdf","comment":"Accepted at Twelfth International Conference on Learning\n  Representations (ICLR 2024)"},{"id":"http://arxiv.org/abs/2403.06702v1","updated":"2024-03-11T13:17:55Z","published":"2024-03-11T13:17:55Z","title":"Fast Text-to-3D-Aware Face Generation and Manipulation via Direct\n  Cross-modal Mapping and Geometric Regularization","summary":"  Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging\nresearch hot spot in machine learning, which still suffers from low efficiency\nand poor quality. In this paper, we propose an End-to-End Efficient and\nEffective network for fast and accurate T3D face generation and manipulation,\ntermed $E^3$-FaceNet. Different from existing complex generation paradigms,\n$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware\nvisual space. We introduce a novel Style Code Enhancer to enhance cross-modal\nsemantic alignment, alongside an innovative Geometric Regularization objective\nto maintain consistency across multi-view generations. Extensive experiments on\nthree benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve\npicture-like 3D face generation and manipulation, but also improve inference\nspeed by orders of magnitudes. For instance, compared with Latent3D,\n$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while\nstill exceeding in generation quality. Our code are released at\nhttps://github.com/Aria-Zhangjl/E3-FaceNet.\n","authors":["Jinlu Zhang","Yiyi Zhou","Qiancheng Zheng","Xiaoxiong Du","Gen Luo","Jun Peng","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.06702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06698v1","updated":"2024-03-11T13:13:10Z","published":"2024-03-11T13:13:10Z","title":"PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification","summary":"  Point clouds are extensively employed in a variety of real-world applications\nsuch as robotics, autonomous driving and augmented reality. Despite the recent\nsuccess of point cloud neural networks, especially for safety-critical tasks,\nit is essential to also ensure the robustness of the model. A typical way to\nassess a model's robustness is through adversarial attacks, where test-time\nexamples are generated based on gradients to deceive the model. While many\ndifferent defense mechanisms are studied in 2D, studies on 3D point clouds have\nbeen relatively limited in the academic field. Inspired from PointDP, which\ndenoises the network inputs by diffusion, we propose Point Cloud Layerwise\nDiffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy.\nUnlike PointDP, we propagated the diffusion denoising after each layer to\nincrementally enhance the results. We apply our defense method to different\ntypes of commonly used point cloud models and adversarial attacks to evaluate\nits robustness. Our experiments demonstrate that the proposed defense method\nachieved results that are comparable to or surpass those of existing\nmethodologies, establishing robustness through a novel technique. Code is\navailable at https://github.com/batuceng/diffusion-layer-robustness-pc.\n","authors":["Mert Gulsen","Batuhan Cengiz","Yusuf H. Sahin","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.06698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09571v6","updated":"2024-03-11T13:05:10Z","published":"2023-04-19T11:19:10Z","title":"LLIC: Large Receptive Field Transform Coding with Adaptive Weights for\n  Learned Image Compression","summary":"  Effective Receptive field (ERF) plays an important role in transform coding,\nwhich determines how much redundancy can be removed at most during transform\nand how many spatial priors can be utilized to synthesize textures during\ninverse transform. Existing methods rely on stacks of small kernels, whose ERF\nremains not large enough instead, or heavy non-local attention mechanisms,\nwhich limit the potential of high resolution image coding. To tackle this\nissue, we propose Large Receptive Field Transform Coding with Adaptive Weights\nfor Learned Image Compression (LLIC). Specifically, for the first time in\nlearned image compression community, we introduce a few large kernel-based\ndepth-wise convolutions to reduce more redundancy while maintaining modest\ncomplexity. Due to wide range of image diversity, we propose to enhance the\nadaptability of convolutions via generating weights in a self-conditioned\nmanner. The large kernels cooperate with non-linear embedding and gate\nmechanisms for better expressiveness and lighter point-wise interactions. We\nalso investigate improved training techniques to fully exploit the potential of\nlarge kernels. In addition, to enhance the interactions among channels, we\npropose the adaptive channel-wise bit allocation via generating channel\nimportance factor in a self-conditioned manner. To demonstrate the\neffectiveness of proposed transform coding, we align the entropy model to\ncompare with existing transform methods and obtain models LLIC-STF, LLIC-ELIC,\nLLIC-TCM. Extensive experiments demonstrate our proposed LLIC models have\nsignificant improvements over corresponding baselines and achieve\nstate-of-the-art performances and better trade-off between performance and\ncomplexity.\n","authors":["Wei Jiang","Peirong Ning","Jiayu Yang","Yongqi Zhai","Feng Gao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.09571v6.pdf","comment":"Fix typos in Table 1 and Add some related references"},{"id":"http://arxiv.org/abs/2403.06687v1","updated":"2024-03-11T13:04:21Z","published":"2024-03-11T13:04:21Z","title":"Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and\n  Attention Mechanism Approach for Heterogeneous Graph-Structured Data","summary":"  Graph neural networks (GNNs) have proven effective in capturing relationships\namong nodes in a graph. This study introduces a novel perspective by\nconsidering a graph as a simplicial complex, encompassing nodes, edges,\ntriangles, and $k$-simplices, enabling the definition of graph-structured data\non any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous\ngraph attention network (HL-HGAT), designed to learn heterogeneous signal\nrepresentations across $k$-simplices. The HL-HGAT incorporates three key\ncomponents: HL convolutional filters (HL-filters), simplicial projection (SP),\nand simplicial attention pooling (SAP) operators, applied to $k$-simplices.\nHL-filters leverage the unique topology of $k$-simplices encoded by the\nHodge-Laplacian (HL) operator, operating within the spectral domain of the\n$k$-th HL operator. To address computation challenges, we introduce a\npolynomial approximation for HL-filters, exhibiting spatial localization\nproperties. Additionally, we propose a pooling operator to coarsen\n$k$-simplices, combining features through simplicial attention mechanisms of\nself-attention and cross-attention via transformers and SP operators, capturing\ntopological interconnections across multiple dimensions of simplices. The\nHL-HGAT is comprehensively evaluated across diverse graph applications,\nincluding NP-hard problems, graph multi-label and classification challenges,\nand graph regression tasks in logistics, computer vision, biology, chemistry,\nand neuroscience. The results demonstrate the model's efficacy and versatility\nin handling a wide range of graph-based scenarios.\n","authors":["Jinghan Huang","Qiufeng Chen","Yijun Bian","Pengli Zhu","Nanguang Chen","Moo K. Chung","Anqi Qiu"],"pdf_url":"https://arxiv.org/pdf/2403.06687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06683v1","updated":"2024-03-11T12:57:51Z","published":"2024-03-11T12:57:51Z","title":"Transferring Relative Monocular Depth to Surgical Vision with Temporal\n  Consistency","summary":"  Relative monocular depth, inferring depth up to shift and scale from a single\nimage, is an active research topic. Recent deep learning models, trained on\nlarge and varied meta-datasets, now provide excellent performance in the domain\nof natural images. However, few datasets exist which provide ground truth depth\nfor endoscopic images, making training such models from scratch unfeasible.\nThis work investigates the transfer of these models into the surgical domain,\nand presents an effective and simple way to improve on standard supervision\nthrough the use of temporal consistency self-supervision. We show temporal\nconsistency significantly improves supervised training alone when transferring\nto the low-data regime of endoscopy, and outperforms the prevalent\nself-supervision technique for this task. In addition we show our method\ndrastically outperforms the state-of-the-art method from within the domain of\nendoscopy. We also release our code, model and ensembled meta-dataset,\nMeta-MED, establishing a strong benchmark for future work.\n","authors":["Charlie Budd","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06682v1","updated":"2024-03-11T12:57:28Z","published":"2024-03-11T12:57:28Z","title":"Restoring Ancient Ideograph: A Multimodal Multitask Neural Network\n  Approach","summary":"  Cultural heritage serves as the enduring record of human thought and history.\nDespite significant efforts dedicated to the preservation of cultural relics,\nmany ancient artefacts have been ravaged irreversibly by natural deterioration\nand human actions. Deep learning technology has emerged as a valuable tool for\nrestoring various kinds of cultural heritages, including ancient text\nrestoration. Previous research has approached ancient text restoration from\neither visual or textual perspectives, often overlooking the potential of\nsynergizing multimodal information. This paper proposes a novel Multimodal\nMultitask Restoring Model (MMRM) to restore ancient texts, particularly\nemphasising the ideograph. This model combines context understanding with\nresidual visual information from damaged ancient artefacts, enabling it to\npredict damaged characters and generate restored images simultaneously. We\ntested the MMRM model through experiments conducted on both simulated datasets\nand authentic ancient inscriptions. The results show that the proposed method\ngives insightful restoration suggestions in both simulation experiments and\nreal-world scenarios. To the best of our knowledge, this work represents the\npioneering application of multimodal deep learning in ancient text restoration,\nwhich will contribute to the understanding of ancient society and culture in\ndigital humanities fields.\n","authors":["Siyu Duan","Jun Wang","Qi Su"],"pdf_url":"https://arxiv.org/pdf/2403.06682v1.pdf","comment":"Accept by Lrec-Coling 2024"},{"id":"http://arxiv.org/abs/2403.06681v1","updated":"2024-03-11T12:56:36Z","published":"2024-03-11T12:56:36Z","title":"Trustworthy Partial Label Learning with Out-of-distribution Detection","summary":"  Partial Label Learning (PLL) grapples with learning from ambiguously labelled\ndata, and it has been successfully applied in fields such as image recognition.\nNevertheless, traditional PLL methods rely on the closed-world assumption,\nwhich can be limiting in open-world scenarios and negatively impact model\nperformance and generalization. To tackle these challenges, our study\nintroduces a novel method called PLL-OOD, which is the first to incorporate\nOut-of-Distribution (OOD) detection into the PLL framework. PLL-OOD\nsignificantly enhances model adaptability and accuracy by merging\nself-supervised learning with partial label loss and pioneering the\nPartial-Energy (PE) score for OOD detection. This approach improves data\nfeature representation and effectively disambiguates candidate labels, using a\ndynamic label confidence matrix to refine predictions. The PE score, adjusted\nby label confidence, precisely identifies OOD instances, optimizing model\ntraining towards in-distribution data. This innovative method markedly boosts\nPLL model robustness and performance in open-world settings. To validate our\napproach, we conducted a comprehensive comparative experiment combining the\nexisting state-of-the-art PLL model with multiple OOD scores on the CIFAR-10\nand CIFAR-100 datasets with various OOD datasets. The results demonstrate that\nthe proposed PLL-OOD framework is highly effective and effectiveness\noutperforms existing models, showcasing its superiority and effectiveness.\n","authors":["Jintao Huang","Yiu-Ming Cheung"],"pdf_url":"https://arxiv.org/pdf/2403.06681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06679v1","updated":"2024-03-11T12:51:37Z","published":"2024-03-11T12:51:37Z","title":"Answering Diverse Questions via Text Attached with Key Audio-Visual\n  Clues","summary":"  Audio-visual question answering (AVQA) requires reference to video content\nand auditory information, followed by correlating the question to predict the\nmost precise answer. Although mining deeper layers of audio-visual information\nto interact with questions facilitates the multimodal fusion process, the\nredundancy of audio-visual parameters tends to reduce the generalization of the\ninference engine to multiple question-answer pairs in a single video. Indeed,\nthe natural heterogeneous relationship between audiovisuals and text makes the\nperfect fusion challenging, to prevent high-level audio-visual semantics from\nweakening the network's adaptability to diverse question types, we propose a\nframework for performing mutual correlation distillation (MCD) to aid question\ninference. MCD is divided into three main steps: 1) firstly, the residual\nstructure is utilized to enhance the audio-visual soft associations based on\nself-attention, then key local audio-visual features relevant to the question\ncontext are captured hierarchically by shared aggregators and coupled in the\nform of clues with specific question vectors. 2) Secondly, knowledge\ndistillation is enforced to align audio-visual-text pairs in a shared latent\nspace to narrow the cross-modal semantic gap. 3) And finally, the audio-visual\ndependencies are decoupled by discarding the decision-level integrations. We\nevaluate the proposed method on two publicly available datasets containing\nmultiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments show\nthat our method outperforms other state-of-the-art methods, and one interesting\nfinding behind is that removing deep audio-visual features during inference can\neffectively mitigate overfitting. The source code is released at\nhttp://github.com/rikeilong/MCD-forAVQA.\n","authors":["Qilang Ye","Zitong Yu","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19101v2","updated":"2024-03-11T12:48:37Z","published":"2023-05-30T15:06:02Z","title":"Which Models have Perceptually-Aligned Gradients? An Explanation via\n  Off-Manifold Robustness","summary":"  One of the remarkable properties of robust computer vision models is that\ntheir input-gradients are often aligned with human perception, referred to in\nthe literature as perceptually-aligned gradients (PAGs). Despite only being\ntrained for classification, PAGs cause robust models to have rudimentary\ngenerative capabilities, including image generation, denoising, and\nin-painting. However, the underlying mechanisms behind these phenomena remain\nunknown. In this work, we provide a first explanation of PAGs via\n\\emph{off-manifold robustness}, which states that models must be more robust\noff- the data manifold than they are on-manifold. We first demonstrate\ntheoretically that off-manifold robustness leads input gradients to lie\napproximately on the data manifold, explaining their perceptual alignment. We\nthen show that Bayes optimal models satisfy off-manifold robustness, and\nconfirm the same empirically for robust models trained via gradient norm\nregularization, randomized smoothing, and adversarial training with projected\ngradient descent. Quantifying the perceptual alignment of model gradients via\ntheir similarity with the gradients of generative models, we show that\noff-manifold robustness correlates well with perceptual alignment. Finally,\nbased on the levels of on- and off-manifold robustness, we identify three\ndifferent regimes of robustness that affect both perceptual alignment and model\naccuracy: weak robustness, bayes-aligned robustness, and excessive robustness.\nCode is available at \\url{https://github.com/tml-tuebingen/pags}.\n","authors":["Suraj Srinivas","Sebastian Bordt","Hima Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2305.19101v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.06676v1","updated":"2024-03-11T12:48:22Z","published":"2024-03-11T12:48:22Z","title":"CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object\n  Localization Perspective","summary":"  Recently, convolutional neural networks (CNNs) with large size kernels have\nattracted much attention in the computer vision field, following the success of\nthe Vision Transformers. Large kernel CNNs have been reported to perform well\nin downstream vision tasks as well as in classification performance. The reason\nfor the high-performance of large kernel CNNs in downstream tasks has been\nattributed to the large effective receptive field (ERF) produced by large size\nkernels, but this view has not been fully tested. We therefore revisit the\nperformance of large kernel CNNs in downstream task, focusing on the weakly\nsupervised object localization (WSOL) task. WSOL, a difficult downstream task\nthat is not fully supervised, provides a new angle to explore the capabilities\nof the large kernel CNNs. Our study compares the modern large kernel CNNs\nConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that\nERF size is important for improving downstream task performance. Our analysis\nof the factors contributing to high performance provides a different\nperspective, in which the main factor is feature map improvement. Furthermore,\nwe find that modern CNNs are robust to the CAM problems of local regions of\nobjects being activated, which has long been discussed in WSOL. CAM is the most\nclassic WSOL method, but because of the above-mentioned problems, it is often\nused as a baseline method for comparison. However, experiments on the\nCUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and\nsimple data augmentation methods can achieve performance (90.99% MaxBoxAcc)\ncomparable to the latest WSOL method, which is CNN-based and requires special\ntraining or complex post-processing. The code is available at\nhttps://github.com/snskysk/CAM-Back-Again.\n","authors":["Shunsuke Yasuki","Masato Taki"],"pdf_url":"https://arxiv.org/pdf/2403.06676v1.pdf","comment":"Accepted by CVPR2024. Code: https://github.com/snskysk/CAM-Back-Again"},{"id":"http://arxiv.org/abs/2403.06674v1","updated":"2024-03-11T12:46:53Z","published":"2024-03-11T12:46:53Z","title":"Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment","summary":"  Most computer vision applications aim to identify pixels in a scene and use\nthem for diverse purposes. One intriguing application is car damage detection\nfor insurance carriers which tends to detect all car damages by comparing both\npre-trip and post-trip images, even requiring two components: (i) car damage\ndetection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to\ndetect car damages on custom images. Whereas for the image alignment section,\nwe especially propose a novel self-supervised Patch-to-Patch SimCLR inspired\nalignment approach to find perspective transformations between custom pre/post\ncar rental images except for traditional computer vision methods.\n","authors":["Hanxiao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.06674v1.pdf","comment":"The paper has been accepted and given a poster presentation at\n  NeurIPS 2021 WiML Workshop\n  (https://nips.cc/virtual/2021/affinity-workshop/22882)"},{"id":"http://arxiv.org/abs/2402.00680v3","updated":"2024-03-11T12:41:20Z","published":"2024-02-01T15:43:43Z","title":"LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video\n  Compression","summary":"  Existing learned video compression models employ flow net or deformable\nconvolutional networks (DCN) to estimate motion information. However, the\nlimited receptive fields of flow net and DCN inherently direct their\nattentiveness towards the local contexts. Global contexts, such as large-scale\nmotions and global correlations among frames are ignored, presenting a\nsignificant bottleneck for capturing accurate motions. To address this issue,\nwe propose a joint local and global motion compensation module (LGMC) for\nleaned video coding. More specifically, we adopt flow net for local motion\ncompensation. To capture global context, we employ the cross attention in\nfeature domain for motion compensation. In addition, to avoid the quadratic\ncomplexity of vanilla cross attention, we divide the softmax operations in\nattention into two independent softmax operations, leading to linear\ncomplexity. To validate the effectiveness of our proposed LGMC, we integrate it\nwith DCVC-TCM and obtain learned video compression with joint local and global\nmotion compensation (LVC-LGMC). Extensive experiments demonstrate that our\nLVC-LGMC has significant rate-distortion performance improvements over baseline\nDCVC-TCM.\n","authors":["Wei Jiang","Junru Li","Kai Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.00680v3.pdf","comment":"Accepted to ICASSP 2024 (lecture presentation). The first attempt to\n  use cross attention for bits-free motion estimation and motion compensation"},{"id":"http://arxiv.org/abs/2403.06670v1","updated":"2024-03-11T12:40:12Z","published":"2024-03-11T12:40:12Z","title":"CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar\n  Class-Incremental Learnin","summary":"  In real-world applications, dynamic scenarios require the models to possess\nthe capability to learn new tasks continuously without forgetting the old\nknowledge. Experience-Replay methods store a subset of the old images for joint\ntraining. In the scenario of more strict privacy protection, storing the old\nimages becomes infeasible, which leads to a more severe plasticity-stability\ndilemma and classifier bias. To meet the above challenges, we propose a new\narchitecture, named continual expansion and absorption transformer~(CEAT). The\nmodel can learn the novel knowledge by extending the expanded-fusion layers in\nparallel with the frozen previous parameters. After the task ends, we\nlosslessly absorb the extended parameters into the backbone to ensure that the\nnumber of parameters remains constant. To improve the learning ability of the\nmodel, we designed a novel prototype contrastive loss to reduce the overlap\nbetween old and new classes in the feature space. Besides, to address the\nclassifier bias towards the new classes, we propose a novel approach to\ngenerate the pseudo-features to correct the classifier. We experiment with our\nmethods on three standard Non-Exemplar Class-Incremental Learning~(NECIL)\nbenchmarks. Extensive experiments demonstrate that our model gets a significant\nimprovement compared with the previous works and achieves 5.38%, 5.20%, and\n4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset.\n","authors":["Xinyuan Gao","Songlin Dong","Yuhang He","Xing Wei","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2403.06670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04151v2","updated":"2024-03-11T12:40:09Z","published":"2024-03-07T02:17:59Z","title":"Dual-path Frequency Discriminators for Few-shot Anomaly Detection","summary":"  Few-shot anomaly detection (FSAD) is essential in industrial manufacturing.\nHowever, existing FSAD methods struggle to effectively leverage a limited\nnumber of normal samples, and they may fail to detect and locate inconspicuous\nanomalies in the spatial domain. We further discover that these subtle\nanomalies would be more noticeable in the frequency domain. In this paper, we\npropose a Dual-Path Frequency Discriminators (DFD) network from a frequency\nperspective to tackle these issues. Specifically, we generate anomalies at both\nimage-level and feature-level. Differential frequency components are extracted\nby the multi-frequency information construction module and supplied into the\nfine-grained feature construction module to provide adapted features. We\nconsider anomaly detection as a discriminative classification problem,\nwherefore the dual-path feature discrimination module is employed to detect and\nlocate the image-level and feature-level anomalies in the feature space. The\ndiscriminators aim to learn a joint representation of anomalous features and\nnormal features in the latent space. Extensive experiments conducted on MVTec\nAD and VisA benchmarks demonstrate that our DFD surpasses current\nstate-of-the-art methods. Source code will be available.\n","authors":["Yuhu Bai","Jiangning Zhang","Yuhang Dong","Guanzhong Tian","Liang Liu","Yunkang Cao","Yabiao Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.04151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06547v2","updated":"2024-03-11T12:36:37Z","published":"2023-09-12T19:46:15Z","title":"AmodalSynthDrive: A Synthetic Amodal Perception Dataset for Autonomous\n  Driving","summary":"  Unlike humans, who can effortlessly estimate the entirety of objects even\nwhen partially occluded, modern computer vision algorithms still find this\naspect extremely challenging. Leveraging this amodal perception for autonomous\ndriving remains largely untapped due to the lack of suitable datasets. The\ncuration of these datasets is primarily hindered by significant annotation\ncosts and mitigating annotator subjectivity in accurately labeling occluded\nregions. To address these limitations, we introduce AmodalSynthDrive, a\nsynthetic multi-task multi-modal amodal perception dataset. The dataset\nprovides multi-view camera images, 3D bounding boxes, LiDAR data, and odometry\nfor 150 driving sequences with over 1M object annotations in diverse traffic,\nweather, and lighting conditions. AmodalSynthDrive supports multiple amodal\nscene understanding tasks including the introduced amodal depth estimation for\nenhanced spatial understanding. We evaluate several baselines for each of these\ntasks to illustrate the challenges and set up public benchmarking servers. The\ndataset is available at http://amodalsynthdrive.cs.uni-freiburg.de.\n","authors":["Ahmed Rida Sekkat","Rohit Mohan","Oliver Sawade","Elmar Matthes","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2309.06547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06668v1","updated":"2024-03-11T12:36:14Z","published":"2024-03-11T12:36:14Z","title":"PeerAiD: Improving Adversarial Distillation from a Specialized Peer\n  Tutor","summary":"  Adversarial robustness of the neural network is a significant concern when it\nis applied to security-critical domains. In this situation, adversarial\ndistillation is a promising option which aims to distill the robustness of the\nteacher network to improve the robustness of a small student network. Previous\nworks pretrain the teacher network to make it robust to the adversarial\nexamples aimed at itself. However, the adversarial examples are dependent on\nthe parameters of the target network. The fixed teacher network inevitably\ndegrades its robustness against the unseen transferred adversarial examples\nwhich targets the parameters of the student network in the adversarial\ndistillation process. We propose PeerAiD to make a peer network learn the\nadversarial examples of the student network instead of adversarial examples\naimed at itself. PeerAiD is an adversarial distillation that trains the peer\nnetwork and the student network simultaneously in order to make the peer\nnetwork specialized for defending the student network. We observe that such\npeer networks surpass the robustness of pretrained robust teacher network\nagainst student-attacked adversarial samples. With this peer network and\nadversarial distillation, PeerAiD achieves significantly higher robustness of\nthe student network with AutoAttack (AA) accuracy up to 1.66%p and improves the\nnatural accuracy of the student network up to 4.72%p with ResNet-18 and\nTinyImageNet dataset.\n","authors":["Jaewon Jung","Hongsun Jang","Jaeyong Song","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06668v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06661v1","updated":"2024-03-11T12:29:55Z","published":"2024-03-11T12:29:55Z","title":"epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for\n  Facial Expression Recognition","summary":"  Point clouds and meshes are widely used 3D data structures for many computer\nvision applications. While the meshes represent the surfaces of an object,\npoint cloud represents sampled points from the surface which is also the output\nof modern sensors such as LiDAR and RGB-D cameras. Due to the wide application\narea of point clouds and the recent advancements in deep neural networks,\nstudies focusing on robust classification of the 3D point cloud data emerged.\nTo evaluate the robustness of deep classifier networks, a common method is to\nuse adversarial attacks where the gradient direction is followed to change the\ninput slightly. The previous studies on adversarial attacks are generally\nevaluated on point clouds of daily objects. However, considering 3D faces,\nthese adversarial attacks tend to affect the person's facial structure more\nthan the desired amount and cause malformation. Specifically for facial\nexpressions, even a small adversarial attack can have a significant effect on\nthe face structure. In this paper, we suggest an adversarial attack called\n$\\epsilon$-Mesh Attack, which operates on point cloud data via limiting\nperturbations to be on the mesh surface. We also parameterize our attack by\n$\\epsilon$ to scale the perturbation mesh. Our surface-based attack has tighter\nperturbation bounds compared to $L_2$ and $L_\\infty$ norm bounded attacks that\noperate on unit-ball. Even though our method has additional constraints, our\nexperiments on CoMA, Bosphorus and FaceWarehouse datasets show that\n$\\epsilon$-Mesh Attack (Perpendicular) successfully confuses trained DGCNN and\nPointNet models $99.72\\%$ and $97.06\\%$ of the time, with indistinguishable\nfacial deformations. The code is available at\nhttps://github.com/batuceng/e-mesh-attack.\n","authors":["Batuhan Cengiz","Mert Gulsen","Yusuf H. Sahin","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.06661v1.pdf","comment":"Accepted at 18th IEEE International Conference on Automatic Face &\n  Gesture Recognition (FG 2024)"},{"id":"http://arxiv.org/abs/2403.06658v1","updated":"2024-03-11T12:27:20Z","published":"2024-03-11T12:27:20Z","title":"Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration\n  Framework","summary":"  Large vision models based in deep learning architectures have been\nconsistently advancing the state-of-the-art in biometric recognition. However,\nthree weaknesses are commonly reported for such kind of approaches: 1) their\nextreme demands in terms of learning data; 2) the difficulties in generalising\nbetween different domains; and 3) the lack of interpretability/explainability,\nwith biometrics being of particular interest, as it is important to provide\nevidence able to be used for forensics/legal purposes (e.g., in courts). To the\nbest of our knowledge, this paper describes the first recognition\nframework/strategy that aims at addressing the three weaknesses simultaneously.\nAt first, it relies exclusively in synthetic samples for learning purposes.\nInstead of requiring a large amount and variety of samples for each subject,\nthe idea is to exclusively enroll a 3D point cloud per identity. Then, using\ngenerative strategies, we synthesize a very large (potentially infinite) number\nof samples, containing all the desired covariates (poses, clothing, distances,\nperspectives, lighting, occlusions,...). Upon the synthesizing method used, it\nis possible to adapt precisely to different kind of domains, which accounts for\ngeneralization purposes. Such data are then used to learn a model that performs\nlocal registration between image pairs, establishing positive correspondences\nbetween body parts that are the key, not only to recognition (according to\ncardinality and distribution), but also to provide an interpretable description\nof the response (e.g.: \"both samples are from the same person, as they have\nsimilar facial shape, hair color and legs thickness\").\n","authors":["Henrique Jesus","Hugo Proença"],"pdf_url":"https://arxiv.org/pdf/2403.06658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06645v1","updated":"2024-03-11T12:07:33Z","published":"2024-03-11T12:07:33Z","title":"Ricci flow-based brain surface covariance descriptors for Alzheimer\n  disease","summary":"  Automated feature extraction from MRI brain scans and diagnosis of\nAlzheimer's disease are ongoing challenges. With advances in 3D imaging\ntechnology, 3D data acquisition is becoming more viable and efficient than its\n2D counterpart. Rather than using feature-based vectors, in this paper, for the\nfirst time, we suggest a pipeline to extract novel covariance-based descriptors\nfrom the cortical surface using the Ricci energy optimization. The covariance\ndescriptors are components of the nonlinear manifold of symmetric\npositive-definite matrices, thus we focus on using the Gaussian radial basis\nfunction to apply manifold-based classification to the 3D shape problem.\nApplying this novel signature to the analysis of abnormal cortical brain\nmorphometry allows for diagnosing Alzheimer's disease. Experimental studies\nperformed on about two hundred 3D MRI brain models, gathered from Alzheimer's\nDisease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of\nour descriptors in achieving remarkable classification accuracy.\n","authors":["Fatemeh Ahmadi","Mohamad Ebrahim Shiri","Behroz Bidabad","Maral Sedaghat","Pooran Memari"],"pdf_url":"https://arxiv.org/pdf/2403.06645v1.pdf","comment":"Accepted for publication in Biomedical Signal Processing and Control\n  journal"},{"id":"http://arxiv.org/abs/2403.06631v1","updated":"2024-03-11T11:41:30Z","published":"2024-03-11T11:41:30Z","title":"Evaluating the Energy Efficiency of Few-Shot Learning for Object\n  Detection in Industrial Settings","summary":"  In the ever-evolving era of Artificial Intelligence (AI), model performance\nhas constituted a key metric driving innovation, leading to an exponential\ngrowth in model size and complexity. However, sustainability and energy\nefficiency have been critical requirements during deployment in contemporary\nindustrial settings, necessitating the use of data-efficient approaches such as\nfew-shot learning. In this paper, to alleviate the burden of lengthy model\ntraining and minimize energy consumption, a finetuning approach to adapt\nstandard object detection models to downstream tasks is examined. Subsequently,\na thorough case study and evaluation of the energy demands of the developed\nmodels, applied in object detection benchmark datasets from volatile industrial\nenvironments is presented. Specifically, different finetuning strategies as\nwell as utilization of ancillary evaluation data during training are examined,\nand the trade-off between performance and efficiency is highlighted in this\nlow-data regime. Finally, this paper introduces a novel way to quantify this\ntrade-off through a customized Efficiency Factor metric.\n","authors":["Georgios Tsoumplekas","Vladislav Li","Ilias Siniosoglou","Vasileios Argyriou","Sotirios K. Goudos","Ioannis D. Moscholios","Panagiotis Radoglou-Grammatikis","Panagiotis Sarigiannidis"],"pdf_url":"https://arxiv.org/pdf/2403.06631v1.pdf","comment":"7 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.06621v1","updated":"2024-03-11T11:26:44Z","published":"2024-03-11T11:26:44Z","title":"Forest Inspection Dataset for Aerial Semantic Segmentation and Depth\n  Estimation","summary":"  Humans use UAVs to monitor changes in forest environments since they are\nlightweight and provide a large variety of surveillance data. However, their\ninformation does not present enough details for understanding the scene which\nis needed to assess the degree of deforestation. Deep learning algorithms must\nbe trained on large amounts of data to output accurate interpretations, but\nground truth recordings of annotated forest imagery are not available. To solve\nthis problem, we introduce a new large aerial dataset for forest inspection\nwhich contains both real-world and virtual recordings of natural environments,\nwith densely annotated semantic segmentation labels and depth maps, taken in\ndifferent illumination conditions, at various altitudes and recording angles.\nWe test the performance of two multi-scale neural networks for solving the\nsemantic segmentation task (HRNet and PointFlow network), studying the impact\nof the various acquisition conditions and the capabilities of transfer learning\nfrom virtual to real data. Our results showcase that the best results are\nobtained when the training is done on a dataset containing a large variety of\nscenarios, rather than separating the data into specific categories. We also\ndevelop a framework to assess the deforestation degree of an area.\n","authors":["Bianca-Cerasela-Zelia Blaga","Sergiu Nedevschi"],"pdf_url":"https://arxiv.org/pdf/2403.06621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05845v4","updated":"2024-03-11T11:17:36Z","published":"2022-08-11T14:28:21Z","title":"Analyzing Fairness in Deepfake Detection With Massively Annotated\n  Databases","summary":"  In recent years, image and video manipulations with Deepfake have become a\nsevere concern for security and society. Many detection models and datasets\nhave been proposed to detect Deepfake data reliably. However, there is an\nincreased concern that these models and training databases might be biased and,\nthus, cause Deepfake detectors to fail. In this work, we investigate factors\ncausing biased detection in public Deepfake datasets by (a) creating\nlarge-scale demographic and non-demographic attribute annotations with 47\ndifferent attributes for five popular Deepfake datasets and (b) comprehensively\nanalysing attributes resulting in AI-bias of three state-of-the-art Deepfake\ndetection backbone models on these datasets. The analysis shows how various\nattributes influence a large variety of distinctive attributes (from over 65M\nlabels) on the detection performance which includes demographic (age, gender,\nethnicity) and non-demographic (hair, skin, accessories, etc.) attributes. The\nresults examined datasets show limited diversity and, more importantly, show\nthat the utilised Deepfake detection backbone models are strongly affected by\ninvestigated attributes making them not fair across attributes. The Deepfake\ndetection backbone methods trained on such imbalanced/biased datasets result in\nincorrect detection results leading to generalisability, fairness, and security\nissues. Our findings and annotated datasets will guide future research to\nevaluate and mitigate bias in Deepfake detection techniques. The annotated\ndatasets and the corresponding code are publicly available.\n","authors":["Ying Xu","Philipp Terhörst","Kiran Raja","Marius Pedersen"],"pdf_url":"https://arxiv.org/pdf/2208.05845v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14025v2","updated":"2024-03-11T11:14:15Z","published":"2023-07-26T08:14:18Z","title":"Topologically Regularized Multiple Instance Learning to Harness Data\n  Scarcity","summary":"  In biomedical data analysis, Multiple Instance Learning (MIL) models have\nemerged as a powerful tool to classify patients' microscopy samples. However,\nthe data-intensive requirement of these models poses a significant challenge in\nscenarios with scarce data availability, e.g., in rare diseases. We introduce a\ntopological regularization term to MIL to mitigate this challenge. It provides\na shape-preserving inductive bias that compels the encoder to maintain the\nessential geometrical-topological structure of input bags during projection\ninto latent space. This enhances the performance and generalization of the MIL\nclassifier regardless of the aggregation function, particularly for scarce\ntraining data. The effectiveness of our method is confirmed through experiments\nacross a range of datasets, showing an average enhancement of 2.8% for MIL\nbenchmarks, 15.3% for synthetic MIL datasets, and 5.5% for real-world\nbiomedical datasets over the current state-of-the-art.\n","authors":["Salome Kazeminia","Carsten Marr","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2307.14025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06616v1","updated":"2024-03-11T11:06:41Z","published":"2024-03-11T11:06:41Z","title":"Density-Guided Label Smoothing for Temporal Localization of Driving\n  Actions","summary":"  Temporal localization of driving actions plays a crucial role in advanced\ndriver-assistance systems and naturalistic driving studies. However, this is a\nchallenging task due to strict requirements for robustness, reliability and\naccurate localization. In this work, we focus on improving the overall\nperformance by efficiently utilizing video action recognition networks and\nadapting these to the problem of action localization. To this end, we first\ndevelop a density-guided label smoothing technique based on label probability\ndistributions to facilitate better learning from boundary video-segments that\ntypically include multiple labels. Second, we design a post-processing step to\nefficiently fuse information from video-segments and multiple camera views into\nscene-level predictions, which facilitates elimination of false positives. Our\nmethodology yields a competitive performance on the A2 test set of the\nnaturalistic driving action recognition track of the 2022 NVIDIA AI City\nChallenge with an F1 score of 0.271.\n","authors":["Tunc Alkanat","Erkut Akdag","Egor Bondarev","Peter H. N. De With"],"pdf_url":"https://arxiv.org/pdf/2403.06616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06024v3","updated":"2024-03-11T10:51:03Z","published":"2023-05-10T10:19:31Z","title":"A Survey on the Robustness of Computer Vision Models against Common\n  Corruptions","summary":"  The performance of computer vision models are susceptible to unexpected\nchanges in input images, known as common corruptions (e.g. noise, blur,\nillumination changes, etc.), that can hinder their reliability when deployed in\nreal scenarios. These corruptions are not always considered to test model\ngeneralization and robustness. In this survey, we present a comprehensive\noverview of methods that improve the robustness of computer vision models\nagainst common corruptions. We categorize methods into four groups based on the\nmodel part and training method addressed: data augmentation, representation\nlearning, knowledge distillation, and network components. We also cover\nindirect methods for generalization and mitigation of shortcut learning,\npotentially useful for corruption robustness. We release a unified benchmark\nframework to compare robustness performance on several datasets, and address\nthe inconsistencies of evaluation in the literature. We provide an experimental\noverview of the base corruption robustness of popular vision backbones, and\nshow that corruption robustness does not necessarily scale with model size. The\nvery large models (above 100M parameters) gain negligible robustness,\nconsidering the increased computational requirements. To achieve generalizable\nand robust computer vision models, we foresee the need of developing new\nlearning strategies to efficiently exploit limited data and mitigate unwanted\nor unreliable learning behaviors.\n","authors":["Shunxin Wang","Raymond Veldhuis","Christoph Brune","Nicola Strisciuglio"],"pdf_url":"https://arxiv.org/pdf/2305.06024v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06606v1","updated":"2024-03-11T10:50:53Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06601v1","updated":"2024-03-11T10:48:56Z","published":"2024-03-11T10:48:56Z","title":"Cross-domain and Cross-dimension Learning for Image-to-Graph\n  Transformers","summary":"  Direct image-to-graph transformation is a challenging task that solves object\ndetection and relationship prediction in a single model. Due to the complexity\nof this task, large training datasets are rare in many domains, which makes the\ntraining of large networks challenging. This data sparsity necessitates the\nestablishment of pre-training strategies akin to the state-of-the-art in\ncomputer vision. In this work, we introduce a set of methods enabling\ncross-domain and cross-dimension transfer learning for image-to-graph\ntransformers. We propose (1) a regularized edge sampling loss for sampling the\noptimal number of object relationships (edges) across domains, (2) a domain\nadaptation framework for image-to-graph transformers that aligns features from\ndifferent domains, and (3) a simple projection function that allows us to\npretrain 3D transformers on 2D input data. We demonstrate our method's utility\nin cross-domain and cross-dimension experiments, where we pretrain our models\non 2D satellite images before applying them to vastly different target domains\nin 2D and 3D. Our method consistently outperforms a series of baselines on\nchallenging benchmarks, such as retinal or whole-brain vessel graph extraction.\n","authors":["Alexander H. Berger","Laurin Lux","Suprosanna Shit","Ivan Ezhov","Georgios Kaissis","Martin J. Menten","Daniel Rueckert","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2403.06601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06600v1","updated":"2024-03-11T10:46:43Z","published":"2024-03-11T10:46:43Z","title":"BEV2PR: BEV-Enhanced Visual Place Recognition with Structural Cues","summary":"  In this paper, we propose a new image-based visual place recognition (VPR)\nframework by exploiting the structural cues in bird's-eye view (BEV) from a\nsingle monocular camera. The motivation arises from two key observations about\nVPR: 1) For the methods based on both camera and LiDAR sensors, the integration\nof LiDAR in robotic systems has led to increased expenses, while the alignment\nof data between different sensors is also a major challenge. 2) Other\nimage-/camera-based methods, involving integrating RGB images and their derived\nvariants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several\nlimitations, such as the failure to effectively exploit the explicit spatial\nrelationships between different objects. To tackle the above issues, we design\na new BEV-enhanced VPR framework, nemely BEV2PR, which can generate a composite\ndescriptor with both visual cues and spatial awareness solely based on a single\ncamera. For the visual cues, any popular aggregation module for RGB global\nfeatures can be integrated into our framework. The key points lie in: 1) We use\nBEV segmentation features as an explicit source of structural knowledge in\nconstructing global features. 2) The lower layers of the pre-trained backbone\nfrom BEV map generation are shared for visual and structural streams in VPR,\nfacilitating the learning of fine-grained local features in the visual stream.\n3) The complementary visual features and structural features can jointly\nenhance VPR performance. Our BEV2PR framework enables consistent performance\nimprovements over several popular camera-based VPR aggregation modules when\nintegrating them. The experiments on our collected VPR-NuScenes dataset\ndemonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP\nbaseline to achieve the best performance in our setting, and notably, a 18.06%\ngain on the hard set.\n","authors":["Fudong Ge","Yiwei Zhang","Shuhan Shen","Yue Wang","Weiming Hu","Jin Gao"],"pdf_url":"https://arxiv.org/pdf/2403.06600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10450v2","updated":"2024-03-11T10:45:29Z","published":"2023-08-21T03:50:19Z","title":"COCA: Classifier-Oriented Calibration via Textual Prototype for\n  Source-Free Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) aims to address domain and category\nshifts across data sources. Recently, due to more stringent data restrictions,\nresearchers have introduced source-free UniDA (SF-UniDA). SF-UniDA methods\neliminate the need for direct access to source samples when performing\nadaptation to the target domain. However, existing SF-UniDA methods still\nrequire an extensive quantity of labeled source samples to train a source\nmodel, resulting in significant labeling costs. To tackle this issue, we\npresent a novel plug-and-play classifier-oriented calibration (COCA) method.\nCOCA, which exploits textual prototypes, is designed for the source models\nbased on few-shot learning with vision-language models (VLMs). It endows the\nVLM-powered few-shot learners, which are built for closed-set classification,\nwith the unknown-aware ability to distinguish common and unknown classes in the\nSF-UniDA scenario. Crucially, COCA is a new paradigm to tackle SF-UniDA\nchallenges based on VLMs, which focuses on classifier instead of image encoder\noptimization. Experiments show that COCA outperforms state-of-the-art UniDA and\nSF-UniDA models.\n","authors":["Xinghong Liu","Yi Zhou","Tao Zhou","Chun-Mei Feng","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2308.10450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16503v3","updated":"2024-03-11T10:40:40Z","published":"2023-11-27T12:59:52Z","title":"TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models","summary":"  The Diffusion model, a prevalent framework for image generation, encounters\nsignificant challenges in terms of broad applicability due to its extended\ninference times and substantial memory requirements. Efficient Post-training\nQuantization (PTQ) is pivotal for addressing these issues in traditional\nmodels. Different from traditional models, diffusion models heavily depend on\nthe time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$\nfrom the finite set $\\{1, \\ldots, T\\}$ is encoded to a temporal feature by a\nfew modules totally irrespective of the sampling data. However, existing PTQ\nmethods do not optimize these modules separately. They adopt inappropriate\nreconstruction targets and complex calibration methods, resulting in a severe\ndisturbance of the temporal feature and denoising trajectory, as well as a low\ncompression efficiency. To solve these, we propose a Temporal Feature\nMaintenance Quantization (TFMQ) framework building upon a Temporal Information\nBlock which is just related to the time-step $t$ and unrelated to the sampling\ndata. Powered by the pioneering block design, we devise temporal information\naware reconstruction (TIAR) and finite set calibration (FSC) to align the\nfull-precision temporal features in a limited time. Equipped with the\nframework, we can maintain the most temporal information and ensure the\nend-to-end generation quality. Extensive experiments on various datasets and\ndiffusion models prove our state-of-the-art results. Remarkably, our\nquantization approach, for the first time, achieves model performance nearly on\npar with the full-precision model under 4-bit weight quantization.\nAdditionally, our method incurs almost no extra computational cost and\naccelerates quantization time by $2.0 \\times$ on LSUN-Bedrooms $256 \\times 256$\ncompared to previous works. Our code is publicly available at\nhttps://github.com/ModelTC/TFMQ-DM.\n","authors":["Yushi Huang","Ruihao Gong","Jing Liu","Tianlong Chen","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.16503v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17336v2","updated":"2024-03-11T10:37:29Z","published":"2023-09-29T15:46:59Z","title":"Robust 3D Object Detection from LiDAR-Radar Point Clouds via Cross-Modal\n  Feature Augmentation","summary":"  This paper presents a novel framework for robust 3D object detection from\npoint clouds via cross-modal hallucination. Our proposed approach is agnostic\nto either hallucination direction between LiDAR and 4D radar. We introduce\nmultiple alignments on both spatial and feature levels to achieve simultaneous\nbackbone refinement and hallucination generation. Specifically, spatial\nalignment is proposed to deal with the geometry discrepancy for better instance\nmatching between LiDAR and radar. The feature alignment step further bridges\nthe intrinsic attribute gap between the sensing modalities and stabilizes the\ntraining. The trained object detection models can deal with difficult detection\ncases better, even though only single-modal data is used as the input during\nthe inference stage. Extensive experiments on the View-of-Delft (VoD) dataset\nshow that our proposed method outperforms the state-of-the-art (SOTA) methods\nfor both radar and LiDAR object detection while maintaining competitive\nefficiency in runtime.\n","authors":["Jianning Deng","Gabriel Chan","Hantao Zhong","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2309.17336v2.pdf","comment":"Accepted to ICRA 2024. 8 pages, 4 figures. Equal contribution for\n  Gabriel Chan and Hantao Zhong, listed randomly"},{"id":"http://arxiv.org/abs/2306.12152v2","updated":"2024-03-11T10:37:00Z","published":"2023-06-21T09:56:55Z","title":"Exploiting Multimodal Synthetic Data for Egocentric Human-Object\n  Interaction Detection in an Industrial Scenario","summary":"  In this paper, we tackle the problem of Egocentric Human-Object Interaction\n(EHOI) detection in an industrial setting. To overcome the lack of public\ndatasets in this context, we propose a pipeline and a tool for generating\nsynthetic images of EHOIs paired with several annotations and data signals\n(e.g., depth maps or segmentation masks). Using the proposed pipeline, we\npresent EgoISM-HOI a new multimodal dataset composed of synthetic EHOI images\nin an industrial environment with rich annotations of hands and objects. To\ndemonstrate the utility and effectiveness of synthetic EHOI data produced by\nthe proposed tool, we designed a new method that predicts and combines\ndifferent multimodal signals to detect EHOIs in RGB images. Our study shows\nthat exploiting synthetic data to pre-train the proposed method significantly\nimproves performance when tested on real-world data. Moreover, to fully\nunderstand the usefulness of our method, we conducted an in-depth analysis in\nwhich we compared and highlighted the superiority of the proposed approach over\ndifferent state-of-the-art class-agnostic methods. To support research in this\nfield, we publicly release the datasets, source code, and pre-trained models at\nhttps://iplab.dmi.unict.it/egoism-hoi.\n","authors":["Rosario Leonardi","Francesco Ragusa","Antonino Furnari","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2306.12152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06592v1","updated":"2024-03-11T10:35:58Z","published":"2024-03-11T10:35:58Z","title":"Exploiting Style Latent Flows for Generalizing Deepfake Detection Video\n  Detection","summary":"  This paper presents a new approach for the detection of fake videos, based on\nthe analysis of style latent vectors and their abnormal behavior in temporal\nchanges in the generated videos. We discovered that the generated facial videos\nsuffer from the temporal distinctiveness in the temporal changes of style\nlatent vectors, which are inevitable during the generation of temporally stable\nvideos with various facial expressions and geometric transformations. Our\nframework utilizes the StyleGRU module, trained by contrastive learning, to\nrepresent the dynamic properties of style latent vectors. Additionally, we\nintroduce a style attention module that integrates StyleGRU-generated features\nwith content-based features, enabling the detection of visual and temporal\nartifacts. We demonstrate our approach across various benchmark scenarios in\ndeepfake detection, showing its superiority in cross-dataset and\ncross-manipulation scenarios. Through further analysis, we also validate the\nimportance of using temporal changes of style latent vectors to improve the\ngenerality of deepfake video detection.\n","authors":["Jongwook Choi","Taehoon Kim","Yonghyun Jeong","Seungryul Baek","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06592v1.pdf","comment":"Preprint version, final version will be available at\n  https://openaccess.thecvf.com The IEEE / CVF Computer Vision and Pattern\n  Recognition Conference (CVPR) (2024) Published by: IEEE & CVF"},{"id":"http://arxiv.org/abs/2308.14316v2","updated":"2024-03-11T10:28:41Z","published":"2023-08-28T05:38:43Z","title":"UniPT: Universal Parallel Tuning for Transfer Learning with Efficient\n  Parameter and Memory","summary":"  Parameter-efficient transfer learning (PETL), i.e., fine-tuning a small\nportion of parameters, is an effective strategy for adapting pre-trained models\nto downstream domains. To further reduce the memory demand, recent PETL works\nfocus on the more valuable memory-efficient characteristic. In this paper, we\nargue that the scalability, adaptability, and generalizability of\nstate-of-the-art methods are hindered by structural dependency and pertinency\non specific pre-trained backbones. To this end, we propose a new\nmemory-efficient PETL strategy, Universal Parallel Tuning (UniPT), to mitigate\nthese weaknesses. Specifically, we facilitate the transfer process via a\nlightweight and learnable parallel network, which consists of: 1) A parallel\ninteraction module that decouples the sequential connections and processes the\nintermediate activations detachedly from the pre-trained network. 2) A\nconfidence aggregation module that learns optimal strategies adaptively for\nintegrating cross-layer features. We evaluate UniPT with different backbones\n(e.g., T5, VSE$\\infty$, CLIP4Clip, Clip-ViL, and MDETR) on various\nvision-and-language and pure NLP tasks. Extensive ablations on 18 datasets have\nvalidated that UniPT can not only dramatically reduce memory consumption and\noutperform the best competitor, but also achieve competitive performance over\nother plain PETL methods with lower training memory overhead. Our code is\npublicly available at: https://github.com/Paranioar/UniPT.\n","authors":["Haiwen Diao","Bo Wan","Ying Zhang","Xu Jia","Huchuan Lu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2308.14316v2.pdf","comment":"15 pages, 11 figures, Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.06577v1","updated":"2024-03-11T10:26:38Z","published":"2024-03-11T10:26:38Z","title":"Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for\n  Distracted Driver Action Recognition","summary":"  Classification and localization of driving actions over time is important for\nadvanced driver-assistance systems and naturalistic driving studies. Temporal\nlocalization is challenging because it requires robustness, reliability, and\naccuracy. In this study, we aim to improve the temporal localization and\nclassification accuracy performance by adapting video action recognition and 2D\nhuman-pose estimation networks to one model. Therefore, we design a\ntransformer-based fusion architecture to effectively combine 2D-pose features\nand spatio-temporal features. The model uses 2D-pose features as the positional\nembedding of the transformer architecture and spatio-temporal features as the\nmain input to the encoder of the transformer. The proposed solution is generic\nand independent of the camera numbers and positions, giving frame-based class\nprobabilities as output. Finally, the post-processing step combines information\nfrom different camera views to obtain final predictions and eliminate false\npositives. The model performs well on the A2 test set of the 2023 NVIDIA AI\nCity Challenge for naturalistic driving action recognition, achieving the\noverlap score of the organizer-defined distracted driver behaviour metric of\n0.5079.\n","authors":["Erkut Akdag","Zeqi Zhu","Egor Bondarev","Peter H. N. De With"],"pdf_url":"https://arxiv.org/pdf/2403.06577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12577v3","updated":"2024-03-11T10:25:58Z","published":"2023-07-24T07:49:01Z","title":"PRIOR: Prototype Representation Joint Learning from Medical Images and\n  Reports","summary":"  Contrastive learning based vision-language joint pre-training has emerged as\na successful representation learning strategy. In this paper, we present a\nprototype representation learning framework incorporating both global and local\nalignment between medical images and reports. In contrast to standard global\nmulti-modality alignment methods, we employ a local alignment module for\nfine-grained representation. Furthermore, a cross-modality conditional\nreconstruction module is designed to interchange information across modalities\nin the training phase by reconstructing masked images and reports. For\nreconstructing long reports, a sentence-wise prototype memory bank is\nconstructed, enabling the network to focus on low-level localized visual and\nhigh-level clinical linguistic features. Additionally, a non-auto-regressive\ngeneration paradigm is proposed for reconstructing non-sequential reports.\nExperimental results on five downstream tasks, including supervised\nclassification, zero-shot classification, image-to-text retrieval, semantic\nsegmentation, and object detection, show the proposed method outperforms other\nstate-of-the-art methods across multiple datasets and under different dataset\nsize settings. The code is available at https://github.com/QtacierP/PRIOR.\n","authors":["Pujin Cheng","Li Lin","Junyan Lyu","Yijin Huang","Wenhan Luo","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2307.12577v3.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2403.06567v1","updated":"2024-03-11T10:06:45Z","published":"2024-03-11T10:06:45Z","title":"Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology","summary":"  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n","authors":["Stefan Denner","David Zimmerer","Dimitrios Bounias","Markus Bujotzek","Shuhan Xiao","Lisa Kausch","Philipp Schader","Tobias Penzkofer","Paul F. Jäger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.06567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06552v1","updated":"2024-03-11T09:53:19Z","published":"2024-03-11T09:53:19Z","title":"Detection of Object Throwing Behavior in Surveillance Videos","summary":"  Anomalous behavior detection is a challenging research area within computer\nvision. Progress in this area enables automated detection of dangerous behavior\nusing surveillance camera feeds. A dangerous behavior that is often overlooked\nin other research is the throwing action in traffic flow, which is one of the\nunique requirements of our Smart City project to enhance public safety. This\npaper proposes a solution for throwing action detection in surveillance videos\nusing deep learning. At present, datasets for throwing actions are not publicly\navailable. To address the use-case of our Smart City project, we first generate\nthe novel public 'Throwing Action' dataset, consisting of 271 videos of\nthrowing actions performed by traffic participants, such as pedestrians,\nbicyclists, and car drivers, and 130 normal videos without throwing actions.\nSecond, we compare the performance of different feature extractors for our\nanomaly detection method on the UCF-Crime and Throwing-Action datasets. The\nexplored feature extractors are the Convolutional 3D (C3D) network, the\nInflated 3D ConvNet (I3D) network, and the Multi-Fiber Network (MFNet).\nFinally, the performance of the anomaly detection algorithm is improved by\napplying the Adam optimizer instead of Adadelta, and proposing a mean normal\nloss function that covers the multitude of normal situations in traffic. Both\naspects yield better anomaly detection performance. Besides this, the proposed\nmean normal loss function lowers the false alarm rate on the combined dataset.\nThe experimental results reach an area under the ROC curve of 86.10 for the\nThrowing-Action dataset, and 80.13 on the combined dataset, respectively.\n","authors":["Ivo P. C. Kersten","Erkut Akdag","Egor Bondarev","Peter H. N. De With"],"pdf_url":"https://arxiv.org/pdf/2403.06552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06546v1","updated":"2024-03-11T09:46:41Z","published":"2024-03-11T09:46:41Z","title":"OMH: Structured Sparsity via Optimally Matched Hierarchy for\n  Unsupervised Semantic Segmentation","summary":"  Unsupervised Semantic Segmentation (USS) involves segmenting images without\nrelying on predefined labels, aiming to alleviate the burden of extensive human\nlabeling. Existing methods utilize features generated by self-supervised models\nand specific priors for clustering. However, their clustering objectives are\nnot involved in the optimization of the features during training. Additionally,\ndue to the lack of clear class definitions in USS, the resulting segments may\nnot align well with the clustering objective. In this paper, we introduce a\nnovel approach called Optimally Matched Hierarchy (OMH) to simultaneously\naddress the above issues. The core of our method lies in imposing structured\nsparsity on the feature space, which allows the features to encode information\nwith different levels of granularity. The structure of this sparsity stems from\nour hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy\namong parallel clusters through Optimal Transport. Our OMH yields better\nunsupervised segmentation performance compared to existing USS methods. Our\nextensive experiments demonstrate the benefits of OMH when utilizing our\ndifferentiable paradigm. We will make our code publicly available.\n","authors":["Baran Ozaydin","Tong Zhang","Deblina Bhattacharjee","Sabine Süsstrunk","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2403.06546v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.06545v1","updated":"2024-03-11T09:45:34Z","published":"2024-03-11T09:45:34Z","title":"ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico\n  Data Generation","summary":"  The creation of in-silico datasets can expand the utility of existing\nannotations to new domains with different staining patterns in computational\npathology. As such, it has the potential to significantly lower the cost\nassociated with building large and pixel precise datasets needed to train\nsupervised deep learning models. We propose a novel approach for the generation\nof in-silico immunohistochemistry (IHC) images by disentangling morphology\nspecific IHC stains into separate image channels in immunofluorescence (IF)\nimages. The proposed approach qualitatively and quantitatively outperforms\nbaseline methods as proven by training nucleus segmentation models on the\ncreated in-silico datasets.\n","authors":["Dominik Winter","Nicolas Triltsch","Philipp Plewa","Marco Rosati","Thomas Padel","Ross Hill","Markus Schick","Nicolas Brieu"],"pdf_url":"https://arxiv.org/pdf/2403.06545v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2309.14859v2","updated":"2024-03-11T09:39:33Z","published":"2023-09-26T11:36:26Z","title":"Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to\n  Model Evaluation","summary":"  Text-to-image generative models have garnered immense attention for their\nability to produce high-fidelity images from text prompts. Among these, Stable\nDiffusion distinguishes itself as a leading open-source model in this\nfast-growing field. However, the intricacies of fine-tuning these models pose\nmultiple challenges from new methodology integration to systematic evaluation.\nAddressing these issues, this paper introduces LyCORIS (Lora beYond\nConventional methods, Other Rank adaptation Implementations for Stable\ndiffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library\nthat offers a wide selection of fine-tuning methodologies for Stable Diffusion.\nFurthermore, we present a thorough framework for the systematic assessment of\nvaried fine-tuning techniques. This framework employs a diverse suite of\nmetrics and delves into multiple facets of fine-tuning, including\nhyperparameter adjustments and the evaluation with different prompt types\nacross various concept categories. Through this comprehensive approach, our\nwork provides essential insights into the nuanced effects of fine-tuning\nparameters, bridging the gap between state-of-the-art research and practical\napplication.\n","authors":["Shih-Ying Yeh","Yu-Guan Hsieh","Zhidong Gao","Bernard B W Yang","Giyeong Oh","Yanmin Gong"],"pdf_url":"https://arxiv.org/pdf/2309.14859v2.pdf","comment":"In International Conference on Learning Representations 12 (ICLR\n  2024) [79 pages, 54 figures, 7 tables]"},{"id":"http://arxiv.org/abs/2403.04954v2","updated":"2024-03-11T09:37:39Z","published":"2024-03-07T23:44:10Z","title":"Fooling Neural Networks for Motion Forecasting via Adversarial Attacks","summary":"  Human motion prediction is still an open problem, which is extremely\nimportant for autonomous driving and safety applications. Although there are\ngreat advances in this area, the widely studied topic of adversarial attacks\nhas not been applied to multi-regression models such as GCNs and MLP-based\narchitectures in human motion prediction. This work intends to reduce this gap\nusing extensive quantitative and qualitative experiments in state-of-the-art\narchitectures similar to the initial stages of adversarial attacks in image\nclassification. The results suggest that models are susceptible to attacks even\non low levels of perturbation. We also show experiments with 3D transformations\nthat affect the model performance, in particular, we show that most models are\nsensitive to simple rotations and translations which do not alter joint\ndistances. We conclude that similar to earlier CNN models, motion forecasting\ntasks are susceptible to small perturbations and simple 3D transformations.\n","authors":["Edgar Medina","Leyong Loh"],"pdf_url":"https://arxiv.org/pdf/2403.04954v2.pdf","comment":"11 pages, 8 figures, VISSAP 2024"},{"id":"http://arxiv.org/abs/2403.06538v1","updated":"2024-03-11T09:29:44Z","published":"2024-03-11T09:29:44Z","title":"3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and\n  Lidar Data","summary":"  Reflective surfaces present a persistent challenge for reliable 3D mapping\nand perception in robotics and autonomous systems. However, existing reflection\ndatasets and benchmarks remain limited to sparse 2D data. This paper introduces\nthe first large-scale 3D reflection detection dataset containing more than\n50,000 aligned samples of multi-return Lidar, RGB images, and 2D/3D semantic\nlabels across diverse indoor environments with various reflections. Textured 3D\nground truth meshes enable automatic point cloud labeling to provide precise\nground truth annotations. Detailed benchmarks evaluate three Lidar point cloud\nsegmentation methods, as well as current state-of-the-art image segmentation\nnetworks for glass and mirror detection. The proposed dataset advances\nreflection detection by providing a comprehensive testbed with precise global\nalignment, multi-modal data, and diverse reflective objects and materials. It\nwill drive future research towards reliable reflection detection. The dataset\nis publicly available at http://3dref.github.io\n","authors":["Xiting Zhao","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2403.06538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06536v1","updated":"2024-03-11T09:23:20Z","published":"2024-03-11T09:23:20Z","title":"Multi-Scale Implicit Transformer with Re-parameterize for\n  Arbitrary-Scale Super-Resolution","summary":"  Recently, the methods based on implicit neural representations have shown\nexcellent capabilities for arbitrary-scale super-resolution (ASSR). Although\nthese methods represent the features of an image by generating latent codes,\nthese latent codes are difficult to adapt for different magnification factors\nof super-resolution, which seriously affects their performance. Addressing\nthis, we design Multi-Scale Implicit Transformer (MSIT), consisting of an\nMulti-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among\nthem, MSNO obtains multi-scale latent codes through feature enhancement,\nmulti-scale characteristics extraction, and multi-scale characteristics\nmerging. MSSA further enhances the multi-scale characteristics of latent codes,\nresulting in better performance. Furthermore, to improve the performance of\nnetwork, we propose the Re-Interaction Module (RIM) combined with the\ncumulative training strategy to improve the diversity of learned information\nfor the network. We have systematically introduced multi-scale characteristics\nfor the first time in ASSR, extensive experiments are performed to validate the\neffectiveness of MSIT, and our method achieves state-of-the-art performance in\narbitrary super-resolution tasks.\n","authors":["Jinchen Zhu","Mingjian Zhang","Ling Zheng","Shizhuang Weng"],"pdf_url":"https://arxiv.org/pdf/2403.06536v1.pdf","comment":"Super-resolution, Arbitrary-Scale Super-Resolution, Multi-Scale,\n  Transformer"},{"id":"http://arxiv.org/abs/2303.16058v2","updated":"2024-03-11T09:21:50Z","published":"2023-03-28T15:39:28Z","title":"Unmasked Teacher: Towards Training-Efficient Video Foundation Models","summary":"  Video Foundation Models (VFMs) have received limited exploration due to high\ncomputational costs and data scarcity. Previous VFMs rely on Image Foundation\nModels (IFMs), which face challenges in transferring to the video domain.\nAlthough VideoMAE has trained a robust ViT from limited data, its low-level\nreconstruction poses convergence difficulties and conflicts with high-level\ncross-modal alignment. This paper proposes a training-efficient method for\ntemporal-sensitive VFMs that integrates the benefits of existing methods. To\nincrease data efficiency, we mask out most of the low-semantics video tokens,\nbut selectively align the unmasked tokens with IFM, which serves as the\nUnMasked Teacher (UMT). By providing semantic guidance, our method enables\nfaster convergence and multimodal friendliness. With a progressive pre-training\nframework, our model can handle various tasks including scene-related,\ntemporal-related, and complex video-language understanding. Using only public\nsources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16\nachieves state-of-the-art performances on various video tasks. The code and\nmodels will be released at https://github.com/OpenGVLab/unmasked_teacher.\n","authors":["Kunchang Li","Yali Wang","Yizhuo Li","Yi Wang","Yinan He","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16058v2.pdf","comment":"ICCV2023"},{"id":"http://arxiv.org/abs/2403.06534v1","updated":"2024-03-11T09:20:40Z","published":"2024-03-11T09:20:40Z","title":"SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale\n  SAR Object Detection","summary":"  Synthetic Aperture Radar (SAR) object detection has gained significant\nattention recently due to its irreplaceable all-weather imaging capabilities.\nHowever, this research field suffers from both limited public datasets (mostly\ncomprising <2K images with only mono-category objects) and inaccessible source\ncode. To tackle these challenges, we establish a new benchmark dataset and an\nopen-source method for large-scale SAR object detection. Our dataset,\nSARDet-100K, is a result of intense surveying, collecting, and standardizing 10\nexisting SAR detection datasets, providing a large-scale and diverse dataset\nfor research purposes. To the best of our knowledge, SARDet-100K is the first\nCOCO-level large-scale multi-class SAR object detection dataset ever created.\nWith this high-quality dataset, we conducted comprehensive experiments and\nuncovered a crucial challenge in SAR object detection: the substantial\ndisparities between the pretraining on RGB datasets and finetuning on SAR\ndatasets in terms of both data domain and model structure. To bridge these\ngaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)\npretraining framework that tackles the problems from the perspective of data\ninput, domain transition, and model migration. The proposed MSFA method\nsignificantly enhances the performance of SAR object detection models while\ndemonstrating exceptional generalizability and flexibility across diverse\nmodels. This work aims to pave the way for further advancements in SAR object\ndetection. The dataset and code is available at\nhttps://github.com/zcablii/SARDet_100K.\n","authors":["Yuxuan Li","Xiang Li","Weijie Li","Qibin Hou","Li Liu","Ming-Ming Cheng","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2403.06534v1.pdf","comment":"22 Pages, 10 Figures, 9 Tables"},{"id":"http://arxiv.org/abs/2403.06532v1","updated":"2024-03-11T09:19:09Z","published":"2024-03-11T09:19:09Z","title":"Reconstructing Visual Stimulus Images from EEG Signals Based on Deep\n  Visual Representation Model","summary":"  Reconstructing visual stimulus images is a significant task in neural\ndecoding, and up to now, most studies consider the functional magnetic\nresonance imaging (fMRI) as the signal source. However, the fMRI-based image\nreconstruction methods are difficult to widely applied because of the\ncomplexity and high cost of the acquisition equipments. Considering the\nadvantages of low cost and easy portability of the electroencephalogram (EEG)\nacquisition equipments, we propose a novel image reconstruction method based on\nEEG signals in this paper. Firstly, to satisfy the high recognizability of\nvisual stimulus images in fast switching manner, we build a visual stimuli\nimage dataset, and obtain the EEG dataset by a corresponding EEG signals\ncollection experiment. Secondly, the deep visual representation model(DVRM)\nconsisting of a primary encoder and a subordinate decoder is proposed to\nreconstruct visual stimuli. The encoder is designed based on the\nresidual-in-residual dense blocks to learn the distribution characteristics\nbetween EEG signals and visual stimulus images, while the decoder is designed\nbased on the deep neural network to reconstruct the visual stimulus image from\nthe learned deep visual representation. The DVRM can fit the deep and multiview\nvisual features of human natural state and make the reconstructed images more\nprecise. Finally, we evaluate the DVRM in the quality of the generated images\non our EEG dataset. The results show that the DVRM have good performance in the\ntask of learning deep visual representation from EEG signals and generating\nreconstructed images that are realistic and highly resemble the original\nimages.\n","authors":["Hongguang Pan","Zhuoyi Li","Yunpeng Fu","Xuebin Qin","Jianchen Hu"],"pdf_url":"https://arxiv.org/pdf/2403.06532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06529v1","updated":"2024-03-11T09:12:24Z","published":"2024-03-11T09:12:24Z","title":"Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis","summary":"  2D face recognition encounters challenges in unconstrained environments due\nto varying illumination, occlusion, and pose. Recent studies focus on RGB-D\nface recognition to improve robustness by incorporating depth information.\nHowever, collecting sufficient paired RGB-D training data is expensive and\ntime-consuming, hindering wide deployment. In this work, we first construct a\ndiverse depth dataset generated by 3D Morphable Models for depth model\npre-training. Then, we propose a domain-independent pre-training framework that\nutilizes readily available pre-trained RGB and depth models to separately\nperform face recognition without needing additional paired data for retraining.\nTo seamlessly integrate the two distinct networks and harness the complementary\nbenefits of RGB and depth information for improved accuracy, we propose an\ninnovative Adaptive Confidence Weighting (ACW). This mechanism is designed to\nlearn confidence estimates for each modality to achieve modality fusion at the\nscore level. Our method is simple and lightweight, only requiring ACW training\nbeyond the backbone models. Experiments on multiple public RGB-D face\nrecognition benchmarks demonstrate state-of-the-art performance surpassing\nprevious methods based on depth estimation and feature fusion, validating the\nefficacy of our approach.\n","authors":["Zijian Chen","Mei Wang","Weihong Deng","Hongzhi Shi","Dongchao Wen","Yingjie Zhang","Xingchen Cui","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.06529v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.12070v2","updated":"2024-03-11T09:11:44Z","published":"2023-07-22T12:37:34Z","title":"Fast and Stable Diffusion Inverse Solver with History Gradient Update","summary":"  Diffusion models have recently been recognised as efficient inverse problem\nsolvers due to their ability to produce high-quality reconstruction results\nwithout relying on pairwise data training. Existing diffusion-based solvers\nutilize Gradient Descent strategy to get a optimal sample solution. However,\nthese solvers only calculate the current gradient and have not utilized any\nhistory information of sampling process, thus resulting in unstable\noptimization progresses and suboptimal solutions. To address this issue, we\npropose to utilize the history information of the diffusion-based inverse\nsolvers. In this paper, we first prove that, in previous work, using the\ngradient descent method to optimize the data fidelity term is convergent.\nBuilding on this, we introduce the incorporation of historical gradients into\nthis optimization process, termed History Gradient Update (HGU). We also\nprovide theoretical evidence that HGU ensures the convergence of the entire\nalgorithm. It's worth noting that HGU is applicable to both pixel-based and\nlatent-based diffusion model solvers. Experimental results demonstrate that,\ncompared to previous sampling algorithms, sampling algorithms with HGU achieves\nstate-of-the-art results in medical image reconstruction, surpassing even\nsupervised learning methods. Additionally, it achieves competitive results on\nnatural images.\n","authors":["Linchao He","Hongyu Yan","Mengting Luo","Hongjie Wu","Kunming Luo","Wang Wang","Wenchao Du","Hu Chen","Hongyu Yang","Yi Zhang","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2307.12070v2.pdf","comment":"17 pages, 7 figures. Provision of theoretical proofs to demonstrate\n  the convergence of the methods"},{"id":"http://arxiv.org/abs/2401.13934v3","updated":"2024-03-11T08:46:23Z","published":"2024-01-25T04:16:45Z","title":"MambaMorph: a Mamba-based Framework for Medical MR-CT Deformable\n  Registration","summary":"  Capturing voxel-wise spatial correspondence across distinct modalities is\ncrucial for medical image analysis. However, current registration approaches\nare not practical enough in terms of registration accuracy and clinical\napplicability. In this paper, we introduce MambaMorph, a novel multi-modality\ndeformable registration framework. Specifically, MambaMorph utilizes a\nMamba-based registration module and a fine-grained, yet simple, feature\nextractor for efficient long-range correspondence modeling and high-dimensional\nfeature learning, respectively. Additionally, we develop a well-annotated brain\nMR-CT registration dataset, SR-Reg, to address the scarcity of data in\nmulti-modality registration. To validate MambaMorph's multi-modality\nregistration capabilities, we conduct quantitative experiments on both our\nSR-Reg dataset and a public T1-T2 dataset. The experimental results on both\ndatasets demonstrate that MambaMorph significantly outperforms the current\nstate-of-the-art learning-based registration methods in terms of registration\naccuracy. Further study underscores the efficiency of the Mamba-based\nregistration module and the lightweight feature extractor, which achieve\nnotable registration quality while maintaining reasonable computational costs\nand speeds. We believe that MambaMorph holds significant potential for\npractical applications in medical image registration. The code for MambaMorph\nis available at: https://github.com/Guo-Stone/MambaMorph.\n","authors":["Tao Guo","Yinuo Wang","Shihao Shu","Diansheng Chen","Zhouping Tang","Cai Meng","Xiangzhi Bai"],"pdf_url":"https://arxiv.org/pdf/2401.13934v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06517v1","updated":"2024-03-11T08:45:31Z","published":"2024-03-11T08:45:31Z","title":"Active Generation for Image Classification","summary":"  Recently, the growing capabilities of deep generative models have underscored\ntheir potential in enhancing image classification accuracy. However, existing\nmethods often demand the generation of a disproportionately large number of\nimages compared to the original dataset, while having only marginal\nimprovements in accuracy. This computationally expensive and time-consuming\nprocess hampers the practicality of such approaches. In this paper, we propose\nto address the efficiency of image generation by focusing on the specific needs\nand characteristics of the model. With a central tenet of active learning, our\nmethod, named ActGen, takes a training-aware approach to image generation. It\naims to create images akin to the challenging or misclassified samples\nencountered by the current model and incorporates these generated images into\nthe training set to augment model performance. ActGen introduces an attentive\nimage guidance technique, using real images as guides during the denoising\nprocess of a diffusion model. The model's attention on class prompt is\nleveraged to ensure the preservation of similar foreground object while\ndiversifying the background. Furthermore, we introduce a gradient-based\ngeneration guidance method, which employs two losses to generate more\nchallenging samples and prevent the generated images from being too similar to\npreviously generated ones. Experimental results on the CIFAR and ImageNet\ndatasets demonstrate that our method achieves better performance with a\nsignificantly reduced number of generated images.\n","authors":["Tao Huang","Jiaqi Liu","Shan You","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.06517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06516v1","updated":"2024-03-11T08:43:57Z","published":"2024-03-11T08:43:57Z","title":"Advancing Text-Driven Chest X-Ray Generation with Policy-Based\n  Reinforcement Learning","summary":"  Recent advances in text-conditioned image generation diffusion models have\nbegun paving the way for new opportunities in modern medical domain, in\nparticular, generating Chest X-rays (CXRs) from diagnostic reports.\nNonetheless, to further drive the diffusion models to generate CXRs that\nfaithfully reflect the complexity and diversity of real data, it has become\nevident that a nontrivial learning approach is needed. In light of this, we\npropose CXRL, a framework motivated by the potential of reinforcement learning\n(RL). Specifically, we integrate a policy gradient RL approach with\nwell-designed multiple distinctive CXR-domain specific reward models. This\napproach guides the diffusion denoising trajectory, achieving precise CXR\nposture and pathological details. Here, considering the complex medical image\nenvironment, we present \"RL with Comparative Feedback\" (RLCF) for the reward\nmechanism, a human-like comparative evaluation that is known to be more\neffective and reliable in complex scenarios compared to direct evaluation. Our\nCXRL framework includes jointly optimizing learnable adaptive condition\nembeddings (ACE) and the image generator, enabling the model to produce more\naccurate and higher perceptual CXR quality. Our extensive evaluation of the\nMIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning\napproach. Consequently, our CXRL generates pathologically realistic CXRs,\nestablishing a new standard for generating CXRs with high fidelity to\nreal-world clinical scenarios.\n","authors":["Woojung Han","Chanyoung Kim","Dayun Ju","Yumin Shim","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.06516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06514v1","updated":"2024-03-11T08:40:37Z","published":"2024-03-11T08:40:37Z","title":"Structure Your Data: Towards Semantic Graph Counterfactuals","summary":"  Counterfactual explanations (CEs) based on concepts are explanations that\nconsider alternative scenarios to understand which high-level semantic features\ncontributed to particular model predictions. In this work, we propose CEs based\non the semantic graphs accompanying input data to achieve more descriptive,\naccurate, and human-aligned explanations. Building upon state-of-the-art (SoTA)\nconceptual attempts, we adopt a model-agnostic edit-based approach and\nintroduce leveraging GNNs for efficient Graph Edit Distance (GED) computation.\nWith a focus on the visual domain, we represent images as scene graphs and\nobtain their GNN embeddings to bypass solving the NP-hard graph similarity\nproblem for all input pairs, an integral part of the CE computation process. We\napply our method to benchmark and real-world datasets with varying difficulty\nand availability of semantic annotations. Testing on diverse classifiers, we\nfind that our CEs outperform previous SoTA explanation models based on\nsemantics, including both white and black-box as well as conceptual and\npixel-level approaches. Their superiority is proven quantitatively and\nqualitatively, as validated by human subjects, highlighting the significance of\nleveraging semantic edges in the presence of intricate relationships. Our\nmodel-agnostic graph-based approach is widely applicable and easily extensible,\nproducing actionable explanations across different contexts.\n","authors":["Angeliki Dimitriou","Maria Lymperaiou","Giorgos Filandrianos","Konstantinos Thomas","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2403.06514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06510v1","updated":"2024-03-11T08:37:03Z","published":"2024-03-11T08:37:03Z","title":"Skeleton Supervised Airway Segmentation","summary":"  Fully-supervised airway segmentation has accomplished significant triumphs\nover the years in aiding pre-operative diagnosis and intra-operative\nnavigation. However, full voxel-level annotation constitutes a labor-intensive\nand time-consuming task, often plagued by issues such as missing branches,\nbranch annotation discontinuity, or erroneous edge delineation. label-efficient\nsolutions for airway extraction are rarely explored yet primarily demanding in\nmedical practice. To this end, we introduce a novel skeleton-level annotation\n(SkA) tailored to the airway, which simplifies the annotation workflow while\nenhancing annotation consistency and accuracy, preserving the complete\ntopology. Furthermore, we propose a skeleton-supervised learning framework to\nachieve accurate airway segmentation. Firstly, a dual-stream buffer inference\nis introduced to realize initial label propagation from SkA, avoiding the\ncollapse of direct learning from SkA. Then, we construct a geometry-aware\ndual-path propagation framework (GDP) to further promote complementary\npropagation learning, composed of hard geometry-aware propagation learning and\nsoft geometry-aware propagation guidance. Experiments reveal that our proposed\nframework outperforms the competing methods with SKA, which amounts to only\n1.96% airways, and achieves comparable performance with the baseline model that\nis fully supervised with 100% airways, demonstrating its significant potential\nin achieving label-efficient segmentation for other tubular structures, such as\nvessels.\n","authors":["Mingyue Zhao","Han Li","Li Fan","Shiyuan Liu","Xiaolan Qiu","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.06510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06505v1","updated":"2024-03-11T08:28:51Z","published":"2024-03-11T08:28:51Z","title":"Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis","summary":"  The neural radiance field (NeRF) has emerged as a prominent methodology for\nsynthesizing realistic images of novel views. While neural radiance\nrepresentations based on voxels or mesh individually offer distinct advantages,\nexcelling in either rendering quality or speed, each has limitations in the\nother aspect. In response, we propose a pioneering hybrid representation named\nVosh, seamlessly combining both voxel and mesh components in hybrid rendering\nfor view synthesis. Vosh is meticulously crafted by optimizing the voxel grid\nof NeRF, strategically with selected voxels replaced by mesh. Therefore, it\nexcels in fast rendering scenes with simple geometry and textures through its\nmesh component, while simultaneously enabling high-quality rendering in\nintricate regions by leveraging voxel component. The flexibility of Vosh is\nshowcased through the ability to adjust hybrid ratios, providing users the\nability to control the balance between rendering quality and speed based on\nflexible usage. Experimental results demonstrates that our method achieves\ncommendable trade-off between rendering quality and speed, and notably has\nreal-time performance on mobile devices.\n","authors":["Chenhao Zhang","Yongyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06501v1","updated":"2024-03-11T08:17:56Z","published":"2024-03-11T08:17:56Z","title":"3D Semantic Segmentation-Driven Representations for 3D Object Detection","summary":"  In autonomous driving, 3D detection provides more precise information to\ndownstream tasks, including path planning and motion estimation, compared to 2D\ndetection. Therefore, the need for 3D detection research has emerged. However,\nalthough single and multi-view images and depth maps obtained from the camera\nwere used, detection accuracy was relatively low compared to other\nmodality-based detectors due to the lack of geometric information. The proposed\nmulti-modal 3D object detection combines semantic features obtained from images\nand geometric features obtained from point clouds, but there are difficulties\nin defining unified representation to fuse data existing in different domains\nand synchronization between them. In this paper, we propose SeSame : point-wise\nsemantic feature as a new presentation to ensure sufficient semantic\ninformation of the existing LiDAR-only based 3D detection. Experiments show\nthat our approach outperforms previous state-of-the-art at different levels of\ndifficulty in car and performance improvement on the KITTI object detection\nbenchmark. Our code is available at https://github.com/HAMA-DL-dev/SeSame\n","authors":["Hayeon O","Kunsoo Huh"],"pdf_url":"https://arxiv.org/pdf/2403.06501v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.06498v1","updated":"2024-03-11T08:11:46Z","published":"2024-03-11T08:11:46Z","title":"Incorporating Improved Sinusoidal Threshold-based Semi-supervised Method\n  and Diffusion Models for Osteoporosis Diagnosis","summary":"  Osteoporosis is a common skeletal disease that seriously affects patients'\nquality of life. Traditional osteoporosis diagnosis methods are expensive and\ncomplex. The semi-supervised model based on diffusion model and class threshold\nsinusoidal decay proposed in this paper can automatically diagnose osteoporosis\nbased on patient's imaging data, which has the advantages of convenience,\naccuracy, and low cost. Unlike previous semi-supervised models, all the\nunlabeled data used in this paper are generated by the diffusion model.\nCompared with real unlabeled data, synthetic data generated by the diffusion\nmodel show better performance. In addition, this paper proposes a novel\npseudo-label threshold adjustment mechanism, Sinusoidal Threshold Decay, which\ncan make the semi-supervised model converge more quickly and improve its\nperformance. Specifically, the method is tested on a dataset including 749\ndental panoramic images, and its achieved leading detect performance and\nproduces a 80.10% accuracy.\n","authors":["Wenchi Ke"],"pdf_url":"https://arxiv.org/pdf/2403.06498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06497v1","updated":"2024-03-11T08:09:30Z","published":"2024-03-11T08:09:30Z","title":"QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven\n  Fine Tuning","summary":"  Transformer-based models have gained widespread popularity in both the\ncomputer vision (CV) and natural language processing (NLP) fields. However,\nsignificant challenges arise during post-training linear quantization, leading\nto noticeable reductions in inference accuracy. Our study focuses on uncovering\nthe underlying causes of these accuracy drops and proposing a\nquantization-friendly fine-tuning method, \\textbf{QuantTune}. Firstly, our\nanalysis revealed that, on average, 65\\% of quantization errors result from the\nprecision loss incurred by the dynamic range amplification effect of outliers\nacross the target Transformer-based models. Secondly, \\textbf{QuantTune}\nadjusts weights based on the deviation of outlier activations and effectively\nconstrains the dynamic ranges of the problematic activations. As a result, it\nsuccessfully mitigates the negative impact of outliers on the inference\naccuracy of quantized models. Lastly, \\textbf{QuantTune} can be seamlessly\nintegrated into the back-propagation pass in the fine-tuning process without\nrequiring extra complexity in inference software and hardware design. Our\napproach showcases significant improvements in post-training quantization\nacross a range of Transformer-based models, including ViT, Bert-base, and OPT.\nQuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at\n7-bit compared to top calibration methods, outperforming state-of-the-art\nsolutions by over 18.84\\% across ViT models.\n","authors":["Jiun-Man Chen","Yu-Hsuan Chao","Yu-Jie Wang","Ming-Der Shieh","Chih-Chung Hsu","Wei-Fen Lin"],"pdf_url":"https://arxiv.org/pdf/2403.06497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06495v1","updated":"2024-03-11T08:07:46Z","published":"2024-03-11T08:07:46Z","title":"Toward Generalist Anomaly Detection via In-context Residual Learning\n  with Few-shot Sample Prompts","summary":"  This paper explores the problem of Generalist Anomaly Detection (GAD), aiming\nto train one single detection model that can generalize to detect anomalies in\ndiverse datasets from different application domains without any further\ntraining on the target data. Some recent studies have shown that large\npre-trained Visual-Language Models (VLMs) like CLIP have strong generalization\ncapabilities on detecting industrial defects from various datasets, but their\nmethods rely heavily on handcrafted text prompts about defects, making them\ndifficult to generalize to anomalies in other applications, e.g., medical image\nanomalies or semantic anomalies in natural images. In this work, we propose to\ntrain a GAD model with few-shot normal images as sample prompts for AD on\ndiverse datasets on the fly. To this end, we introduce a novel approach that\nlearns an in-context residual learning model for GAD, termed InCTRL. It is\ntrained on an auxiliary dataset to discriminate anomalies from normal samples\nbased on a holistic evaluation of the residuals between query images and\nfew-shot normal sample prompts. Regardless of the datasets, per definition of\nanomaly, larger residuals are expected for anomalies than normal samples,\nthereby enabling InCTRL to generalize across different domains without further\ntraining.\n","authors":["Jiawen Zhu","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2403.06495v1.pdf","comment":"Accepted to CVPR 2024; 17 pages; 5 figures"},{"id":"http://arxiv.org/abs/2312.16245v2","updated":"2024-03-11T07:52:22Z","published":"2023-12-25T11:48:55Z","title":"iKUN: Speak to Trackers without Retraining","summary":"  Referring multi-object tracking (RMOT) aims to track multiple objects based\non input textual descriptions. Previous works realize it by simply integrating\nan extra textual module into the multi-object tracker. However, they typically\nneed to retrain the entire framework and have difficulties in optimization. In\nthis work, we propose an insertable Knowledge Unification Network, termed iKUN,\nto enable communication with off-the-shelf trackers in a plug-and-play manner.\nConcretely, a knowledge unification module (KUM) is designed to adaptively\nextract visual features based on textual guidance. Meanwhile, to improve the\nlocalization accuracy, we present a neural version of Kalman filter (NKF) to\ndynamically adjust process noise and observation noise based on the current\nmotion status. Moreover, to address the problem of open-set long-tail\ndistribution of textual descriptions, a test-time similarity calibration method\nis proposed to refine the confidence score with pseudo frequency. Extensive\nexperiments on Refer-KITTI dataset verify the effectiveness of our framework.\nFinally, to speed up the development of RMOT, we also contribute a more\nchallenging dataset, Refer-Dance, by extending public DanceTrack dataset with\nmotion and dressing descriptions. The codes and dataset are available at\nhttps://github.com/dyhBUPT/iKUN.\n","authors":["Yunhao Du","Cheng Lei","Zhicheng Zhao","Fei Su"],"pdf_url":"https://arxiv.org/pdf/2312.16245v2.pdf","comment":"CVPR 2024 camera-ready"},{"id":"http://arxiv.org/abs/2403.06488v1","updated":"2024-03-11T07:50:40Z","published":"2024-03-11T07:50:40Z","title":"Query-guided Prototype Evolution Network for Few-Shot Segmentation","summary":"  Previous Few-Shot Segmentation (FSS) approaches exclusively utilize support\nfeatures for prototype generation, neglecting the specific requirements of the\nquery. To address this, we present the Query-guided Prototype Evolution Network\n(QPENet), a new method that integrates query features into the generation\nprocess of foreground and background prototypes, thereby yielding customized\nprototypes attuned to specific queries. The evolution of the foreground\nprototype is accomplished through a \\textit{support-query-support} iterative\nprocess involving two new modules: Pseudo-prototype Generation (PPG) and Dual\nPrototype Evolution (DPE). The PPG module employs support features to create an\ninitial prototype for the preliminary segmentation of the query image,\nresulting in a pseudo-prototype reflecting the unique needs of the current\nquery. Subsequently, the DPE module performs reverse segmentation on support\nimages using this pseudo-prototype, leading to the generation of evolved\nprototypes, which can be considered as custom solutions. As for the background\nprototype, the evolution begins with a global background prototype that\nrepresents the generalized features of all training images. We also design a\nGlobal Background Cleansing (GBC) module to eliminate potential adverse\ncomponents mirroring the characteristics of the current foreground class.\nExperimental results on the PASCAL-$5^i$ and COCO-$20^i$ datasets attest to the\nsubstantial enhancements achieved by QPENet over prevailing state-of-the-art\ntechniques, underscoring the validity of our ideas.\n","authors":["Runmin Cong","Hang Xiong","Jinpeng Chen","Wei Zhang","Qingming Huang","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.06488v1.pdf","comment":"Accepted by IEEE TMM 2024"},{"id":"http://arxiv.org/abs/2403.06479v1","updated":"2024-03-11T07:42:40Z","published":"2024-03-11T07:42:40Z","title":"Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template\n  Matching","summary":"  Soft tissue tracking is crucial for computer-assisted interventions. Existing\napproaches mainly rely on extracting discriminative features from the template\nand videos to recover corresponding matches. However, it is difficult to adopt\nthese techniques in surgical scenes, where tissues are changing in shape and\nappearance throughout the surgery. To address this problem, we exploit optical\nflow to naturally capture the pixel-wise tissue deformations and adaptively\ncorrect the tracked template. Specifically, we first implement an inter-frame\nmatching mechanism to extract a coarse region of interest based on optical flow\nfrom consecutive frames. To accommodate appearance change and alleviate drift,\nwe then propose an adaptive-template matching method, which updates the tracked\ntemplate based on the reliability of the estimates. Our approach, Ada-Tracker,\nenjoys both short-term dynamics modeling by capturing local deformations and\nlong-term dynamics modeling by introducing global temporal compensation. We\nevaluate our approach on the public SurgT benchmark, which is generated from\nHamlyn, SCARED, and Kidney boundary datasets. The experimental results show\nthat Ada-Tracker achieves superior accuracy and performs more robustly against\nprior works. Code is available at https://github.com/wrld/Ada-Tracker.\n","authors":["Jiaxin Guo","Jiangliu Wang","Zhaoshuo Li","Tongyu Jia","Qi Dou","Yun-Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05136v5","updated":"2024-03-11T07:32:31Z","published":"2023-10-08T12:10:44Z","title":"InstructDET: Diversifying Referring Object Detection with Generalized\n  Instructions","summary":"  We propose InstructDET, a data-centric method for referring object detection\n(ROD) that localizes target objects based on user instructions. While deriving\nfrom referring expressions (REC), the instructions we leverage are greatly\ndiversified to encompass common user intentions related to object detection.\nFor one image, we produce tremendous instructions that refer to every single\nobject and different combinations of multiple objects. Each instruction and its\ncorresponding object bounding boxes (bbxs) constitute one training data pair.\nIn order to encompass common detection expressions, we involve emerging\nvision-language model (VLM) and large language model (LLM) to generate\ninstructions guided by text prompts and object bbxs, as the generalizations of\nfoundation models are effective to produce human-like expressions (e.g.,\ndescribing object property, category, and relationship). We name our\nconstructed dataset as InDET. It contains images, bbxs and generalized\ninstructions that are from foundation models. Our InDET is developed from\nexisting REC datasets and object detection datasets, with the expanding\npotential that any image with object bbxs can be incorporated through using our\nInstructDET method. By using our InDET dataset, we show that a conventional ROD\nmodel surpasses existing methods on standard REC datasets and our InDET test\nset. Our data-centric method InstructDET, with automatic data expansion by\nleveraging foundation models, directs a promising field that ROD can be greatly\ndiversified to execute common object detection instructions.\n","authors":["Ronghao Dang","Jiangyan Feng","Haodong Zhang","Chongjian Ge","Lin Song","Lijun Gong","Chengju Liu","Qijun Chen","Feng Zhu","Rui Zhao","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2310.05136v5.pdf","comment":"29 pages (include Appendix) Published in ICLR"},{"id":"http://arxiv.org/abs/2312.07943v2","updated":"2024-03-11T07:25:26Z","published":"2023-12-13T07:40:39Z","title":"ReFusion: Learning Image Fusion from Reconstruction with Learnable Loss\n  via Meta-Learning","summary":"  Image fusion aims to combine information from multiple source images into a\nsingle one with more comprehensive informational content. The significant\nchallenges for deep learning-based image fusion algorithms are the lack of a\ndefinitive ground truth as well as the corresponding distance measurement, with\ncurrent manually given loss functions constrain the flexibility of model and\ngeneralizability for unified fusion tasks. To overcome these limitations, we\nintroduce a unified image fusion framework based on meta-learning, named\nReFusion, which provides a learning paradigm that obtains the optimal fusion\nloss for various fusion tasks based on reconstructing the source images.\nCompared to existing methods, ReFusion employs a parameterized loss function,\ndynamically adjusted by the training framework according to the specific\nscenario and task. ReFusion is constituted by three components: a fusion\nmodule, a loss proposal module, and a source reconstruction module. To ensure\nthe fusion module maximally preserves the information from the source images,\nenabling the reconstruction of the source images from the fused image, we adopt\na meta-learning strategy to train the loss proposal module using reconstruction\nloss. The update of the fusion module relies on the fusion loss proposed by the\nloss proposal module. The alternating updates of the three modules mutually\nfacilitate each other, aiming to propose an appropriate fusion loss for\ndifferent tasks and yield satisfactory fusion results. Extensive experiments\ndemonstrate that ReFusion is capable of adapting to various tasks, including\ninfrared-visible, medical, multi-focus, and multi-exposure image fusion. The\ncode will be released.\n","authors":["Haowen Bai","Zixiang Zhao","Jiangshe Zhang","Yichen Wu","Lilun Deng","Yukun Cui","Shuang Xu","Baisong Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.07943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06471v1","updated":"2024-03-11T07:19:29Z","published":"2024-03-11T07:19:29Z","title":"Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment\n  Network-Based Few-Shot Segmentation in Veterinary Medicine","summary":"  In the cutting-edge domain of medical artificial intelligence (AI),\nremarkable advances have been achieved in areas such as diagnosis, prediction,\nand therapeutic interventions. Despite these advances, the technology for image\nsegmentation faces the significant barrier of having to produce extensively\nannotated datasets. To address this challenge, few-shot segmentation (FSS) has\nbeen recognized as one of the innovative solutions. Although most of the FSS\nresearch has focused on human health care, its application in veterinary\nmedicine, particularly for pet care, remains largely limited. This study has\nfocused on accurate segmentation of the heart and left atrial enlargement on\ncanine chest radiographs using the proposed deep prototype alignment network\n(DPANet). The PANet architecture is adopted as the backbone model, and\nexperiments are conducted using various encoders based on VGG-19, ResNet-18,\nand ResNet-50 to extract features. Experimental results demonstrate that the\nproposed DPANet achieves the highest performance. In the 2way-1shot scenario,\nit achieves the highest intersection over union (IoU) value of 0.6966, and in\nthe 2way-5shot scenario, it achieves the highest IoU value of 0.797. The DPANet\nnot only signifies a performance improvement, but also shows an improved\ntraining speed in the 2way-5shot scenario. These results highlight our model's\nexceptional capability as a trailblazing solution for segmenting the heart and\nleft atrial enlargement in veterinary applications through FSS, setting a new\nbenchmark in veterinary AI research, and demonstrating its superior potential\nto veterinary medicine advances.\n","authors":["Jun-Young Oh","In-Gyu Lee","Tae-Eui Kam","Ji-Hoon Jeong"],"pdf_url":"https://arxiv.org/pdf/2403.06471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11570v2","updated":"2024-03-11T07:15:17Z","published":"2023-12-18T04:49:03Z","title":"Understanding the Multi-modal Prompts of the Pre-trained Vision-Language\n  Model","summary":"  Prompt learning has emerged as an efficient alternative for fine-tuning\nfoundational models, such as CLIP, for various downstream tasks. However, there\nis no work that provides a comprehensive explanation for the working mechanism\nof the multi-modal prompts. In this paper, we conduct a direct analysis of the\nmulti-modal prompts by asking the following questions: $(i)$ How do the learned\nmulti-modal prompts improve the recognition performance? $(ii)$ What do the\nmulti-modal prompts learn? To answer these questions, we begin by isolating the\ncomponent of the formula where the prompt influences the calculation of\nself-attention at each layer in two distinct ways, \\ie, $(1)$ introducing\nprompt embeddings makes the $[cls]$ token focus on foreground objects. $(2)$\nthe prompts learn a bias term during the update of token embeddings, allowing\nthe model to adapt to the target domain. Subsequently, we conduct extensive\nvisualization and statistical experiments on the eleven diverse downstream\nrecognition datasets. From the experiments, we reveal that the learned prompts\nimprove the performance mainly through the second way, which acts as the\ndataset bias to improve the recognition performance of the pre-trained model on\nthe corresponding dataset. Based on this finding, we propose the bias tuning\nway and demonstrate that directly incorporating the learnable bias outperforms\nthe learnable prompts in the same parameter settings. In datasets with limited\ncategory information, \\ie, EuroSAT, bias tuning surpasses prompt tuning by a\nlarge margin. With a deeper understanding of the multi-modal prompt, we hope\nour work can inspire new and solid research in this direction.\n","authors":["Shuailei Ma","Chen-Wei Xie","Ying Wei","Siyang Sun","Jiaqi Fan","Xiaoyi Bao","Yuxin Guo","Yun Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.11570v2.pdf","comment":"We find that the statistical information in Figure 2 neglect the\n  statistics for tSOS, so we make corrections. Additionally, we change the\n  statistical samples to those where CLIP misidentify, but prompt tuning\n  identify correctly. At the same time, we also revise some of the\n  descriptions. The changes to the supplementary materials will be updated\n  shortly"},{"id":"http://arxiv.org/abs/2403.05246v2","updated":"2024-03-11T07:14:36Z","published":"2024-03-08T12:07:42Z","title":"LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image\n  Segmentation","summary":"  UNet and its variants have been widely used in medical image segmentation.\nHowever, these models, especially those based on Transformer architectures,\npose challenges due to their large number of parameters and computational\nloads, making them unsuitable for mobile health applications. Recently, State\nSpace Models (SSMs), exemplified by Mamba, have emerged as competitive\nalternatives to CNN and Transformer architectures. Building upon this, we\nemploy Mamba as a lightweight substitute for CNN and Transformer within UNet,\naiming at tackling challenges stemming from computational resource limitations\nin real medical settings. To this end, we introduce the Lightweight Mamba UNet\n(LightM-UNet) that integrates Mamba and UNet in a lightweight framework.\nSpecifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure\nMamba fashion to extract deep semantic features and model long-range spatial\ndependencies, with linear computational complexity. Extensive experiments\nconducted on two real-world 2D/3D datasets demonstrate that LightM-UNet\nsurpasses existing state-of-the-art literature. Notably, when compared to the\nrenowned nnU-Net, LightM-UNet achieves superior segmentation performance while\ndrastically reducing parameter and computation costs by 116x and 21x,\nrespectively. This highlights the potential of Mamba in facilitating model\nlightweighting. Our code implementation is publicly available at\nhttps://github.com/MrBlankness/LightM-UNet.\n","authors":["Weibin Liao","Yinghao Zhu","Xinyuan Wang","Chengwei Pan","Yasha Wang","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2403.05246v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06470v1","updated":"2024-03-11T07:10:37Z","published":"2024-03-11T07:10:37Z","title":"3D-aware Image Generation and Editing with Multi-modal Conditions","summary":"  3D-consistent image generation from a single 2D semantic label is an\nimportant and challenging research topic in computer graphics and computer\nvision. Although some related works have made great progress in this field,\nmost of the existing methods suffer from poor disentanglement performance of\nshape and appearance, and lack multi-modal control. In this paper, we propose a\nnovel end-to-end 3D-aware image generation and editing model incorporating\nmultiple types of conditional inputs, including pure noise, text and reference\nimage. On the one hand, we dive into the latent space of 3D Generative\nAdversarial Networks (GANs) and propose a novel disentanglement strategy to\nseparate appearance features from shape features during the generation process.\nOn the other hand, we propose a unified framework for flexible image generation\nand editing tasks with multi-modal conditions. Our method can generate diverse\nimages with distinct noises, edit the attribute through a text description and\nconduct style transfer by giving a reference RGB image. Extensive experiments\ndemonstrate that the proposed method outperforms alternative approaches both\nqualitatively and quantitatively on image generation and editing.\n","authors":["Bo Li","Yi-ke Li","Zhi-fen He","Bin Liu","Yun-Kun Lai"],"pdf_url":"https://arxiv.org/pdf/2403.06470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06467v1","updated":"2024-03-11T07:07:39Z","published":"2024-03-11T07:07:39Z","title":"Point Mamba: A Novel Point Cloud Backbone Based on State Space Model\n  with Octree-Based Ordering Strategy","summary":"  Recently, state space model (SSM) has gained great attention due to its\npromising performance, linear complexity, and long sequence modeling ability in\nboth language and image domains. However, it is non-trivial to extend SSM to\nthe point cloud field, because of the causality requirement of SSM and the\ndisorder and irregularity nature of point clouds. In this paper, we propose a\nnovel SSM-based point cloud processing backbone, named Point Mamba, with a\ncausality-aware ordering mechanism. To construct the causal dependency\nrelationship, we design an octree-based ordering strategy on raw irregular\npoints, globally sorting points in a z-order sequence and also retaining their\nspatial proximity. Our method achieves state-of-the-art performance compared\nwith transformer-based counterparts, with 93.4% accuracy and 75.7 mIOU\nrespectively on the ModelNet40 classification dataset and ScanNet semantic\nsegmentation dataset. Furthermore, our Point Mamba has linear complexity, which\nis more efficient than transformer-based methods. Our method demonstrates the\ngreat potential that SSM can serve as a generic backbone in point cloud\nunderstanding. Codes are released at https://github.com/IRMVLab/Point-Mamba.\n","authors":["Jiuming Liu","Ruiji Yu","Yian Wang","Yu Zheng","Tianchen Deng","Weicai Ye","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11106v2","updated":"2024-03-11T07:06:29Z","published":"2023-11-18T15:44:57Z","title":"ShapeMatcher: Self-Supervised Joint Shape Canonicalization,\n  Segmentation, Retrieval and Deformation","summary":"  In this paper, we present ShapeMatcher, a unified self-supervised learning\nframework for joint shape canonicalization, segmentation, retrieval and\ndeformation. Given a partially-observed object in an arbitrary pose, we first\ncanonicalize the object by extracting point-wise affine-invariant features,\ndisentangling inherent structure of the object with its pose and size. These\nlearned features are then leveraged to predict semantically consistent part\nsegmentation and corresponding part centers. Next, our lightweight retrieval\nmodule aggregates the features within each part as its retrieval token and\ncompare all the tokens with source shapes from a pre-established database to\nidentify the most geometrically similar shape. Finally, we deform the retrieved\nshape in the deformation module to tightly fit the input object by harnessing\npart center guided neural cage deformation. The key insight of ShapeMaker is\nthe simultaneous training of the four highly-associated processes:\ncanonicalization, segmentation, retrieval, and deformation, leveraging\ncross-task consistency losses for mutual supervision. Extensive experiments on\nsynthetic datasets PartNet, ComplementMe, and real-world dataset Scan2CAD\ndemonstrate that ShapeMaker surpasses competitors by a large margin.\n","authors":["Yan Di","Chenyangguang Zhang","Chaowei Wang","Ruida Zhang","Guangyao Zhai","Yanyan Li","Bowen Fu","Xiangyang Ji","Shan Gao"],"pdf_url":"https://arxiv.org/pdf/2311.11106v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.06462v1","updated":"2024-03-11T06:59:05Z","published":"2024-03-11T06:59:05Z","title":"Towards the Uncharted: Density-Descending Feature Perturbation for\n  Semi-supervised Semantic Segmentation","summary":"  Semi-supervised semantic segmentation allows model to mine effective\nsupervision from unlabeled data to complement label-guided training. Recent\nresearch has primarily focused on consistency regularization techniques,\nexploring perturbation-invariant training at both the image and feature levels.\nIn this work, we proposed a novel feature-level consistency learning framework\nnamed Density-Descending Feature Perturbation (DDFP). Inspired by the\nlow-density separation assumption in semi-supervised learning, our key insight\nis that feature density can shed a light on the most promising direction for\nthe segmentation classifier to explore, which is the regions with lower\ndensity. We propose to shift features with confident predictions towards\nlower-density regions by perturbation injection. The perturbed features are\nthen supervised by the predictions on the original features, thereby compelling\nthe classifier to explore less dense regions to effectively regularize the\ndecision boundary. Central to our method is the estimation of feature density.\nTo this end, we introduce a lightweight density estimator based on normalizing\nflow, allowing for efficient capture of the feature density distribution in an\nonline manner. By extracting gradients from the density estimator, we can\ndetermine the direction towards less dense regions for each feature. The\nproposed DDFP outperforms other designs on feature-level perturbations and\nshows state of the art performances on both Pascal VOC and Cityscapes dataset\nunder various partition protocols. The project is available at\nhttps://github.com/Gavinwxy/DDFP.\n","authors":["Xiaoyang Wang","Huihui Bai","Limin Yu","Yao Zhao","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.06462v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2402.04676v2","updated":"2024-03-11T06:56:54Z","published":"2024-02-07T09:03:04Z","title":"Group Distributionally Robust Dataset Distillation with Risk\n  Minimization","summary":"  Dataset distillation (DD) has emerged as a widely adopted technique for\ncrafting a synthetic dataset that captures the essential information of a\ntraining dataset, facilitating the training of accurate neural models. Its\napplications span various domains, including transfer learning, federated\nlearning, and neural architecture search. The most popular methods for\nconstructing the synthetic data rely on matching the convergence properties of\ntraining the model with the synthetic dataset and the training dataset.\nHowever, targeting the training dataset must be thought of as auxiliary in the\nsame sense that the training set is an approximate substitute for the\npopulation distribution, and the latter is the data of interest. Yet despite\nits popularity, an aspect that remains unexplored is the relationship of DD to\nits generalization, particularly across uncommon subgroups. That is, how can we\nensure that a model trained on the synthetic dataset performs well when faced\nwith samples from regions with low population density? Here, the\nrepresentativeness and coverage of the dataset become salient over the\nguaranteed training error at inference. Drawing inspiration from\ndistributionally robust optimization, we introduce an algorithm that combines\nclustering with the minimization of a risk measure on the loss to conduct DD.\nWe provide a theoretical rationale for our approach and demonstrate its\neffective generalization and robustness across subgroups through numerical\nexperiments. The source code is available in\nhttps://github.com/Mming11/RobustDatasetDistillation.\n","authors":["Saeed Vahidian","Mingyu Wang","Jianyang Gu","Vyacheslav Kungurtsev","Wei Jiang","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2402.04676v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06461v1","updated":"2024-03-11T06:56:08Z","published":"2024-03-11T06:56:08Z","title":"Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation","summary":"  Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an\nunlabeled target domain by leveraging the complementary multi-modal inputs in\nan online manner. Previous MM-TTA methods rely on predictions of cross-modal\ninformation in each input frame, while they ignore the fact that predictions of\ngeometric neighborhoods within consecutive frames are highly correlated,\nleading to unstable predictions across time. To fulfill this gap, we propose\nReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages\nreliable cross-modal spatial-temporal correspondences for multi-modal 3D\nsegmentation. Motivated by the fact that reliable predictions should be\nconsistent with their spatial-temporal correspondences, Latte aggregates\nconsecutive frames in a slide window manner and constructs ST voxel to capture\ntemporally local prediction consistency for each modality. After filtering out\nST voxels with high ST entropy, Latte conducts cross-modal learning for each\npoint and pixel by attending to those with reliable and consistent predictions\namong both spatial and temporal neighborhoods. Experimental results show that\nLatte achieves state-of-the-art performance on three different MM-TTA\nbenchmarks compared to previous MM-TTA or TTA methods.\n","authors":["Haozhi Cao","Yuecong Xu","Jianfei Yang","Pengyu Yin","Xingyu Ji","Shenghai Yuan","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.06461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06459v1","updated":"2024-03-11T06:46:31Z","published":"2024-03-11T06:46:31Z","title":"From Pixel to Cancer: Cellular Automata in Computed Tomography","summary":"  AI for cancer detection encounters the bottleneck of data scarcity,\nannotation difficulty, and low prevalence of early tumors. Tumor synthesis\nseeks to create artificial tumors in medical images, which can greatly\ndiversify the data and annotations for AI training. However, current tumor\nsynthesis approaches are not applicable across different organs due to their\nneed for specific expertise and design. This paper establishes a set of generic\nrules to simulate tumor development. Each cell (pixel) is initially assigned a\nstate between zero and ten to represent the tumor population, and a tumor can\nbe developed based on three rules to describe the process of growth, invasion,\nand death. We apply these three generic rules to simulate tumor\ndevelopment--from pixel to cancer--using cellular automata. We then integrate\nthe tumor state into the original computed tomography (CT) images to generate\nsynthetic tumors across different organs. This tumor synthesis approach allows\nfor sampling tumors at multiple stages and analyzing tumor-organ interaction.\nClinically, a reader study involving three expert radiologists reveals that the\nsynthetic tumors and their developing trajectories are convincingly realistic.\nTechnically, we generate tumors at varied stages in 9,262 raw, unlabeled CT\nimages sourced from 68 hospitals worldwide. The performance in segmenting\ntumors in the liver, pancreas, and kidneys exceeds prevailing literature\nbenchmarks, underlining the immense potential of tumor synthesis, especially\nfor earlier cancer detection. The code and models are available at\nhttps://github.com/MrGiovanni/Pixel2Cancer\n","authors":["Yuxiang Lai","Xiaoxi Chen","Angtian Wang","Alan Yuille","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.06459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11816v2","updated":"2024-03-11T06:46:22Z","published":"2024-02-19T04:13:33Z","title":"Learning the Unlearned: Mitigating Feature Suppression in Contrastive\n  Learning","summary":"  Self-Supervised Contrastive Learning has proven effective in deriving\nhigh-quality representations from unlabeled data. However, a major challenge\nthat hinders both unimodal and multimodal contrastive learning is feature\nsuppression, a phenomenon where the trained model captures only a limited\nportion of the information from the input data while overlooking other\npotentially valuable content. This issue often leads to indistinguishable\nrepresentations for visually similar but semantically different inputs,\nadversely affecting downstream task performance, particularly those requiring\nrigorous semantic comprehension. To address this challenge, we propose a novel\nmodel-agnostic Multistage Contrastive Learning (MCL) framework. Unlike standard\ncontrastive learning which inherently captures one single biased feature\ndistribution, MCL progressively learns previously unlearned features through\nfeature-aware negative sampling at each stage, where the negative samples of an\nanchor are exclusively selected from the cluster it was assigned to in\npreceding stages. Meanwhile, MCL preserves the previously well-learned features\nby cross-stage representation integration, integrating features across all\nstages to form final representations. Our comprehensive evaluation demonstrates\nMCL's effectiveness and superiority across both unimodal and multimodal\ncontrastive learning, spanning a range of model architectures from ResNet to\nVision Transformers (ViT). Remarkably, in tasks where the original CLIP model\nhas shown limitations, MCL dramatically enhances performance, with improvements\nup to threefold on specific attributes in the recently proposed MMVP benchmark.\n","authors":["Jihai Zhang","Xiang Lan","Xiaoye Qu","Yu Cheng","Mengling Feng","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2402.11816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06457v1","updated":"2024-03-11T06:34:05Z","published":"2024-03-11T06:34:05Z","title":"Ensemble Quadratic Assignment Network for Graph Matching","summary":"  Graph matching is a commonly used technique in computer vision and pattern\nrecognition. Recent data-driven approaches have improved the graph matching\naccuracy remarkably, whereas some traditional algorithm-based methods are more\nrobust to feature noises, outlier nodes, and global transformation\n(e.g.~rotation). In this paper, we propose a graph neural network (GNN) based\napproach to combine the advantages of data-driven and traditional methods. In\nthe GNN framework, we transform traditional graph-matching solvers as\nsingle-channel GNNs on the association graph and extend the single-channel\narchitecture to the multi-channel network. The proposed model can be seen as an\nensemble method that fuses multiple algorithms at every iteration. Instead of\naveraging the estimates at the end of the ensemble, in our approach, the\nindependent iterations of the ensembled algorithms exchange their information\nafter each iteration via a 1x1 channel-wise convolution layer. Experiments show\nthat our model improves the performance of traditional algorithms\nsignificantly. In addition, we propose a random sampling strategy to reduce the\ncomputational complexity and GPU memory usage, so the model applies to matching\ngraphs with thousands of nodes. We evaluate the performance of our method on\nthree tasks: geometric graph matching, semantic feature matching, and few-shot\n3D shape classification. The proposed model performs comparably or outperforms\nthe best existing GNN-based methods.\n","authors":["Haoru Tan","Chuang Wang","Sitong Wu","Xu-Yao Zhang","Fei Yin","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06457v1.pdf","comment":"Accepted by IJCV in 2024"},{"id":"http://arxiv.org/abs/2306.15620v3","updated":"2024-03-11T06:20:07Z","published":"2023-06-27T16:59:15Z","title":"SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating\n  Replicable Scenes","summary":"  We present a new reproducible benchmark for evaluating robot manipulation in\nthe real world, specifically focusing on pick-and-place. Our benchmark uses the\nYCB objects, a commonly used dataset in the robotics community, to ensure that\nour results are comparable to other studies. Additionally, the benchmark is\ndesigned to be easily reproducible in the real world, making it accessible to\nresearchers and practitioners. We also provide our experimental results and\nanalyzes for model-based and model-free 6D robotic grasping on the benchmark,\nwhere representative algorithms are evaluated for object perception, grasping\nplanning, and motion planning. We believe that our benchmark will be a valuable\ntool for advancing the field of robot manipulation. By providing a standardized\nevaluation framework, researchers can more easily compare different techniques\nand algorithms, leading to faster progress in developing robot manipulation\nmethods.\n","authors":["Ninad Khargonkar","Sai Haneesh Allu","Yangxiao Lu","Jishnu Jaykumar P","Balakrishnan Prabhakaran","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2306.15620v3.pdf","comment":"Accepted to ICRA 2024. Project page is available at\n  https://irvlutd.github.io/SceneReplica"},{"id":"http://arxiv.org/abs/2305.18766v4","updated":"2024-03-11T06:14:31Z","published":"2023-05-30T05:56:58Z","title":"HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion\n  Guidance","summary":"  The advancements in automatic text-to-3D generation have been remarkable.\nMost existing methods use pre-trained text-to-image diffusion models to\noptimize 3D representations like Neural Radiance Fields (NeRFs) via\nlatent-space denoising score matching. Yet, these methods often result in\nartifacts and inconsistencies across different views due to their suboptimal\noptimization approaches and limited understanding of 3D geometry. Moreover, the\ninherent constraints of NeRFs in rendering crisp geometry and stable textures\nusually lead to a two-stage optimization to attain high-resolution details.\nThis work proposes holistic sampling and smoothing approaches to achieve\nhigh-quality text-to-3D generation, all in a single-stage optimization. We\ncompute denoising scores in the text-to-image diffusion model's latent and\nimage spaces. Instead of randomly sampling timesteps (also referred to as noise\nlevels in denoising score matching), we introduce a novel timestep annealing\napproach that progressively reduces the sampled timestep throughout\noptimization. To generate high-quality renderings in a single-stage\noptimization, we propose regularization for the variance of z-coordinates along\nNeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel\nsmoothing technique that refines importance sampling weights coarse-to-fine,\nensuring accurate and thorough sampling in high-density regions. Extensive\nexperiments demonstrate the superiority of our method over previous approaches,\nenabling the generation of highly detailed and view-consistent 3D assets\nthrough a single-stage training process.\n","authors":["Junzhe Zhu","Peiye Zhuang","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2305.18766v4.pdf","comment":"Project page: https://hifa-team.github.io/HiFA-site/"},{"id":"http://arxiv.org/abs/2403.06453v1","updated":"2024-03-11T06:08:16Z","published":"2024-03-11T06:08:16Z","title":"FontCLIP: A Semantic Typography Visual-Language Model for Multilingual\n  Font Applications","summary":"  Acquiring the desired font for various design tasks can be challenging and\nrequires professional typographic knowledge. While previous font retrieval or\ngeneration works have alleviated some of these difficulties, they often lack\nsupport for multiple languages and semantic attributes beyond the training data\ndomains. To solve this problem, we present FontCLIP: a model that connects the\nsemantic understanding of a large vision-language model with typographical\nknowledge. We integrate typography-specific knowledge into the comprehensive\nvision-language knowledge of a pretrained CLIP model through a novel finetuning\napproach. We propose to use a compound descriptive prompt that encapsulates\nadaptively sampled attributes from a font attribute dataset focusing on Roman\nalphabet characters. FontCLIP's semantic typographic latent space demonstrates\ntwo unprecedented generalization abilities. First, FontCLIP generalizes to\ndifferent languages including Chinese, Japanese, and Korean (CJK), capturing\nthe typographical features of fonts across different languages, even though it\nwas only finetuned using fonts of Roman characters. Second, FontCLIP can\nrecognize the semantic attributes that are not presented in the training data.\nFontCLIP's dual-modality and generalization abilities enable multilingual and\ncross-lingual font retrieval and letter shape optimization, reducing the burden\nof obtaining desired fonts.\n","authors":["Yuki Tatsukawa","I-Chao Shen","Anran Qi","Yuki Koyama","Takeo Igarashi","Ariel Shamir"],"pdf_url":"https://arxiv.org/pdf/2403.06453v1.pdf","comment":"11 pages. Eurographics 2024.\n  https://yukistavailable.github.io/fontclip.github.io/"},{"id":"http://arxiv.org/abs/2403.06452v1","updated":"2024-03-11T06:03:31Z","published":"2024-03-11T06:03:31Z","title":"Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for\n  Text-Guided QR Code Generation","summary":"  In the digital era, QR codes serve as a linchpin connecting virtual and\nphysical realms. Their pervasive integration across various applications\nhighlights the demand for aesthetically pleasing codes without compromised\nscannability. However, prevailing methods grapple with the intrinsic challenge\nof balancing customization and scannability. Notably, stable-diffusion models\nhave ushered in an epoch of high-quality, customizable content generation. This\npaper introduces Text2QR, a pioneering approach leveraging these advancements\nto address a fundamental challenge: concurrently achieving user-defined\naesthetics and scanning robustness. To ensure stable generation of aesthetic QR\ncodes, we introduce the QR Aesthetic Blueprint (QAB) module, generating a\nblueprint image exerting control over the entire generation process.\nSubsequently, the Scannability Enhancing Latent Refinement (SELR) process\nrefines the output iteratively in the latent space, enhancing scanning\nrobustness. This approach harnesses the potent generation capabilities of\nstable-diffusion models, navigating the trade-off between image aesthetics and\nQR code scannability. Our experiments demonstrate the seamless fusion of visual\nappeal with the practical utility of aesthetic QR codes, markedly outperforming\nprior methods. Codes are available at \\url{https://github.com/mulns/Text2QR}\n","authors":["Guangyang Wu","Xiaohong Liu","Jun Jia","Xuehao Cui","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2403.06452v1.pdf","comment":"Accepted by CVPR"},{"id":"http://arxiv.org/abs/2403.00269v2","updated":"2024-03-11T05:58:55Z","published":"2024-03-01T04:16:08Z","title":"Large Convolutional Model Tuning via Filter Subspace","summary":"  Efficient fine-tuning methods are critical to address the high computational\nand parameter complexity while adapting large pre-trained models to downstream\ntasks. Our study is inspired by prior research that represents each convolution\nfilter as a linear combination of a small set of filter subspace elements,\nreferred to as filter atoms. In this paper, we propose to fine-tune pre-trained\nmodels by adjusting only filter atoms, which are responsible for spatial-only\nconvolution, while preserving spatially-invariant channel combination knowledge\nin atom coefficients. In this way, we bring a new filter subspace view for\nmodel tuning. Furthermore, each filter atom can be recursively decomposed as a\ncombination of another set of atoms, which naturally expands the number of\ntunable parameters in the filter subspace. By only adapting filter atoms\nconstructed by a small number of parameters, while maintaining the rest of\nmodel parameters constant, the proposed approach is highly parameter-efficient.\nIt effectively preserves the capabilities of pre-trained models and prevents\noverfitting to downstream tasks. Extensive experiments show that such a simple\nscheme surpasses previous tuning baselines for both discriminate and generative\ntasks.\n","authors":["Wei Chen","Zichen Miao","Qiang Qiu"],"pdf_url":"https://arxiv.org/pdf/2403.00269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16500v3","updated":"2024-03-11T05:48:19Z","published":"2023-11-27T13:37:26Z","title":"LLMGA: Multimodal Large Language Model based Generation Assistant","summary":"  In this paper, we introduce a Multimodal Large Language Model-based\nGeneration Assistant (LLMGA), leveraging the vast reservoir of knowledge and\nproficiency in reasoning, comprehension, and response inherent in Large\nLanguage Models (LLMs) to assist users in image generation and editing.\nDiverging from existing approaches where Multimodal Large Language Models\n(MLLMs) generate fixed-size embeddings to control Stable Diffusion (SD), our\nLLMGA provides a detailed language generation prompt for precise control over\nSD. This not only augments LLM context understanding but also reduces noise in\ngeneration prompts, yields images with more intricate and precise content, and\nelevates the interpretability of the network. To this end, we curate a\ncomprehensive dataset comprising prompt refinement, similar image generation,\ninpainting \\& outpainting, and instruction-based editing. Moreover, we propose\na two-stage training scheme. In the first stage, we train the MLLM to grasp the\nproperties of image generation and editing, enabling it to generate detailed\nprompts. In the second stage, we optimize SD to align with the MLLM's\ngeneration prompts. Additionally, we propose a reference-based restoration\nnetwork to alleviate texture, brightness, and contrast disparities between\ngenerated and preserved regions during inpainting and outpainting. Extensive\nresults show that LLMGA has promising generation and editing capabilities and\ncan enable more flexible and expansive applications in an interactive manner.\n","authors":["Bin Xia","Shiyin Wang","Yingfan Tao","Yitong Wang","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2311.16500v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04161v2","updated":"2024-03-11T05:37:36Z","published":"2024-03-07T02:40:42Z","title":"Estimating Neural Network Performance through Sample-Wise Activation\n  Patterns","summary":"  Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid\nresource-intensive neural network training, especially in Neural Architecture\nSearch (NAS). Recent studies show that existing training-free metrics have\nseveral limitations, such as limited correlation and poor generalisation across\ndifferent search spaces and tasks. Hence, we propose Sample-Wise Activation\nPatterns and its derivative, SWAP-Score, a novel high-performance training-free\nmetric. It measures the expressivity of networks over a batch of input samples.\nThe SWAP-Score is strongly correlated with ground-truth performance across\nvarious search spaces and tasks, outperforming 15 existing training-free\nmetrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be\nfurther enhanced by regularisation, which leads to even higher correlations in\ncell-based search space and enables model size control during the search. For\nexample, Spearman's rank correlation coefficient between regularised SWAP-Score\nand CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90,\nsignificantly higher than 0.80 from the second-best metric, NWOT. When\nintegrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves\ncompetitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and\n9 minutes of GPU time respectively.\n","authors":["Yameng Peng","Andy Song","Haytham M. Fayek","Vic Ciesielski","Xiaojun Chang"],"pdf_url":"https://arxiv.org/pdf/2403.04161v2.pdf","comment":"ICLR2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.06444v1","updated":"2024-03-11T05:35:38Z","published":"2024-03-11T05:35:38Z","title":"Latent Semantic Consensus For Deterministic Geometric Model Fitting","summary":"  Estimating reliable geometric model parameters from the data with severe\noutliers is a fundamental and important task in computer vision. This paper\nattempts to sample high-quality subsets and select model instances to estimate\nparameters in the multi-structural data. To address this, we propose an\neffective method called Latent Semantic Consensus (LSC). The principle of LSC\nis to preserve the latent semantic consensus in both data points and model\nhypotheses. Specifically, LSC formulates the model fitting problem into two\nlatent semantic spaces based on data points and model hypotheses, respectively.\nThen, LSC explores the distributions of points in the two latent semantic\nspaces, to remove outliers, generate high-quality model hypotheses, and\neffectively estimate model instances. Finally, LSC is able to provide\nconsistent and reliable solutions within only a few milliseconds for general\nmulti-structural model fitting, due to its deterministic fitting nature and\nefficiency. Compared with several state-of-the-art model fitting methods, our\nLSC achieves significant superiority for the performance of both accuracy and\nspeed on synthetic data and real images. The code will be available at\nhttps://github.com/guobaoxiao/LSC.\n","authors":["Guobao Xiao","Jun Yu","Jiayi Ma","Deng-Ping Fan","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2403.06444v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.06789v1","updated":"2024-03-11T15:04:55Z","published":"2024-03-11T15:04:55Z","title":"SPLADE-v3: New baselines for SPLADE","summary":"  A companion to the release of the latest version of the SPLADE library. We\ndescribe changes to the training structure and present our latest series of\nmodels -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as\nre-rankers, and showcase its effectiveness via a meta-analysis over more than\n40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is\nstatistically significantly more effective than both BM25 and SPLADE++, while\ncomparing well to cross-encoder re-rankers. Specifically, it gets more than 40\nMRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on\nthe BEIR benchmark.\n","authors":["Carlos Lassance","Hervé Déjean","Thibault Formal","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2403.06789v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2403.06747v1","updated":"2024-03-11T14:13:41Z","published":"2024-03-11T14:13:41Z","title":"MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation","summary":"  Compared to business-to-consumer (B2C) e-commerce systems,\nconsumer-to-consumer (C2C) e-commerce platforms usually encounter the\nlimited-stock problem, that is, a product can only be sold one time in a C2C\nsystem. This poses several unique challenges for click-through rate (CTR)\nprediction. Due to limited user interactions for each product (i.e. item), the\ncorresponding item embedding in the CTR model may not easily converge. This\nmakes the conventional sequence modeling based approaches cannot effectively\nutilize user history information since historical user behaviors contain a\nmixture of items with different volume of stocks. Particularly, the attention\nmechanism in a sequence model tends to assign higher score to products with\nmore accumulated user interactions, making limited-stock products being ignored\nand contribute less to the final output. To this end, we propose the Meta-Split\nNetwork (MSN) to split user history sequence regarding to the volume of stock\nfor each product, and adopt differentiated modeling approaches for different\nsequences. As for the limited-stock products, a meta-learning approach is\napplied to address the problem of inconvergence, which is achieved by designing\nmeta scaling and shifting networks with ID and side information. In addition,\ntraditional approach can hardly update item embedding once the product is\nconsumed. Thereby, we propose an auxiliary loss that makes the parameters\nupdatable even when the product is no longer in distribution. To the best of\nour knowledge, this is the first solution addressing the recommendation of\nlimited-stock product. Experimental results on the production dataset and\nonline A/B testing demonstrate the effectiveness of our proposed method.\n","authors":["Wenhao Wu","Jialiang Zhou","Ailong He","Shuguang Han","Jufeng Chen","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.06747v1.pdf","comment":"WWW'24 Industry Track"},{"id":"http://arxiv.org/abs/2403.06737v1","updated":"2024-03-11T14:02:24Z","published":"2024-03-11T14:02:24Z","title":"Post-Training Attribute Unlearning in Recommender Systems","summary":"  With the growing privacy concerns in recommender systems, recommendation\nunlearning is getting increasing attention. Existing studies predominantly use\ntraining data, i.e., model inputs, as unlearning target. However, attackers can\nextract private information from the model even if it has not been explicitly\nencountered during training. We name this unseen information as\n\\textit{attribute} and treat it as unlearning target. To protect the sensitive\nattribute of users, Attribute Unlearning (AU) aims to make target attributes\nindistinguishable. In this paper, we focus on a strict but practical setting of\nAU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can\nonly be performed after the training of the recommendation model is completed.\nTo address the PoT-AU problem in recommender systems, we propose a\ntwo-component loss function. The first component is distinguishability loss,\nwhere we design a distribution-based measurement to make attribute labels\nindistinguishable from attackers. We further extend this measurement to handle\nmulti-class attribute cases with efficient computational overhead. The second\ncomponent is regularization loss, where we explore a function-space measurement\nthat effectively maintains recommendation performance compared to\nparameter-space regularization. We use stochastic gradient descent algorithm to\noptimize our proposed loss. Extensive experiments on four real-world datasets\ndemonstrate the effectiveness of our proposed methods.\n","authors":["Chaochao Chen","Yizhao Zhang","Yuyuan Li","Dan Meng","Jun Wang","Xiaoli Zheng","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.06737v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.05847"},{"id":"http://arxiv.org/abs/2403.06716v1","updated":"2024-03-11T13:40:46Z","published":"2024-03-11T13:40:46Z","title":"Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based\n  Method for Dynamic Observation Processing in Spatially Distributed\n  Emergencies","summary":"  In emergencies, high stake decisions often have to be made under time\npressure and strain. In order to support such decisions, information from\nvarious sources needs to be collected and processed rapidly. The information\navailable tends to be temporally and spatially variable, uncertain, and\nsometimes conflicting, leading to potential biases in decisions. Currently,\nthere is a lack of systematic approaches for information processing and\nsituation assessment which meet the particular demands of emergency situations.\nTo address this gap, we present a Bayesian network-based method called ERIMap\nthat is tailored to the complex information-scape during emergencies. The\nmethod enables the systematic and rapid processing of heterogeneous and\npotentially uncertain observations and draws inferences about key variables of\nan emergency. It thereby reduces complexity and cognitive load for decision\nmakers. The output of the ERIMap method is a dynamically evolving and spatially\nresolved map of beliefs about key variables of an emergency that is updated\neach time a new observation becomes available. The method is illustrated in a\ncase study in which an emergency response is triggered by an accident causing a\ngas leakage on a chemical plant site.\n","authors":["Moritz Schneider","Lukas Halekotte","Tina Comes","Daniel Lichte","Frank Fiedrich"],"pdf_url":"https://arxiv.org/pdf/2403.06716v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.06642v1","updated":"2024-03-11T12:04:20Z","published":"2024-03-11T12:04:20Z","title":"KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation","summary":"  The utilization of semantic information is an important research problem in\nthe field of recommender systems, which aims to complement the missing parts of\nmainstream ID-based approaches. With the rise of LLM, its ability to act as a\nknowledge base and its reasoning capability have opened up new possibilities\nfor this research area, making LLM-based recommendation an emerging research\ndirection. However, directly using LLM to process semantic information for\nrecommendation scenarios is unreliable and sub-optimal due to several problems\nsuch as hallucination. A promising way to cope with this is to use external\nknowledge to aid LLM in generating truthful and usable text. Inspired by the\nabove motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to\nusing external knowledge in prompts, the proposed method also includes a\nknowledge-based contrastive learning scheme for training. Experiments on public\ndatasets and in-enterprise datasets validate the effectiveness of the proposed\nmethod.\n","authors":["Weiqing Luo","Chonggang Song","Lingling Yi","Gong Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.06642v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.06567v1","updated":"2024-03-11T10:06:45Z","published":"2024-03-11T10:06:45Z","title":"Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology","summary":"  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n","authors":["Stefan Denner","David Zimmerer","Dimitrios Bounias","Markus Bujotzek","Shuhan Xiao","Lisa Kausch","Philipp Schader","Tobias Penzkofer","Paul F. Jäger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.06567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06551v1","updated":"2024-03-11T09:52:32Z","published":"2024-03-11T09:52:32Z","title":"ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval","summary":"  Tool learning aims to extend the capabilities of large language models (LLMs)\nwith external tools. A major challenge in tool learning is how to support a\nlarge number of tools, including unseen tools. To address this challenge,\nprevious studies have proposed retrieving suitable tools for the LLM based on\nthe user query. However, previously proposed methods do not consider the\ndifferences between seen and unseen tools, nor do they take the hierarchy of\nthe tool library into account, which may lead to suboptimal performance for\ntool retrieval. Therefore, to address the aforementioned issues, we propose\nToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval\nto further refine the retrieval results. Specifically, our proposed ToolRerank\nincludes Adaptive Truncation, which truncates the retrieval results related to\nseen and unseen tools at different positions, and Hierarchy-Aware Reranking,\nwhich makes retrieval results more concentrated for single-tool queries and\nmore diverse for multi-tool queries. Experimental results show that ToolRerank\ncan improve the quality of the retrieval results, leading to better execution\nresults generated by the LLM.\n","authors":["Yuanhang Zheng","Peng Li","Wei Liu","Yang Liu","Jian Luan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06551v1.pdf","comment":"This paper is accepted for LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.06465v1","updated":"2024-03-11T07:07:02Z","published":"2024-03-11T07:07:02Z","title":"RecAI: Leveraging Large Language Models for Next-Generation Recommender\n  Systems","summary":"  This paper introduces RecAI, a practical toolkit designed to augment or even\nrevolutionize recommender systems with the advanced capabilities of Large\nLanguage Models (LLMs). RecAI provides a suite of tools, including Recommender\nAI Agent, Recommendation-oriented Language Models, Knowledge Plugin,\nRecExplainer, and Evaluator, to facilitate the integration of LLMs into\nrecommender systems from multifaceted perspectives. The new generation of\nrecommender systems, empowered by LLMs, are expected to be more versatile,\nexplainable, conversational, and controllable, paving the way for more\nintelligent and user-centric recommendation experiences. We hope the\nopen-source of RecAI can help accelerate evolution of new advanced recommender\nsystems. The source code of RecAI is available at\n\\url{https://github.com/microsoft/RecAI}.\n","authors":["Jianxun Lian","Yuxuan Lei","Xu Huang","Jing Yao","Wei Xu","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2403.06465v1.pdf","comment":"4 pages. Webconf 2024 demo track"},{"id":"http://arxiv.org/abs/2403.06447v1","updated":"2024-03-11T05:49:34Z","published":"2024-03-11T05:49:34Z","title":"CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve\n  Long-tail Recommendation","summary":"  The long-tail recommendation is a challenging task for traditional\nrecommender systems, due to data sparsity and data imbalance issues. The recent\ndevelopment of large language models (LLMs) has shown their abilities in\ncomplex reasoning, which can help to deduce users' preferences based on very\nfew previous interactions. However, since most LLM-based systems rely on items'\nsemantic meaning as the sole evidence for reasoning, the collaborative\ninformation of user-item interactions is neglected, which can cause the LLM's\nreasoning to be misaligned with task-specific collaborative information of the\ndataset. To further align LLMs' reasoning to task-specific user-item\ninteraction knowledge, we introduce collaborative retrieval-augmented LLMs,\nCoRAL, which directly incorporate collaborative evidence into the prompts.\nBased on the retrieved user-item interactions, the LLM can analyze shared and\ndistinct preferences among users, and summarize the patterns indicating which\ntypes of users would be attracted by certain items. The retrieved collaborative\nevidence prompts the LLM to align its reasoning with the user-item interaction\npatterns in the dataset. However, since the capacity of the input prompt is\nlimited, finding the minimally-sufficient collaborative information for\nrecommendation tasks can be challenging. We propose to find the optimal\ninteraction set through a sequential decision-making process and develop a\nretrieval policy learned through a reinforcement learning (RL) framework,\nCoRAL. Our experimental results show that CoRAL can significantly improve LLMs'\nreasoning abilities on specific recommendation tasks. Our analysis also reveals\nthat CoRAL can more efficiently explore collaborative information through\nreinforcement learning.\n","authors":["Junda Wu","Cheng-Chun Chang","Tong Yu","Zhankui He","Jianing Wang","Yupeng Hou","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.06447v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2305.13931v2","updated":"2024-03-11T05:13:49Z","published":"2023-05-10T02:16:55Z","title":"Improving position bias estimation against sparse and skewed dataset\n  with item embedding","summary":"  Estimating position bias is a well-known challenge in Learning to Rank (L2R).\nClick data in e-commerce applications, such as targeted advertisements and\nsearch engines, provides implicit but abundant feedback to improve personalized\nrankings. However, click data inherently includes various biases like position\nbias. Based on the position-based click model, Result Randomization and\nRegression Expectation-Maximization algorithm (REM) have been proposed to\nestimate position bias, but they require various paired observations of (item,\nposition). In real-world scenarios of advertising, marketers frequently display\nadvertisements in a fixed pre-determined order, which creates difficulties in\nestimation due to the limited availability of various pairs in the training\ndata, resulting in a sparse dataset. We propose a variant of the REM that\nutilizes item embeddings to alleviate the sparsity of (item, position). Using a\npublic dataset and internal carousel advertisement click dataset, we\nempirically show that item embedding with Latent Semantic Indexing (LSI) and\nVariational Auto-Encoder (VAE) improves the accuracy of position bias\nestimation and the estimated position bias enhances Learning to Rank\nperformance. We also show that LSI is more effective as an embedding creation\nmethod for position bias estimation.\n","authors":["Shion Ishikawa","Yun Ching Liu","Young-Joo Chung","Yu Hirate"],"pdf_url":"https://arxiv.org/pdf/2305.13931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06372v1","updated":"2024-03-11T01:50:41Z","published":"2024-03-11T01:50:41Z","title":"Repeated Padding as Data Augmentation for Sequential Recommendation","summary":"  Sequential recommendation aims to provide users with personalized suggestions\nbased on their historical interactions. When training sequential models,\npadding is a widely adopted technique for two main reasons: 1) The vast\nmajority of models can only handle fixed-length sequences; 2) Batching-based\ntraining needs to ensure that the sequences in each batch have the same length.\nThe special value \\emph{0} is usually used as the padding content, which does\nnot contain the actual information and is ignored in the model calculations.\nThis common-sense padding strategy leads us to a problem that has never been\nexplored before: \\emph{Can we fully utilize this idle input space by padding\nother content to further improve model performance and training efficiency?}\n  In this paper, we propose a simple yet effective padding method called\n\\textbf{Rep}eated \\textbf{Pad}ding (\\textbf{RepPad}). Specifically, we use the\noriginal interaction sequences as the padding content and fill it to the\npadding positions during model training. This operation can be performed a\nfinite number of times or repeated until the input sequences' length reaches\nthe maximum limit. Our RepPad can be viewed as a sequence-level data\naugmentation strategy. Unlike most existing works, our method contains no\ntrainable parameters or hyperparameters and is a plug-and-play data\naugmentation operation. Extensive experiments on various categories of\nsequential models and five real-world datasets demonstrate the effectiveness\nand efficiency of our approach. The average recommendation performance\nimprovement is up to 60.3\\% on GRU4Rec and 24.3\\% on SASRec. We also provide\nin-depth analysis and explanation of what makes RepPad effective from multiple\nperspectives. The source code will be released to ensure the reproducibility of\nour experiments.\n","authors":["Yizhou Dang","Yuting Liu","Enneng Yang","Guibing Guo","Linying Jiang","Xingwei Wang","Jianzhe Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.06372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15479v2","updated":"2024-03-11T23:37:36Z","published":"2024-01-27T19:04:30Z","title":"Navigating the Post-API Dilemma Search Engine Results Pages Present a\n  Biased View of Social Media Data","summary":"  Recent decisions to discontinue access to social media APIs are having\ndetrimental effects on Internet research and the field of computational social\nscience as a whole. This lack of access to data has been dubbed the Post-API\nera of Internet research. Fortunately, popular search engines have the means to\ncrawl, capture, and surface social media data on their Search Engine Results\nPages (SERP) if provided the proper search query, and may provide a solution to\nthis dilemma. In the present work we ask: does SERP provide a complete and\nunbiased sample of social media data? Is SERP a viable alternative to direct\nAPI-access? To answer these questions, we perform a comparative analysis\nbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. We\nfind that SERP results are highly biased in favor of popular posts; against\npolitical, pornographic, and vulgar posts; are more positive in their\nsentiment; and have large topical gaps. Overall, we conclude that SERP is not a\nviable alternative to social media API access.\n","authors":["Amrit Poudel","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2401.15479v2.pdf","comment":"Proceedings of the ACM Web Conference 2024 (WWW '24)"},{"id":"http://arxiv.org/abs/2403.07090v1","updated":"2024-03-11T18:33:56Z","published":"2024-03-11T18:33:56Z","title":"Time Series Analysis of Key Societal Events as Reflected in Complex\n  Social Media Data Streams","summary":"  Social media platforms hold valuable insights, yet extracting essential\ninformation can be challenging. Traditional top-down approaches often struggle\nto capture critical signals in rapidly changing events. As global events evolve\nswiftly, social media narratives, including instances of disinformation, become\nsignificant sources of insights. To address the need for an inductive strategy,\nwe explore a niche social media platform GAB and an established messaging\nservice Telegram, to develop methodologies applicable on a broader scale. This\nstudy investigates narrative evolution on these platforms using quantitative\ncorpus-based discourse analysis techniques. Our approach is a novel mode to\nstudy multiple social media domains to distil key information which may be\nobscured otherwise, allowing for useful and actionable insights. The paper\ndetails the technical and methodological aspects of gathering and preprocessing\nGAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying\ncrucial nouns and verbs for deeper exploration. Empirically, this approach is\napplied to a case study of a well defined event that had global impact: the\n2023 Wagner mutiny. The main findings are: (1) the time line can be\ndeconstructed to provide useful data features allowing for improved\ninterpretation; (2) a methodology is applied which provides a basis for\ngeneralization. The key contribution is an approach, that in some cases,\nprovides the ability to capture the dynamic narrative shifts over time with\nelevated confidence. The approach can augment near-real-time assessment of key\nsocial movements, allowing for informed governance choices. This research is\nimportant because it lays out a useful methodology for time series relevant\ninfo-culling, which can enable proactive modes for positive social engagement.\n","authors":["Andy Skumanich","Han Kyul Kim"],"pdf_url":"https://arxiv.org/pdf/2403.07090v1.pdf","comment":"AAAI2024 Workshop on AI for Time Series Analysis (AI4TS)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.06973v1","updated":"2024-03-11T17:55:53Z","published":"2024-03-11T17:55:53Z","title":"Bayesian Diffusion Models for 3D Shape Reconstruction","summary":"  We present Bayesian Diffusion Models (BDM), a prediction algorithm that\nperforms effective Bayesian inference by tightly coupling the top-down (prior)\ninformation with the bottom-up (data-driven) procedure via joint diffusion\nprocesses. We show the effectiveness of BDM on the 3D shape reconstruction\ntask. Compared to prototypical deep learning data-driven approaches trained on\npaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM\nbrings in rich prior information from standalone labels (e.g. point clouds) to\nimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesian\nframeworks where explicit prior and likelihood are required for the inference,\nBDM performs seamless information fusion via coupled diffusion processes with\nlearned gradient computation networks. The specialty of our BDM lies in its\ncapability to engage the active and effective information exchange and fusion\nof the top-down and bottom-up processes where each itself is a diffusion\nprocess. We demonstrate state-of-the-art results on both synthetic and\nreal-world benchmarks for 3D shape reconstruction.\n","authors":["Haiyang Xu","Yu Lei","Zeyuan Chen","Xiang Zhang","Yue Zhao","Yilin Wang","Zhuowen Tu"],"pdf_url":"https://arxiv.org/pdf/2403.06973v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06971v1","updated":"2024-03-11T17:54:42Z","published":"2024-03-11T17:54:42Z","title":"A representation-learning game for classes of prediction tasks","summary":"  We propose a game-based formulation for learning dimensionality-reducing\nrepresentations of feature vectors, when only a prior knowledge on future\nprediction tasks is available. In this game, the first player chooses a\nrepresentation, and then the second player adversarially chooses a prediction\ntask from a given class, representing the prior knowledge. The first player\naims is to minimize, and the second player to maximize, the regret: The minimal\nprediction loss using the representation, compared to the same loss using the\noriginal features. For the canonical setting in which the representation, the\nresponse to predict and the predictors are all linear functions, and under the\nmean squared error loss function, we derive the theoretically optimal\nrepresentation in pure strategies, which shows the effectiveness of the prior\nknowledge, and the optimal regret in mixed strategies, which shows the\nusefulness of randomizing the representation. For general representations and\nloss functions, we propose an efficient algorithm to optimize a randomized\nrepresentation. The algorithm only requires the gradients of the loss function,\nand is based on incrementally adding a representation rule to a mixture of such\nrules.\n","authors":["Neria Uzan","Nir Weinberger"],"pdf_url":"https://arxiv.org/pdf/2403.06971v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2402.05210v3","updated":"2024-03-11T17:50:31Z","published":"2024-02-07T19:35:09Z","title":"Anatomically-Controllable Medical Image Generation with\n  Segmentation-Guided Diffusion Models","summary":"  Diffusion models have enabled remarkably high-quality medical image\ngeneration, yet it is challenging to enforce anatomical constraints in\ngenerated images. This hampers many useful applications, including\npre-registered image generation, counterfactual scenarios, and others. To this\nend, we propose a diffusion model-based method that supports\nanatomically-controllable medical image generation, by following a multi-class\nanatomical segmentation mask at each sampling step. We additionally introduce a\nrandom mask ablation training algorithm to enable conditioning on a selected\ncombination of anatomical constraints while allowing flexibility in other\nanatomical areas. We compare our model (\"Seg-Diff\") to existing methods on\nbreast MRI and abdominal/neck-to-pelvis CT datasets with a wide range of\nanatomical objects. Results show that it reaches a new state-of-the-art in the\nfaithfulness of generated images to input anatomical masks on both datasets,\nand is on par for general anatomical realism. Finally, our model also enjoys\nthe extra benefit of being able to adjust the anatomical similarity of\ngenerated images to real images of choice through interpolation in its latent\nspace.\n","authors":["Nicholas Konz","Yuwen Chen","Haoyu Dong","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2402.05210v3.pdf","comment":"Code and synthetic dataset:\n  https://github.com/mazurowski-lab/segmentation-guided-diffusion"},{"id":"http://arxiv.org/abs/2403.06966v1","updated":"2024-03-11T17:49:18Z","published":"2024-03-11T17:49:18Z","title":"Acquiring Diverse Skills using Curriculum Reinforcement Learning with\n  Mixture of Experts","summary":"  Reinforcement learning (RL) is a powerful approach for acquiring a\ngood-performing policy. However, learning diverse skills is challenging in RL\ndue to the commonly used Gaussian policy parameterization. We propose\n\\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL), an RL method for\nlearning diverse skills using Mixture of Experts, where each expert formalizes\na skill as a contextual motion primitive. Di-SkilL optimizes each expert and\nits associate context distribution to a maximum entropy objective that\nincentivizes learning diverse skills in similar contexts. The per-expert\ncontext distribution enables automatic curricula learning, allowing each expert\nto focus on its best-performing sub-region of the context space. To overcome\nhard discontinuities and multi-modalities without any prior knowledge of the\nenvironment's unknown context probability space, we leverage energy-based\nmodels to represent the per-expert context distributions and demonstrate how we\ncan efficiently train them using the standard policy gradient objective. We\nshow on challenging robot simulation tasks that Di-SkilL can learn diverse and\nperformant skills.\n","authors":["Onur Celik","Aleksandar Taranovic","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2403.06966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06963v1","updated":"2024-03-11T17:47:30Z","published":"2024-03-11T17:47:30Z","title":"The pitfalls of next-token prediction","summary":"  Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this intuitive concern, which is fragmented in the literature. As a\nstarting point, we argue that the two often-conflated phases of next-token\nprediction -- autoregressive inference and teacher-forced training -- must be\ntreated distinctly. The popular criticism that errors can compound during\nautoregressive inference, crucially assumes that teacher-forcing has learned an\naccurate next-token predictor. This assumption sidesteps a more deep-rooted\nproblem we expose: in certain classes of tasks, teacher-forcing can simply fail\nto learn an accurate next-token predictor in the first place. We describe a\ngeneral mechanism of how teacher-forcing can fail, and design a minimal\nplanning task where both the Transformer and the Mamba architecture empirically\nfail in that manner -- remarkably, despite the task being straightforward to\nlearn. We provide preliminary evidence that this failure can be resolved when\ntraining to predict multiple tokens in advance. We hope this finding can ground\nfuture debates and inspire explorations beyond the next-token prediction\nparadigm. We make our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures\n","authors":["Gregor Bachmann","Vaishnavh Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2403.06963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06955v1","updated":"2024-03-11T17:39:08Z","published":"2024-03-11T17:39:08Z","title":"Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic\n  Perovskites","summary":"  Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a\npromising class of electronically active materials for both light absorption\nand emission. The design space of HOIPs is extremely large, since a diverse\nspace of organic cations can be combined with different inorganic frameworks.\nThis immense design space allows for tunable electronic and mechanical\nproperties, but also necessitates the development of new tools for in silico\nhigh throughput analysis of candidate structures. In this work, we present an\naccurate, efficient, transferable and widely applicable machine learning\ninteratomic potential (MLIP) for predicting the structure of new 2D HOIPs.\nUsing the MACE architecture, an MLIP is trained on 86 diverse experimentally\nreported HOIP structures. The model is tested on 73 unseen perovskite\ncompositions, and achieves chemical accuracy with respect to the reference\nelectronic structure method. Our model is then combined with a simple random\nstructure search algorithm to predict the structure of hypothetical HOIPs given\nonly the proposed composition. Success is demonstrated by correctly and\nreliably recovering the crystal structure of a set of experimentally known 2D\nperovskites. Such a random structure search is impossible with ab initio\nmethods due to the associated computational cost, but is relatively inexpensive\nwith the MACE potential. Finally, the procedure is used to predict the\nstructure formed by a new organic cation with no previously known corresponding\nperovskite. Laboratory synthesis of the new hybrid perovskite confirms the\naccuracy of our prediction. This capability, applied at scale, enables\nefficient screening of thousands of combinations of organic cations and\ninorganic layers.\n","authors":["Nima Karimitari","William J. Baldwin","Evan W. Muller","Zachary J. L. Bare","W. Joshua Kennedy","Gábor Csányi","Christopher Sutton"],"pdf_url":"https://arxiv.org/pdf/2403.06955v1.pdf","comment":"14 pages and 9 figures in the main text. Supplementary included in\n  pdf"},{"id":"http://arxiv.org/abs/2305.17283v2","updated":"2024-03-11T17:37:33Z","published":"2023-05-26T22:06:24Z","title":"Sharpened Lazy Incremental Quasi-Newton Method","summary":"  The problem of minimizing the sum of $n$ functions in $d$ dimensions is\nubiquitous in machine learning and statistics. In many applications where the\nnumber of observations $n$ is large, it is necessary to use incremental or\nstochastic methods, as their per-iteration cost is independent of $n$. Of\nthese, Quasi-Newton (QN) methods strike a balance between the per-iteration\ncost and the convergence rate. Specifically, they exhibit a superlinear rate\nwith $O(d^2)$ cost in contrast to the linear rate of first-order methods with\n$O(d)$ cost and the quadratic rate of second-order methods with $O(d^3)$ cost.\nHowever, existing incremental methods have notable shortcomings: Incremental\nQuasi-Newton (IQN) only exhibits asymptotic superlinear convergence. In\ncontrast, Incremental Greedy BFGS (IGS) offers explicit superlinear convergence\nbut suffers from poor empirical performance and has a per-iteration cost of\n$O(d^3)$. To address these issues, we introduce the Sharpened Lazy Incremental\nQuasi-Newton Method (SLIQN) that achieves the best of both worlds: an explicit\nsuperlinear convergence rate, and superior empirical performance at a\nper-iteration $O(d^2)$ cost. SLIQN features two key changes: first, it\nincorporates a hybrid strategy of using both classic and greedy BFGS updates,\nallowing it to empirically outperform both IQN and IGS. Second, it employs a\nclever constant multiplicative factor along with a lazy propagation strategy,\nwhich enables it to have a cost of $O(d^2)$. Additionally, our experiments\ndemonstrate the superiority of SLIQN over other incremental and stochastic\nQuasi-Newton variants and establish its competitiveness with second-order\nincremental methods.\n","authors":["Aakash Lahoti","Spandan Senapati","Ketan Rajawat","Alec Koppel"],"pdf_url":"https://arxiv.org/pdf/2305.17283v2.pdf","comment":"39 pages, 3 figures; Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.06952v1","updated":"2024-03-11T17:35:33Z","published":"2024-03-11T17:35:33Z","title":"SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with\n  Auto-Generated Data","summary":"  Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.\n","authors":["Jialu Li","Jaemin Cho","Yi-Lin Sung","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2403.06952v1.pdf","comment":"First two authors contributed equally; Project website:\n  https://selma-t2i.github.io/"},{"id":"http://arxiv.org/abs/2307.11888v2","updated":"2024-03-11T17:30:54Z","published":"2023-07-21T20:09:06Z","title":"Universality of Linear Recurrences Followed by Non-linear Projections:\n  Finite-Width Guarantees and Benefits of Complex Eigenvalues","summary":"  Deep neural networks based on linear complex-valued RNNs interleaved with\nposition-wise MLPs are gaining traction as competitive approaches to sequence\nmodeling. Examples of such architectures include state-space models (SSMs) like\nS4, LRU, and Mamba: recently proposed models that achieve promising performance\non text, genetics, and other data that require long-range reasoning. Despite\nexperimental evidence highlighting these architectures' effectiveness and\ncomputational efficiency, their expressive power remains relatively unexplored,\nespecially in connection to specific choices crucial in practice - e.g.,\ncarefully designed initialization distribution and use of complex numbers. In\nthis paper, we show that combining MLPs with both real or complex linear\ndiagonal recurrences leads to arbitrarily precise approximation of regular\ncausal sequence-to-sequence maps. At the heart of our proof, we rely on a\nseparation of concerns: the linear RNN provides a lossless encoding of the\ninput sequence, and the MLP performs non-linear processing on this encoding.\nWhile we show that using real diagonal linear recurrences is enough to achieve\nuniversality in this architecture, we prove that employing complex eigenvalues\nnear unit disk - i.e., empirically the most successful strategy in SSMs -\ngreatly helps the RNN in storing information. We connect this finding with the\nvanishing gradient issue and provide experimental evidence supporting our\nclaims.\n","authors":["Antonio Orvieto","Soham De","Caglar Gulcehre","Razvan Pascanu","Samuel L. Smith"],"pdf_url":"https://arxiv.org/pdf/2307.11888v2.pdf","comment":"v1: Accepted at HLD 2023: 1st Workshop on High-dimensional Learning\n  Dynamics v2: Preprint"},{"id":"http://arxiv.org/abs/2403.06942v1","updated":"2024-03-11T17:28:46Z","published":"2024-03-11T17:28:46Z","title":"Grid Monitoring and Protection with Continuous Point-on-Wave\n  Measurements and Generative AI","summary":"  Purpose This article presents a case for a next-generation grid monitoring\nand control system, leveraging recent advances in generative artificial\nintelligence (AI), machine learning, and statistical inference. Advancing\nbeyond earlier generations of wide-area monitoring systems built upon\nsupervisory control and data acquisition (SCADA) and synchrophasor\ntechnologies, we argue for a monitoring and control framework based on the\nstreaming of continuous point-on-wave (CPOW) measurements with AI-powered data\ncompression and fault detection.\n  Methods and Results: The architecture of the proposed design originates from\nthe Wiener-Kallianpur innovation representation of a random process that\ntransforms causally a stationary random process into an innovation sequence\nwith independent and identically distributed random variables. This work\npresents a generative AI approach that (i) learns an innovation autoencoder\nthat extracts innovation sequence from CPOW time series, (ii) compresses the\nCPOW streaming data with innovation autoencoder and subband coding, and (iii)\ndetects unknown faults and novel trends via nonparametric sequential hypothesis\ntesting.\n  Conclusion: This work argues that conventional monitoring using SCADA and\nphasor measurement unit (PMU) technologies is ill-suited for a future grid with\ndeep penetration of inverter-based renewable generations and distributed energy\nresources. A monitoring system based on CPOW data streaming and AI data\nanalytics should be the basic building blocks for situational awareness of a\nhighly dynamic future grid.\n","authors":["Lang Tong","Xinyi Wang","Qing Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.06942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06940v1","updated":"2024-03-11T17:26:18Z","published":"2024-03-11T17:26:18Z","title":"Conditional Score-Based Diffusion Model for Cortical Thickness\n  Trajectory Prediction","summary":"  Alzheimer's Disease (AD) is a neurodegenerative condition characterized by\ndiverse progression rates among individuals, with changes in cortical thickness\n(CTh) closely linked to its progression. Accurately forecasting CTh\ntrajectories can significantly enhance early diagnosis and intervention\nstrategies, providing timely care. However, the longitudinal data essential for\nthese studies often suffer from temporal sparsity and incompleteness,\npresenting substantial challenges in modeling the disease's progression\naccurately. Existing methods are limited, focusing primarily on datasets\nwithout missing entries or requiring predefined assumptions about CTh\nprogression. To overcome these obstacles, we propose a conditional score-based\ndiffusion model specifically designed to generate CTh trajectories with the\ngiven baseline information, such as age, sex, and initial diagnosis. Our\nconditional diffusion model utilizes all available data during the training\nphase to make predictions based solely on baseline information during inference\nwithout needing prior history about CTh progression. The prediction accuracy of\nthe proposed CTh prediction pipeline using a conditional score-based model was\ncompared for sub-groups consisting of cognitively normal, mild cognitive\nimpairment, and AD subjects. The Bland-Altman analysis shows our\ndiffusion-based prediction model has a near-zero bias with narrow 95%\nconfidential interval compared to the ground-truth CTh in 6-36 months. In\naddition, our conditional diffusion model has a stochastic generative nature,\ntherefore, we demonstrated an uncertainty analysis of patient-specific CTh\nprediction through multiple realizations.\n","authors":["Qing Xiao","Siyeop Yoon","Hui Ren","Matthew Tivnan","Lichao Sun","Quanzheng Li","Tianming Liu","Yu Zhang","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.06940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02645v2","updated":"2024-03-11T17:25:14Z","published":"2024-03-05T04:29:31Z","title":"DT-DDNN: A Physical Layer Security Attack Detector in 5G RF Domain for\n  CAVs","summary":"  The Synchronization Signal Block (SSB) is a fundamental component of the 5G\nNew Radio (NR) air interface, crucial for the initial access procedure of\nConnected and Automated Vehicles (CAVs), and serves several key purposes in the\nnetwork's operation. However, due to the predictable nature of SSB\ntransmission, including the Primary and Secondary Synchronization Signals (PSS\nand SSS), jamming attacks are critical threats. These attacks, which can be\nexecuted without requiring high power or complex equipment, pose substantial\nrisks to the 5G network, particularly as a result of the unencrypted\ntransmission of control signals. Leveraging RF domain knowledge, this work\npresents a novel deep learning-based technique for detecting jammers in CAV\nnetworks. Unlike the existing jamming detection algorithms that mostly rely on\nnetwork parameters, we introduce a double-threshold deep learning jamming\ndetector by focusing on the SSB. The detection method is focused on RF domain\nfeatures and improves the robustness of the network without requiring\nintegration with the pre-existing network infrastructure. By integrating a\npreprocessing block to extract PSS correlation and energy per null resource\nelements (EPNRE) characteristics, our method distinguishes between normal and\njammed received signals with high precision. Additionally, by incorporating of\nDiscrete Wavelet Transform (DWT), the efficacy of training and detection are\noptimized. A double-threshold double Deep Neural Network (DT-DDNN) is also\nintroduced to the architecture complemented by a deep cascade learning model to\nincrease the sensitivity of the model to variations of signal-to-jamming noise\nratio (SJNR). Results show that the proposed method achieves 96.4% detection\nrate in extra low jamming power, i.e., SJNR between 15 to 30 dB. Further,\nperformance of DT-DDNN is validated by analyzing real 5G signals obtained from\na practical testbed.\n","authors":["Ghazal Asemian","Mohammadreza Amini","Burak Kantarci","Melike Erol-Kantarci"],"pdf_url":"https://arxiv.org/pdf/2403.02645v2.pdf","comment":"15 pages, 16 figures"},{"id":"http://arxiv.org/abs/2311.15100v2","updated":"2024-03-11T17:23:24Z","published":"2023-11-25T18:58:15Z","title":"Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation","summary":"  In optimal transport (OT), a Monge map is known as a mapping that transports\na source distribution to a target distribution in the most cost-efficient way.\nRecently, multiple neural estimators for Monge maps have been developed and\napplied in diverse unpaired domain translation tasks, e.g. in single-cell\nbiology and computer vision. However, the classic OT framework enforces mass\nconservation, which makes it prone to outliers and limits its applicability in\nreal-world scenarios. The latter can be particularly harmful in OT domain\ntranslation tasks, where the relative position of a sample within a\ndistribution is explicitly taken into account. While unbalanced OT tackles this\nchallenge in the discrete setting, its integration into neural Monge map\nestimators has received limited attention. We propose a theoretically grounded\nmethod to incorporate unbalancedness into any Monge map estimator. We improve\nexisting estimators to model cell trajectories over time and to predict\ncellular responses to perturbations. Moreover, our approach seamlessly\nintegrates with the OT flow matching (OT-FM) framework. While we show that\nOT-FM performs competitively in image translation, we further improve\nperformance by incorporating unbalancedness (UOT-FM), which better preserves\nrelevant features. We hence establish UOT-FM as a principled method for\nunpaired image translation.\n","authors":["Luca Eyring","Dominik Klein","Théo Uscidda","Giovanni Palla","Niki Kilbertus","Zeynep Akata","Fabian Theis"],"pdf_url":"https://arxiv.org/pdf/2311.15100v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.06936v1","updated":"2024-03-11T17:21:39Z","published":"2024-03-11T17:21:39Z","title":"Counterfactual Reasoning with Knowledge Graph Embeddings","summary":"  Knowledge graph embeddings (KGEs) were originally developed to infer true but\nmissing facts in incomplete knowledge repositories. In this paper, we link\nknowledge graph completion and counterfactual reasoning via our new task CFKGR.\nWe model the original world state as a knowledge graph, hypothetical scenarios\nas edges added to the graph, and plausible changes to the graph as inferences\nfrom logical rules. We create corresponding benchmark datasets, which contain\ndiverse hypothetical scenarios with plausible changes to the original knowledge\ngraph and facts that should be retained. We develop COULDD, a general method\nfor adapting existing knowledge graph embeddings given a hypothetical premise,\nand evaluate it on our benchmark. Our results indicate that KGEs learn patterns\nin the graph without explicit training. We further observe that KGEs adapted\nwith COULDD solidly detect plausible counterfactual changes to the graph that\nfollow these patterns. An evaluation on human-annotated data reveals that KGEs\nadapted with COULDD are mostly unable to recognize changes to the graph that do\nnot follow learned inference rules. In contrast, ChatGPT mostly outperforms\nKGEs in detecting plausible changes to the graph but has poor knowledge\nretention. In summary, CFKGR connects two previously distinct areas, namely KG\ncompletion and counterfactual reasoning.\n","authors":["Lena Zellinger","Andreas Stephan","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2403.06936v1.pdf","comment":"Accepted to EACL 2024"},{"id":"http://arxiv.org/abs/2403.06925v1","updated":"2024-03-11T17:12:09Z","published":"2024-03-11T17:12:09Z","title":"Simplicity Bias of Transformers to Learn Low Sensitivity Functions","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of the inductive biases that they have and how\nthose biases are different from other neural network architectures remains\nelusive. Various neural network architectures such as fully connected networks\nhave been found to have a simplicity bias towards simple functions of the data;\none version of this simplicity bias is a spectral bias to learn simple\nfunctions in the Fourier space. In this work, we identify the notion of\nsensitivity of the model to random changes in the input as a notion of\nsimplicity bias which provides a unified metric to explain the simplicity and\nspectral bias of transformers across different data modalities. We show that\ntransformers have lower sensitivity than alternative architectures, such as\nLSTMs, MLPs and CNNs, across both vision and language tasks. We also show that\nlow-sensitivity bias correlates with improved robustness; furthermore, it can\nalso be used as an efficient intervention to further improve the robustness of\ntransformers.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v1.pdf","comment":"24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.10695v2","updated":"2024-03-11T17:08:36Z","published":"2024-02-16T13:58:23Z","title":"Unlink to Unlearn: Simplifying Edge Unlearning in GNNs","summary":"  As concerns over data privacy intensify, unlearning in Graph Neural Networks\n(GNNs) has emerged as a prominent research frontier in academia. This concept\nis pivotal in enforcing the \\textit{right to be forgotten}, which entails the\nselective removal of specific data from trained GNNs upon user request. Our\nresearch focuses on edge unlearning, a process of particular relevance to\nreal-world applications. Current state-of-the-art approaches like GNNDelete can\neliminate the influence of specific edges yet suffer from\n\\textit{over-forgetting}, which means the unlearning process inadvertently\nremoves excessive information beyond needed, leading to a significant\nperformance decline for remaining edges. Our analysis identifies the loss\nfunctions of GNNDelete as the primary source of over-forgetting and also\nsuggests that loss functions may be redundant for effective edge unlearning.\nBuilding on these insights, we simplify GNNDelete to develop \\textbf{Unlink to\nUnlearn} (UtU), a novel method that facilitates unlearning exclusively through\nunlinking the forget edges from graph structure. Our extensive experiments\ndemonstrate that UtU delivers privacy protection on par with that of a\nretrained model while preserving high accuracy in downstream tasks, by\nupholding over 97.3\\% of the retrained model's privacy protection capabilities\nand 99.8\\% of its link prediction accuracy. Meanwhile, UtU requires only\nconstant computational demands, underscoring its advantage as a highly\nlightweight and practical edge unlearning solution.\n","authors":["Jiajun Tan","Fei Sun","Ruichen Qiu","Du Su","Huawei Shen"],"pdf_url":"https://arxiv.org/pdf/2402.10695v2.pdf","comment":"Accepted by WWW 2024 as a Short Research Paper"},{"id":"http://arxiv.org/abs/2403.06910v1","updated":"2024-03-11T17:01:13Z","published":"2024-03-11T17:01:13Z","title":"Responsible Artificial Intelligence: A Structured Literature Review","summary":"  Our research endeavors to advance the concept of responsible artificial\nintelligence (AI), a topic of increasing importance within EU policy\ndiscussions. The EU has recently issued several publications emphasizing the\nnecessity of trust in AI, underscoring the dual nature of AI as both a\nbeneficial tool and a potential weapon. This dichotomy highlights the urgent\nneed for international regulation. Concurrently, there is a need for frameworks\nthat guide companies in AI development, ensuring compliance with such\nregulations. Our research aims to assist lawmakers and machine learning\npractitioners in navigating the evolving landscape of AI regulation,\nidentifying focal areas for future attention. This paper introduces a\ncomprehensive and, to our knowledge, the first unified definition of\nresponsible AI. Through a structured literature review, we elucidate the\ncurrent understanding of responsible AI. Drawing from this analysis, we propose\nan approach for developing a future framework centered around this concept. Our\nfindings advocate for a human-centric approach to Responsible AI. This approach\nencompasses the implementation of AI methods with a strong emphasis on ethics,\nmodel explainability, and the pillars of privacy, security, and trust.\n","authors":["Sabrina Goellner","Marina Tropmann-Frick","Bostjan Brumen"],"pdf_url":"https://arxiv.org/pdf/2403.06910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06906v1","updated":"2024-03-11T16:57:20Z","published":"2024-03-11T16:57:20Z","title":"Cost-Sensitive Learning to Defer to Multiple Experts with Workload\n  Constraints","summary":"  Learning to defer (L2D) aims to improve human-AI collaboration systems by\nlearning how to defer decisions to humans when they are more likely to be\ncorrect than an ML classifier. Existing research in L2D overlooks key aspects\nof real-world systems that impede its practical adoption, namely: i) neglecting\ncost-sensitive scenarios, where type 1 and type 2 errors have different costs;\nii) requiring concurrent human predictions for every instance of the training\ndataset and iii) not dealing with human work capacity constraints. To address\nthese issues, we propose the deferral under cost and capacity constraints\nframework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised\nlearning to model the probability of human error under less restrictive data\nrequirements (only one expert prediction per instance) and using constraint\nprogramming to globally minimize the error cost subject to workload\nlimitations. We test DeCCaF in a series of cost-sensitive fraud detection\nscenarios with different teams of 9 synthetic fraud analysts, with individual\nwork capacity constraints. The results demonstrate that our approach performs\nsignificantly better than the baselines in a wide array of scenarios, achieving\nan average 8.4% reduction in the misclassification cost.\n","authors":["Jean V. Alves","Diogo Leitão","Sérgio Jesus","Marco O. P. Sampaio","Javier Liébana","Pedro Saleiro","Mário A. T. Figueiredo","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2403.06906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06903v1","updated":"2024-03-11T16:56:01Z","published":"2024-03-11T16:56:01Z","title":"Benign overfitting in leaky ReLU networks with moderate input dimension","summary":"  The problem of benign overfitting asks whether it is possible for a model to\nperfectly fit noisy training data and still generalize well. We study benign\noverfitting in two-layer leaky ReLU networks trained with the hinge loss on a\nbinary classification task. We consider input data which can be decomposed into\nthe sum of a common signal and a random noise component, which lie on subspaces\northogonal to one another. We characterize conditions on the signal to noise\nratio (SNR) of the model parameters giving rise to benign versus non-benign, or\nharmful, overfitting: in particular, if the SNR is high then benign overfitting\noccurs, conversely if the SNR is low then harmful overfitting occurs. We\nattribute both benign and non-benign overfitting to an approximate margin\nmaximization property and show that leaky ReLU networks trained on hinge loss\nwith Gradient Descent (GD) satisfy this property. In contrast to prior work we\ndo not require near orthogonality conditions on the training data: notably, for\ninput dimension $d$ and training sample size $n$, while prior work shows\nasymptotically optimal error when $d = \\Omega(n^2 \\log n)$, here we require\nonly $d = \\Omega\\left(n \\log \\frac{1}{\\epsilon}\\right)$ to obtain error within\n$\\epsilon$ of optimal.\n","authors":["Kedar Karhadkar","Erin George","Michael Murray","Guido Montúfar","Deanna Needell"],"pdf_url":"https://arxiv.org/pdf/2403.06903v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.06901v1","updated":"2024-03-11T16:54:44Z","published":"2024-03-11T16:54:44Z","title":"LIBR+: Improving Intraoperative Liver Registration by Learning the\n  Residual of Biomechanics-Based Deformable Registration","summary":"  The surgical environment imposes unique challenges to the intraoperative\nregistration of organ shapes to their preoperatively-imaged geometry.\nBiomechanical model-based registration remains popular, while deep learning\nsolutions remain limited due to the sparsity and variability of intraoperative\nmeasurements and the limited ground-truth deformation of an organ that can be\nobtained during the surgery. In this paper, we propose a novel \\textit{hybrid}\nregistration approach that leverage a linearized iterative boundary\nreconstruction (LIBR) method based on linear elastic biomechanics, and use deep\nneural networks to learn its residual to the ground-truth deformation (LIBR+).\nWe further formulate a dual-branch spline-residual graph convolutional neural\nnetwork (SR-GCN) to assimilate information from sparse and variable\nintraoperative measurements and effectively propagate it through the geometry\nof the 3D organ. Experiments on a large intraoperative liver registration\ndataset demonstrated the consistent improvements achieved by LIBR+ in\ncomparison to existing rigid, biomechnical model-based non-rigid, and\ndeep-learning based non-rigid approaches to intraoperative liver registration.\n","authors":["Dingrong Wang","Soheil Azadvar","Jon Heiselman","Xiajun Jiang","Michael Miga","Linwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06901v1.pdf","comment":"12 pages, Medical Image Computing and Computer Assisted Intervention\n  2024"},{"id":"http://arxiv.org/abs/2212.14400v2","updated":"2024-03-11T16:49:16Z","published":"2022-12-29T18:14:38Z","title":"Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided\n  Quadruped Locomotion","summary":"  We present a framework for learning visually-guided quadruped locomotion by\nintegrating exteroceptive sensing and central pattern generators (CPGs), i.e.\nsystems of coupled oscillators, into the deep reinforcement learning (DRL)\nframework. Through both exteroceptive and proprioceptive sensing, the agent\nlearns to coordinate rhythmic behavior among different oscillators to track\nvelocity commands, while at the same time override these commands to avoid\ncollisions with the environment. We investigate several open robotics and\nneuroscience questions: 1) What is the role of explicit interoscillator\ncouplings between oscillators, and can such coupling improve sim-to-real\ntransfer for navigation robustness? 2) What are the effects of using a\nmemory-enabled vs. a memory-free policy network with respect to robustness,\nenergy-efficiency, and tracking performance in sim-to-real navigation tasks? 3)\nHow do animals manage to tolerate high sensorimotor delays, yet still produce\nsmooth and robust gaits? To answer these questions, we train our perceptive\nlocomotion policies in simulation and perform sim-to-real transfers to the\nUnitree Go1 quadruped, where we observe robust navigation in a variety of\nscenarios. Our results show that the CPG, explicit interoscillator couplings,\nand memory-enabled policy representations are all beneficial for energy\nefficiency, robustness to noise and sensory delays of 90 ms, and tracking\nperformance for successful sim-to-real transfer for navigation tasks. Video\nresults can be found at https://youtu.be/wpsbSMzIwgM.\n","authors":["Guillaume Bellegarda","Milad Shafiee","Auke Ijspeert"],"pdf_url":"https://arxiv.org/pdf/2212.14400v2.pdf","comment":"Accepted for 2024 IEEE International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2403.06890v1","updated":"2024-03-11T16:47:09Z","published":"2024-03-11T16:47:09Z","title":"Application of Quantum Tensor Networks for Protein Classification","summary":"  We show that protein sequences can be thought of as sentences in natural\nlanguage processing and can be parsed using the existing Quantum Natural\nLanguage framework into parameterized quantum circuits of reasonable qubits,\nwhich can be trained to solve various protein-related machine-learning\nproblems. We classify proteins based on their subcellular locations, a pivotal\ntask in bioinformatics that is key to understanding biological processes and\ndisease mechanisms. Leveraging the quantum-enhanced processing capabilities, we\ndemonstrate that Quantum Tensor Networks (QTN) can effectively handle the\ncomplexity and diversity of protein sequences. We present a detailed\nmethodology that adapts QTN architectures to the nuanced requirements of\nprotein data, supported by comprehensive experimental results. We demonstrate\ntwo distinct QTNs, inspired by classical recurrent neural networks (RNN) and\nconvolutional neural networks (CNN), to solve the binary classification task\nmentioned above. Our top-performing quantum model has achieved a 94% accuracy\nrate, which is comparable to the performance of a classical model that uses the\nESM2 protein language model embeddings. It's noteworthy that the ESM2 model is\nextremely large, containing 8 million parameters in its smallest configuration,\nwhereas our best quantum model requires only around 800 parameters. We\ndemonstrate that these hybrid models exhibit promising performance, showcasing\ntheir potential to compete with classical models of similar complexity.\n","authors":["Debarshi Kundu","Archisman Ghosh","Srinivasan Ekambaram","Jian Wang","Nikolay Dokholyan","Swaroop Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.06890v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.06888v1","updated":"2024-03-11T16:45:19Z","published":"2024-03-11T16:45:19Z","title":"HiRA-Pro: High resolution alignment of multimodal spatio-temporal data:\n  a process physics driven approach","summary":"  We present HiRA-Pro, a novel procedure to align, at high spatio-temporal\nresolutions, multimodal signals from real-world processes and systems that\nexhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing\nmachines. It is based on discerning and synchronizing the process signatures of\nsalient kinematic and dynamic events in these disparate signals. HiRA-Pro\naddresses the challenge of aligning data with sub-millisecond phenomena, where\ntraditional timestamp, external trigger, or clock-based alignment methods fall\nshort. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing\ncontext, where it aligns data from 13+ channels acquired during 3D-printing and\nmilling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data\nis then voxelized to generate 0.25 second aligned data chunks that correspond\nto physical voxels on the produced part. The superiority of HiRA-Pro is further\nshowcased through case studies in additive manufacturing, demonstrating\nimproved machine learning-based predictive performance due to precise\nmultimodal data alignment. Specifically, testing classification accuracies\nimproved by almost 35% with the application of HiRA-Pro, even with limited\ndata, allowing for precise localization of artifacts. The paper also provides a\ncomprehensive discussion on the proposed method, its applications, and\ncomparative qualitative analysis with a few other alignment methods. HiRA-Pro\nachieves temporal-spatial resolutions of 10-1000 us and 100 um in order to\ngenerate datasets that register with physical voxels on the 3D-printed and\nmilled part. These resolutions are at least an order of magnitude finer than\nthe existing alignment methods that employ individual timestamps, statistical\ncorrelations, or common clocks, which achieve precision of hundreds of\nmilliseconds.\n","authors":["Abhishek Hanchate","Himanshu Balhara","Vishal S. Chindepalli","Satish T. S. Bukkapatnam"],"pdf_url":"https://arxiv.org/pdf/2403.06888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06880v1","updated":"2024-03-11T16:34:23Z","published":"2024-03-11T16:34:23Z","title":"Unveiling the Significance of Toddler-Inspired Reward Transition in\n  Goal-Oriented Reinforcement Learning","summary":"  Toddlers evolve from free exploration with sparse feedback to exploiting\nprior experiences for goal-directed learning with denser rewards. Drawing\ninspiration from this Toddler-Inspired Reward Transition, we set out to explore\nthe implications of varying reward transitions when incorporated into\nReinforcement Learning (RL) tasks. Central to our inquiry is the transition\nfrom sparse to potential-based dense rewards, which share optimal strategies\nregardless of reward changes. Through various experiments, including those in\negocentric navigation and robotic arm manipulation tasks, we found that proper\nreward transitions significantly influence sample efficiency and success rates.\nOf particular note is the efficacy of the toddler-inspired Sparse-to-Dense\n(S2D) transition. Beyond these performance metrics, using Cross-Density\nVisualizer technique, we observed that transitions, especially the S2D, smooth\nthe policy loss landscape, promoting wide minima that enhance generalization in\nRL models.\n","authors":["Junseok Park","Yoonsung Kim","Hee Bin Yoo","Min Whoo Lee","Kibeom Kim","Won-Seok Choi","Minsu Lee","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06880v1.pdf","comment":"Accepted as a full paper at AAAI 2024 (Oral presentation): The 38th\n  Annual AAAI Conference on Artificial Intelligence (Main Tech Track). 7 pages\n  (main paper), 2 pages (references), 17 pages (appendix) each"},{"id":"http://arxiv.org/abs/2309.15257v2","updated":"2024-03-11T16:29:17Z","published":"2023-09-26T20:31:19Z","title":"STARC: A General Framework For Quantifying Differences Between Reward\n  Functions","summary":"  In order to solve a task using reinforcement learning, it is necessary to\nfirst formalise the goal of that task as a reward function. However, for many\nreal-world tasks, it is very difficult to manually specify a reward function\nthat never incentivises undesirable behaviour. As a result, it is increasingly\npopular to use \\emph{reward learning algorithms}, which attempt to \\emph{learn}\na reward function from data. However, the theoretical foundations of reward\nlearning are not yet well-developed. In particular, it is typically not known\nwhen a given reward learning algorithm with high probability will learn a\nreward function that is safe to optimise. This means that reward learning\nalgorithms generally must be evaluated empirically, which is expensive, and\nthat their failure modes are difficult to anticipate in advance. One of the\nroadblocks to deriving better theoretical guarantees is the lack of good\nmethods for quantifying the difference between reward functions. In this paper\nwe provide a solution to this problem, in the form of a class of pseudometrics\non the space of all reward functions that we call STARC (STAndardised Reward\nComparison) metrics. We show that STARC metrics induce both an upper and a\nlower bound on worst-case regret, which implies that our metrics are tight, and\nthat any metric with the same properties must be bilipschitz equivalent to\nours. Moreover, we also identify a number of issues with reward metrics\nproposed by earlier works. Finally, we evaluate our metrics empirically, to\ndemonstrate their practical efficacy. STARC metrics can be used to make both\ntheoretical and empirical analysis of reward learning algorithms both easier\nand more principled.\n","authors":["Joar Skalse","Lucy Farnik","Sumeet Ramesh Motwani","Erik Jenner","Adam Gleave","Alessandro Abate"],"pdf_url":"https://arxiv.org/pdf/2309.15257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14675v2","updated":"2024-03-11T16:28:15Z","published":"2023-09-26T05:03:13Z","title":"FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous\n  Client Devices using a Computing Power Aware Scheduler","summary":"  Cross-silo federated learning offers a promising solution to collaboratively\ntrain robust and generalized AI models without compromising the privacy of\nlocal datasets, e.g., healthcare, financial, as well as scientific projects\nthat lack a centralized data facility. Nonetheless, because of the disparity of\ncomputing resources among different clients (i.e., device heterogeneity),\nsynchronous federated learning algorithms suffer from degraded efficiency when\nwaiting for straggler clients. Similarly, asynchronous federated learning\nalgorithms experience degradation in the convergence rate and final model\naccuracy on non-identically and independently distributed (non-IID)\nheterogeneous datasets due to stale local models and client drift. To address\nthese limitations in cross-silo federated learning with heterogeneous clients\nand data, we propose FedCompass, an innovative semi-asynchronous federated\nlearning algorithm with a computing power-aware scheduler on the server side,\nwhich adaptively assigns varying amounts of training tasks to different clients\nusing the knowledge of the computing power of individual clients. FedCompass\nensures that multiple locally trained models from clients are received almost\nsimultaneously as a group for aggregation, effectively reducing the staleness\nof local models. At the same time, the overall training process remains\nasynchronous, eliminating prolonged waiting periods from straggler clients.\nUsing diverse non-IID heterogeneous distributed datasets, we demonstrate that\nFedCompass achieves faster convergence and higher accuracy than other\nasynchronous algorithms while remaining more efficient than synchronous\nalgorithms when performing federated learning on heterogeneous clients. The\nsource code for FedCompass is available at https://github.com/APPFL/FedCompass.\n","authors":["Zilinghan Li","Pranshu Chaturvedi","Shilan He","Han Chen","Gagandeep Singh","Volodymyr Kindratenko","E. A. Huerta","Kibaek Kim","Ravi Madduri"],"pdf_url":"https://arxiv.org/pdf/2309.14675v2.pdf","comment":"Accepted as poster at The Twelfth International Conference on\n  Learning Representations (ICLR 2024)"},{"id":"http://arxiv.org/abs/2403.06874v1","updated":"2024-03-11T16:26:35Z","published":"2024-03-11T16:26:35Z","title":"COOD: Combined out-of-distribution detection using multiple measures for\n  anomaly & novel class detection in large-scale hierarchical classification","summary":"  High-performing out-of-distribution (OOD) detection, both anomaly and novel\nclass, is an important prerequisite for the practical use of classification\nmodels. In this paper, we focus on the species recognition task in images\nconcerned with large databases, a large number of fine-grained hierarchical\nclasses, severe class imbalance, and varying image quality. We propose a\nframework for combining individual OOD measures into one combined OOD (COOD)\nmeasure using a supervised model. The individual measures are several existing\nstate-of-the-art measures and several novel OOD measures developed with novel\nclass detection and hierarchical class structure in mind. COOD was extensively\nevaluated on three large-scale (500k+ images) biodiversity datasets in the\ncontext of anomaly and novel class detection. We show that COOD outperforms\nindividual, including state-of-the-art, OOD measures by a large margin in terms\nof TPR@1% FPR in the majority of experiments, e.g., improving detecting\nImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.\nSHAP (feature contribution) analysis shows that different individual OOD\nmeasures are essential for various tasks, indicating that multiple OOD measures\nand combinations are needed to generalize. Additionally, we show that\nexplicitly considering ID images that are incorrectly classified for the\noriginal (species) recognition task is important for constructing\nhigh-performing OOD detection methods and for practical applicability. The\nframework can easily be extended or adapted to other tasks and media\nmodalities.\n","authors":["L. E. Hogeweg","R. Gangireddy","D. Brunink","V. J. Kalkman","L. Cornelissen","J. W. Kamminga"],"pdf_url":"https://arxiv.org/pdf/2403.06874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06873v1","updated":"2024-03-11T16:24:26Z","published":"2024-03-11T16:24:26Z","title":"Last Iterate Convergence of Incremental Methods and Applications in\n  Continual Learning","summary":"  Incremental gradient methods and incremental proximal methods are a\nfundamental class of optimization algorithms used for solving finite sum\nproblems, broadly studied in the literature. Yet, when it comes to their\nconvergence guarantees, nonasymptotic (first-order or proximal) oracle\ncomplexity bounds have been obtained fairly recently, almost exclusively\napplying to the average iterate. Motivated by applications in continual\nlearning, we obtain the first convergence guarantees for the last iterate of\nboth incremental gradient and incremental proximal methods, in general convex\nsmooth (for both) and convex Lipschitz (for the proximal variants) settings.\nOur oracle complexity bounds for the last iterate nearly match (i.e., match up\nto a square-root-log or a log factor) the best known oracle complexity bounds\nfor the average iterate, for both classes of methods. We further obtain\ngeneralizations of our results to weighted averaging of the iterates with\nincreasing weights, which can be seen as interpolating between the last iterate\nand the average iterate guarantees. Additionally, we discuss how our results\ncan be generalized to variants of studied incremental methods with permuted\nordering of updates. Our results generalize last iterate guarantees for\nincremental methods compared to state of the art, as such results were\npreviously known only for overparameterized linear models, which correspond to\nconvex quadratic problems with infinitely many solutions.\n","authors":["Xufeng Cai","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2403.06873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06871v1","updated":"2024-03-11T16:23:42Z","published":"2024-03-11T16:23:42Z","title":"On the Generalization Ability of Unsupervised Pretraining","summary":"  Recent advances in unsupervised learning have shown that unsupervised\npre-training, followed by fine-tuning, can improve model generalization.\nHowever, a rigorous understanding of how the representation function learned on\nan unlabeled dataset affects the generalization of the fine-tuned model is\nlacking. Existing theoretical research does not adequately account for the\nheterogeneity of the distribution and tasks in pre-training and fine-tuning\nstage. To bridge this gap, this paper introduces a novel theoretical framework\nthat illuminates the critical factor influencing the transferability of\nknowledge acquired during unsupervised pre-training to the subsequent\nfine-tuning phase, ultimately affecting the generalization capabilities of the\nfine-tuned model on downstream tasks. We apply our theoretical framework to\nanalyze generalization bound of two distinct scenarios: Context Encoder\npre-training with deep neural networks and Masked Autoencoder pre-training with\ndeep transformers, followed by fine-tuning on a binary classification task.\nFinally, inspired by our findings, we propose a novel regularization method\nduring pre-training to further enhances the generalization of fine-tuned model.\nOverall, our results contribute to a better understanding of unsupervised\npre-training and fine-tuning paradigm, and can shed light on the design of more\neffective pre-training algorithms.\n","authors":["Yuyang Deng","Junyuan Hong","Jiayu Zhou","Mehrdad Mahdavi"],"pdf_url":"https://arxiv.org/pdf/2403.06871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06870v1","updated":"2024-03-11T16:23:38Z","published":"2024-03-11T16:23:38Z","title":"Semantic Residual Prompts for Continual Learning","summary":"  Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained\nmodel and focus training on a few parameter vectors termed prompts. Most of\nthese methods organize these vectors in a pool of key-value pairs, and use the\ninput image as query to retrieve the prompts (values). However, as keys are\nlearned while tasks progress, the prompting selection strategy is itself\nsubject to catastrophic forgetting, an issue often overlooked by existing\napproaches. For instance, prompts introduced to accommodate new tasks might end\nup interfering with previously learned prompts. To make the selection strategy\nmore stable, we ask a foundational model (CLIP) to select our prompt within a\ntwo-level adaptation mechanism. Specifically, the first level leverages\nstandard textual prompts for the CLIP textual encoder, leading to stable class\nprototypes. The second level, instead, uses these prototypes along with the\nquery image as keys to index a second pool. The retrieved prompts serve to\nadapt a pre-trained ViT, granting plasticity. In doing so, we also propose a\nnovel residual mechanism to transfer CLIP semantics to the ViT layers. Through\nextensive analysis on established CL benchmarks, we show that our method\nsignificantly outperforms both state-of-the-art CL approaches and the zero-shot\nCLIP test. Notably, our findings hold true even for datasets with a substantial\ndomain gap w.r.t. the pre-training knowledge of the backbone model, as\nshowcased by experiments on satellite imagery and medical datasets.\n","authors":["Martin Menabue","Emanuele Frascaroli","Matteo Boschini","Enver Sangineto","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2403.06870v1.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.06869v1","updated":"2024-03-11T16:22:41Z","published":"2024-03-11T16:22:41Z","title":"Learning with Noisy Foundation Models","summary":"  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n","authors":["Hao Chen","Jindong Wang","Zihan Wang","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2403.06869v1.pdf","comment":"18 pages, 10 figures, 6 tables, preprint. arXiv admin note:\n  substantial text overlap with arXiv:2309.17002"},{"id":"http://arxiv.org/abs/2403.06860v1","updated":"2024-03-11T16:13:58Z","published":"2024-03-11T16:13:58Z","title":"A Geospatial Approach to Predicting Desert Locust Breeding Grounds in\n  Africa","summary":"  Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.\n","authors":["Ibrahim Salihu Yusuf","Mukhtar Opeyemi Yusuf","Kobby Panford-Quainoo","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2403.06860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06854v1","updated":"2024-03-11T16:09:39Z","published":"2024-03-11T16:09:39Z","title":"Quantifying the Sensitivity of Inverse Reinforcement Learning to\n  Misspecification","summary":"  Inverse reinforcement learning (IRL) aims to infer an agent's preferences\n(represented as a reward function $R$) from their behaviour (represented as a\npolicy $\\pi$). To do this, we need a behavioural model of how $\\pi$ relates to\n$R$. In the current literature, the most common behavioural models are\noptimality, Boltzmann-rationality, and causal entropy maximisation. However,\nthe true relationship between a human's preferences and their behaviour is much\nmore complex than any of these behavioural models. This means that the\nbehavioural models are misspecified, which raises the concern that they may\nlead to systematic errors if applied to real data. In this paper, we analyse\nhow sensitive the IRL problem is to misspecification of the behavioural model.\nSpecifically, we provide necessary and sufficient conditions that completely\ncharacterise how the observed data may differ from the assumed behavioural\nmodel without incurring an error above a given threshold. In addition to this,\nwe also characterise the conditions under which a behavioural model is robust\nto small perturbations of the observed policy, and we analyse how robust many\nbehavioural models are to misspecification of their parameter values (such as\ne.g.\\ the discount rate). Our analysis suggests that the IRL problem is highly\nsensitive to misspecification, in the sense that very mild misspecification can\nlead to very large errors in the inferred reward function.\n","authors":["Joar Skalse","Alessandro Abate"],"pdf_url":"https://arxiv.org/pdf/2403.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06843v1","updated":"2024-03-11T16:03:21Z","published":"2024-03-11T16:03:21Z","title":"Towards an educational tool for supporting neonatologists in the\n  delivery room","summary":"  Nowadays, there is evidence that several factors may increase the risk, for\nan infant, to require stabilisation or resuscitation manoeuvres at birth.\nHowever, this risk factors are not completely known, and a universally\napplicable model for predicting high-risk situations is not available yet.\nConsidering both these limitations and the fact that the need for resuscitation\nat birth is a rare event, periodic training of the healthcare personnel\nresponsible for newborn caring in the delivery room is mandatory.\n  In this paper, we propose a machine learning approach for identifying risk\nfactors and their impact on the birth event from real data, which can be used\nby personnel to progressively increase and update their knowledge. Our final\ngoal will be the one of designing a user-friendly mobile application, able to\nimprove the recognition rate and the planning of the appropriate interventions\non high-risk patients.\n","authors":["Giorgio Leonardi","Clara Maldarizzi","Stefania Montani","Manuel Striani","Mariachiara Martina Strozzi"],"pdf_url":"https://arxiv.org/pdf/2403.06843v1.pdf","comment":"9 pages, 5 figures, conference paper"},{"id":"http://arxiv.org/abs/2309.17002v2","updated":"2024-03-11T15:59:28Z","published":"2023-09-29T06:18:15Z","title":"Understanding and Mitigating the Label Noise in Pre-training on\n  Downstream Tasks","summary":"  Pre-training on large-scale datasets and then fine-tuning on downstream tasks\nhave become a standard practice in deep learning. However, pre-training data\noften contain label noise that may adversely affect the generalization of the\nmodel. This paper aims to understand the nature of noise in pre-training\ndatasets and to mitigate its impact on downstream tasks. More specifically,\nthrough extensive experiments of supervised pre-training models on synthetic\nnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\nin pre-training can benefit in-domain (ID) transfer performance, where the\ntraining and testing data share the same distribution, it always deteriorates\nout-of-domain (OOD) performance, where training and testing data distribution\nare different. We empirically verify that the reason behind is noise in\npre-training shapes the feature space differently. We then propose a\nlight-weight black-box tuning method (NMTune) to affine the feature space to\nmitigate the malignant effect of noise and improve generalization on both ID\nand OOD tasks, considering one may not be able to fully fine-tune or even\naccess the pre-trained models. We conduct practical experiments on popular\nvision and language models that are pre-trained on noisy data for evaluation of\nour approach. Our analysis and results show the importance of this interesting\nand novel research direction, which we term Noisy Model Learning.\n","authors":["Hao Chen","Jindong Wang","Ankit Shah","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2309.17002v2.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.05385v2","updated":"2024-03-11T15:59:08Z","published":"2024-03-08T15:30:58Z","title":"Switching the Loss Reduces the Cost in Batch Reinforcement Learning","summary":"  We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch\nreinforcement learning (RL). We show that the number of samples needed to learn\na near-optimal policy with FQI-LOG scales with the accumulated cost of the\noptimal policy, which is zero in problems where acting optimally achieves the\ngoal and incurs no cost. In doing so, we provide a general framework for\nproving $\\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal\nachievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses\nfewer samples than FQI trained with squared loss on problems where the optimal\npolicy reliably achieves the goal.\n","authors":["Alex Ayoub","Kaiwen Wang","Vincent Liu","Samuel Robertson","James McInerney","Dawen Liang","Nathan Kallus","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2403.05385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12020v4","updated":"2024-03-11T15:50:55Z","published":"2022-11-22T05:24:30Z","title":"PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated\n  Catalyst Design","summary":"  Mitigating the climate crisis requires a rapid transition towards\nlower-carbon energy. Catalyst materials play a crucial role in the\nelectrochemical reactions involved in numerous industrial processes key to this\ntransition, such as renewable energy storage and electrofuel synthesis. To\nreduce the energy spent on such activities, we must quickly discover more\nefficient catalysts to drive electrochemical reactions. Machine learning (ML)\nholds the potential to efficiently model materials properties from large\namounts of data, accelerating electrocatalyst design. The Open Catalyst Project\nOC20 dataset was constructed to that end. However, ML models trained on OC20\nare still neither scalable nor accurate enough for practical applications. In\nthis paper, we propose task-specific innovations applicable to most\narchitectures, enhancing both computational efficiency and accuracy. This\nincludes improvements in (1) the graph creation step, (2) atom representations,\n(3) the energy prediction head, and (4) the force prediction head. We describe\nthese contributions, referred to as PhAST, and evaluate them thoroughly on\nmultiple architectures. Overall, PhAST improves energy MAE by 4 to 42$\\%$ while\ndividing compute time by 3 to 8$\\times$ depending on the targeted task/model.\nPhAST also enables CPU training, leading to 40$\\times$ speedups in highly\nparallelized settings. Python package: \\url{https://phast.readthedocs.io}.\n","authors":["Alexandre Duval","Victor Schmidt","Santiago Miret","Yoshua Bengio","Alex Hernández-García","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2211.12020v4.pdf","comment":"Journal of Machine Learning Research (JMLR)"},{"id":"http://arxiv.org/abs/2403.06833v1","updated":"2024-03-11T15:48:56Z","published":"2024-03-11T15:48:56Z","title":"Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?","summary":"  Instruction-tuned Large Language Models (LLMs) have achieved breakthrough\nresults, opening countless new possibilities for many practical applications.\nHowever, LLMs lack elementary safety features that are established norms in\nother areas of computer science, such as the separation between instructions\nand data, causing them to malfunction or rendering them vulnerable to\nmanipulation and interference by third parties e.g., via indirect\nprompt/command injection. Even worse, so far, there is not even an established\ndefinition of what precisely such a separation would mean and how its violation\ncould be tested. In this work, we aim to close this gap. We introduce a formal\nmeasure to quantify the phenomenon of instruction-data separation as well as an\nempirical variant of the measure that can be computed from a model`s black-box\noutputs. We also introduce a new dataset, SEP (Should it be Executed or\nProcessed?), which allows estimating the measure, and we report results on\nseveral state-of-the-art open-source and closed LLMs. Finally, we\nquantitatively demonstrate that all evaluated LLMs fail to achieve a high\namount of separation, according to our measure. The source code and SEP dataset\nare openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.\n","authors":["Egor Zverev","Sahar Abdelnabi","Mario Fritz","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2403.06833v1.pdf","comment":"Accepted for ICLR 2024 Workshop on Secure and Trustworthy Large\n  Language Models, GitHub:\n  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 5 pages main\n  text, 17 pages in total"},{"id":"http://arxiv.org/abs/2307.05358v3","updated":"2024-03-11T15:48:08Z","published":"2023-07-11T15:45:03Z","title":"Combating Data Imbalances in Federated Semi-supervised Learning with\n  Dual Regulators","summary":"  Federated learning has become a popular method to learn from decentralized\nheterogeneous data. Federated semi-supervised learning (FSSL) emerges to train\nmodels from a small fraction of labeled data due to label scarcity on\ndecentralized clients. Existing FSSL methods assume independent and identically\ndistributed (IID) labeled data across clients and consistent class distribution\nbetween labeled and unlabeled data within a client. This work studies a more\npractical and challenging scenario of FSSL, where data distribution is\ndifferent not only across clients but also within a client between labeled and\nunlabeled data. To address this challenge, we propose a novel FSSL framework\nwith dual regulators, FedDure. FedDure lifts the previous assumption with a\ncoarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg\nregularizes the updating of the local model by tracking the learning effect on\nlabeled data distribution; F-reg learns an adaptive weighting scheme tailored\nfor unlabeled instances in each client. We further formulate the client model\ntraining as bi-level optimization that adaptively optimizes the model in the\nclient with two regulators. Theoretically, we show the convergence guarantee of\nthe dual regulators. Empirically, we demonstrate that FedDure is superior to\nthe existing methods across a wide range of settings, notably by more than 11\non CIFAR-10 and CINIC-10 datasets.\n","authors":["Sikai Bai","Shuaicheng Li","Weiming Zhuang","Jie Zhang","Song Guo","Kunlin Yang","Jun Hou","Shuai Zhang","Junyu Gao","Shuai Yi"],"pdf_url":"https://arxiv.org/pdf/2307.05358v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03143v2","updated":"2024-03-11T15:46:57Z","published":"2023-06-05T18:00:07Z","title":"Machine learning reveals features of spinon Fermi surface","summary":"  With rapid progress in simulation of strongly interacting quantum\nHamiltonians, the challenge in characterizing unknown phases becomes a\nbottleneck for scientific progress. We demonstrate that a Quantum-Classical\nhybrid approach (QuCl) of mining sampled projective snapshots with\ninterpretable classical machine learning can unveil signatures of seemingly\nfeatureless quantum states. The Kitaev-Heisenberg model on a honeycomb lattice\nunder external magnetic field presents an ideal system to test QuCl, where\nsimulations have found an intermediate gapless phase (IGP) sandwiched between\nknown phases, launching a debate over its elusive nature. We use the correlator\nconvolutional neural network, trained on labeled projective snapshots, in\nconjunction with regularization path analysis to identify signatures of phases.\nWe show that QuCl reproduces known features of established phases.\nSignificantly, we also identify a signature of the IGP in the spin channel\nperpendicular to the field direction, which we interpret as a signature of\nFriedel oscillations of gapless spinons forming a Fermi surface. Our\npredictions can guide future experimental searches for spin liquids.\n","authors":["Kevin Zhang","Shi Feng","Yuri D. Lensky","Nandini Trivedi","Eun-Ah Kim"],"pdf_url":"https://arxiv.org/pdf/2306.03143v2.pdf","comment":"9 pages + 7 pages supplemental"},{"id":"http://arxiv.org/abs/2403.06829v1","updated":"2024-03-11T15:44:40Z","published":"2024-03-11T15:44:40Z","title":"Constructing Variables Using Classifiers as an Aid to Regression: An\n  Empirical Assessment","summary":"  This paper proposes a method for the automatic creation of variables (in the\ncase of regression) that complement the information contained in the initial\ninput vector. The method works as a pre-processing step in which the continuous\nvalues of the variable to be regressed are discretized into a set of intervals\nwhich are then used to define value thresholds. Then classifiers are trained to\npredict whether the value to be regressed is less than or equal to each of\nthese thresholds. The different outputs of the classifiers are then\nconcatenated in the form of an additional vector of variables that enriches the\ninitial vector of the regression problem. The implemented system can thus be\nconsidered as a generic pre-processing tool. We tested the proposed enrichment\nmethod with 5 types of regressors and evaluated it in 33 regression datasets.\nOur experimental results confirm the interest of the approach.\n","authors":["Colin Troisemaine","Vincent Lemaire"],"pdf_url":"https://arxiv.org/pdf/2403.06829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06826v1","updated":"2024-03-11T15:43:14Z","published":"2024-03-11T15:43:14Z","title":"In-context Exploration-Exploitation for Reinforcement Learning","summary":"  In-context learning is a promising approach for online policy learning of\noffline reinforcement learning (RL) methods, which can be achieved at inference\ntime without gradient optimization. However, this method is hindered by\nsignificant computational costs resulting from the gathering of large training\ntrajectory sets and the need to train large Transformer models. We address this\nchallenge by introducing an In-context Exploration-Exploitation (ICEE)\nalgorithm, designed to optimize the efficiency of in-context policy learning.\nUnlike existing models, ICEE performs an exploration-exploitation trade-off at\ninference time within a Transformer model, without the need for explicit\nBayesian inference. Consequently, ICEE can solve Bayesian optimization problems\nas efficiently as Gaussian process biased methods do, but in significantly less\ntime. Through experiments in grid world environments, we demonstrate that ICEE\ncan learn to solve new RL tasks using only tens of episodes, marking a\nsubstantial improvement over the hundreds of episodes needed by the previous\nin-context learning method.\n","authors":["Zhenwen Dai","Federico Tomasi","Sina Ghiassian"],"pdf_url":"https://arxiv.org/pdf/2403.06826v1.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2402.14515v2","updated":"2024-03-11T15:40:18Z","published":"2024-02-22T13:04:50Z","title":"Spectral invariance and maximality properties of the frequency spectrum\n  of quantum neural networks","summary":"  Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine\nLearning due to their close connection to Variational Quantum Circuits, making\nthem a promising candidate for practical applications on Noisy\nIntermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite\nFourier series, where the set of frequencies is called the frequency spectrum.\nWe analyse this frequency spectrum and prove, for a large class of models,\nvarious maximality results. Furthermore, we prove that under some mild\nconditions there exists a bijection between classes of models with the same\narea $A = RL$ that preserves the frequency spectrum, where $R$ denotes the\nnumber of qubits and $L$ the number of layers, which we consequently call\nspectral invariance under area-preserving transformations. With this we explain\nthe symmetry in $R$ and $L$ in the results often observed in the literature and\nshow that the maximal frequency spectrum depends only on the area $A = RL$ and\nnot on the individual values of $R$ and $L$. Moreover, we extend existing\nresults and specify the maximum possible frequency spectrum of a QNN with\narbitrarily many layers as a function of the spectrum of its generators. If the\ngenerators of the QNN can be further decomposed into 2-dimensional\nsub-generators, then this specification follows from elementary\nnumber-theoretical considerations. In the case of arbitrary dimensional\ngenerators, we extend existing results based on the so-called Golomb ruler and\nintroduce a second novel approach based on a variation of the turnpike problem,\nwhich we call the relaxed turnpike problem.\n","authors":["Patrick Holzer","Ivica Turkalj"],"pdf_url":"https://arxiv.org/pdf/2402.14515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05179v2","updated":"2024-03-11T15:36:19Z","published":"2023-10-08T14:32:23Z","title":"Distributional Reinforcement Learning with Online Risk-awareness\n  Adaption","summary":"  The use of reinforcement learning (RL) in practical applications requires\nconsidering sub-optimal outcomes, which depend on the agent's familiarity with\nthe uncertain environment. Dynamically adjusting the level of epistemic risk\nover the course of learning can tactically achieve reliable optimal policy in\nsafety-critical environments and tackle the sub-optimality of a static risk\nlevel. In this work, we introduce a novel framework, Distributional RL with\nOnline Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic\nuncertainties compositely and dynamically select the epistemic risk levels via\nsolving a total variation minimization problem online. The risk level selection\ncan be efficiently achieved through grid search using a Follow-The-Leader type\nalgorithm, and its offline oracle is related to \"satisficing measure\" (in the\ndecision analysis community) under a special modification of the loss function.\nWe show multiple classes of tasks where DRL-ORA outperforms existing methods\nthat rely on either a fixed risk level or manually predetermined risk level\nadaption. Given the simplicity of our modifications, we believe the framework\ncan be easily incorporated into most RL algorithm variants.\n","authors":["Yupeng Wu","Wenjie Huang"],"pdf_url":"https://arxiv.org/pdf/2310.05179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06817v1","updated":"2024-03-11T15:34:57Z","published":"2024-03-11T15:34:57Z","title":"Are Targeted Messages More Effective?","summary":"  Graph neural networks (GNN) are deep learning architectures for graphs.\nEssentially, a GNN is a distributed message passing algorithm, which is\ncontrolled by parameters learned from data. It operates on the vertices of a\ngraph: in each iteration, vertices receive a message on each incoming edge,\naggregate these messages, and then update their state based on their current\nstate and the aggregated messages. The expressivity of GNNs can be\ncharacterised in terms of certain fragments of first-order logic with counting\nand the Weisfeiler-Lehman algorithm.\n  The core GNN architecture comes in two different versions. In the first\nversion, a message only depends on the state of the source vertex, whereas in\nthe second version it depends on the states of the source and target vertices.\nIn practice, both of these versions are used, but the theory of GNNs so far\nmostly focused on the first one. On the logical side, the two versions\ncorrespond to two fragments of first-order logic with counting that we call\nmodal and guarded.\n  The question whether the two versions differ in their expressivity has been\nmostly overlooked in the GNN literature and has only been asked recently\n(Grohe, LICS'23). We answer this question here. It turns out that the answer is\nnot as straightforward as one might expect. By proving that the modal and\nguarded fragment of first-order logic with counting have the same expressivity\nover labelled undirected graphs, we show that in a non-uniform setting the two\nGNN versions have the same expressivity. However, we also prove that in a\nuniform setting the second version is strictly more expressive.\n","authors":["Martin Grohe","Eran Rosenbluth"],"pdf_url":"https://arxiv.org/pdf/2403.06817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06816v1","updated":"2024-03-11T15:33:55Z","published":"2024-03-11T15:33:55Z","title":"Efficient first-order algorithms for large-scale, non-smooth maximum\n  entropy models with application to wildfire science","summary":"  Maximum entropy (Maxent) models are a class of statistical models that use\nthe maximum entropy principle to estimate probability distributions from data.\nDue to the size of modern data sets, Maxent models need efficient optimization\nalgorithms to scale well for big data applications. State-of-the-art algorithms\nfor Maxent models, however, were not originally designed to handle big data\nsets; these algorithms either rely on technical devices that may yield\nunreliable numerical results, scale poorly, or require smoothness assumptions\nthat many practical Maxent models lack. In this paper, we present novel\noptimization algorithms that overcome the shortcomings of state-of-the-art\nalgorithms for training large-scale, non-smooth Maxent models. Our proposed\nfirst-order algorithms leverage the Kullback-Leibler divergence to train\nlarge-scale and non-smooth Maxent models efficiently. For Maxent models with\ndiscrete probability distribution of $n$ elements built from samples, each\ncontaining $m$ features, the stepsize parameters estimation and iterations in\nour algorithms scale on the order of $O(mn)$ operations and can be trivially\nparallelized. Moreover, the strong $\\ell_{1}$ convexity of the\nKullback--Leibler divergence allows for larger stepsize parameters, thereby\nspeeding up the convergence rate of our algorithms. To illustrate the\nefficiency of our novel algorithms, we consider the problem of estimating\nprobabilities of fire occurrences as a function of ecological features in the\nWestern US MTBS-Interagency wildfire data set. Our numerical results show that\nour algorithms outperform the state of the arts by one order of magnitude and\nyield results that agree with physical models of wildfire occurrence and\nprevious statistical analyses of wildfire drivers.\n","authors":["Gabriel P. Langlois","Jatan Buch","Jérôme Darbon"],"pdf_url":"https://arxiv.org/pdf/2403.06816v1.pdf","comment":"The main text of our manuscript is 20 pages long, the appendices are\n  4 pages long, and the references are 4 pages long,for a total of 28 pages"},{"id":"http://arxiv.org/abs/2403.06814v1","updated":"2024-03-11T15:33:40Z","published":"2024-03-11T15:33:40Z","title":"ε-Neural Thompson Sampling of Deep Brain Stimulation for\n  Parkinson Disease Treatment","summary":"  Deep Brain Stimulation (DBS) stands as an effective intervention for\nalleviating the motor symptoms of Parkinson's disease (PD). Traditional\ncommercial DBS devices are only able to deliver fixed-frequency periodic pulses\nto the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS).\nHowever, they in general suffer from energy inefficiency and side effects, such\nas speech impairment. Recent research has focused on adaptive DBS (aDBS) to\nresolve the limitations of cDBS. Specifically, reinforcement learning (RL)\nbased approaches have been developed to adapt the frequencies of the stimuli in\norder to achieve both energy efficiency and treatment efficacy. However, RL\napproaches in general require significant amount of training data and\ncomputational resources, making it intractable to integrate RL policies into\nreal-time embedded systems as needed in aDBS. In contrast, contextual\nmulti-armed bandits (CMAB) in general lead to better sample efficiency compared\nto RL. In this study, we propose a CMAB solution for aDBS. Specifically, we\ndefine the context as the signals capturing irregular neuronal firing\nactivities in the BG regions (i.e., beta-band power spectral density), while\neach arm signifies the (discretized) pulse frequency of the stimulation.\nMoreover, an {\\epsilon}-exploring strategy is introduced on top of the classic\nThompson sampling method, leading to an algorithm called {\\epsilon}-Neural\nThompson sampling ({\\epsilon}-NeuralTS), such that the learned CMAB policy can\nbetter balance exploration and exploitation of the BG environment. The\n{\\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that\ncaptures the neuronal activities in PD patients' brains. The results show that\nour method outperforms both existing cDBS methods and CMAB baselines.\n","authors":["Hao-Lun Hsu","Qitong Gao","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2403.06814v1.pdf","comment":"11 pages, 12 figures, 2 tables. To appear in the 15th ACM/IEEE\n  International Conference on Cyber-Physical Systems (ICCPS'2024)"},{"id":"http://arxiv.org/abs/2403.06812v1","updated":"2024-03-11T15:32:56Z","published":"2024-03-11T15:32:56Z","title":"Monotone Individual Fairness","summary":"  We revisit the problem of online learning with individual fairness, where an\nonline learner strives to maximize predictive accuracy while ensuring that\nsimilar individuals are treated similarly. We first extend the frameworks of\nGillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human\nauditors regarding fairness violations, as we consider auditing schemes that\nare capable of aggregating feedback from any number of auditors, using a rich\nclass we term monotone aggregation functions. We then prove a characterization\nfor such auditing schemes, practically reducing the analysis of auditing for\nindividual fairness by multiple auditors to that of auditing by\n(instance-specific) single auditors. Using our generalized framework, we\npresent an oracle-efficient algorithm achieving an upper bound frontier of\n$(\\mathcal{O}(T^{1/2+2b}),\\mathcal{O}(T^{3/4-b}))$ respectively for regret,\nnumber of fairness violations, for $0\\leq b \\leq 1/4$. We then study an online\nclassification setting where label feedback is available for\npositively-predicted individuals only, and present an oracle-efficient\nalgorithm achieving an upper bound frontier of\n$(\\mathcal{O}(T^{2/3+2b}),\\mathcal{O}(T^{5/6-b}))$ for regret, number of\nfairness violations, for $0\\leq b \\leq 1/6$. In both settings, our algorithms\nimprove on the best known bounds for oracle-efficient algorithms. Furthermore,\nour algorithms offer significant improvements in computational efficiency,\ngreatly reducing the number of required calls to an (offline) optimization\noracle per round, to $\\tilde{\\mathcal{O}}(\\alpha^{-2})$ in the full information\nsetting, and $\\tilde{\\mathcal{O}}(\\alpha^{-2} + k^2T^{1/3})$ in the partial\ninformation setting, where $\\alpha$ is the sensitivity for reporting fairness\nviolations, and $k$ is the number of individuals in a round.\n","authors":["Yahav Bechavod"],"pdf_url":"https://arxiv.org/pdf/2403.06812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06807v1","updated":"2024-03-11T15:26:34Z","published":"2024-03-11T15:26:34Z","title":"Multistep Consistency Models","summary":"  Diffusion models are relatively easy to train but require many steps to\ngenerate samples. Consistency models are far more difficult to train, but\ngenerate samples in a single step.\n  In this paper we propose Multistep Consistency Models: A unification between\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\ncan interpolate between a consistency model and a diffusion model: a trade-off\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\nmodel is a conventional consistency model whereas we show that a $\\infty$-step\nconsistency model is a diffusion model.\n  Multistep Consistency Models work really well in practice. By increasing the\nsample budget from a single step to 2-8 steps, we can train models more easily\nthat generate higher quality samples, while retaining much of the sampling\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\nFID on Imagenet128 in 8 steps with consistency distillation. We also show that\nour method scales to a text-to-image diffusion model, generating samples that\nare very close to the quality of the original model.\n","authors":["Jonathan Heek","Emiel Hoogeboom","Tim Salimans"],"pdf_url":"https://arxiv.org/pdf/2403.06807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13171v2","updated":"2024-03-11T15:25:57Z","published":"2024-01-24T01:33:39Z","title":"Compositional Generative Inverse Design","summary":"  Inverse design, where we seek to design input variables in order to optimize\nan underlying objective function, is an important problem that arises across\nfields such as mechanical engineering to aerospace engineering. Inverse design\nis typically formulated as an optimization problem, with recent works\nleveraging optimization across learned dynamics models. However, as models are\noptimized they tend to fall into adversarial modes, preventing effective\nsampling. We illustrate that by instead optimizing over the learned energy\nfunction captured by the diffusion model, we can avoid such adversarial\nexamples and significantly improve design performance. We further illustrate\nhow such a design system is compositional, enabling us to combine multiple\ndifferent diffusion models representing subcomponents of our desired system to\ndesign systems with every specified component. In an N-body interaction task\nand a challenging 2D multi-airfoil design task, we demonstrate that by\ncomposing the learned diffusion model at test time, our method allows us to\ndesign initial states and boundary shapes that are more complex than those in\nthe training data. Our method generalizes to more objects for N-body dataset\nand discovers formation flying to minimize drag in the multi-airfoil design\ntask. Project website and code can be found at\nhttps://github.com/AI4Science-WestlakeU/cindm.\n","authors":["Tailin Wu","Takashi Maruyama","Long Wei","Tao Zhang","Yilun Du","Gianluca Iaccarino","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2401.13171v2.pdf","comment":"ICLR 2024 spotlight. 30 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.06806v1","updated":"2024-03-11T15:25:03Z","published":"2024-03-11T15:25:03Z","title":"On the Global Convergence of Policy Gradient in Average Reward Markov\n  Decision Processes","summary":"  We present the first finite time global convergence analysis of policy\ngradient in the context of infinite horizon average reward Markov decision\nprocesses (MDPs). Specifically, we focus on ergodic tabular MDPs with finite\nstate and action spaces. Our analysis shows that the policy gradient iterates\nconverge to the optimal policy at a sublinear rate of\n$O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$\nregret, where $T$ represents the number of iterations. Prior work on\nperformance bounds for discounted reward MDPs cannot be extended to average\nreward MDPs because the bounds grow proportional to the fifth power of the\neffective horizon. Thus, our primary contribution is in proving that the policy\ngradient algorithm converges for average-reward MDPs and in obtaining\nfinite-time performance guarantees. In contrast to the existing discounted\nreward performance bounds, our performance bounds have an explicit dependence\non constants that capture the complexity of the underlying MDP. Motivated by\nthis observation, we reexamine and improve the existing performance bounds for\ndiscounted reward MDPs. We also present simulations to empirically evaluate the\nperformance of average reward policy gradient algorithm.\n","authors":["Navdeep Kumar","Yashaswini Murthy","Itai Shufaro","Kfir Y. Levy","R. Srikant","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2403.06806v1.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.06798v1","updated":"2024-03-11T15:16:20Z","published":"2024-03-11T15:16:20Z","title":"Dynamic Perturbation-Adaptive Adversarial Training on Medical Image\n  Classification","summary":"  Remarkable successes were made in Medical Image Classification (MIC)\nrecently, mainly due to wide applications of convolutional neural networks\n(CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity\nwith raw data, raising serious concerns on network robustness. Although\nadversarial training (AT), in responding to malevolent AEs, was recognized as\nan effective approach to improve robustness, it was challenging to overcome\ngeneralization decline of networks caused by the AT. In this paper, in order to\nreserve high generalization while improving robustness, we proposed a dynamic\nperturbation-adaptive adversarial training (DPAAT) method, which placed AT in a\ndynamic learning environment to generate adaptive data-level perturbations and\nprovided a dynamically updated criterion by loss information collections to\nhandle the disadvantage of fixed perturbation sizes in conventional AT methods\nand the dependence on external transference. Comprehensive testing on\ndermatology HAM10000 dataset showed that the DPAAT not only achieved better\nrobustness improvement and generalization preservation but also significantly\nenhanced mean average precision and interpretability on various CNNs,\nindicating its great potential as a generic adversarial training method on the\nMIC.\n","authors":["Shuai Li","Xiaoguang Ma","Shancheng Jiang","Lu Meng"],"pdf_url":"https://arxiv.org/pdf/2403.06798v1.pdf","comment":"9 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.06797v1","updated":"2024-03-11T15:15:50Z","published":"2024-03-11T15:15:50Z","title":"Leveraging Internal Representations of Model for Magnetic Image\n  Classification","summary":"  Data generated by edge devices has the potential to train intelligent\nautonomous systems across various domains. Despite the emergence of diverse\nmachine learning approaches addressing privacy concerns and utilizing\ndistributed data, security issues persist due to the sensitive storage of data\nshards in disparate locations. This paper introduces a potentially\ngroundbreaking paradigm for machine learning model training, specifically\ndesigned for scenarios with only a single magnetic image and its corresponding\nlabel image available. We harness the capabilities of Deep Learning to generate\nconcise yet informative samples, aiming to overcome data scarcity. Through the\nutilization of deep learning's internal representations, our objective is to\nefficiently address data scarcity issues and produce meaningful results. This\nmethodology presents a promising avenue for training machine learning models\nwith minimal data.\n","authors":["Adarsh N L","Arun P V","Alok Porwal","Malcolm Aranha"],"pdf_url":"https://arxiv.org/pdf/2403.06797v1.pdf","comment":"5 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2310.16047v2","updated":"2024-03-11T15:14:51Z","published":"2023-10-24T17:58:54Z","title":"From Posterior Sampling to Meaningful Diversity in Image Restoration","summary":"  Image restoration problems are typically ill-posed in the sense that each\ndegraded image can be restored in infinitely many valid ways. To accommodate\nthis, many works generate a diverse set of outputs by attempting to randomly\nsample from the posterior distribution of natural images given the degraded\ninput. Here we argue that this strategy is commonly of limited practical value\nbecause of the heavy tail of the posterior distribution. Consider for example\ninpainting a missing region of the sky in an image. Since there is a high\nprobability that the missing region contains no object but clouds, any set of\nsamples from the posterior would be entirely dominated by (practically\nidentical) completions of sky. However, arguably, presenting users with only\none clear sky completion, along with several alternative solutions such as\nairships, birds, and balloons, would better outline the set of possibilities.\nIn this paper, we initiate the study of meaningfully diverse image restoration.\nWe explore several post-processing approaches that can be combined with any\ndiverse image restoration method to yield semantically meaningful diversity.\nMoreover, we propose a practical approach for allowing diffusion based image\nrestoration methods to generate meaningfully diverse outputs, while incurring\nonly negligent computational overhead. We conduct extensive user studies to\nanalyze the proposed techniques, and find the strategy of reducing similarity\nbetween outputs to be significantly favorable over posterior sampling. Code and\nexamples are available at https://noa-cohen.github.io/MeaningfulDiversityInIR.\n","authors":["Noa Cohen","Hila Manor","Yuval Bahat","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2310.16047v2.pdf","comment":"Accepted for ICLR 2024. Code and examples are available at\n  https://noa-cohen.github.io/MeaningfulDiversityInIR"},{"id":"http://arxiv.org/abs/2308.13139v2","updated":"2024-03-11T14:50:03Z","published":"2023-08-25T02:32:36Z","title":"MatchXML: An Efficient Text-label Matching Framework for Extreme\n  Multi-label Text Classification","summary":"  The eXtreme Multi-label text Classification(XMC) refers to training a\nclassifier that assigns a text sample with relevant labels from an extremely\nlarge-scale label set (e.g., millions of labels). We propose MatchXML, an\nefficient text-label matching framework for XMC. We observe that the label\nembeddings generated from the sparse Term Frequency-Inverse Document\nFrequency(TF-IDF) features have several limitations. We thus propose label2vec\nto effectively train the semantic dense label embeddings by the Skip-gram\nmodel. The dense label embeddings are then used to build a Hierarchical Label\nTree by clustering. In fine-tuning the pre-trained encoder Transformer, we\nformulate the multi-label text classification as a text-label matching problem\nin a bipartite graph. We then extract the dense text representations from the\nfine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also\nextract the static dense sentence embeddings from a pre-trained Sentence\nTransformer. Finally, a linear ranker is trained by utilizing the sparse TF-IDF\nfeatures, the fine-tuned dense text representations and static dense sentence\nfeatures. Experimental results demonstrate that MatchXML achieves\nstate-of-the-art accuracy on five out of six datasets. As for the speed,\nMatchXML outperforms the competing methods on all the six datasets. Our source\ncode is publicly available at https://github.com/huiyegit/MatchXML.\n","authors":["Hui Ye","Rajshekhar Sunderraman","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2308.13139v2.pdf","comment":"Accepted to TKDE 2024"},{"id":"http://arxiv.org/abs/2306.02090v2","updated":"2024-03-11T14:48:34Z","published":"2023-06-03T11:45:16Z","title":"Deep Classifier Mimicry without Data Access","summary":"  Access to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.\n","authors":["Steven Braun","Martin Mundt","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2306.02090v2.pdf","comment":"11 pages main, 4 figures, 2 tables, 4 pages appendix"},{"id":"http://arxiv.org/abs/2401.08819v2","updated":"2024-03-11T14:43:52Z","published":"2024-01-16T20:42:15Z","title":"Learning from Sparse Offline Datasets via Conservative Density\n  Estimation","summary":"  Offline reinforcement learning (RL) offers a promising direction for learning\npolicies from pre-collected datasets without requiring further interactions\nwith the environment. However, existing methods struggle to handle\nout-of-distribution (OOD) extrapolation errors, especially in sparse reward or\nscarce data settings. In this paper, we propose a novel training algorithm\ncalled Conservative Density Estimation (CDE), which addresses this challenge by\nexplicitly imposing constraints on the state-action occupancy stationary\ndistribution. CDE overcomes the limitations of existing approaches, such as the\nstationary distribution correction method, by addressing the support mismatch\nissue in marginal importance sampling. Our method achieves state-of-the-art\nperformance on the D4RL benchmark. Notably, CDE consistently outperforms\nbaselines in challenging tasks with sparse rewards or insufficient data,\ndemonstrating the advantages of our approach in addressing the extrapolation\nerror problem in offline RL.\n","authors":["Zhepeng Cen","Zuxin Liu","Zitong Wang","Yihang Yao","Henry Lam","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.08819v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2307.03672v3","updated":"2024-03-11T14:42:58Z","published":"2023-07-07T15:42:35Z","title":"Simulation-free Schrödinger bridges via score and flow matching","summary":"  We present simulation-free score and flow matching ([SF]$^2$M), a\nsimulation-free objective for inferring stochastic dynamics given unpaired\nsamples drawn from arbitrary source and target distributions. Our method\ngeneralizes both the score-matching loss used in the training of diffusion\nmodels and the recently proposed flow matching loss used in the training of\ncontinuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic\ngenerative modeling as a Schr\\\"odinger bridge problem. It relies on static\nentropy-regularized optimal transport, or a minibatch approximation, to\nefficiently learn the SB without simulating the learned stochastic process. We\nfind that [SF]$^2$M is more efficient and gives more accurate solutions to the\nSB problem than simulation-based methods from prior work. Finally, we apply\n[SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably,\n[SF]$^2$M is the first method to accurately model cell dynamics in high\ndimensions and can recover known gene regulatory networks from simulated data.\nOur code is available in the TorchCFM package at\nhttps://github.com/atong01/conditional-flow-matching.\n","authors":["Alexander Tong","Nikolay Malkin","Kilian Fatras","Lazar Atanackovic","Yanlei Zhang","Guillaume Huguet","Guy Wolf","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2307.03672v3.pdf","comment":"AISTATS 2024. Code:\n  https://github.com/atong01/conditional-flow-matching"},{"id":"http://arxiv.org/abs/2403.06771v1","updated":"2024-03-11T14:39:24Z","published":"2024-03-11T14:39:24Z","title":"Redefining Event Types and Group Evolution in Temporal Data","summary":"  Groups -- such as clusters of points or communities of nodes -- are\nfundamental when addressing various data mining tasks. In temporal data, the\npredominant approach for characterizing group evolution has been through the\nidentification of ``events\". However, the events usually described in the\nliterature, e.g., shrinks/growths, splits/merges, are often arbitrarily\ndefined, creating a gap between such theoretical/predefined types and real-data\ngroup observations. Moving beyond existing taxonomies, we think of events as\n``archetypes\" characterized by a unique combination of quantitative dimensions\nthat we call ``facets\". Group dynamics are defined by their position within the\nfacet space, where archetypal events occupy extremities. Thus, rather than\nenforcing strict event types, our approach can allow for hybrid descriptions of\ndynamics involving group proximity to multiple archetypes. We apply our\nframework to evolving groups from several face-to-face interaction datasets,\nshowing it enables richer, more reliable characterization of group dynamics\nwith respect to state-of-the-art methods, especially when the groups are\nsubject to complex relationships. Our approach also offers intuitive solutions\nto common tasks related to dynamic group analysis, such as choosing an\nappropriate aggregation scale, quantifying partition stability, and evaluating\nevent quality.\n","authors":["Andrea Failla","Rémy Cazabet","Giulio Rossetti","Salvatore Citraro"],"pdf_url":"https://arxiv.org/pdf/2403.06771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06768v1","updated":"2024-03-11T14:37:57Z","published":"2024-03-11T14:37:57Z","title":"XB-MAML: Learning Expandable Basis Parameters for Effective\n  Meta-Learning with Wide Task Coverage","summary":"  Meta-learning, which pursues an effective initialization model, has emerged\nas a promising approach to handling unseen tasks. However, a limitation remains\nto be evident when a meta-learner tries to encompass a wide range of task\ndistribution, e.g., learning across distinctive datasets or domains. Recently,\na group of works has attempted to employ multiple model initializations to\ncover widely-ranging tasks, but they are limited in adaptively expanding\ninitializations. We introduce XB-MAML, which learns expandable basis\nparameters, where they are linearly combined to form an effective\ninitialization to a given task. XB-MAML observes the discrepancy between the\nvector space spanned by the basis and fine-tuned parameters to decide whether\nto expand the basis. Our method surpasses the existing works in the\nmulti-domain meta-learning benchmarks and opens up new chances of meta-learning\nfor obtaining the diverse inductive bias that can be combined to stretch toward\nthe effective initialization for diverse unseen tasks.\n","authors":["Jae-Jun Lee","Sung Whan Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.06768v1.pdf","comment":"In Proceedings of the International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024, Valencia, Spain"},{"id":"http://arxiv.org/abs/2403.03353v2","updated":"2024-03-11T14:37:42Z","published":"2024-03-05T22:42:29Z","title":"Hypothesis Spaces for Deep Learning","summary":"  This paper introduces a hypothesis space for deep learning that employs deep\nneural networks (DNNs). By treating a DNN as a function of two variables, the\nphysical variable and parameter variable, we consider the primitive set of the\nDNNs for the parameter variable located in a set of the weight matrices and\nbiases determined by a prescribed depth and widths of the DNNs. We then\ncomplete the linear span of the primitive DNN set in a weak* topology to\nconstruct a Banach space of functions of the physical variable. We prove that\nthe Banach space so constructed is a reproducing kernel Banach space (RKBS) and\nconstruct its reproducing kernel. We investigate two learning models,\nregularized learning and minimum interpolation problem in the resulting RKBS,\nby establishing representer theorems for solutions of the learning models. The\nrepresenter theorems unfold that solutions of these learning models can be\nexpressed as linear combination of a finite number of kernel sessions\ndetermined by given data and the reproducing kernel.\n","authors":["Rui Wang","Yuesheng Xu","Mingsong Yan"],"pdf_url":"https://arxiv.org/pdf/2403.03353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06759v1","updated":"2024-03-11T14:31:03Z","published":"2024-03-11T14:31:03Z","title":"Average Calibration Error: A Differentiable Loss for Improved\n  Reliability in Image Segmentation","summary":"  Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS\n","authors":["Theodore Barfoot","Luis Garcia-Peraza-Herrera","Ben Glocker","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06757v1","updated":"2024-03-11T14:29:56Z","published":"2024-03-11T14:29:56Z","title":"Koopman Ensembles for Probabilistic Time Series Forecasting","summary":"  In the context of an increasing popularity of data-driven models to represent\ndynamical systems, many machine learning-based implementations of the Koopman\noperator have recently been proposed. However, the vast majority of those works\nare limited to deterministic predictions, while the knowledge of uncertainty is\ncritical in fields like meteorology and climatology. In this work, we\ninvestigate the training of ensembles of models to produce stochastic outputs.\nWe show through experiments on real remote sensing image time series that\nensembles of independently trained models are highly overconfident and that\nusing a training criterion that explicitly encourages the members to produce\npredictions with high inter-model variances greatly improves the uncertainty\nquantification of the ensembles.\n","authors":["Anthony Frion","Lucas Drumetz","Guillaume Tochon","Mauro Dalla Mura","Albdeldjalil Aïssa El Bey"],"pdf_url":"https://arxiv.org/pdf/2403.06757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06754v1","updated":"2024-03-11T14:28:40Z","published":"2024-03-11T14:28:40Z","title":"ALaRM: Align Language Models via Hierarchical Rewards Modeling","summary":"  We introduce ALaRM, the first framework modeling hierarchical rewards in\nreinforcement learning from human feedback (RLHF), which is designed to enhance\nthe alignment of large language models (LLMs) with human preferences. The\nframework addresses the limitations of current alignment approaches, which\noften struggle with the inconsistency and sparsity of human supervision\nsignals, by integrating holistic rewards with aspect-specific rewards. This\nintegration enables more precise and consistent guidance of language models\ntowards desired outcomes, particularly in complex and open text generation\ntasks. By employing a methodology that filters and combines multiple rewards\nbased on their consistency, the framework provides a reliable mechanism for\nimproving model alignment. We validate our approach through applications in\nlong-form question answering and machine translation tasks, employing\ngpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over\nexisting baselines. Our work underscores the effectiveness of hierarchical\nrewards modeling in refining LLM training processes for better human preference\nalignment. We release our code at https://ALaRM-fdu.github.io.\n","authors":["Yuhang Lai","Siyuan Wang","Shujun Liu","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2403.06754v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2302.00482v4","updated":"2024-03-11T14:27:48Z","published":"2023-02-01T14:47:17Z","title":"Improving and generalizing flow-based generative models with minibatch\n  optimal transport","summary":"  Continuous normalizing flows (CNFs) are an attractive generative modeling\ntechnique, but they have been held back by limitations in their\nsimulation-based maximum likelihood training. We introduce the generalized\nconditional flow matching (CFM) technique, a family of simulation-free training\nobjectives for CNFs. CFM features a stable regression objective like that used\nto train the stochastic flow in diffusion models but enjoys the efficient\ninference of deterministic flow models. In contrast to both diffusion models\nand prior CNF training algorithms, CFM does not require the source distribution\nto be Gaussian or require evaluation of its density. A variant of our objective\nis optimal transport CFM (OT-CFM), which creates simpler flows that are more\nstable to train and lead to faster inference, as evaluated in our experiments.\nFurthermore, we show that when the true OT plan is available, our OT-CFM method\napproximates dynamic OT. Training CNFs with CFM improves results on a variety\nof conditional and unconditional generation tasks, such as inferring single\ncell dynamics, unsupervised image translation, and Schr\\\"odinger bridge\ninference.\n","authors":["Alexander Tong","Kilian Fatras","Nikolay Malkin","Guillaume Huguet","Yanlei Zhang","Jarrid Rector-Brooks","Guy Wolf","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2302.00482v4.pdf","comment":"TMLR. Code: https://github.com/atong01/conditional-flow-matching"},{"id":"http://arxiv.org/abs/2403.06750v1","updated":"2024-03-11T14:20:13Z","published":"2024-03-11T14:20:13Z","title":"Generalising Multi-Agent Cooperation through Task-Agnostic Communication","summary":"  Existing communication methods for multi-agent reinforcement learning (MARL)\nin cooperative multi-robot problems are almost exclusively task-specific,\ntraining new communication strategies for each unique task. We address this\ninefficiency by introducing a communication strategy applicable to any task\nwithin a given environment. We pre-train the communication strategy without\ntask-specific reward guidance in a self-supervised manner using a set\nautoencoder. Our objective is to learn a fixed-size latent Markov state from a\nvariable number of agent observations. Under mild assumptions, we prove that\npolicies using our latent representations are guaranteed to converge, and upper\nbound the value error introduced by our Markov state approximation. Our method\nenables seamless adaptation to novel tasks without fine-tuning the\ncommunication strategy, gracefully supports scaling to more agents than present\nduring training, and detects out-of-distribution events in an environment.\nEmpirical results on diverse MARL scenarios validate the effectiveness of our\napproach, surpassing task-specific communication strategies in unseen tasks.\nOur implementation of this work is available at\nhttps://github.com/proroklab/task-agnostic-comms.\n","authors":["Dulhan Jayalath","Steven Morad","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2403.06750v1.pdf","comment":"12 pages, 6 figures, submitted to Distributed Autonomous Robotic\n  Systems (DARS 2024)"},{"id":"http://arxiv.org/abs/2305.16363v2","updated":"2024-03-11T14:18:30Z","published":"2023-05-25T09:22:12Z","title":"Subpopulation-Specific Synthetic EHR for Better Mortality Prediction","summary":"  Electronic health records (EHR) often contain different rates of\nrepresentation of certain subpopulations (SP). Factors like patient\ndemographics, clinical condition prevalence, and medical center type contribute\nto this underrepresentation. Consequently, when training machine learning\nmodels on such datasets, the models struggle to generalize well and perform\npoorly on underrepresented SPs. To address this issue, we propose a novel\nensemble framework that utilizes generative models. Specifically, we train a\nGAN-based synthetic data generator for each SP and incorporate synthetic\nsamples into each SP training set. Ultimately, we train SP-specific prediction\nmodels. To properly evaluate this method, we design an evaluation pipeline with\n2 real-world use case datasets, queried from the MIMIC database. Our approach\nshows increased model performance over underrepresented SPs. Our code and\nmodels are given as supplementary and will be made available on a public\nrepository.\n","authors":["Oriel Perets","Nadav Rappoport"],"pdf_url":"https://arxiv.org/pdf/2305.16363v2.pdf","comment":"10 pages, 4 figures, submitted to AIME 2024"},{"id":"http://arxiv.org/abs/2403.06748v1","updated":"2024-03-11T14:14:52Z","published":"2024-03-11T14:14:52Z","title":"Shortcut Learning in Medical Image Segmentation","summary":"  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation.\n","authors":["Manxi Lin","Nina Weng","Kamil Mikolaj","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark Christensen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.06748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09744v2","updated":"2024-03-11T14:09:44Z","published":"2023-05-16T18:32:26Z","title":"Assessment of few-hits machine learning classification algorithms for\n  low energy physics in liquid argon detectors","summary":"  The physics potential of massive liquid argon TPCs in the low-energy regime\nis still to be fully reaped because few-hits events encode information that can\nhardly be exploited by conventional classification algorithms. Machine learning\n(ML) techniques give their best in these types of classification problems. In\nthis paper, we evaluate their performance against conventional (deterministic)\nalgorithms. We demonstrate that both Convolutional Neural Networks (CNN) and\nTransformer-Encoder methods outperform deterministic algorithms in one of the\nmost challenging classification problems of low-energy physics (single- versus\ndouble-beta events). We discuss the advantages and pitfalls of\nTransformer-Encoder methods versus CNN and employ these methods to optimize the\ndetector parameters, with an emphasis on the DUNE Phase II detectors (\"Module\nof Opportunity\").\n","authors":["Roberto Moretti","Marco Rossi","Matteo Biassoni","Andrea Giachero","Michele Grossi","Daniele Guffanti","Danilo Labranca","Francesco Terranova","Sofia Vallecorsa"],"pdf_url":"https://arxiv.org/pdf/2305.09744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05653v3","updated":"2024-03-11T13:55:01Z","published":"2024-01-11T04:24:19Z","title":"Quantifying Marketing Performance at Channel-Partner Level by Using\n  Marketing Mix Modeling (MMM) and Shapley Value Regression","summary":"  This paper explores the application of Shapley Value Regression in dissecting\nmarketing performance at channel-partner level, complementing channel-level\nMarketing Mix Modeling (MMM). Utilizing real-world data from the financial\nservices industry, we demonstrate the practicality of Shapley Value Regression\nin evaluating individual partner contributions. Although structured in-field\ntesting along with cooperative game theory is most accurate, it can often be\nhighly complex and expensive to conduct. Shapley Value Regression is thus a\nmore feasible approach to disentangle the influence of each marketing partner\nwithin a marketing channel. We also propose a simple method to derive adjusted\ncoefficients of Shapley Value Regression and compare it with alternative\napproaches.\n","authors":["Sean Tang","Sriya Musunuru","Baoshi Zong","Brooks Thornton"],"pdf_url":"https://arxiv.org/pdf/2401.05653v3.pdf","comment":"Corrected typos"},{"id":"http://arxiv.org/abs/2403.06731v1","updated":"2024-03-11T13:50:07Z","published":"2024-03-11T13:50:07Z","title":"On the Approximation of Kernel functions","summary":"  Various methods in statistical learning build on kernels considered in\nreproducing kernel Hilbert spaces. In applications, the kernel is often\nselected based on characteristics of the problem and the data. This kernel is\nthen employed to infer response variables at points, where no explanatory data\nwere observed. The data considered here are located in compact sets in higher\ndimensions and the paper addresses approximations of the kernel itself. The new\napproach considers Taylor series approximations of radial kernel functions. For\nthe Gauss kernel on the unit cube, the paper establishes an upper bound of the\nassociated eigenfunctions, which grows only polynomially with respect to the\nindex. The novel approach substantiates smaller regularization parameters than\nconsidered in the literature, overall leading to better approximations. This\nimprovement confirms low rank approximation methods such as the Nystr\\\"om\nmethod.\n","authors":["Paul Dommel","Alois Pichler"],"pdf_url":"https://arxiv.org/pdf/2403.06731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06726v1","updated":"2024-03-11T13:44:49Z","published":"2024-03-11T13:44:49Z","title":"Probabilistic Contrastive Learning for Long-Tailed Visual Recognition","summary":"  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n","authors":["Chaoqun Du","Yulin Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.06726v1.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)"},{"id":"http://arxiv.org/abs/2403.06725v1","updated":"2024-03-11T13:44:43Z","published":"2024-03-11T13:44:43Z","title":"Improving Low-Resource Knowledge Tracing Tasks by Supervised\n  Pre-training and Importance Mechanism Fine-tuning","summary":"  Knowledge tracing (KT) aims to estimate student's knowledge mastery based on\ntheir historical interactions. Recently, the deep learning based KT (DLKT)\napproaches have achieved impressive performance in the KT task. These DLKT\nmodels heavily rely on the large number of available student interactions.\nHowever, due to various reasons such as budget constraints and privacy\nconcerns, observed interactions are very limited in many real-world scenarios,\na.k.a, low-resource KT datasets. Directly training a DLKT model on a\nlow-resource KT dataset may lead to overfitting and it is difficult to choose\nthe appropriate deep neural architecture. Therefore, in this paper, we propose\na low-resource KT framework called LoReKT to address above challenges. Inspired\nby the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn\ntransferable parameters and representations from rich-resource KT datasets\nduring the pre-training stage and subsequently facilitate effective adaptation\nto low-resource KT datasets. Specifically, we simplify existing sophisticated\nDLKT model architectures with purely a stack of transformer decoders. We design\nan encoding mechanism to incorporate student interactions from multiple KT data\nsources and develop an importance mechanism to prioritize updating parameters\nwith high importance while constraining less important ones during the\nfine-tuning stage. We evaluate LoReKT on six public KT datasets and\nexperimental results demonstrate the superiority of our approach in terms of\nAUC and Accuracy. To encourage reproducible research, we make our data and code\npublicly available at https://anonymous.4open.science/r/LoReKT-C619.\n","authors":["Hengyuan Zhang","Zitao Liu","Shuyan Huang","Chenming Shang","Bojun Zhan","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.06725v1.pdf","comment":"29 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.03917v2","updated":"2024-03-11T13:35:29Z","published":"2024-02-06T11:35:02Z","title":"Elastic Feature Consolidation for Cold Start Exemplar-free Incremental\n  Learning","summary":"  Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a\nsequence of tasks without having access to previous task data. In this paper,\nwe consider the challenging Cold Start scenario in which insufficient data is\navailable in the first task to learn a high-quality backbone. This is\nespecially challenging for EFCIL since it requires high plasticity, which\nresults in feature drift which is difficult to compensate for in the\nexemplar-free setting. To address this problem, we propose a simple and\neffective approach that consolidates feature representations by regularizing\ndrift in directions highly relevant to previous tasks and employs prototypes to\nreduce task-recency bias. Our method, called Elastic Feature Consolidation\n(EFC), exploits a tractable second-order approximation of feature drift based\non an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in\nfeature space which we use to regularize feature drift in important directions\nand to update Gaussian prototypes used in a novel asymmetric cross entropy loss\nwhich effectively balances prototype rehearsal with data from new tasks.\nExperimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and\nImageNet-1K demonstrate that Elastic Feature Consolidation is better able to\nlearn new tasks by maintaining model plasticity and significantly outperform\nthe state-of-the-art.\n","authors":["Simone Magistri","Tomaso Trinci","Albin Soutif-Cormerais","Joost van de Weijer","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2402.03917v2.pdf","comment":"Accepted at Twelfth International Conference on Learning\n  Representations (ICLR 2024)"},{"id":"http://arxiv.org/abs/2207.02829v6","updated":"2024-03-11T13:25:29Z","published":"2022-07-06T17:36:59Z","title":"Online Bilevel Optimization: Regret Analysis of Online Alternating\n  Gradient Methods","summary":"  This paper introduces \\textit{online bilevel optimization} in which a\nsequence of time-varying bilevel problems is revealed one after the other. We\nextend the known regret bounds for single-level online algorithms to the\nbilevel setting. Specifically, we provide new notions of \\textit{bilevel\nregret}, develop an online alternating time-averaged gradient method that is\ncapable of leveraging smoothness, and give regret bounds in terms of the\npath-length of the inner and outer minimizer sequences.\n","authors":["Davoud Ataee Tarzanagh","Parvin Nazari","Bojian Hou","Li Shen","Laura Balzano"],"pdf_url":"https://arxiv.org/pdf/2207.02829v6.pdf","comment":"Accepted for publication at AISTATS 2024. v6: minor edits"},{"id":"http://arxiv.org/abs/2403.01865v2","updated":"2024-03-11T13:11:51Z","published":"2024-03-04T09:21:10Z","title":"Improving generalisation via anchor multivariate analysis","summary":"  We introduce a causal regularisation extension to anchor regression (AR) for\nimproved out-of-distribution (OOD) generalisation. We present anchor-compatible\nlosses, aligning with the anchor framework to ensure robustness against\ndistribution shifts. Various multivariate analysis (MVA) algorithms, such as\n(Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We\nobserve that simple regularisation enhances robustness in OOD settings.\nEstimators for selected algorithms are provided, showcasing consistency and\nefficacy in synthetic and real-world climate science problems. The empirical\nvalidation highlights the versatility of anchor regularisation, emphasizing its\ncompatibility with MVA approaches and its role in enhancing replicability while\nguarding against distribution shifts. The extended AR framework advances causal\ninference methodologies, addressing the need for reliable OOD generalisation.\n","authors":["Homer Durand","Gherardo Varando","Nathan Mankovich","Gustau Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2403.01865v2.pdf","comment":"21 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.06687v1","updated":"2024-03-11T13:04:21Z","published":"2024-03-11T13:04:21Z","title":"Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and\n  Attention Mechanism Approach for Heterogeneous Graph-Structured Data","summary":"  Graph neural networks (GNNs) have proven effective in capturing relationships\namong nodes in a graph. This study introduces a novel perspective by\nconsidering a graph as a simplicial complex, encompassing nodes, edges,\ntriangles, and $k$-simplices, enabling the definition of graph-structured data\non any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous\ngraph attention network (HL-HGAT), designed to learn heterogeneous signal\nrepresentations across $k$-simplices. The HL-HGAT incorporates three key\ncomponents: HL convolutional filters (HL-filters), simplicial projection (SP),\nand simplicial attention pooling (SAP) operators, applied to $k$-simplices.\nHL-filters leverage the unique topology of $k$-simplices encoded by the\nHodge-Laplacian (HL) operator, operating within the spectral domain of the\n$k$-th HL operator. To address computation challenges, we introduce a\npolynomial approximation for HL-filters, exhibiting spatial localization\nproperties. Additionally, we propose a pooling operator to coarsen\n$k$-simplices, combining features through simplicial attention mechanisms of\nself-attention and cross-attention via transformers and SP operators, capturing\ntopological interconnections across multiple dimensions of simplices. The\nHL-HGAT is comprehensively evaluated across diverse graph applications,\nincluding NP-hard problems, graph multi-label and classification challenges,\nand graph regression tasks in logistics, computer vision, biology, chemistry,\nand neuroscience. The results demonstrate the model's efficacy and versatility\nin handling a wide range of graph-based scenarios.\n","authors":["Jinghan Huang","Qiufeng Chen","Yijun Bian","Pengli Zhu","Nanguang Chen","Moo K. Chung","Anqi Qiu"],"pdf_url":"https://arxiv.org/pdf/2403.06687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06677v1","updated":"2024-03-11T12:49:37Z","published":"2024-03-11T12:49:37Z","title":"Streamlining in the Riemannian Realm: Efficient Riemannian Optimization\n  with Loopless Variance Reduction","summary":"  In this study, we investigate stochastic optimization on Riemannian\nmanifolds, focusing on the crucial variance reduction mechanism used in both\nEuclidean and Riemannian settings. Riemannian variance-reduced methods usually\ninvolve a double-loop structure, computing a full gradient at the start of each\nloop. Determining the optimal inner loop length is challenging in practice, as\nit depends on strong convexity or smoothness constants, which are often unknown\nor hard to estimate. Motivated by Euclidean methods, we introduce the\nRiemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods\nreplace the outer loop with probabilistic gradient computation triggered by a\ncoin flip in each iteration, ensuring simpler proofs, efficient hyperparameter\nselection, and sharp convergence guarantees. Using R-PAGE as a framework for\nnon-convex Riemannian optimization, we demonstrate its applicability to various\nimportant settings. For example, we derive Riemannian MARINA (R-MARINA) for\ndistributed settings with communication compression, providing the best\ntheoretical communication complexity guarantees for non-convex distributed\noptimization over Riemannian manifolds. Experimental results support our\ntheoretical findings.\n","authors":["Yury Demidovich","Grigory Malinovsky","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2403.06677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19101v2","updated":"2024-03-11T12:48:37Z","published":"2023-05-30T15:06:02Z","title":"Which Models have Perceptually-Aligned Gradients? An Explanation via\n  Off-Manifold Robustness","summary":"  One of the remarkable properties of robust computer vision models is that\ntheir input-gradients are often aligned with human perception, referred to in\nthe literature as perceptually-aligned gradients (PAGs). Despite only being\ntrained for classification, PAGs cause robust models to have rudimentary\ngenerative capabilities, including image generation, denoising, and\nin-painting. However, the underlying mechanisms behind these phenomena remain\nunknown. In this work, we provide a first explanation of PAGs via\n\\emph{off-manifold robustness}, which states that models must be more robust\noff- the data manifold than they are on-manifold. We first demonstrate\ntheoretically that off-manifold robustness leads input gradients to lie\napproximately on the data manifold, explaining their perceptual alignment. We\nthen show that Bayes optimal models satisfy off-manifold robustness, and\nconfirm the same empirically for robust models trained via gradient norm\nregularization, randomized smoothing, and adversarial training with projected\ngradient descent. Quantifying the perceptual alignment of model gradients via\ntheir similarity with the gradients of generative models, we show that\noff-manifold robustness correlates well with perceptual alignment. Finally,\nbased on the levels of on- and off-manifold robustness, we identify three\ndifferent regimes of robustness that affect both perceptual alignment and model\naccuracy: weak robustness, bayes-aligned robustness, and excessive robustness.\nCode is available at \\url{https://github.com/tml-tuebingen/pags}.\n","authors":["Suraj Srinivas","Sebastian Bordt","Hima Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2305.19101v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.06672v1","updated":"2024-03-11T12:43:44Z","published":"2024-03-11T12:43:44Z","title":"Provable Mutual Benefits from Federated Learning in Privacy-Sensitive\n  Domains","summary":"  Cross-silo federated learning (FL) allows data owners to train accurate\nmachine learning models by benefiting from each others private datasets.\nUnfortunately, the model accuracy benefits of collaboration are often\nundermined by privacy defenses. Therefore, to incentivize client participation\nin privacy-sensitive domains, a FL protocol should strike a delicate balance\nbetween privacy guarantees and end-model accuracy. In this paper, we study the\nquestion of when and how a server could design a FL protocol provably\nbeneficial for all participants. First, we provide necessary and sufficient\nconditions for the existence of mutually beneficial protocols in the context of\nmean estimation and convex stochastic optimization. We also derive protocols\nthat maximize the total clients' utility, given symmetric privacy preferences.\nFinally, we design protocols maximizing end-model accuracy and demonstrate\ntheir benefits in synthetic experiments.\n","authors":["Nikita Tsoy","Anna Mihalkova","Teodora Todorova","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2403.06672v1.pdf","comment":"AISTATS 2024; Camera-ready version"},{"id":"http://arxiv.org/abs/2403.06671v1","updated":"2024-03-11T12:42:31Z","published":"2024-03-11T12:42:31Z","title":"Untangling Gaussian Mixtures","summary":"  Tangles were originally introduced as a concept to formalize regions of high\nconnectivity in graphs. In recent years, they have also been discovered as a\nlink between structural graph theory and data science: when interpreting\nsimilarity in data sets as connectivity between points, finding clusters in the\ndata essentially amounts to finding tangles in the underlying graphs. This\npaper further explores the potential of tangles in data sets as a means for a\nformal study of clusters. Real-world data often follow a normal distribution.\nAccounting for this, we develop a quantitative theory of tangles in data sets\ndrawn from Gaussian mixtures. To this end, we equip the data with a graph\nstructure that models similarity between the points and allows us to apply\ntangle theory to the data. We provide explicit conditions under which tangles\nassociated with the marginal Gaussian distributions exist asymptotically almost\nsurely. This can be considered as a sufficient formal criterion for the\nseparabability of clusters in the data.\n","authors":["Eva Fluck","Sandra Kiefer","Christoph Standke"],"pdf_url":"https://arxiv.org/pdf/2403.06671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06668v1","updated":"2024-03-11T12:36:14Z","published":"2024-03-11T12:36:14Z","title":"PeerAiD: Improving Adversarial Distillation from a Specialized Peer\n  Tutor","summary":"  Adversarial robustness of the neural network is a significant concern when it\nis applied to security-critical domains. In this situation, adversarial\ndistillation is a promising option which aims to distill the robustness of the\nteacher network to improve the robustness of a small student network. Previous\nworks pretrain the teacher network to make it robust to the adversarial\nexamples aimed at itself. However, the adversarial examples are dependent on\nthe parameters of the target network. The fixed teacher network inevitably\ndegrades its robustness against the unseen transferred adversarial examples\nwhich targets the parameters of the student network in the adversarial\ndistillation process. We propose PeerAiD to make a peer network learn the\nadversarial examples of the student network instead of adversarial examples\naimed at itself. PeerAiD is an adversarial distillation that trains the peer\nnetwork and the student network simultaneously in order to make the peer\nnetwork specialized for defending the student network. We observe that such\npeer networks surpass the robustness of pretrained robust teacher network\nagainst student-attacked adversarial samples. With this peer network and\nadversarial distillation, PeerAiD achieves significantly higher robustness of\nthe student network with AutoAttack (AA) accuracy up to 1.66%p and improves the\nnatural accuracy of the student network up to 4.72%p with ResNet-18 and\nTinyImageNet dataset.\n","authors":["Jaewon Jung","Hongsun Jang","Jaeyong Song","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06668v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06664v1","updated":"2024-03-11T12:32:14Z","published":"2024-03-11T12:32:14Z","title":"Smart-Infinity: Fast Large Language Model Training using Near-Storage\n  Processing on a Real System","summary":"  The recent huge advance of Large Language Models (LLMs) is mainly driven by\nthe increase in the number of parameters. This has led to substantial memory\ncapacity requirements, necessitating the use of dozens of GPUs just to meet the\ncapacity. One popular solution to this is storage-offloaded training, which\nuses host memory and storage as an extended memory hierarchy. However, this\nobviously comes at the cost of storage bandwidth bottleneck because storage\ndevices have orders of magnitude lower bandwidth compared to that of GPU device\nmemories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck\nof storage-offloaded LLM training using near-storage processing devices on a\nreal system. The main component of Smart-Infinity is SmartUpdate, which\nperforms parameter updates on custom near-storage accelerators. We identify\nthat moving parameter updates to the storage side removes most of the storage\ntraffic. In addition, we propose an efficient data transfer handler structure\nto address the system integration issues for Smart-Infinity. The handler allows\noverlapping data transfers with fixed memory consumption by reusing the device\nbuffer. Lastly, we propose accelerator-assisted gradient\ncompression/decompression to enhance the scalability of Smart-Infinity. When\nscaling to multiple near-storage processing devices, the write traffic on the\nshared channel becomes the bottleneck. To alleviate this, we compress the\ngradients on the GPU and decompress them on the accelerators. It provides\nfurther acceleration from reduced traffic. As a result, Smart-Infinity achieves\na significant speedup compared to the baseline. Notably, Smart-Infinity is a\nready-to-use approach that is fully integrated into PyTorch on a real system.\nWe will open-source Smart-Infinity to facilitate its use.\n","authors":["Hongsun Jang","Jaeyong Song","Jaewon Jung","Jaeyoung Park","Youngsok Kim","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06664v1.pdf","comment":"Published at HPCA 2024 (Best Paper Award Honorable Mention)"},{"id":"http://arxiv.org/abs/2403.06659v1","updated":"2024-03-11T12:28:55Z","published":"2024-03-11T12:28:55Z","title":"Zero-Shot ECG Classification with Multimodal Learning and Test-time\n  Clinical Knowledge Enhancement","summary":"  Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for\ndetecting cardiac arrhythmic diseases in clinical practice. While ECG\nSelf-supervised Learning (eSSL) methods show promise in representation learning\nfrom unannotated ECG data, they often overlook the clinical knowledge that can\nbe found in reports. This oversight and the requirement for annotated samples\nfor downstream tasks limit eSSL's versatility. In this work, we address these\nissues with the Multimodal ECG Representation Learning (MERL}) framework.\nThrough multimodal learning on ECG records and associated reports, MERL is\ncapable of performing zero-shot ECG classification with text prompts,\neliminating the need for training data in downstream tasks. At test time, we\npropose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach,\nwhich uses Large Language Models (LLMs) to exploit external expert-verified\nclinical knowledge databases, generating more descriptive prompts and reducing\nhallucinations in LLM-generated content to boost zero-shot classification.\nBased on MERL, we perform the first benchmark across six public ECG datasets,\nshowing the superior performance of MERL compared against eSSL methods.\nNotably, MERL achieves an average AUC score of 75.2% in zero-shot\nclassification (without training data), 3.2% higher than linear probed eSSL\nmethods with 10\\% annotated training data, averaged across all six datasets.\n","authors":["Che Liu","Zhongwei Wan","Cheng Ouyang","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2403.06659v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2311.17740v2","updated":"2024-03-11T12:17:58Z","published":"2023-11-29T15:44:00Z","title":"A transductive few-shot learning approach for classification of digital\n  histopathological slides from liver cancer","summary":"  This paper presents a new approach for classifying 2D histopathology patches\nusing few-shot learning. The method is designed to tackle a significant\nchallenge in histopathology, which is the limited availability of labeled data.\nBy applying a sliding window technique to histopathology slides, we illustrate\nthe practical benefits of transductive learning (i.e., making joint predictions\non patches) to achieve consistent and accurate classification. Our approach\ninvolves an optimization-based strategy that actively penalizes the prediction\nof a large number of distinct classes within each window. We conducted\nexperiments on histopathological data to classify tissue classes in digital\nslides of liver cancer, specifically hepatocellular carcinoma. The initial\nresults show the effectiveness of our method and its potential to enhance the\nprocess of automated cancer diagnosis and treatment, all while reducing the\ntime and effort required for expert annotation.\n","authors":["Aymen Sadraoui","Ségolène Martin","Eliott Barbot","Astrid Laurent-Bellue","Jean-Christophe Pesquet","Catherine Guettier","Ismail Ben Ayed"],"pdf_url":"https://arxiv.org/pdf/2311.17740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03838v2","updated":"2024-03-11T12:16:24Z","published":"2024-02-06T09:35:40Z","title":"Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman\n  graph kernels","summary":"  Supervised learning has recently garnered significant attention in the field\nof computational physics due to its ability to effectively extract complex\npatterns for tasks like solving partial differential equations, or predicting\nmaterial properties. Traditionally, such datasets consist of inputs given as\nmeshes with a large number of nodes representing the problem geometry (seen as\ngraphs), and corresponding outputs obtained with a numerical solver. This means\nthe supervised learning model must be able to handle large and sparse graphs\nwith continuous node attributes. In this work, we focus on Gaussian process\nregression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman\n(SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL\nkernel enjoys positive definiteness and a drastic complexity reduction, which\nmakes it possible to process datasets that were previously impossible to\nhandle. The new kernel is first validated on graph classification for molecular\ndatasets, where the input graphs have a few tens of nodes. The efficiency of\nthe SWWL kernel is then illustrated on graph regression in computational fluid\ndynamics and solid mechanics, where the input graphs are made up of tens of\nthousands of nodes.\n","authors":["Raphaël Carpintero Perez","Sébastien da Veiga","Josselin Garnier","Brian Staber"],"pdf_url":"https://arxiv.org/pdf/2402.03838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06645v1","updated":"2024-03-11T12:07:33Z","published":"2024-03-11T12:07:33Z","title":"Ricci flow-based brain surface covariance descriptors for Alzheimer\n  disease","summary":"  Automated feature extraction from MRI brain scans and diagnosis of\nAlzheimer's disease are ongoing challenges. With advances in 3D imaging\ntechnology, 3D data acquisition is becoming more viable and efficient than its\n2D counterpart. Rather than using feature-based vectors, in this paper, for the\nfirst time, we suggest a pipeline to extract novel covariance-based descriptors\nfrom the cortical surface using the Ricci energy optimization. The covariance\ndescriptors are components of the nonlinear manifold of symmetric\npositive-definite matrices, thus we focus on using the Gaussian radial basis\nfunction to apply manifold-based classification to the 3D shape problem.\nApplying this novel signature to the analysis of abnormal cortical brain\nmorphometry allows for diagnosing Alzheimer's disease. Experimental studies\nperformed on about two hundred 3D MRI brain models, gathered from Alzheimer's\nDisease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of\nour descriptors in achieving remarkable classification accuracy.\n","authors":["Fatemeh Ahmadi","Mohamad Ebrahim Shiri","Behroz Bidabad","Maral Sedaghat","Pooran Memari"],"pdf_url":"https://arxiv.org/pdf/2403.06645v1.pdf","comment":"Accepted for publication in Biomedical Signal Processing and Control\n  journal"},{"id":"http://arxiv.org/abs/2403.06644v1","updated":"2024-03-11T12:07:13Z","published":"2024-03-11T12:07:13Z","title":"Elephants Never Forget: Testing Language Models for Memorization of\n  Tabular Data","summary":"  While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Starting with simple qualitative tests for whether an LLM knows\nthe names and values of features, we introduce a variety of different\ntechniques to assess the degrees of contamination, including statistical tests\nfor conditional distribution modeling and four tests that identify\nmemorization. Our investigation reveals that LLMs are pre-trained on many\npopular tabular datasets. This exposure can lead to invalid performance\nevaluation on downstream tasks because the LLMs have, in effect, been fit to\nthe test set. Interestingly, we also identify a regime where the language model\nreproduces important statistics of the data, but fails to reproduce the dataset\nverbatim. On these datasets, although seen during training, good performance on\ndownstream tasks might not be due to overfitting. Our findings underscore the\nneed for ensuring data integrity in machine learning tasks with LLMs. To\nfacilitate future research, we release an open-source tool that can perform\nvarious tests for memorization\n\\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.\n","authors":["Sebastian Bordt","Harsha Nori","Rich Caruana"],"pdf_url":"https://arxiv.org/pdf/2403.06644v1.pdf","comment":"Table Representation Learning Workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.06643v1","updated":"2024-03-11T12:04:28Z","published":"2024-03-11T12:04:28Z","title":"Spatial features of CO2 for occupancy detection in a naturally\n  ventilated school building","summary":"  Accurate occupancy information helps to improve building energy efficiency\nand occupant comfort. Occupancy detection methods based on CO2 sensors have\nreceived attention due to their low cost and low intrusiveness. In naturally\nventilated buildings, the accuracy of CO2-based occupancy detection is\ngenerally low in related studies due to the complex ventilation behavior and\nthe difficulty in measuring the actual air exchange through windows. In this\nstudy, we present two novel features for occupancy detection based on the\nspatial distribution of the CO2 concentration. After a quantitative analysis\nwith Support Vector Machine (SVM) as classifier, it was found that the accuracy\nof occupancy state detection in naturally ventilated rooms could be improved by\nup to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1\nscore 0.84) without any ventilation information. With ventilation information,\nthe accuracy reached 87.6 % (F1 score 0.89). The performance of occupancy\nquantity detection was significantly improved by up to 25.3 percentage points\nversus baseline, reaching 56 %, with root mean square error (RMSE) of 11.44\noccupants, using only CO2-related features. Additional ventilation information\nfurther enhanced the performance to 61.8 % (RMSE 9.02 occupants). By\nincorporating spatial features, the model using only CO2-related features\nrevealed similar performance as the model containing additional ventilation\ninformation, resulting in a better low-cost occupancy detection method for\nnaturally ventilated buildings.\n","authors":["Qirui Huang","Marc Syndicus","Jérôme Frisch","Christoph van Treeck"],"pdf_url":"https://arxiv.org/pdf/2403.06643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04493v3","updated":"2024-03-11T12:01:43Z","published":"2024-03-07T13:49:43Z","title":"What makes an image realistic?","summary":"  The last decade has seen tremendous progress in our ability to generate\nrealistic-looking data, be it images, text, audio, or video. Here, we discuss\nthe closely related problem of quantifying realism, that is, designing\nfunctions that can reliably tell realistic data from unrealistic data. This\nproblem turns out to be significantly harder to solve and remains poorly\nunderstood, despite its prevalence in machine learning and recent breakthroughs\nin generative AI. Drawing on insights from algorithmic information theory, we\ndiscuss why this problem is challenging, why a good generative model alone is\ninsufficient to solve it, and what a good solution would look like. In\nparticular, we introduce the notion of a universal critic, which unlike\nadversarial critics does not require adversarial training. While universal\ncritics are not immediately practical, they can serve both as a North Star for\nguiding practical implementations and as a tool for analyzing existing attempts\nto capture realism.\n","authors":["Lucas Theis"],"pdf_url":"https://arxiv.org/pdf/2403.04493v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06631v1","updated":"2024-03-11T11:41:30Z","published":"2024-03-11T11:41:30Z","title":"Evaluating the Energy Efficiency of Few-Shot Learning for Object\n  Detection in Industrial Settings","summary":"  In the ever-evolving era of Artificial Intelligence (AI), model performance\nhas constituted a key metric driving innovation, leading to an exponential\ngrowth in model size and complexity. However, sustainability and energy\nefficiency have been critical requirements during deployment in contemporary\nindustrial settings, necessitating the use of data-efficient approaches such as\nfew-shot learning. In this paper, to alleviate the burden of lengthy model\ntraining and minimize energy consumption, a finetuning approach to adapt\nstandard object detection models to downstream tasks is examined. Subsequently,\na thorough case study and evaluation of the energy demands of the developed\nmodels, applied in object detection benchmark datasets from volatile industrial\nenvironments is presented. Specifically, different finetuning strategies as\nwell as utilization of ancillary evaluation data during training are examined,\nand the trade-off between performance and efficiency is highlighted in this\nlow-data regime. Finally, this paper introduces a novel way to quantify this\ntrade-off through a customized Efficiency Factor metric.\n","authors":["Georgios Tsoumplekas","Vladislav Li","Ilias Siniosoglou","Vasileios Argyriou","Sotirios K. Goudos","Ioannis D. Moscholios","Panagiotis Radoglou-Grammatikis","Panagiotis Sarigiannidis"],"pdf_url":"https://arxiv.org/pdf/2403.06631v1.pdf","comment":"7 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2308.08742v6","updated":"2024-03-11T11:35:48Z","published":"2023-08-17T02:33:43Z","title":"PMET: Precise Model Editing in a Transformer","summary":"  Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.\n","authors":["Xiaopeng Li","Shasha Li","Shezheng Song","Jing Yang","Jun Ma","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2308.08742v6.pdf","comment":"AAAI24"},{"id":"http://arxiv.org/abs/2308.14412v2","updated":"2024-03-11T11:19:32Z","published":"2023-08-28T08:50:12Z","title":"Task-Aware Machine Unlearning and Its Application in Load Forecasting","summary":"  Data privacy and security have become a non-negligible factor in load\nforecasting. Previous researches mainly focus on training stage enhancement.\nHowever, once the model is trained and deployed, it may need to `forget' (i.e.,\nremove the impact of) part of training data if the these data are found to be\nmalicious or as requested by the data owner. This paper introduces the concept\nof machine unlearning which is specifically designed to remove the influence of\npart of the dataset on an already trained forecaster. However, direct\nunlearning inevitably degrades the model generalization ability. To balance\nbetween unlearning completeness and model performance, a performance-aware\nalgorithm is proposed by evaluating the sensitivity of local model parameter\nchange using influence function and sample re-weighting. Furthermore, we\nobserve that the statistical criterion such as mean squared error, cannot fully\nreflect the operation cost of the downstream tasks in power system. Therefore,\na task-aware machine unlearning is proposed whose objective is a trilevel\noptimization with dispatch and redispatch problems considered. We theoretically\nprove the existence of the gradient of such an objective, which is key to\nre-weighting the remaining samples. We tested the unlearning algorithms on\nlinear, CNN, and MLP-Mixer based load forecasters with a realistic load\ndataset. The simulation demonstrates the balance between unlearning\ncompleteness and operational cost. All codes can be found at\nhttps://github.com/xuwkk/task_aware_machine_unlearning.\n","authors":["Wangkun Xu","Fei Teng"],"pdf_url":"https://arxiv.org/pdf/2308.14412v2.pdf","comment":"This paper has been accepted by IEEE trans on Power Systems. The\n  copyright is transfered and preserved by IEEE"},{"id":"http://arxiv.org/abs/2208.05845v4","updated":"2024-03-11T11:17:36Z","published":"2022-08-11T14:28:21Z","title":"Analyzing Fairness in Deepfake Detection With Massively Annotated\n  Databases","summary":"  In recent years, image and video manipulations with Deepfake have become a\nsevere concern for security and society. Many detection models and datasets\nhave been proposed to detect Deepfake data reliably. However, there is an\nincreased concern that these models and training databases might be biased and,\nthus, cause Deepfake detectors to fail. In this work, we investigate factors\ncausing biased detection in public Deepfake datasets by (a) creating\nlarge-scale demographic and non-demographic attribute annotations with 47\ndifferent attributes for five popular Deepfake datasets and (b) comprehensively\nanalysing attributes resulting in AI-bias of three state-of-the-art Deepfake\ndetection backbone models on these datasets. The analysis shows how various\nattributes influence a large variety of distinctive attributes (from over 65M\nlabels) on the detection performance which includes demographic (age, gender,\nethnicity) and non-demographic (hair, skin, accessories, etc.) attributes. The\nresults examined datasets show limited diversity and, more importantly, show\nthat the utilised Deepfake detection backbone models are strongly affected by\ninvestigated attributes making them not fair across attributes. The Deepfake\ndetection backbone methods trained on such imbalanced/biased datasets result in\nincorrect detection results leading to generalisability, fairness, and security\nissues. Our findings and annotated datasets will guide future research to\nevaluate and mitigate bias in Deepfake detection techniques. The annotated\ndatasets and the corresponding code are publicly available.\n","authors":["Ying Xu","Philipp Terhörst","Kiran Raja","Marius Pedersen"],"pdf_url":"https://arxiv.org/pdf/2208.05845v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09450v2","updated":"2024-03-11T11:16:33Z","published":"2024-02-02T10:04:13Z","title":"Guiding Masked Representation Learning to Capture Spatio-Temporal\n  Relationship of Electrocardiogram","summary":"  Electrocardiograms (ECG) are widely employed as a diagnostic tool for\nmonitoring electrical signals originating from a heart. Recent machine learning\nresearch efforts have focused on the application of screening various diseases\nusing ECG signals. However, adapting to the application of screening disease is\nchallenging in that labeled ECG data are limited. Achieving general\nrepresentation through self-supervised learning (SSL) is a well-known approach\nto overcome the scarcity of labeled data; however, a naive application of SSL\nto ECG data, without considering the spatial-temporal relationships inherent in\nECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM\n(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn\nspatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM\noutperforms other SSL baseline methods in various experimental settings for\narrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is\nadaptable to various lead combinations. Through quantitative and qualitative\nanalysis, we show a spatio-temporal relationship within ECG data. Our code is\navailable at https://github.com/bakqui/ST-MEM.\n","authors":["Yeongyeon Na","Minje Park","Yunwon Tae","Sunghoon Joo"],"pdf_url":"https://arxiv.org/pdf/2402.09450v2.pdf","comment":"ICLR 2024. The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2307.14025v2","updated":"2024-03-11T11:14:15Z","published":"2023-07-26T08:14:18Z","title":"Topologically Regularized Multiple Instance Learning to Harness Data\n  Scarcity","summary":"  In biomedical data analysis, Multiple Instance Learning (MIL) models have\nemerged as a powerful tool to classify patients' microscopy samples. However,\nthe data-intensive requirement of these models poses a significant challenge in\nscenarios with scarce data availability, e.g., in rare diseases. We introduce a\ntopological regularization term to MIL to mitigate this challenge. It provides\na shape-preserving inductive bias that compels the encoder to maintain the\nessential geometrical-topological structure of input bags during projection\ninto latent space. This enhances the performance and generalization of the MIL\nclassifier regardless of the aggregation function, particularly for scarce\ntraining data. The effectiveness of our method is confirmed through experiments\nacross a range of datasets, showing an average enhancement of 2.8% for MIL\nbenchmarks, 15.3% for synthetic MIL datasets, and 5.5% for real-world\nbiomedical datasets over the current state-of-the-art.\n","authors":["Salome Kazeminia","Carsten Marr","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2307.14025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06612v1","updated":"2024-03-11T10:59:55Z","published":"2024-03-11T10:59:55Z","title":"Pulling back symmetric Riemannian geometry for data analysis","summary":"  Data sets tend to live in low-dimensional non-linear subspaces. Ideal data\nanalysis tools for such data sets should therefore account for such non-linear\ngeometry. The symmetric Riemannian geometry setting can be suitable for a\nvariety of reasons. First, it comes with a rich mathematical structure to\naccount for a wide range of non-linear geometries that has been shown to be\nable to capture the data geometry through empirical evidence from classical\nnon-linear embedding. Second, many standard data analysis tools initially\ndeveloped for data in Euclidean space can also be generalised efficiently to\ndata on a symmetric Riemannian manifold. A conceptual challenge comes from the\nlack of guidelines for constructing a symmetric Riemannian structure on the\ndata space itself and the lack of guidelines for modifying successful\nalgorithms on symmetric Riemannian manifolds for data analysis to this setting.\nThis work considers these challenges in the setting of pullback Riemannian\ngeometry through a diffeomorphism. The first part of the paper characterises\ndiffeomorphisms that result in proper, stable and efficient data analysis. The\nsecond part then uses these best practices to guide construction of such\ndiffeomorphisms through deep learning. As a proof of concept, different types\nof pullback geometries -- among which the proposed construction -- are tested\non several data analysis tasks and on several toy data sets. The numerical\nexperiments confirm the predictions from theory, i.e., that the diffeomorphisms\ngenerating the pullback geometry need to map the data manifold into a geodesic\nsubspace of the pulled back Riemannian manifold while preserving local isometry\naround the data manifold for proper, stable and efficient data analysis, and\nthat pulling back positive curvature can be problematic in terms of stability.\n","authors":["Willem Diepeveen"],"pdf_url":"https://arxiv.org/pdf/2403.06612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02807v3","updated":"2024-03-11T10:51:14Z","published":"2023-10-04T13:34:34Z","title":"A Deep Instance Generative Framework for MILP Solvers Under Limited Data\n  Availability","summary":"  In the past few years, there has been an explosive surge in the use of\nmachine learning (ML) techniques to address combinatorial optimization (CO)\nproblems, especially mixed-integer linear programs (MILPs). Despite the\nachievements, the limited availability of real-world instances often leads to\nsub-optimal decisions and biased solver assessments, which motivates a suite of\nsynthetic MILP instance generation techniques. However, existing methods either\nrely heavily on expert-designed formulations or struggle to capture the rich\nfeatures of real-world instances. To tackle this problem, we propose G2MILP,\nthe first deep generative framework for MILP instances. Specifically, G2MILP\nrepresents MILP instances as bipartite graphs, and applies a masked variational\nautoencoder to iteratively corrupt and replace parts of the original graphs to\ngenerate new ones. The appealing feature of G2MILP is that it can learn to\ngenerate novel and realistic MILP instances without prior expert-designed\nformulations, while preserving the structures and computational hardness of\nreal-world datasets, simultaneously. Thus the generated instances can\nfacilitate downstream tasks for enhancing MILP solvers under limited data\navailability. We design a suite of benchmarks to evaluate the quality of the\ngenerated MILP instances. Experiments demonstrate that our method can produce\ninstances that closely resemble real-world datasets in terms of both structures\nand computational hardness. The deliverables are released at\nhttps://miralab-ustc.github.io/L2O-G2MILP.\n","authors":["Zijie Geng","Xijun Li","Jie Wang","Xiao Li","Yongdong Zhang","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2310.02807v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06606v1","updated":"2024-03-11T10:50:53Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2307.13709v5","updated":"2024-03-11T10:45:41Z","published":"2023-07-24T20:56:42Z","title":"Neural Bradley-Terry Rating: Quantifying Properties from Comparisons","summary":"  Many properties in the real world don't have metrics and can't be numerically\nobserved, making them difficult to learn. To deal with this challenging\nproblem, prior works have primarily focused on estimating those properties by\nusing graded human scores as the target label in the training. Meanwhile,\nrating algorithms based on the Bradley-Terry model are extensively studied to\nevaluate the competitiveness of players based on their match history. In this\npaper, we introduce the Neural Bradley-Terry Rating (NBTR), a novel machine\nlearning framework designed to quantify and evaluate properties of unknown\nitems. Our method seamlessly integrates the Bradley-Terry model into the neural\nnetwork structure. Moreover, we generalize this architecture further to\nasymmetric environments with unfairness, a condition more commonly encountered\nin real-world settings. Through experimental analysis, we demonstrate that NBTR\nsuccessfully learns to quantify and estimate desired properties.\n","authors":["Satoru Fujii"],"pdf_url":"https://arxiv.org/pdf/2307.13709v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02557v2","updated":"2024-03-11T10:44:54Z","published":"2023-11-05T03:33:44Z","title":"Fast Minimization of Expected Logarithmic Loss via Stochastic Dual\n  Averaging","summary":"  Consider the problem of minimizing an expected logarithmic loss over either\nthe probability simplex or the set of quantum density matrices. This problem\nincludes tasks such as solving the Poisson inverse problem, computing the\nmaximum-likelihood estimate for quantum state tomography, and approximating\npositive semi-definite matrix permanents with the currently tightest\napproximation ratio. Although the optimization problem is convex, standard\niteration complexity guarantees for first-order methods do not directly apply\ndue to the absence of Lipschitz continuity and smoothness in the loss function.\n  In this work, we propose a stochastic first-order algorithm named $B$-sample\nstochastic dual averaging with the logarithmic barrier. For the Poisson inverse\nproblem, our algorithm attains an $\\varepsilon$-optimal solution in\n$\\smash{\\tilde{O}}(d^2/\\varepsilon^2)$ time, matching the state of the art,\nwhere $d$ denotes the dimension. When computing the maximum-likelihood estimate\nfor quantum state tomography, our algorithm yields an $\\varepsilon$-optimal\nsolution in $\\smash{\\tilde{O}}(d^3/\\varepsilon^2)$ time. This improves on the\ntime complexities of existing stochastic first-order methods by a factor of\n$d^{\\omega-2}$ and those of batch methods by a factor of $d^2$, where $\\omega$\ndenotes the matrix multiplication exponent. Numerical experiments demonstrate\nthat empirically, our algorithm outperforms existing methods with explicit\ncomplexity guarantees.\n","authors":["Chung-En Tsai","Hao-Chung Cheng","Yen-Huan Li"],"pdf_url":"https://arxiv.org/pdf/2311.02557v2.pdf","comment":"26 pages, AISTATS 2024"},{"id":"http://arxiv.org/abs/2311.16503v3","updated":"2024-03-11T10:40:40Z","published":"2023-11-27T12:59:52Z","title":"TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models","summary":"  The Diffusion model, a prevalent framework for image generation, encounters\nsignificant challenges in terms of broad applicability due to its extended\ninference times and substantial memory requirements. Efficient Post-training\nQuantization (PTQ) is pivotal for addressing these issues in traditional\nmodels. Different from traditional models, diffusion models heavily depend on\nthe time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$\nfrom the finite set $\\{1, \\ldots, T\\}$ is encoded to a temporal feature by a\nfew modules totally irrespective of the sampling data. However, existing PTQ\nmethods do not optimize these modules separately. They adopt inappropriate\nreconstruction targets and complex calibration methods, resulting in a severe\ndisturbance of the temporal feature and denoising trajectory, as well as a low\ncompression efficiency. To solve these, we propose a Temporal Feature\nMaintenance Quantization (TFMQ) framework building upon a Temporal Information\nBlock which is just related to the time-step $t$ and unrelated to the sampling\ndata. Powered by the pioneering block design, we devise temporal information\naware reconstruction (TIAR) and finite set calibration (FSC) to align the\nfull-precision temporal features in a limited time. Equipped with the\nframework, we can maintain the most temporal information and ensure the\nend-to-end generation quality. Extensive experiments on various datasets and\ndiffusion models prove our state-of-the-art results. Remarkably, our\nquantization approach, for the first time, achieves model performance nearly on\npar with the full-precision model under 4-bit weight quantization.\nAdditionally, our method incurs almost no extra computational cost and\naccelerates quantization time by $2.0 \\times$ on LSUN-Bedrooms $256 \\times 256$\ncompared to previous works. Our code is publicly available at\nhttps://github.com/ModelTC/TFMQ-DM.\n","authors":["Yushi Huang","Ruihao Gong","Jing Liu","Tianlong Chen","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.16503v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06586v1","updated":"2024-03-11T10:32:23Z","published":"2024-03-11T10:32:23Z","title":"ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity\n  Recognition Models","summary":"  Context-aware Human Activity Recognition (HAR) is a hot research area in\nmobile computing, and the most effective solutions in the literature are based\non supervised deep learning models. However, the actual deployment of these\nsystems is limited by the scarcity of labeled data that is required for\ntraining. Neuro-Symbolic AI (NeSy) provides an interesting research direction\nto mitigate this issue, by infusing common-sense knowledge about human\nactivities and the contexts in which they can be performed into HAR deep\nlearning classifiers. Existing NeSy methods for context-aware HAR rely on\nknowledge encoded in logic-based models (e.g., ontologies) whose design,\nimplementation, and maintenance to capture new activities and contexts require\nsignificant human engineering efforts, technical knowledge, and domain\nexpertise. Recent works show that pre-trained Large Language Models (LLMs)\neffectively encode common-sense knowledge about human activities. In this work,\nwe propose ContextGPT: a novel prompt engineering approach to retrieve from\nLLMs common-sense knowledge about the relationship between human activities and\nthe context in which they are performed. Unlike ontologies, ContextGPT requires\nlimited human effort and expertise. An extensive evaluation carried out on two\npublic datasets shows how a NeSy model obtained by infusing common-sense\nknowledge from ContextGPT is effective in data scarcity scenarios, leading to\nsimilar (and sometimes better) recognition rates than logic-based approaches\nwith a fraction of the effort.\n","authors":["Luca Arrotta","Claudio Bettini","Gabriele Civitarese","Michele Fiori"],"pdf_url":"https://arxiv.org/pdf/2403.06586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06576v1","updated":"2024-03-11T10:26:04Z","published":"2024-03-11T10:26:04Z","title":"FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing\n  Fourier Transform and Auto-encoder","summary":"  The success of deep learning-based generative models in producing realistic\nimages, videos, and audios has led to a crucial consideration: how to\neffectively assess the quality of synthetic samples. While the Fr\\'{e}chet\nInception Distance (FID) serves as the standard metric for evaluating\ngenerative models in image synthesis, a comparable metric for time series data\nis notably absent. This gap in assessment capabilities stems from the absence\nof a widely accepted feature vector extractor pre-trained on benchmark time\nseries datasets. In addressing these challenges related to assessing the\nquality of time series, particularly in the context of Fr\\'echet Distance, this\nwork proposes a novel solution leveraging the Fourier transform and\nAuto-encoder, termed the Fr\\'{e}chet Fourier-transform Auto-encoder Distance\n(FFAD). Through our experimental results, we showcase the potential of FFAD for\neffectively distinguishing samples from different classes. This novel metric\nemerges as a fundamental tool for the evaluation of generative time series\ndata, contributing to the ongoing efforts of enhancing assessment methodologies\nin the realm of deep learning-based generative models.\n","authors":["Yang Chen","Dustin J. Kempton","Rafal A. Angryk"],"pdf_url":"https://arxiv.org/pdf/2403.06576v1.pdf","comment":"13 pages, 6 figures, accepted by ICTIS-2024 on March 8th, 2024"},{"id":"http://arxiv.org/abs/2403.06571v1","updated":"2024-03-11T10:14:06Z","published":"2024-03-11T10:14:06Z","title":"Scalable Online Exploration via Coverability","summary":"  Exploration is a major challenge in reinforcement learning, especially for\nhigh-dimensional domains that require function approximation. We propose\nexploration objectives -- policy optimization objectives that enable downstream\nmaximization of any reward function -- as a conceptual framework to systematize\nthe study of exploration. Within this framework, we introduce a new objective,\n$L_1$-Coverage, which generalizes previous exploration schemes and supports\nthree fundamental desiderata:\n  1. Intrinsic complexity control. $L_1$-Coverage is associated with a\nstructural parameter, $L_1$-Coverability, which reflects the intrinsic\nstatistical difficulty of the underlying MDP, subsuming Block and Low-Rank\nMDPs.\n  2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently\nreduces to standard policy optimization, allowing flexible integration with\noff-the-shelf methods such as policy gradient and Q-learning approaches.\n  3. Efficient exploration. $L_1$-Coverage enables the first computationally\nefficient model-based and model-free algorithms for online (reward-free or\nreward-driven) reinforcement learning in MDPs with low coverability.\n  Empirically, we find that $L_1$-Coverage effectively drives off-the-shelf\npolicy optimization algorithms to explore the state space.\n","authors":["Philip Amortila","Dylan J. Foster","Akshay Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2403.06571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06569v1","updated":"2024-03-11T10:10:45Z","published":"2024-03-11T10:10:45Z","title":"Enhancing Joint Motion Prediction for Individuals with Limb Loss Through\n  Model Reprogramming","summary":"  Mobility impairment caused by limb loss is a significant challenge faced by\nmillions of individuals worldwide. The development of advanced assistive\ntechnologies, such as prosthetic devices, has the potential to greatly improve\nthe quality of life for amputee patients. A critical component in the design of\nsuch technologies is the accurate prediction of reference joint motion for the\nmissing limb. However, this task is hindered by the scarcity of joint motion\ndata available for amputee patients, in contrast to the substantial quantity of\ndata from able-bodied subjects. To overcome this, we leverage deep learning's\nreprogramming property to repurpose well-trained models for a new goal without\naltering the model parameters. With only data-level manipulation, we adapt\nmodels originally designed for able-bodied people to forecast joint motion in\namputees. The findings in this study have significant implications for\nadvancing assistive tech and amputee mobility.\n","authors":["Sharmita Dey","Sarath R. Nair"],"pdf_url":"https://arxiv.org/pdf/2403.06569v1.pdf","comment":"Accepted at ICLR 2024 Workshop on Learning from Time Series for\n  Health"},{"id":"http://arxiv.org/abs/2310.04742v3","updated":"2024-03-11T10:07:08Z","published":"2023-10-07T08:55:54Z","title":"Parameter Efficient Multi-task Model Fusion with Partial Linearization","summary":"  Large pre-trained models have enabled significant advances in machine\nlearning and served as foundation components. Model fusion methods, such as\ntask arithmetic, have been proven to be powerful and scalable to incorporate\nfine-tuned weights from different tasks into a multi-task model. However,\nefficiently fine-tuning large pre-trained models on multiple downstream tasks\nremains challenging, leading to inefficient multi-task model fusion. In this\nwork, we propose a novel method to improve multi-task fusion for\nparameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically,\nour approach partially linearizes only the adapter modules and applies task\narithmetic over the linearized adapters. This allows us to leverage the the\nadvantages of model fusion over linearized fine-tuning, while still performing\nfine-tuning and inference efficiently. We demonstrate that our partial\nlinearization technique enables a more effective fusion of multiple tasks into\na single model, outperforming standard adapter tuning and task arithmetic\nalone. Experimental results demonstrate the capabilities of our proposed\npartial linearization technique to effectively construct unified multi-task\nmodels via the fusion of fine-tuned task vectors. We evaluate performance over\nan increasing number of tasks and find that our approach outperforms standard\nparameter-efficient fine-tuning techniques. The results highlight the benefits\nof partial linearization for scalable and efficient multi-task model fusion.\nThe code is available at https://github.com/tanganke/peta\n","authors":["Anke Tang","Li Shen","Yong Luo","Yibing Zhan","Han Hu","Bo Du","Yixin Chen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2310.04742v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17000v3","updated":"2024-03-11T10:07:03Z","published":"2023-05-26T14:59:28Z","title":"DistriBlock: Identifying adversarial audio samples by leveraging\n  characteristics of the output distribution","summary":"  Adversarial attacks can mislead automatic speech recognition (ASR) systems\ninto predicting an arbitrary target text, thus posing a clear security threat.\nTo prevent such attacks, we propose DistriBlock, an efficient detection\nstrategy applicable to any ASR system that predicts a probability distribution\nover output tokens in each time step. We measure a set of characteristics of\nthis distribution: the median, maximum, and minimum over the output\nprobabilities, the entropy of the distribution, as well as the Kullback-Leibler\nand the Jensen-Shannon divergence with respect to the distributions of the\nsubsequent time step. Then, by leveraging the characteristics observed for both\nbenign and adversarial data, we apply binary classifiers, including simple\nthreshold-based classification, ensembles of such classifiers, and neural\nnetworks. Through extensive analysis across different state-of-the-art ASR\nsystems and language data sets, we demonstrate the supreme performance of this\napproach, with a mean area under the receiver operating characteristic for\ndistinguishing target adversarial examples against clean and noisy data of 99%\nand 97%, respectively. To assess the robustness of our method, we show that\nadaptive adversarial examples that can circumvent DistriBlock are much noisier,\nwhich makes them easier to detect through filtering and creates another avenue\nfor preserving the system's robustness.\n","authors":["Matías Pizarro","Dorothea Kolossa","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2305.17000v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14712v2","updated":"2024-03-11T10:06:37Z","published":"2023-12-22T14:10:07Z","title":"Robustness, Efficiency, or Privacy: Pick Two in Machine Learning","summary":"  The success of machine learning (ML) applications relies on vast datasets and\ndistributed architectures which, as they grow, present major challenges. In\nreal-world scenarios, where data often contains sensitive information, issues\nlike data poisoning and hardware failures are common. Ensuring privacy and\nrobustness is vital for the broad adoption of ML in public life. This paper\nexamines the costs associated with achieving these objectives in distributed ML\narchitectures, from both theoretical and empirical perspectives. We overview\nthe meanings of privacy and robustness in distributed ML, and clarify how they\ncan be achieved efficiently in isolation. However, we contend that the\nintegration of these two objectives entails a notable compromise in\ncomputational efficiency. In short, traditional noise injection hurts accuracy\nby concealing poisoned inputs, while cryptographic methods clash with poisoning\ndefenses due to their non-linear nature. However, we outline future research\ndirections aimed at reconciling this compromise with efficiency by considering\nweaker threat models.\n","authors":["Youssef Allouah","Rachid Guerraoui","John Stephan"],"pdf_url":"https://arxiv.org/pdf/2312.14712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.03396v3","updated":"2024-03-11T10:06:17Z","published":"2021-09-08T02:05:40Z","title":"A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with\n  an Arbitrary Opponent","summary":"  In this paper, we propose Posterior Sampling Reinforcement Learning for\nZero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that\nachieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon\nzero-sum stochastic games with average-reward criterion. Here $H$ is an upper\nbound on the span of the bias function, $S$ is the number of states, $A$ is the\nnumber of joint actions and $T$ is the horizon. We consider the online setting\nwhere the opponent can not be controlled and can take any arbitrary\ntime-adaptive history-dependent strategy. Our regret bound improves on the best\nexisting regret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the\nsame assumption and matches the theoretical lower bound in $T$.\n","authors":["Mehdi Jafarnia-Jahromi","Rahul Jain","Ashutosh Nayyar"],"pdf_url":"https://arxiv.org/pdf/2109.03396v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06563v1","updated":"2024-03-11T10:05:29Z","published":"2024-03-11T10:05:29Z","title":"Unraveling the Mystery of Scaling Laws: Part I","summary":"  Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.\n","authors":["Hui Su","Zhi Tian","Xiaoyu Shen","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.06563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06560v1","updated":"2024-03-11T10:01:21Z","published":"2024-03-11T10:01:21Z","title":"Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds","summary":"  While many Machine Learning methods were developed or transposed on\nRiemannian manifolds to tackle data with known non Euclidean geometry, Optimal\nTransport (OT) methods on such spaces have not received much attention. The\nmain OT tool on these spaces is the Wasserstein distance which suffers from a\nheavy computational burden. On Euclidean spaces, a popular alternative is the\nSliced-Wasserstein distance, which leverages a closed-form solution of the\nWasserstein distance in one dimension, but which is not readily available on\nmanifolds. In this work, we derive general constructions of Sliced-Wasserstein\ndistances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive\ncurvature, which include among others Hyperbolic spaces or the space of\nSymmetric Positive Definite matrices. Then, we propose different applications.\nAdditionally, we derive non-parametric schemes to minimize these new distances\nby approximating their Wasserstein gradient flows.\n","authors":["Clément Bonet","Lucas Drumetz","Nicolas Courty"],"pdf_url":"https://arxiv.org/pdf/2403.06560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06557v1","updated":"2024-03-11T10:00:26Z","published":"2024-03-11T10:00:26Z","title":"Data-driven architecture to encode information in the kinematics of\n  robots and artificial avatars","summary":"  We present a data-driven control architecture for modifying the kinematics of\nrobots and artificial avatars to encode specific information such as the\npresence or not of an emotion in the movements of an avatar or robot driven by\na human operator. We validate our approach on an experimental dataset obtained\nduring the reach-to-grasp phase of a pick-and-place task.\n","authors":["Francesco De Lellis","Marco Coraggio","Nathan C. Foster","Riccardo Villa","Cristina Becchio","Mario di Bernardo"],"pdf_url":"https://arxiv.org/pdf/2403.06557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12570v3","updated":"2024-03-11T09:49:06Z","published":"2023-11-21T12:34:00Z","title":"BEND: Benchmarking DNA Language Models on biologically meaningful tasks","summary":"  The genome sequence contains the blueprint for governing cellular processes.\nWhile the availability of genomes has vastly increased over the last decades,\nexperimental annotation of the various functional, non-coding and regulatory\nelements encoded in the DNA sequence remains both expensive and challenging.\nThis has sparked interest in unsupervised language modeling of genomic DNA, a\nparadigm that has seen great success for protein sequence data. Although\nvarious DNA language models have been proposed, evaluation tasks often differ\nbetween individual works, and might not fully recapitulate the fundamental\nchallenges of genome annotation, including the length, scale and sparsity of\nthe data. In this study, we introduce BEND, a Benchmark for DNA language\nmodels, featuring a collection of realistic and biologically meaningful\ndownstream tasks defined on the human genome. We find that embeddings from\ncurrent DNA LMs can approach performance of expert methods on some tasks, but\nonly capture limited information about long-range features. BEND is available\nat https://github.com/frederikkemarin/BEND.\n","authors":["Frederikke Isa Marin","Felix Teufel","Marc Horlacher","Dennis Madsen","Dennis Pultz","Ole Winther","Wouter Boomsma"],"pdf_url":"https://arxiv.org/pdf/2311.12570v3.pdf","comment":"9 pages, 1 figure, 3 tables, code available at\n  https://github.com/frederikkemarin/BEND, to be published in ICLR 2024"},{"id":"http://arxiv.org/abs/2403.06546v1","updated":"2024-03-11T09:46:41Z","published":"2024-03-11T09:46:41Z","title":"OMH: Structured Sparsity via Optimally Matched Hierarchy for\n  Unsupervised Semantic Segmentation","summary":"  Unsupervised Semantic Segmentation (USS) involves segmenting images without\nrelying on predefined labels, aiming to alleviate the burden of extensive human\nlabeling. Existing methods utilize features generated by self-supervised models\nand specific priors for clustering. However, their clustering objectives are\nnot involved in the optimization of the features during training. Additionally,\ndue to the lack of clear class definitions in USS, the resulting segments may\nnot align well with the clustering objective. In this paper, we introduce a\nnovel approach called Optimally Matched Hierarchy (OMH) to simultaneously\naddress the above issues. The core of our method lies in imposing structured\nsparsity on the feature space, which allows the features to encode information\nwith different levels of granularity. The structure of this sparsity stems from\nour hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy\namong parallel clusters through Optimal Transport. Our OMH yields better\nunsupervised segmentation performance compared to existing USS methods. Our\nextensive experiments demonstrate the benefits of OMH when utilizing our\ndifferentiable paradigm. We will make our code publicly available.\n","authors":["Baran Ozaydin","Tong Zhang","Deblina Bhattacharjee","Sabine Süsstrunk","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2403.06546v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.06545v1","updated":"2024-03-11T09:45:34Z","published":"2024-03-11T09:45:34Z","title":"ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico\n  Data Generation","summary":"  The creation of in-silico datasets can expand the utility of existing\nannotations to new domains with different staining patterns in computational\npathology. As such, it has the potential to significantly lower the cost\nassociated with building large and pixel precise datasets needed to train\nsupervised deep learning models. We propose a novel approach for the generation\nof in-silico immunohistochemistry (IHC) images by disentangling morphology\nspecific IHC stains into separate image channels in immunofluorescence (IF)\nimages. The proposed approach qualitatively and quantitatively outperforms\nbaseline methods as proven by training nucleus segmentation models on the\ncreated in-silico datasets.\n","authors":["Dominik Winter","Nicolas Triltsch","Philipp Plewa","Marco Rosati","Thomas Padel","Ross Hill","Markus Schick","Nicolas Brieu"],"pdf_url":"https://arxiv.org/pdf/2403.06545v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2312.08150v2","updated":"2024-03-11T09:43:47Z","published":"2023-12-13T14:01:58Z","title":"Active learning with biased non-response to label requests","summary":"  Active learning can improve the efficiency of training prediction models by\nidentifying the most informative new labels to acquire. However, non-response\nto label requests can impact active learning's effectiveness in real-world\ncontexts. We conceptualise this degradation by considering the type of\nnon-response present in the data, demonstrating that biased non-response is\nparticularly detrimental to model performance. We argue that biased\nnon-response is likely in contexts where the labelling process, by nature,\nrelies on user interactions. To mitigate the impact of biased non-response, we\npropose a cost-based correction to the sampling strategy--the Upper Confidence\nBound of the Expected Utility (UCB-EU)--that can, plausibly, be applied to any\nactive learning algorithm. Through experiments, we demonstrate that our method\nsuccessfully reduces the harm from labelling non-response in many settings.\nHowever, we also characterise settings where the non-response bias in the\nannotations remains detrimental under UCB-EU for specific sampling methods and\ndata generating processes. Finally, we evaluate our method on a real-world\ndataset from an e-commerce platform. We show that UCB-EU yields substantial\nperformance improvements to conversion models that are trained on clicked\nimpressions. Most generally, this research serves to both better conceptualise\nthe interplay between types of non-response and model improvements via active\nlearning, and to provide a practical, easy-to-implement correction that\nmitigates model degradation.\n","authors":["Thomas Robinson","Niek Tax","Richard Mudd","Ido Guy"],"pdf_url":"https://arxiv.org/pdf/2312.08150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14859v2","updated":"2024-03-11T09:39:33Z","published":"2023-09-26T11:36:26Z","title":"Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to\n  Model Evaluation","summary":"  Text-to-image generative models have garnered immense attention for their\nability to produce high-fidelity images from text prompts. Among these, Stable\nDiffusion distinguishes itself as a leading open-source model in this\nfast-growing field. However, the intricacies of fine-tuning these models pose\nmultiple challenges from new methodology integration to systematic evaluation.\nAddressing these issues, this paper introduces LyCORIS (Lora beYond\nConventional methods, Other Rank adaptation Implementations for Stable\ndiffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library\nthat offers a wide selection of fine-tuning methodologies for Stable Diffusion.\nFurthermore, we present a thorough framework for the systematic assessment of\nvaried fine-tuning techniques. This framework employs a diverse suite of\nmetrics and delves into multiple facets of fine-tuning, including\nhyperparameter adjustments and the evaluation with different prompt types\nacross various concept categories. Through this comprehensive approach, our\nwork provides essential insights into the nuanced effects of fine-tuning\nparameters, bridging the gap between state-of-the-art research and practical\napplication.\n","authors":["Shih-Ying Yeh","Yu-Guan Hsieh","Zhidong Gao","Bernard B W Yang","Giyeong Oh","Yanmin Gong"],"pdf_url":"https://arxiv.org/pdf/2309.14859v2.pdf","comment":"In International Conference on Learning Representations 12 (ICLR\n  2024) [79 pages, 54 figures, 7 tables]"},{"id":"http://arxiv.org/abs/2403.06535v1","updated":"2024-03-11T09:21:11Z","published":"2024-03-11T09:21:11Z","title":"Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning","summary":"  Decentralized and lifelong-adaptive multi-agent collaborative learning aims\nto enhance collaboration among multiple agents without a central server, with\neach agent solving varied tasks over time. To achieve efficient collaboration,\nagents should: i) autonomously identify beneficial collaborative relationships\nin a decentralized manner; and ii) adapt to dynamically changing task\nobservations. In this paper, we propose DeLAMA, a decentralized multi-agent\nlifelong collaborative learning algorithm with dynamic collaboration graphs. To\npromote autonomous collaboration relationship learning, we propose a\ndecentralized graph structure learning algorithm, eliminating the need for\nexternal priors. To facilitate adaptation to dynamic tasks, we design a memory\nunit to capture the agents' accumulated learning history and knowledge, while\npreserving finite storage consumption. To further augment the system's\nexpressive capabilities and computational efficiency, we apply algorithm\nunrolling, leveraging the advantages of both mathematical optimization and\nneural networks. This allows the agents to `learn to collaborate' through the\nsupervision of training tasks. Our theoretical analysis verifies that\ninter-agent collaboration is communication efficient under a small number of\ncommunication rounds. The experimental results verify its ability to facilitate\nthe discovery of collaboration strategies and adaptation to dynamic learning\nscenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in\nclassification accuracy. We expect our work can serve as a foundational\ntechnique to facilitate future works towards an intelligent, decentralized, and\ndynamic multi-agent system. Code is available at\nhttps://github.com/ShuoTang123/DeLAMA.\n","authors":["Shuo Tang","Rui Ye","Chenxin Xu","Xiaowen Dong","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06535v1.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.06534v1","updated":"2024-03-11T09:20:40Z","published":"2024-03-11T09:20:40Z","title":"SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale\n  SAR Object Detection","summary":"  Synthetic Aperture Radar (SAR) object detection has gained significant\nattention recently due to its irreplaceable all-weather imaging capabilities.\nHowever, this research field suffers from both limited public datasets (mostly\ncomprising <2K images with only mono-category objects) and inaccessible source\ncode. To tackle these challenges, we establish a new benchmark dataset and an\nopen-source method for large-scale SAR object detection. Our dataset,\nSARDet-100K, is a result of intense surveying, collecting, and standardizing 10\nexisting SAR detection datasets, providing a large-scale and diverse dataset\nfor research purposes. To the best of our knowledge, SARDet-100K is the first\nCOCO-level large-scale multi-class SAR object detection dataset ever created.\nWith this high-quality dataset, we conducted comprehensive experiments and\nuncovered a crucial challenge in SAR object detection: the substantial\ndisparities between the pretraining on RGB datasets and finetuning on SAR\ndatasets in terms of both data domain and model structure. To bridge these\ngaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)\npretraining framework that tackles the problems from the perspective of data\ninput, domain transition, and model migration. The proposed MSFA method\nsignificantly enhances the performance of SAR object detection models while\ndemonstrating exceptional generalizability and flexibility across diverse\nmodels. This work aims to pave the way for further advancements in SAR object\ndetection. The dataset and code is available at\nhttps://github.com/zcablii/SARDet_100K.\n","authors":["Yuxuan Li","Xiang Li","Weijie Li","Qibin Hou","Li Liu","Ming-Ming Cheng","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2403.06534v1.pdf","comment":"22 Pages, 10 Figures, 9 Tables"},{"id":"http://arxiv.org/abs/2403.06528v1","updated":"2024-03-11T09:10:37Z","published":"2024-03-11T09:10:37Z","title":"Adaptive Federated Learning Over the Air","summary":"  We propose a federated version of adaptive gradient methods, particularly\nAdaGrad and Adam, within the framework of over-the-air model training. This\napproach capitalizes on the inherent superposition property of wireless\nchannels, facilitating fast and scalable parameter aggregation. Meanwhile, it\nenhances the robustness of the model training process by dynamically adjusting\nthe stepsize in accordance with the global gradient update. We derive the\nconvergence rate of the training algorithms, encompassing the effects of\nchannel fading and interference, for a broad spectrum of nonconvex loss\nfunctions. Our analysis shows that the AdaGrad-based algorithm converges to a\nstationary point at the rate of $\\mathcal{O}( \\ln{(T)} /{ T^{ 1 -\n\\frac{1}{\\alpha} } } )$, where $\\alpha$ represents the tail index of the\nelectromagnetic interference. This result indicates that the level of\nheavy-tailedness in interference distribution plays a crucial role in the\ntraining efficiency: the heavier the tail, the slower the algorithm converges.\nIn contrast, an Adam-like algorithm converges at the $\\mathcal{O}( 1/T )$ rate,\ndemonstrating its advantage in expediting the model training process. We\nconduct extensive experiments that corroborate our theoretical findings and\naffirm the practical efficacy of our proposed federated adaptive gradient\nmethods.\n","authors":["Chenhao Wang","Zihan Chen","Nikolaos Pappas","Howard H. Yang","Tony Q. S. Quek","H. Vincent Poor"],"pdf_url":"https://arxiv.org/pdf/2403.06528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00010v2","updated":"2024-03-11T09:10:15Z","published":"2023-05-29T19:40:30Z","title":"Trainable and Explainable Simplicial Map Neural Networks","summary":"  Simplicial map neural networks (SMNNs) are topology-based neural networks\nwith interesting properties such as universal approximation ability and\nrobustness to adversarial examples under appropriate conditions. However, SMNNs\npresent some bottlenecks for their possible application in high-dimensional\ndatasets. First, SMNNs have precomputed fixed weight and no SMNN training\nprocess has been defined so far, so they lack generalization ability. Second,\nSMNNs require the construction of a convex polytope surrounding the input\ndataset. In this paper, we overcome these issues by proposing an SMNN training\nprocedure based on a support subset of the given dataset and replacing the\nconstruction of the convex polytope by a method based on projections to a\nhypersphere. In addition, the explainability capacity of SMNNs and an effective\nimplementation are also newly introduced in this paper.\n","authors":["Eduardo Paluzo-Hidalgo","Miguel A. Gutiérrez-Naranjo","Rocio Gonzalez-Diaz"],"pdf_url":"https://arxiv.org/pdf/2306.00010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06524v1","updated":"2024-03-11T08:58:42Z","published":"2024-03-11T08:58:42Z","title":"Tactical Decision Making for Autonomous Trucks by Deep Reinforcement\n  Learning with Total Cost of Operation Based Reward","summary":"  We develop a deep reinforcement learning framework for tactical decision\nmaking in an autonomous truck, specifically for Adaptive Cruise Control (ACC)\nand lane change maneuvers in a highway scenario. Our results demonstrate that\nit is beneficial to separate high-level decision-making processes and low-level\ncontrol actions between the reinforcement learning agent and the low-level\ncontrollers based on physical models. In the following, we study optimizing the\nperformance with a realistic and multi-objective reward function based on Total\nCost of Operation (TCOP) of the truck using different approaches; by adding\nweights to reward components, by normalizing the reward components and by using\ncurriculum learning techniques.\n","authors":["Deepthi Pathare","Leo Laine","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2403.06524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02879v3","updated":"2024-03-11T08:56:58Z","published":"2023-06-05T13:50:56Z","title":"Neuron Activation Coverage: Rethinking Out-of-distribution Detection and\n  Generalization","summary":"  The out-of-distribution (OOD) problem generally arises when neural networks\nencounter data that significantly deviates from the training data distribution,\ni.e., in-distribution (InD). In this paper, we study the OOD problem from a\nneuron activation view. We first formulate neuron activation states by\nconsidering both the neuron output and its influence on model decisions. Then,\nto characterize the relationship between neurons and OOD issues, we introduce\nthe \\textit{neuron activation coverage} (NAC) -- a simple measure for neuron\nbehaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD\ninputs can be largely separated based on the neuron behavior, which\nsignificantly eases the OOD detection problem and beats the 21 previous methods\nover three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive\ncorrelation between NAC and model generalization ability consistently holds\nacross architectures and datasets, which enables a NAC-based criterion for\nevaluating model robustness. Compared to prevalent InD validation criteria, we\nshow that NAC not only can select more robust models, but also has a stronger\ncorrelation with OOD test performance.\n","authors":["Yibing Liu","Chris Xing Tian","Haoliang Li","Lei Ma","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2306.02879v3.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2312.15698v3","updated":"2024-03-11T08:31:19Z","published":"2023-12-25T11:39:46Z","title":"RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for\n  Program Repair","summary":"  Automated Program Repair (APR) has evolved significantly with the advent of\nLarge Language Models (LLMs). Fine-tuning LLMs for program repair is a recent\navenue of research, with many dimensions which have not been explored. Existing\nwork mostly fine-tunes LLMs with naive code representations and is\nfundamentally limited in its ability to fine-tune larger LLMs. To address this\nproblem, we propose RepairLLaMA, a novel program repair approach that combines\n1) code representations for APR and 2) the state-of-the-art parameter-efficient\nLLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a\nhighly effective `program repair adapter' for fixing bugs with language models.\nOur experiments demonstrate the validity of both concepts. First, fine-tuning\nadapters with program repair specific code representations enables the model to\nuse meaningful repair signals. Second, parameter-efficient fine-tuning helps\nfine-tuning to converge and contributes to the effectiveness of the repair\nadapter to fix data-points outside the fine-tuning data distribution. Overall,\nRepairLLaMA correctly fixes 125 Defects4J v2 and 82 HumanEval-Java bugs,\noutperforming all baselines.\n","authors":["André Silva","Sen Fang","Martin Monperrus"],"pdf_url":"https://arxiv.org/pdf/2312.15698v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06503v1","updated":"2024-03-11T08:25:52Z","published":"2024-03-11T08:25:52Z","title":"Automatic Generation of Python Programs Using Context-Free Grammars","summary":"  In recent years, data has emerged as the new gold, serving as a powerful tool\nfor creating intelligent systems. However, procuring high-quality data remains\nchallenging, especially for code. To address this, we developed TinyPy\nGenerator, a tool that generates random Python programs using a context-free\ngrammar. The generated programs are guaranteed to be correct by construction.\nOur system uses custom production rules (in the Backus-Naur Form (BNF) format)\nto recursively generate code. This allows us to generate code with different\nlevels of complexity, ranging from code containing only assignments to more\ncomplex code containing conditionals and loops. Our proposed tool enables\neffortless large-scale Python code generation, beneficial for a wide range of\napplications. TinyPy Generator is particularly useful in the field of machine\nlearning, where it can generate substantial amounts of Python code for training\nPython language models. Additionally, researchers who are studying programming\nlanguages can utilize this tool to create datasets for their experiments, which\ncan help validate the robustness of code interpreters or compilers. Unlike\nexisting research, we have open-sourced our implementation. This allows\ncustomization according to user needs and extends potential usage to other\nlanguages.\n","authors":["Kamel Yamani","Marwa Naïr","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.06503v1.pdf","comment":"This work was presented at the 2nd Languages, Architectures, and\n  Tools for Heterogeneous Computing (LATHC) Workshop 2024, organized in\n  conjunction with the IEEE/ACM International Symposium on Code Generation and\n  Optimization (CGO)"},{"id":"http://arxiv.org/abs/2403.06499v1","updated":"2024-03-11T08:11:52Z","published":"2024-03-11T08:11:52Z","title":"Detection of Unobserved Common Causes based on NML Code in Discrete,\n  Mixed, and Continuous Variables","summary":"  Causal discovery in the presence of unobserved common causes from\nobservational data only is a crucial but challenging problem. We categorize all\npossible causal relationships between two random variables into the following\nfour categories and aim to identify one from observed data: two cases in which\neither of the direct causality exists, a case that variables are independent,\nand a case that variables are confounded by latent confounders. Although\nexisting methods have been proposed to tackle this problem, they require\nunobserved variables to satisfy assumptions on the form of their equation\nmodels. In our previous study (Kobayashi et al., 2022), the first causal\ndiscovery method without such assumptions is proposed for discrete data and\nnamed CLOUD. Using Normalized Maximum Likelihood (NML) Code, CLOUD selects a\nmodel that yields the minimum codelength of the observed data from a set of\nmodel candidates. This paper extends CLOUD to apply for various data types\nacross discrete, mixed, and continuous. We not only performed theoretical\nanalysis to show the consistency of CLOUD in terms of the model selection, but\nalso demonstrated that CLOUD is more effective than existing methods in\ninferring causal relationships by extensive experiments on both synthetic and\nreal-world data.\n","authors":["Masatoshi Kobayashi","Kohei Miyagichi","Shin Matsushima"],"pdf_url":"https://arxiv.org/pdf/2403.06499v1.pdf","comment":"submitted to Journal of Data Mining and Knowledge Discovery"},{"id":"http://arxiv.org/abs/2403.06489v1","updated":"2024-03-11T07:51:27Z","published":"2024-03-11T07:51:27Z","title":"Graph Neural Network with Two Uplift Estimators for Label-Scarcity\n  Individual Uplift Modeling","summary":"  Uplift modeling aims to measure the incremental effect, which we call uplift,\nof a strategy or action on the users from randomized experiments or\nobservational data. Most existing uplift methods only use individual data,\nwhich are usually not informative enough to capture the unobserved and complex\nhidden factors regarding the uplift. Furthermore, uplift modeling scenario\nusually has scarce labeled data, especially for the treatment group, which also\nposes a great challenge for model training. Considering that the neighbors'\nfeatures and the social relationships are very informative to characterize a\nuser's uplift, we propose a graph neural network-based framework with two\nuplift estimators, called GNUM, to learn from the social graph for uplift\nestimation. Specifically, we design the first estimator based on a\nclass-transformed target. The estimator is general for all types of outcomes,\nand is able to comprehensively model the treatment and control group data\ntogether to approach the uplift. When the outcome is discrete, we further\ndesign the other uplift estimator based on our defined partial labels, which is\nable to utilize more labeled data from both the treatment and control groups,\nto further alleviate the label scarcity problem. Comprehensive experiments on a\npublic dataset and two industrial datasets show a superior performance of our\nproposed framework over state-of-the-art methods under various evaluation\nmetrics. The proposed algorithms have been deployed online to serve real-world\nuplift estimation scenarios.\n","authors":["Dingyuan Zhu","Daixin Wang","Zhiqiang Zhang","Kun Kuang","Yan Zhang","Yulin Kang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.06489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06485v1","updated":"2024-03-11T07:48:35Z","published":"2024-03-11T07:48:35Z","title":"Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid\n  Approach","summary":"  Due to the scale and complexity of cloud systems, a system failure would\ntrigger an \"alert storm\", i.e., massive correlated alerts. Although these\nalerts can be traced back to a few root causes, the overwhelming number makes\nit infeasible for manual handling. Alert aggregation is thus critical to help\nengineers concentrate on the root cause and facilitate failure resolution.\nExisting methods typically utilize semantic similarity-based methods or\nstatistical methods to aggregate alerts. However, semantic similarity-based\nmethods overlook the causal rationale of alerts, while statistical methods can\nhardly handle infrequent alerts.\n  To tackle these limitations, we introduce leveraging external knowledge,\ni.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose\nCOLA, a novel hybrid approach based on correlation mining and LLM (Large\nLanguage Model) reasoning for online alert aggregation. The correlation mining\nmodule effectively captures the temporal and spatial relations between alerts,\nmeasuring their correlations in an efficient manner. Subsequently, only\nuncertain pairs with low confidence are forwarded to the LLM reasoning module\nfor detailed analysis. This hybrid design harnesses both statistical evidence\nfor frequent alerts and the reasoning capabilities of computationally intensive\nLLMs, ensuring the overall efficiency of COLA in handling large volumes of\nalerts in practical scenarios. We evaluate COLA on three datasets collected\nfrom the production environment of a large-scale cloud platform. The\nexperimental results show COLA achieves F1-scores from 0.901 to 0.930,\noutperforming state-of-the-art methods and achieving comparable efficiency. We\nalso share our experience in deploying COLA in our real-world cloud system,\nCloud X.\n","authors":["Jinxi Kuang","Jinyang Liu","Junjie Huang","Renyi Zhong","Jiazhen Gu","Lan Yu","Rui Tan","Zengyin Yang","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.06485v1.pdf","comment":"Accepted by Proceedings of the 46th International Conference on\n  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)"},{"id":"http://arxiv.org/abs/2403.06482v1","updated":"2024-03-11T07:44:56Z","published":"2024-03-11T07:44:56Z","title":"Financial Default Prediction via Motif-preserving Graph Neural Network\n  with Curriculum Learning","summary":"  User financial default prediction plays a critical role in credit risk\nforecasting and management. It aims at predicting the probability that the user\nwill fail to make the repayments in the future. Previous methods mainly extract\na set of user individual features regarding his own profiles and behaviors and\nbuild a binary-classification model to make default predictions. However, these\nmethods cannot get satisfied results, especially for users with limited\ninformation. Although recent efforts suggest that default prediction can be\nimproved by social relations, they fail to capture the higher-order topology\nstructure at the level of small subgraph patterns. In this paper, we fill in\nthis gap by proposing a motif-preserving Graph Neural Network with curriculum\nlearning (MotifGNN) to jointly learn the lower-order structures from the\noriginal graph and higherorder structures from multi-view motif-based graphs\nfor financial default prediction. Specifically, to solve the problem of weak\nconnectivity in motif-based graphs, we design the motif-based gating mechanism.\nIt utilizes the information learned from the original graph with good\nconnectivity to strengthen the learning of the higher-order structure. And\nconsidering that the motif patterns of different samples are highly unbalanced,\nwe propose a curriculum learning mechanism on the whole learning process to\nmore focus on the samples with uncommon motif distributions. Extensive\nexperiments on one public dataset and two industrial datasets all demonstrate\nthe effectiveness of our proposed method.\n","authors":["Daixin Wang","Zhiqiang Zhang","Yeyu Zhao","Kai Huang","Yulin Kang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.06482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07365v3","updated":"2024-03-11T07:33:51Z","published":"2023-10-11T10:30:49Z","title":"GraphControl: Adding Conditional Control to Universal Graph Pre-trained\n  Models for Graph Domain Transfer Learning","summary":"  Graph-structured data is ubiquitous in the world which models complex\nrelationships between objects, enabling various Web applications. Daily\ninfluxes of unlabeled graph data on the Web offer immense potential for these\napplications. Graph self-supervised algorithms have achieved significant\nsuccess in acquiring generic knowledge from abundant unlabeled graph data.\nThese pre-trained models can be applied to various downstream Web applications,\nsaving training time and improving downstream (target) performance. However,\ndifferent graphs, even across seemingly similar domains, can differ\nsignificantly in terms of attribute semantics, posing difficulties, if not\ninfeasibility, for transferring the pre-trained models to downstream tasks.\nConcretely speaking, for example, the additional task-specific node information\nin downstream tasks (specificity) is usually deliberately omitted so that the\npre-trained representation (transferability) can be leveraged. The trade-off as\nsuch is termed as \"transferability-specificity dilemma\" in this work. To\naddress this challenge, we introduce an innovative deployment module coined as\nGraphControl, motivated by ControlNet, to realize better graph domain transfer\nlearning. Specifically, by leveraging universal structural pre-trained models\nand GraphControl, we align the input space across various graphs and\nincorporate unique characteristics of target data as conditional inputs. These\nconditions will be progressively integrated into the model during fine-tuning\nor prompt tuning through ControlNet, facilitating personalized deployment.\nExtensive experiments show that our method significantly enhances the\nadaptability of pre-trained models on target attributed datasets, achieving\n1.4-3x performance gain. Furthermore, it outperforms training-from-scratch\nmethods on target data with a comparable margin and exhibits faster\nconvergence.\n","authors":["Yun Zhu","Yaoke Wang","Haizhou Shi","Zhenshuo Zhang","Dian Jiao","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2310.07365v3.pdf","comment":"Accepted by The Web Conference 2024 (WWW 2024)"},{"id":"http://arxiv.org/abs/2403.06466v1","updated":"2024-03-11T07:07:05Z","published":"2024-03-11T07:07:05Z","title":"RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling\n  Approach","summary":"  Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational\ncost of bus company and guarantee service quality for passengers. Existing\napproaches typically generate a bus scheduling scheme in an offline manner and\nthen schedule buses according to the scheme. In practice, uncertain events such\nas traffic congestion occur frequently, which may make the pre-determined bus\nscheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov\nDecision Process (MDP). A Reinforcement Learning-based Multi-line bus\nScheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline\nand online phases. At the offline phase, deadhead decision is integrated into\nbus selection decision for the first time to simplify the learning problem. At\nthe online phase, deadhead decision is made through a time window mechanism\nbased on the policy learned at the offline phase. We develop several new and\nuseful state features including the features for control points, bus lines and\nbuses. A bus priority screening mechanism is invented to construct bus-related\nfeatures. Considering the interests of both the bus company and passengers, a\nreward function combining the final reward and the step-wise reward is devised.\nExperiments at the offline phase demonstrate that the number of buses used of\nRL-MSA is decreased compared with offline optimization approaches. At the\nonline phase, RL-MSA can cover all departure times in a timetable (i.e.,\nservice quality) without increasing the number of buses used (i.e., operational\ncost).\n","authors":["Yingzhuo Liu"],"pdf_url":"https://arxiv.org/pdf/2403.06466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04224v2","updated":"2024-03-11T07:04:42Z","published":"2024-03-07T04:54:56Z","title":"Aligners: Decoupling LLMs and Alignment","summary":"  Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We illustrate our method by training an \"ethical\" aligner and verify\nits efficacy empirically.\n","authors":["Lilian Ngweta","Mayank Agarwal","Subha Maity","Alex Gittens","Yuekai Sun","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2403.04224v2.pdf","comment":"Tiny Papers Track at the International Conference on Learning\n  Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2402.04676v2","updated":"2024-03-11T06:56:54Z","published":"2024-02-07T09:03:04Z","title":"Group Distributionally Robust Dataset Distillation with Risk\n  Minimization","summary":"  Dataset distillation (DD) has emerged as a widely adopted technique for\ncrafting a synthetic dataset that captures the essential information of a\ntraining dataset, facilitating the training of accurate neural models. Its\napplications span various domains, including transfer learning, federated\nlearning, and neural architecture search. The most popular methods for\nconstructing the synthetic data rely on matching the convergence properties of\ntraining the model with the synthetic dataset and the training dataset.\nHowever, targeting the training dataset must be thought of as auxiliary in the\nsame sense that the training set is an approximate substitute for the\npopulation distribution, and the latter is the data of interest. Yet despite\nits popularity, an aspect that remains unexplored is the relationship of DD to\nits generalization, particularly across uncommon subgroups. That is, how can we\nensure that a model trained on the synthetic dataset performs well when faced\nwith samples from regions with low population density? Here, the\nrepresentativeness and coverage of the dataset become salient over the\nguaranteed training error at inference. Drawing inspiration from\ndistributionally robust optimization, we introduce an algorithm that combines\nclustering with the minimization of a risk measure on the loss to conduct DD.\nWe provide a theoretical rationale for our approach and demonstrate its\neffective generalization and robustness across subgroups through numerical\nexperiments. The source code is available in\nhttps://github.com/Mming11/RobustDatasetDistillation.\n","authors":["Saeed Vahidian","Mingyu Wang","Jianyang Gu","Vyacheslav Kungurtsev","Wei Jiang","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2402.04676v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11816v2","updated":"2024-03-11T06:46:22Z","published":"2024-02-19T04:13:33Z","title":"Learning the Unlearned: Mitigating Feature Suppression in Contrastive\n  Learning","summary":"  Self-Supervised Contrastive Learning has proven effective in deriving\nhigh-quality representations from unlabeled data. However, a major challenge\nthat hinders both unimodal and multimodal contrastive learning is feature\nsuppression, a phenomenon where the trained model captures only a limited\nportion of the information from the input data while overlooking other\npotentially valuable content. This issue often leads to indistinguishable\nrepresentations for visually similar but semantically different inputs,\nadversely affecting downstream task performance, particularly those requiring\nrigorous semantic comprehension. To address this challenge, we propose a novel\nmodel-agnostic Multistage Contrastive Learning (MCL) framework. Unlike standard\ncontrastive learning which inherently captures one single biased feature\ndistribution, MCL progressively learns previously unlearned features through\nfeature-aware negative sampling at each stage, where the negative samples of an\nanchor are exclusively selected from the cluster it was assigned to in\npreceding stages. Meanwhile, MCL preserves the previously well-learned features\nby cross-stage representation integration, integrating features across all\nstages to form final representations. Our comprehensive evaluation demonstrates\nMCL's effectiveness and superiority across both unimodal and multimodal\ncontrastive learning, spanning a range of model architectures from ResNet to\nVision Transformers (ViT). Remarkably, in tasks where the original CLIP model\nhas shown limitations, MCL dramatically enhances performance, with improvements\nup to threefold on specific attributes in the recently proposed MMVP benchmark.\n","authors":["Jihai Zhang","Xiang Lan","Xiaoye Qu","Yu Cheng","Mengling Feng","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2402.11816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06458v1","updated":"2024-03-11T06:36:33Z","published":"2024-03-11T06:36:33Z","title":"Prediction of Wort Density with LSTM Network","summary":"  Many physical target values in technical processes are error-prone,\ncumbersome, or expensive to measure automatically. One example of a physical\ntarget value is the wort density, which is an important value needed for beer\nproduction. This article introduces a system that helps the brewer measure wort\ndensity through sensors in order to reduce errors in manual data collection.\nInstead of a direct measurement of wort density, a method is developed that\ncalculates the density from measured values acquired by inexpensive standard\nsensors such as pressure or temperature. The model behind the calculation is a\nneural network, known as LSTM.\n","authors":["Derk Rembold","Bernd Stauss","Stefan Schwarzkopf"],"pdf_url":"https://arxiv.org/pdf/2403.06458v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.06456v1","updated":"2024-03-11T06:32:32Z","published":"2024-03-11T06:32:32Z","title":"A Survey of Learned Indexes for the Multi-dimensional Space","summary":"  A recent research trend involves treating database index structures as\nMachine Learning (ML) models. In this domain, single or multiple ML models are\ntrained to learn the mapping from keys to positions inside a data set. This\nclass of indexes is known as \"Learned Indexes.\" Learned indexes have\ndemonstrated improved search performance and reduced space requirements for\none-dimensional data. The concept of one-dimensional learned indexes has\nnaturally been extended to multi-dimensional (e.g., spatial) data, leading to\nthe development of \"Learned Multi-dimensional Indexes\". This survey focuses on\nlearned multi-dimensional index structures. Specifically, it reviews the\ncurrent state of this research area, explains the core concepts behind each\nproposed method, and classifies these methods based on several well-defined\ncriteria. We present a taxonomy that classifies and categorizes each learned\nmulti-dimensional index, and survey the existing literature on learned\nmulti-dimensional indexes according to this taxonomy. Additionally, we present\na timeline to illustrate the evolution of research on learned indexes. Finally,\nwe highlight several open challenges and future research directions in this\nemerging and highly active field.\n","authors":["Abdullah Al-Mamun","Hao Wu","Qiyang He","Jianguo Wang","Walid G. Aref"],"pdf_url":"https://arxiv.org/pdf/2403.06456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05773v2","updated":"2024-03-11T06:27:26Z","published":"2023-05-09T21:31:07Z","title":"DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for\n  Identifying Large Language Model Generated Text","summary":"  The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of text generators. With the potential for misuse\nescalating, the importance of discerning whether texts are human-authored or\ngenerated by LLMs has become paramount. Several preceding studies have ventured\nto address this challenge by employing binary classifiers to differentiate\nbetween human-written and LLM-generated text. Nevertheless, the reliability of\nthese classifiers has been subject to question. Given that consequential\ndecisions may hinge on the outcome of such classification, it is imperative\nthat text source detection is of high caliber. In light of this, the present\npaper introduces DeepTextMark, a deep learning-driven text watermarking\nmethodology devised for text source identification. By leveraging Word2Vec and\nSentence Encoding for watermark insertion, alongside a transformer-based\nclassifier for watermark detection, DeepTextMark epitomizes a blend of\nblindness, robustness, imperceptibility, and reliability. As elaborated within\nthe paper, these attributes are crucial for universal text source detection,\nwith a particular emphasis in this paper on text produced by LLMs. DeepTextMark\noffers a viable \"add-on\" solution to prevailing text generation frameworks,\nrequiring no direct access or alterations to the underlying text generation\nmechanism. Experimental evaluations underscore the high imperceptibility,\nelevated detection accuracy, augmented robustness, reliability, and swift\nexecution of DeepTextMark.\n","authors":["Travis Munyer","Abdullah Tanvir","Arjon Das","Xin Zhong"],"pdf_url":"https://arxiv.org/pdf/2305.05773v2.pdf","comment":"The paper has been accpeted for publication by IEEE Access"},{"id":"http://arxiv.org/abs/2306.15620v3","updated":"2024-03-11T06:20:07Z","published":"2023-06-27T16:59:15Z","title":"SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating\n  Replicable Scenes","summary":"  We present a new reproducible benchmark for evaluating robot manipulation in\nthe real world, specifically focusing on pick-and-place. Our benchmark uses the\nYCB objects, a commonly used dataset in the robotics community, to ensure that\nour results are comparable to other studies. Additionally, the benchmark is\ndesigned to be easily reproducible in the real world, making it accessible to\nresearchers and practitioners. We also provide our experimental results and\nanalyzes for model-based and model-free 6D robotic grasping on the benchmark,\nwhere representative algorithms are evaluated for object perception, grasping\nplanning, and motion planning. We believe that our benchmark will be a valuable\ntool for advancing the field of robot manipulation. By providing a standardized\nevaluation framework, researchers can more easily compare different techniques\nand algorithms, leading to faster progress in developing robot manipulation\nmethods.\n","authors":["Ninad Khargonkar","Sai Haneesh Allu","Yangxiao Lu","Jishnu Jaykumar P","Balakrishnan Prabhakaran","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2306.15620v3.pdf","comment":"Accepted to ICRA 2024. Project page is available at\n  https://irvlutd.github.io/SceneReplica"},{"id":"http://arxiv.org/abs/2305.18766v4","updated":"2024-03-11T06:14:31Z","published":"2023-05-30T05:56:58Z","title":"HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion\n  Guidance","summary":"  The advancements in automatic text-to-3D generation have been remarkable.\nMost existing methods use pre-trained text-to-image diffusion models to\noptimize 3D representations like Neural Radiance Fields (NeRFs) via\nlatent-space denoising score matching. Yet, these methods often result in\nartifacts and inconsistencies across different views due to their suboptimal\noptimization approaches and limited understanding of 3D geometry. Moreover, the\ninherent constraints of NeRFs in rendering crisp geometry and stable textures\nusually lead to a two-stage optimization to attain high-resolution details.\nThis work proposes holistic sampling and smoothing approaches to achieve\nhigh-quality text-to-3D generation, all in a single-stage optimization. We\ncompute denoising scores in the text-to-image diffusion model's latent and\nimage spaces. Instead of randomly sampling timesteps (also referred to as noise\nlevels in denoising score matching), we introduce a novel timestep annealing\napproach that progressively reduces the sampled timestep throughout\noptimization. To generate high-quality renderings in a single-stage\noptimization, we propose regularization for the variance of z-coordinates along\nNeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel\nsmoothing technique that refines importance sampling weights coarse-to-fine,\nensuring accurate and thorough sampling in high-density regions. Extensive\nexperiments demonstrate the superiority of our method over previous approaches,\nenabling the generation of highly detailed and view-consistent 3D assets\nthrough a single-stage training process.\n","authors":["Junzhe Zhu","Peiye Zhuang","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2305.18766v4.pdf","comment":"Project page: https://hifa-team.github.io/HiFA-site/"},{"id":"http://arxiv.org/abs/2403.00269v2","updated":"2024-03-11T05:58:55Z","published":"2024-03-01T04:16:08Z","title":"Large Convolutional Model Tuning via Filter Subspace","summary":"  Efficient fine-tuning methods are critical to address the high computational\nand parameter complexity while adapting large pre-trained models to downstream\ntasks. Our study is inspired by prior research that represents each convolution\nfilter as a linear combination of a small set of filter subspace elements,\nreferred to as filter atoms. In this paper, we propose to fine-tune pre-trained\nmodels by adjusting only filter atoms, which are responsible for spatial-only\nconvolution, while preserving spatially-invariant channel combination knowledge\nin atom coefficients. In this way, we bring a new filter subspace view for\nmodel tuning. Furthermore, each filter atom can be recursively decomposed as a\ncombination of another set of atoms, which naturally expands the number of\ntunable parameters in the filter subspace. By only adapting filter atoms\nconstructed by a small number of parameters, while maintaining the rest of\nmodel parameters constant, the proposed approach is highly parameter-efficient.\nIt effectively preserves the capabilities of pre-trained models and prevents\noverfitting to downstream tasks. Extensive experiments show that such a simple\nscheme surpasses previous tuning baselines for both discriminate and generative\ntasks.\n","authors":["Wei Chen","Zichen Miao","Qiang Qiu"],"pdf_url":"https://arxiv.org/pdf/2403.00269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04161v2","updated":"2024-03-11T05:37:36Z","published":"2024-03-07T02:40:42Z","title":"Estimating Neural Network Performance through Sample-Wise Activation\n  Patterns","summary":"  Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid\nresource-intensive neural network training, especially in Neural Architecture\nSearch (NAS). Recent studies show that existing training-free metrics have\nseveral limitations, such as limited correlation and poor generalisation across\ndifferent search spaces and tasks. Hence, we propose Sample-Wise Activation\nPatterns and its derivative, SWAP-Score, a novel high-performance training-free\nmetric. It measures the expressivity of networks over a batch of input samples.\nThe SWAP-Score is strongly correlated with ground-truth performance across\nvarious search spaces and tasks, outperforming 15 existing training-free\nmetrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be\nfurther enhanced by regularisation, which leads to even higher correlations in\ncell-based search space and enables model size control during the search. For\nexample, Spearman's rank correlation coefficient between regularised SWAP-Score\nand CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90,\nsignificantly higher than 0.80 from the second-best metric, NWOT. When\nintegrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves\ncompetitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and\n9 minutes of GPU time respectively.\n","authors":["Yameng Peng","Andy Song","Haytham M. Fayek","Vic Ciesielski","Xiaojun Chang"],"pdf_url":"https://arxiv.org/pdf/2403.04161v2.pdf","comment":"ICLR2024 Spotlight"},{"id":"http://arxiv.org/abs/2301.01333v3","updated":"2024-03-11T05:10:17Z","published":"2023-01-03T19:52:17Z","title":"oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep\n  Learning Compilation","summary":"  With the rapid development of deep learning models and hardware support for\ndense computing, the deep learning workload characteristics changed\nsignificantly from a few hot spots on compute-intensive operations to a broad\nrange of operations scattered across the models. Accelerating a few\ncompute-intensive operations using the expert-tuned implementation of\nprimitives does not fully exploit the performance potential of AI hardware.\nVarious efforts have been made to compile a full deep neural network (DNN)\ngraph. One of the biggest challenges is to achieve high-performance tensor\ncompilation by generating expert level performance code for the dense\ncompute-intensive operations and applying compilation optimization at the scope\nof DNN computation graph across multiple compute-intensive operations.\n  We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid\napproach of using techniques from both compiler optimization and expert-tuned\nkernels for high performance code generation of the deep neural network graph.\noneDNN Graph Compiler addresses unique optimization challenges in the deep\nlearning domain, such as low-precision computation, aggressive fusion of graph\noperations, optimization for static tensor shapes and memory layout, constant\nweight optimization, and memory buffer reuse. Experimental results demonstrate\nsignificant performance gains over existing tensor compiler and primitives\nlibrary for performance-critical DNN computation graphs and end-to-end models\non Intel Xeon Scalable Processors.\n","authors":["Jianhui Li","Zhennan Qin","Yijie Mei","Jingze Cui","Yunfei Song","Ciyong Chen","Yifei Zhang","Longsheng Du","Xianhang Cheng","Baihui Jin","Yan Zhang","Jason Ye","Eric Lin","Dan Lavery"],"pdf_url":"https://arxiv.org/pdf/2301.01333v3.pdf","comment":"10 pages excluding reference, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.10257v3","updated":"2024-03-11T04:59:57Z","published":"2023-03-17T21:53:07Z","title":"Recent Developments in Machine Learning Methods for Stochastic Control\n  and Games","summary":"  Stochastic optimal control and games have a wide range of applications, from\nfinance and economics to social sciences, robotics, and energy management. Many\nreal-world applications involve complex models that have driven the development\nof sophisticated numerical methods. Recently, computational methods based on\nmachine learning have been developed for solving stochastic control problems\nand games. In this review, we focus on deep learning methods that have unlocked\nthe possibility of solving such problems, even in high dimensions or when the\nstructure is very complex, beyond what traditional numerical methods can\nachieve. We consider mostly the continuous time and continuous space setting.\nMany of the new approaches build on recent neural-network-based methods for\nsolving high-dimensional partial differential equations or backward stochastic\ndifferential equations, or on model-free reinforcement learning for Markov\ndecision processes that have led to breakthrough results. This paper provides\nan introduction to these methods and summarizes the state-of-the-art works at\nthe crossroad of machine learning and stochastic control and games.\n","authors":["Ruimeng Hu","Mathieu Laurière"],"pdf_url":"https://arxiv.org/pdf/2303.10257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06432v1","updated":"2024-03-11T04:49:41Z","published":"2024-03-11T04:49:41Z","title":"Joint-Embedding Masked Autoencoder for Self-supervised Learning of\n  Dynamic Functional Connectivity from the Human Brain","summary":"  Graph Neural Networks (GNNs) have shown promise in learning dynamic\nfunctional connectivity for distinguishing phenotypes from human brain\nnetworks. However, obtaining extensive labeled clinical data for training is\noften resource-intensive, making practical application difficult. Leveraging\nunlabeled data thus becomes crucial for representation learning in a\nlabel-scarce setting. Although generative self-supervised learning techniques,\nespecially masked autoencoders, have shown promising results in representation\nlearning in various domains, their application to dynamic graphs for dynamic\nfunctional connectivity remains underexplored, facing challenges in capturing\nhigh-level semantic representations. Here, we introduce the Spatio-Temporal\nJoint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the\nJoint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA\nemploys a JEPA-inspired strategy for reconstructing dynamic graphs, which\nenables the learning of higher-level semantic representations considering\ntemporal perspectives, addressing the challenges in fMRI data representation\nlearning. Utilizing the large-scale UK Biobank dataset for self-supervised\nlearning, ST-JEMA shows exceptional representation learning performance on\ndynamic functional connectivity demonstrating superiority over previous methods\nin predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI\ndatasets even with limited samples and effectiveness of temporal reconstruction\non missing data scenarios. These findings highlight the potential of our\napproach as a robust representation learning method for leveraging label-scarce\nfMRI data.\n","authors":["Jungwon Choi","Hyungi Lee","Byung-Hoon Kim","Juho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06432v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.06425v1","updated":"2024-03-11T04:26:18Z","published":"2024-03-11T04:26:18Z","title":"A Differential Geometric View and Explainability of GNN on Evolving\n  Graphs","summary":"  Graphs are ubiquitous in social networks and biochemistry, where Graph Neural\nNetworks (GNN) are the state-of-the-art models for prediction. Graphs can be\nevolving and it is vital to formally model and understand how a trained GNN\nresponds to graph evolution. We propose a smooth parameterization of the GNN\npredicted distributions using axiomatic attribution, where the distributions\nare on a low-dimensional manifold within a high-dimensional embedding space. We\nexploit the differential geometric viewpoint to model distributional evolution\nas smooth curves on the manifold. We reparameterize families of curves on the\nmanifold and design a convex optimization problem to find a unique curve that\nconcisely approximates the distributional evolution for human interpretation.\nExtensive experiments on node classification, link prediction, and graph\nclassification tasks with evolving graphs demonstrate the better sparsity,\nfaithfulness, and intuitiveness of the proposed method over the\nstate-of-the-art methods.\n","authors":["Yazheng Liu","Xi Zhang","Sihong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.06425v1.pdf","comment":"Accepted into ICLR 2023"},{"id":"http://arxiv.org/abs/2403.06424v1","updated":"2024-03-11T04:25:41Z","published":"2024-03-11T04:25:41Z","title":"Bridging Domains with Approximately Shared Features","summary":"  Multi-source domain adaptation aims to reduce performance degradation when\napplying machine learning models to unseen domains. A fundamental challenge is\ndevising the optimal strategy for feature selection. Existing literature is\nsomewhat paradoxical: some advocate for learning invariant features from source\ndomains, while others favor more diverse features. To address the challenge, we\npropose a statistical framework that distinguishes the utilities of features\nbased on the variance of their correlation to label $y$ across domains. Under\nour framework, we design and analyze a learning procedure consisting of\nlearning approximately shared feature representation from source tasks and\nfine-tuning it on the target task. Our theoretical analysis necessitates the\nimportance of learning approximately shared features instead of only the\nstrictly invariant features and yields an improved population risk compared to\nprevious results on both source and target tasks, thus partly resolving the\nparadox mentioned above. Inspired by our theory, we proposed a more practical\nway to isolate the content (invariant+approximately shared) from environmental\nfeatures and further consolidate our theoretical findings.\n","authors":["Ziliang Samuel Zhong","Xiang Pan","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2403.06424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09476v2","updated":"2024-03-11T04:18:10Z","published":"2023-07-18T17:56:50Z","title":"Overthinking the Truth: Understanding how Language Models Process False\n  Demonstrations","summary":"  Modern language models can imitate complex patterns through few-shot\nlearning, enabling them to complete challenging tasks without fine-tuning.\nHowever, imitation can also lead models to reproduce inaccuracies or harmful\ncontent if present in the context. We study harmful imitation through the lens\nof a model's internal representations, and identify two related phenomena:\n\"overthinking\" and \"false induction heads\". The first phenomenon, overthinking,\nappears when we decode predictions from intermediate layers, given correct vs.\nincorrect few-shot demonstrations. At early layers, both demonstrations induce\nsimilar model behavior, but the behavior diverges sharply at some \"critical\nlayer\", after which the accuracy given incorrect demonstrations progressively\ndecreases. The second phenomenon, false induction heads, are a possible\nmechanistic cause of overthinking: these are heads in late layers that attend\nto and copy false information from previous demonstrations, and whose ablation\nreduces overthinking. Beyond scientific understanding, our results suggest that\nstudying intermediate model computations could be a promising avenue for\nunderstanding and guarding against harmful model behaviors.\n","authors":["Danny Halawi","Jean-Stanislas Denain","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2307.09476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06420v1","updated":"2024-03-11T04:13:26Z","published":"2024-03-11T04:13:26Z","title":"RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic\n  Manipulations With Large Language Models","summary":"  Reinforcement learning (RL) has demonstrated its capability in solving\nvarious tasks but is notorious for its low sample efficiency. In this paper, we\npropose RLingua, a framework that can leverage the internal knowledge of large\nlanguage models (LLMs) to reduce the sample complexity of RL in robotic\nmanipulations. To this end, we first present how to extract the prior knowledge\nof LLMs by prompt engineering so that a preliminary rule-based robot controller\nfor a specific task can be generated. Despite being imperfect, the\nLLM-generated robot controller is utilized to produce action samples during\nrollouts with a decaying probability, thereby improving RL's sample efficiency.\nWe employ the actor-critic framework and modify the actor loss to regularize\nthe policy learning towards the LLM-generated controller. RLingua also provides\na novel method of improving the imperfect LLM-generated robot controllers by\nRL. We demonstrated that RLingua can significantly reduce the sample complexity\nof TD3 in the robot tasks of panda_gym and achieve high success rates in\nsparsely rewarded robot tasks in RLBench, where the standard TD3 fails.\nAdditionally, We validated RLingua's effectiveness in real-world robot\nexperiments through Sim2Real, demonstrating that the learned policies are\neffectively transferable to real robot tasks. Further details and videos about\nour work are available at our project website https://rlingua.github.io.\n","authors":["Liangliang Chen","Yutian Lei","Shiyu Jin","Ying Zhang","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06419v1","updated":"2024-03-11T04:11:48Z","published":"2024-03-11T04:11:48Z","title":"Causal Multi-Label Feature Selection in Federated Setting","summary":"  Multi-label feature selection serves as an effective mean for dealing with\nhigh-dimensional multi-label data. To achieve satisfactory performance,\nexisting methods for multi-label feature selection often require the\ncentralization of substantial data from multiple sources. However, in Federated\nsetting, centralizing data from all sources and merging them into a single\ndataset is not feasible. To tackle this issue, in this paper, we study a\nchallenging problem of causal multi-label feature selection in federated\nsetting and propose a Federated Causal Multi-label Feature Selection (FedCMFS)\nalgorithm with three novel subroutines. Specifically, FedCMFS first uses the\nFedCFL subroutine that considers the correlations among label-label,\nlabel-feature, and feature-feature to learn the relevant features (candidate\nparents and children) of each class label while preserving data privacy without\ncentralizing data. Second, FedCMFS employs the FedCFR subroutine to selectively\nrecover the missed true relevant features. Finally, FedCMFS utilizes the FedCFC\nsubroutine to remove false relevant features. The extensive experiments on 8\ndatasets have shown that FedCMFS is effect for causal multi-label feature\nselection in federated setting.\n","authors":["Yukun Song","Dayuan Cao","Jiali Miao","Shuai Yang","Kui Yu"],"pdf_url":"https://arxiv.org/pdf/2403.06419v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.06776v1","updated":"2024-03-11T14:43:56Z","published":"2024-03-11T14:43:56Z","title":"Born to Run, Programmed to Play: Mapping the Extended Reality Exergames\n  Landscape","summary":"  Many people struggle to exercise regularly, raising the risk of serious\nhealth-related issues. Extended reality (XR) exergames address these hurdles by\ncombining physical exercises with enjoyable, immersive gameplay. While a\ngrowing body of research explores XR exergames, no previous review has\nstructured this rapidly expanding research landscape. We conducted a scoping\nreview of the current state of XR exergame research to (i) provide a structured\noverview, (ii) highlight trends, and (iii) uncover knowledge gaps. After\nidentifying 1318 papers in human-computer interaction and medical databases, we\nultimately included 186 papers in our analysis. We provide a quantitative and\nqualitative summary of XR exergame research, showing current trends and\npotential future considerations. Finally, we provide a taxonomy of XR exergames\nto help future design and methodological investigation and reporting.\n","authors":["Sukran Karaosmanoglu","Sebastian Cmentowski","Lennart E. Nacke","Frank Steinicke"],"pdf_url":"https://arxiv.org/pdf/2403.06776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09571v6","updated":"2024-03-11T13:05:10Z","published":"2023-04-19T11:19:10Z","title":"LLIC: Large Receptive Field Transform Coding with Adaptive Weights for\n  Learned Image Compression","summary":"  Effective Receptive field (ERF) plays an important role in transform coding,\nwhich determines how much redundancy can be removed at most during transform\nand how many spatial priors can be utilized to synthesize textures during\ninverse transform. Existing methods rely on stacks of small kernels, whose ERF\nremains not large enough instead, or heavy non-local attention mechanisms,\nwhich limit the potential of high resolution image coding. To tackle this\nissue, we propose Large Receptive Field Transform Coding with Adaptive Weights\nfor Learned Image Compression (LLIC). Specifically, for the first time in\nlearned image compression community, we introduce a few large kernel-based\ndepth-wise convolutions to reduce more redundancy while maintaining modest\ncomplexity. Due to wide range of image diversity, we propose to enhance the\nadaptability of convolutions via generating weights in a self-conditioned\nmanner. The large kernels cooperate with non-linear embedding and gate\nmechanisms for better expressiveness and lighter point-wise interactions. We\nalso investigate improved training techniques to fully exploit the potential of\nlarge kernels. In addition, to enhance the interactions among channels, we\npropose the adaptive channel-wise bit allocation via generating channel\nimportance factor in a self-conditioned manner. To demonstrate the\neffectiveness of proposed transform coding, we align the entropy model to\ncompare with existing transform methods and obtain models LLIC-STF, LLIC-ELIC,\nLLIC-TCM. Extensive experiments demonstrate our proposed LLIC models have\nsignificant improvements over corresponding baselines and achieve\nstate-of-the-art performances and better trade-off between performance and\ncomplexity.\n","authors":["Wei Jiang","Peirong Ning","Jiayu Yang","Yongqi Zhai","Feng Gao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.09571v6.pdf","comment":"Fix typos in Table 1 and Add some related references"},{"id":"http://arxiv.org/abs/2403.06660v1","updated":"2024-03-11T12:29:35Z","published":"2024-03-11T12:29:35Z","title":"FashionReGen: LLM-Empowered Fashion Report Generation","summary":"  Fashion analysis refers to the process of examining and evaluating trends,\nstyles, and elements within the fashion industry to understand and interpret\nits current state, generating fashion reports. It is traditionally performed by\nfashion professionals based on their expertise and experience, which requires\nhigh labour cost and may also produce biased results for relying heavily on a\nsmall group of people. In this paper, to tackle the Fashion Report Generation\n(FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting\nsystem based the advanced Large Language Models (LLMs), debbed as GPT-FAR.\nSpecifically, it tries to deliver FashionReGen based on effective catwalk\nanalysis, which is equipped with several key procedures, namely, catwalk\nunderstanding, collective organization and analysis, and report generation. By\nposing and exploring such an open-ended, complex and domain-specific task of\nFashionReGen, it is able to test the general capability of LLMs in fashion\ndomain. It also inspires the explorations of more high-level tasks with\nindustrial significance in other domains. Video illustration and more materials\nof GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.\n","authors":["Yujuan Ding","Yunshan Ma","Wenqi Fan","Yige Yao","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.06660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14316v2","updated":"2024-03-11T10:28:41Z","published":"2023-08-28T05:38:43Z","title":"UniPT: Universal Parallel Tuning for Transfer Learning with Efficient\n  Parameter and Memory","summary":"  Parameter-efficient transfer learning (PETL), i.e., fine-tuning a small\nportion of parameters, is an effective strategy for adapting pre-trained models\nto downstream domains. To further reduce the memory demand, recent PETL works\nfocus on the more valuable memory-efficient characteristic. In this paper, we\nargue that the scalability, adaptability, and generalizability of\nstate-of-the-art methods are hindered by structural dependency and pertinency\non specific pre-trained backbones. To this end, we propose a new\nmemory-efficient PETL strategy, Universal Parallel Tuning (UniPT), to mitigate\nthese weaknesses. Specifically, we facilitate the transfer process via a\nlightweight and learnable parallel network, which consists of: 1) A parallel\ninteraction module that decouples the sequential connections and processes the\nintermediate activations detachedly from the pre-trained network. 2) A\nconfidence aggregation module that learns optimal strategies adaptively for\nintegrating cross-layer features. We evaluate UniPT with different backbones\n(e.g., T5, VSE$\\infty$, CLIP4Clip, Clip-ViL, and MDETR) on various\nvision-and-language and pure NLP tasks. Extensive ablations on 18 datasets have\nvalidated that UniPT can not only dramatically reduce memory consumption and\noutperform the best competitor, but also achieve competitive performance over\nother plain PETL methods with lower training memory overhead. Our code is\npublicly available at: https://github.com/Paranioar/UniPT.\n","authors":["Haiwen Diao","Bo Wan","Ying Zhang","Xu Jia","Huchuan Lu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2308.14316v2.pdf","comment":"15 pages, 11 figures, Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.06497v1","updated":"2024-03-11T08:09:30Z","published":"2024-03-11T08:09:30Z","title":"QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven\n  Fine Tuning","summary":"  Transformer-based models have gained widespread popularity in both the\ncomputer vision (CV) and natural language processing (NLP) fields. However,\nsignificant challenges arise during post-training linear quantization, leading\nto noticeable reductions in inference accuracy. Our study focuses on uncovering\nthe underlying causes of these accuracy drops and proposing a\nquantization-friendly fine-tuning method, \\textbf{QuantTune}. Firstly, our\nanalysis revealed that, on average, 65\\% of quantization errors result from the\nprecision loss incurred by the dynamic range amplification effect of outliers\nacross the target Transformer-based models. Secondly, \\textbf{QuantTune}\nadjusts weights based on the deviation of outlier activations and effectively\nconstrains the dynamic ranges of the problematic activations. As a result, it\nsuccessfully mitigates the negative impact of outliers on the inference\naccuracy of quantized models. Lastly, \\textbf{QuantTune} can be seamlessly\nintegrated into the back-propagation pass in the fine-tuning process without\nrequiring extra complexity in inference software and hardware design. Our\napproach showcases significant improvements in post-training quantization\nacross a range of Transformer-based models, including ViT, Bert-base, and OPT.\nQuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at\n7-bit compared to top calibration methods, outperforming state-of-the-art\nsolutions by over 18.84\\% across ViT models.\n","authors":["Jiun-Man Chen","Yu-Hsuan Chao","Yu-Jie Wang","Ming-Der Shieh","Chih-Chung Hsu","Wei-Fen Lin"],"pdf_url":"https://arxiv.org/pdf/2403.06497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05773v2","updated":"2024-03-11T06:27:26Z","published":"2023-05-09T21:31:07Z","title":"DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for\n  Identifying Large Language Model Generated Text","summary":"  The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of text generators. With the potential for misuse\nescalating, the importance of discerning whether texts are human-authored or\ngenerated by LLMs has become paramount. Several preceding studies have ventured\nto address this challenge by employing binary classifiers to differentiate\nbetween human-written and LLM-generated text. Nevertheless, the reliability of\nthese classifiers has been subject to question. Given that consequential\ndecisions may hinge on the outcome of such classification, it is imperative\nthat text source detection is of high caliber. In light of this, the present\npaper introduces DeepTextMark, a deep learning-driven text watermarking\nmethodology devised for text source identification. By leveraging Word2Vec and\nSentence Encoding for watermark insertion, alongside a transformer-based\nclassifier for watermark detection, DeepTextMark epitomizes a blend of\nblindness, robustness, imperceptibility, and reliability. As elaborated within\nthe paper, these attributes are crucial for universal text source detection,\nwith a particular emphasis in this paper on text produced by LLMs. DeepTextMark\noffers a viable \"add-on\" solution to prevailing text generation frameworks,\nrequiring no direct access or alterations to the underlying text generation\nmechanism. Experimental evaluations underscore the high imperceptibility,\nelevated detection accuracy, augmented robustness, reliability, and swift\nexecution of DeepTextMark.\n","authors":["Travis Munyer","Abdullah Tanvir","Arjon Das","Xin Zhong"],"pdf_url":"https://arxiv.org/pdf/2305.05773v2.pdf","comment":"The paper has been accpeted for publication by IEEE Access"}]},"2024-03-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.01381v3","updated":"2024-03-10T22:31:45Z","published":"2023-10-02T17:42:22Z","title":"DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform\n  Generation","summary":"  Diffusion models have recently been shown to be relevant for high-quality\nspeech generation. Most work has been focused on generating spectrograms, and\nas such, they further require a subsequent model to convert the spectrogram to\na waveform (i.e., a vocoder). This work proposes a diffusion probabilistic\nend-to-end model for generating a raw speech waveform. The proposed model is\nautoregressive, generating overlapping frames sequentially, where each frame is\nconditioned on a portion of the previously generated one. Hence, our model can\neffectively synthesize an unlimited speech duration while preserving\nhigh-fidelity synthesis and temporal coherence. We implemented the proposed\nmodel for unconditional and conditional speech generation, where the latter can\nbe driven by an input sequence of phonemes, amplitudes, and pitch values.\nWorking on the waveform directly has some empirical advantages. Specifically,\nit allows the creation of local acoustic behaviors, like vocal fry, which makes\nthe overall waveform sounds more natural. Furthermore, the proposed diffusion\nmodel is stochastic and not deterministic; therefore, each inference generates\na slightly different waveform variation, enabling abundance of valid\nrealizations. Experiments show that the proposed model generates speech with\nsuperior quality compared with other state-of-the-art neural speech generation\nsystems.\n","authors":["Roi Benita","Michael Elad","Joseph Keshet"],"pdf_url":"https://arxiv.org/pdf/2310.01381v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06326v1","updated":"2024-03-10T22:14:54Z","published":"2024-03-10T22:14:54Z","title":"From Instructions to Constraints: Language Model Alignment with\n  Automatic Constraint Verification","summary":"  User alignment is crucial for adapting general-purpose language models (LMs)\nto downstream tasks, but human annotations are often not available for all\ntypes of instructions, especially those with customized constraints. We observe\nthat user instructions typically contain constraints. While assessing response\nquality in terms of the whole instruction is often costly, efficiently\nevaluating the satisfaction rate of constraints is feasible. We investigate\ncommon constraints in NLP tasks, categorize them into three classes based on\nthe types of their arguments, and propose a unified framework, ACT (Aligning to\nConsTraints), to automatically produce supervision signals for user alignment\nwith constraints. Specifically, ACT uses constraint verifiers, which are\ntypically easy to implement in practice, to compute constraint satisfaction\nrate (CSR) of each response. It samples multiple responses for each prompt and\ncollect preference labels based on their CSR automatically. Subsequently, ACT\nadapts the LM to the target task through a ranking-based learning process.\nExperiments on fine-grained entity typing, abstractive summarization, and\ntemporal question answering show that ACT is able to enhance LMs' capability to\nadhere to different classes of constraints, thereby improving task performance.\nFurther experiments show that the constraint-following capabilities are\ntransferable.\n","authors":["Fei Wang","Chao Shang","Sarthak Jain","Shuai Wang","Qiang Ning","Bonan Min","Vittorio Castelli","Yassine Benajiba","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2403.06326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06301v1","updated":"2024-03-10T20:20:16Z","published":"2024-03-10T20:20:16Z","title":"LIEDER: Linguistically-Informed Evaluation for Discourse Entity\n  Recognition","summary":"  Discourse Entity (DE) recognition is the task of identifying novel and known\nentities introduced within a text. While previous work has found that large\nlanguage models have basic, if imperfect, DE recognition abilities (Schuster\nand Linzen, 2022), it remains largely unassessed which of the fundamental\nsemantic properties that govern the introduction and subsequent reference to\nDEs they have knowledge of. We propose the Linguistically-Informed Evaluation\nfor Discourse Entity Recognition (LIEDER) dataset that allows for a detailed\nexamination of language models' knowledge of four crucial semantic properties:\nexistence, uniqueness, plurality, and novelty. We find evidence that\nstate-of-the-art large language models exhibit sensitivity to all of these\nproperties except novelty, which demonstrates that they have yet to reach\nhuman-level language understanding abilities.\n","authors":["Xiaomeng Zhu","Robert Frank"],"pdf_url":"https://arxiv.org/pdf/2403.06301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11998v4","updated":"2024-03-10T19:34:57Z","published":"2023-09-21T12:13:55Z","title":"LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset","summary":"  Studying how people interact with large language models (LLMs) in real-world\nscenarios is increasingly important due to their widespread use in various\napplications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset\ncontaining one million real-world conversations with 25 state-of-the-art LLMs.\nThis dataset is collected from 210K unique IP addresses in the wild on our\nVicuna demo and Chatbot Arena website. We offer an overview of the dataset's\ncontent, including its curation process, basic statistics, and topic\ndistribution, highlighting its diversity, originality, and scale. We\ndemonstrate its versatility through four use cases: developing content\nmoderation models that perform similarly to GPT-4, building a safety benchmark,\ntraining instruction-following models that perform similarly to Vicuna, and\ncreating challenging benchmark questions. We believe that this dataset will\nserve as a valuable resource for understanding and advancing LLM capabilities.\nThe dataset is publicly available at\nhttps://huggingface.co/datasets/lmsys/lmsys-chat-1m.\n","authors":["Lianmin Zheng","Wei-Lin Chiang","Ying Sheng","Tianle Li","Siyuan Zhuang","Zhanghao Wu","Yonghao Zhuang","Zhuohan Li","Zi Lin","Eric P. Xing","Joseph E. Gonzalez","Ion Stoica","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.11998v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06292v1","updated":"2024-03-10T19:31:13Z","published":"2024-03-10T19:31:13Z","title":"Transformer based Multitask Learning for Image Captioning and Object\n  Detection","summary":"  In several real-world scenarios like autonomous navigation and mobility, to\nobtain a better visual understanding of the surroundings, image captioning and\nobject detection play a crucial role. This work introduces a novel multitask\nlearning framework that combines image captioning and object detection into a\njoint model. We propose TICOD, Transformer-based Image Captioning and Object\ndetection model for jointly training both tasks by combining the losses\nobtained from image captioning and object detection networks. By leveraging\njoint training, the model benefits from the complementary information shared\nbetween the two tasks, leading to improved performance for image captioning.\nOur approach utilizes a transformer-based architecture that enables end-to-end\nnetwork integration for image captioning and object detection and performs both\ntasks jointly. We evaluate the effectiveness of our approach through\ncomprehensive experiments on the MS-COCO dataset. Our model outperforms the\nbaselines from image captioning literature by achieving a 3.65% improvement in\nBERTScore.\n","authors":["Debolena Basak","P. K. Srijith","Maunendra Sankar Desarkar"],"pdf_url":"https://arxiv.org/pdf/2403.06292v1.pdf","comment":"Accepted at PAKDD 2024"},{"id":"http://arxiv.org/abs/1808.09334v3","updated":"2024-03-10T18:35:27Z","published":"2018-08-28T14:47:33Z","title":"A Discriminative Latent-Variable Model for Bilingual Lexicon Induction","summary":"  We introduce a novel discriminative latent variable model for bilingual\nlexicon induction. Our model combines the bipartite matching dictionary prior\nof Haghighi et al. (2008) with a representation-based approach (Artetxe et al.,\n2017). To train the model, we derive an efficient Viterbi EM algorithm. We\nprovide empirical results on six language pairs under two metrics and show that\nthe prior improves the induced bilingual lexicons. We also demonstrate how\nprevious work may be viewed as a similarly fashioned latent-variable model,\nalbeit with a different prior.\n","authors":["Sebastian Ruder","Ryan Cotterell","Yova Kementchedjhieva","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/1808.09334v3.pdf","comment":"Proceedings of the 2018 Conference on Empirical Methods in Natural\n  Language Processing"},{"id":"http://arxiv.org/abs/2301.09209v4","updated":"2024-03-10T17:21:25Z","published":"2023-01-22T21:30:12Z","title":"Summarize the Past to Predict the Future: Natural Language Descriptions\n  of Context Boost Multimodal Object Interaction Anticipation","summary":"  We study object interaction anticipation in egocentric videos. This task\nrequires an understanding of the spatio-temporal context formed by past actions\non objects, coined action context. We propose TransFusion, a multimodal\ntransformer-based architecture. It exploits the representational power of\nlanguage by summarizing the action context. TransFusion leverages pre-trained\nimage captioning and vision-language models to extract the action context from\npast video frames. This action context together with the next video frame is\nprocessed by the multimodal fusion module to forecast the next object\ninteraction. Our model enables more efficient end-to-end learning. The large\npre-trained language models add common sense and a generalisation capability.\nExperiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our\nmultimodal fusion model. They also highlight the benefits of using\nlanguage-based context summaries in a task where vision seems to suffice. Our\nmethod outperforms state-of-the-art approaches by 40.4% in relative terms in\noverall mAP on the Ego4D test set. We validate the effectiveness of TransFusion\nvia experiments on EPIC-KITCHENS-100. Video and code are available at\nhttps://eth-ait.github.io/transfusion-proj/.\n","authors":["Razvan-George Pasca","Alexey Gavryushin","Muhammad Hamza","Yen-Ling Kuo","Kaichun Mo","Luc Van Gool","Otmar Hilliges","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2301.09209v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06265v1","updated":"2024-03-10T17:02:53Z","published":"2024-03-10T17:02:53Z","title":"Unpacking Tokenization: Evaluating Text Compression and its Correlation\n  with Model Performance","summary":"  Despite it being the cornerstone of BPE, the most common tokenization\nalgorithm, the importance of compression in the tokenization process is still\nunclear. In this paper, we argue for the theoretical importance of compression,\nthat can be viewed as 0-gram language modeling where equal probability is\nassigned to all tokens. We also demonstrate the empirical importance of\ncompression for downstream success of pre-trained language models. We control\nthe compression ability of several BPE tokenizers by varying the amount of\ndocuments available during their training: from 1 million documents to a\ncharacter-based tokenizer equivalent to no training data at all. We then\npre-train English language models based on those tokenizers and fine-tune them\nover several tasks. We show that there is a correlation between tokenizers'\ncompression and models' downstream performance, suggesting that compression is\na reliable intrinsic indicator of tokenization quality. These correlations are\nmore pronounced for generation tasks (over classification) or for smaller\nmodels (over large ones). We replicated a representative part of our\nexperiments on Turkish and found similar results, confirming that our results\nhold for languages with typological characteristics dissimilar to English. We\nconclude that building better compressing tokenizers is a fruitful avenue for\nfurther research and for improving overall model performance.\n","authors":["Omer Goldman","Avi Caciularu","Matan Eyal","Kris Cao","Idan Szpektor","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2403.06265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06260v1","updated":"2024-03-10T16:57:51Z","published":"2024-03-10T16:57:51Z","title":"SCORE: Self-supervised Correspondence Fine-tuning for Improved Content\n  Representations","summary":"  There is a growing interest in cost-effective self-supervised fine-tuning\n(SSFT) of self-supervised learning (SSL)-based speech models to obtain\ntask-specific representations. These task-specific representations are used for\nrobust performance on various downstream tasks by fine-tuning on the labelled\ndata. This work presents a cost-effective SSFT method named Self-supervised\nCorrespondence (SCORE) fine-tuning to adapt the SSL speech representations for\ncontent-related tasks. The proposed method uses a correspondence training\nstrategy, aiming to learn similar representations from perturbed speech and\noriginal speech. Commonly used data augmentation techniques for content-related\ntasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT\noutperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of\nfine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme\nrecognition, and query-by-example tasks, with relative improvements of 1.09%,\n3.58%, and 12.65%, respectively. SCORE provides competitive results with the\nrecently proposed SSFT method SPIN, using only 1/3 of the processed speech\ncompared to SPIN.\n","authors":["Amit Meghanani","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2403.06260v1.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.06259v1","updated":"2024-03-10T16:57:10Z","published":"2024-03-10T16:57:10Z","title":"Editing Conceptual Knowledge for Large Language Models","summary":"  Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.\n","authors":["Xiaohan Wang","Shengyu Mao","Ningyu Zhang","Shumin Deng","Yunzhi Yao","Yue Shen","Lei Liang","Jinjie Gu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.06259v1.pdf","comment":"Work in progress. Code: https://github.com/zjunlp/EasyEdit Dataset:\n  https://huggingface.co/datasets/zjunlp/ConceptEdit"},{"id":"http://arxiv.org/abs/2403.06249v1","updated":"2024-03-10T16:22:20Z","published":"2024-03-10T16:22:20Z","title":"No Language is an Island: Unifying Chinese and English in Financial\n  Large Language Models, Instruction Data, and Benchmarks","summary":"  While the progression of Large Language Models (LLMs) has notably propelled\nfinancial analysis, their application has largely been confined to singular\nlanguage realms, leaving untapped the potential of bilingual Chinese-English\ncapacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating\nthe ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.\nICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated\nand original English datasets, enriching the breadth and depth of bilingual\nfinancial modeling. It provides unrestricted access to diverse model variants,\na substantial compilation of diverse cross-lingual and multi-modal instruction\ndata, and an evaluation benchmark with expert annotations, comprising 10 NLP\ntasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough\nevaluation emphasizes the advantages of incorporating these bilingual datasets,\nespecially in translation tasks and utilizing original English data, enhancing\nboth linguistic flexibility and analytical acuity in financial contexts.\nNotably, ICE-INTENT distinguishes itself by showcasing significant enhancements\nover conventional LLMs and existing financial LLMs in bilingual milieus,\nunderscoring the profound impact of robust bilingual data on the accuracy and\nefficacy of financial NLP.\n","authors":["Gang Hu","Ke Qin","Chenhan Yuan","Min Peng","Alejandro Lopez-Lira","Benyou Wang","Sophia Ananiadou","Wanlong Yu","Jimin Huang","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2403.06249v1.pdf","comment":"23 pages, 5 figures, 11 tables, including Appendix"},{"id":"http://arxiv.org/abs/2402.13350v2","updated":"2024-03-10T16:13:37Z","published":"2024-02-20T19:53:36Z","title":"PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text\n  Retrieval Methods","summary":"  We present Polish Information Retrieval Benchmark (PIRB), a comprehensive\nevaluation framework encompassing 41 text information retrieval tasks for\nPolish. The benchmark incorporates existing datasets as well as 10 new,\npreviously unpublished datasets covering diverse topics such as medicine, law,\nbusiness, physics, and linguistics. We conduct an extensive evaluation of over\n20 dense and sparse retrieval models, including the baseline models trained by\nus as well as other available Polish and multilingual methods. Finally, we\nintroduce a three-step process for training highly effective language-specific\nretrievers, consisting of knowledge distillation, supervised fine-tuning, and\nbuilding sparse-dense hybrid retrievers using a lightweight rescoring model. In\norder to validate our approach, we train new text encoders for Polish and\ncompare their results with previously evaluated methods. Our dense models\noutperform the best solutions available to date, and the use of hybrid methods\nfurther improves their performance.\n","authors":["Sławomir Dadas","Michał Perełkiewicz","Rafał Poświata"],"pdf_url":"https://arxiv.org/pdf/2402.13350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16034v2","updated":"2024-03-10T15:58:56Z","published":"2024-02-25T09:17:22Z","title":"Emotion Classification in Short English Texts using Deep Learning\n  Techniques","summary":"  Detecting emotions in limited text datasets from under-resourced languages\npresents a formidable obstacle, demanding specialized frameworks and\ncomputational strategies. This study conducts a thorough examination of deep\nlearning techniques for discerning emotions in short English texts. Deep\nlearning approaches employ transfer learning and word embedding, notably BERT,\nto attain superior accuracy. To evaluate these methods, we introduce the\n\"SmallEnglishEmotions\" dataset, comprising 6372 varied short English texts\nannotated with five primary emotion categories. Our experiments reveal that\ntransfer learning and BERT-based text embedding outperform alternative methods\nin accurately categorizing the text in the dataset.\n","authors":["Siddhanth Bhat"],"pdf_url":"https://arxiv.org/pdf/2402.16034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17505v2","updated":"2024-03-10T14:33:49Z","published":"2024-01-30T23:46:35Z","title":"Arrows of Time for Large Language Models","summary":"  We study the probabilistic modeling performed by Autoregressive Large\nLanguage Models through the angle of time directionality. We empirically find a\ntime asymmetry exhibited by such models in their ability to model natural\nlanguage: a difference in the average log-perplexity when trying to predict the\nnext token versus when trying to predict the previous one. This difference is\nat the same time subtle and very consistent across various modalities\n(language, model size, training time, ...). Theoretically, this is surprising:\nfrom an information-theoretic point of view, there should be no such\ndifference. We provide a theoretical framework to explain how such an asymmetry\ncan appear from sparsity and computational complexity considerations, and\noutline a number of perspectives opened by our results.\n","authors":["Vassilis Papadopoulos","Jérémie Wenger","Clément Hongler"],"pdf_url":"https://arxiv.org/pdf/2401.17505v2.pdf","comment":"Updated 1 figure, minor corrections to text. 11 figures, 16 pages"},{"id":"http://arxiv.org/abs/2403.06221v1","updated":"2024-03-10T13:58:38Z","published":"2024-03-10T13:58:38Z","title":"TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned\n  Decision","summary":"  Numerous large language model (LLM) agents have been built for different\ntasks like web navigation and online shopping due to LLM's wide knowledge and\ntext-understanding ability. Among these works, many of them utilize in-context\nexamples to achieve generalization without the need for fine-tuning, while few\nof them have considered the problem of how to select and effectively utilize\nthese examples. Recently, methods based on trajectory-level retrieval with task\nmeta-data and using trajectories as in-context examples have been proposed to\nimprove the agent's overall performance in some sequential decision making\ntasks. However, these methods can be problematic due to plausible examples\nretrieved without task-specific state transition dynamics and long input with\nplenty of irrelevant context. In this paper, we propose a novel framework\n(TRAD) to address these issues. TRAD first conducts Thought Retrieval,\nachieving step-level demonstration selection via thought matching, leading to\nmore helpful demonstrations and less irrelevant input noise. Then, TRAD\nintroduces Aligned Decision, complementing retrieved demonstration steps with\ntheir previous or subsequent steps, which enables tolerance for imperfect\nthought and provides a choice for balance between more context and less noise.\nExtensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not\nonly outperforms state-of-the-art models but also effectively helps in reducing\nnoise and promoting generalization. Furthermore, TRAD has been deployed in\nreal-world scenarios of a global business insurance company and improves the\nsuccess rate of robotic process automation.\n","authors":["Ruiwen Zhou","Yingxuan Yang","Muning Wen","Ying Wen","Wenhao Wang","Chunling Xi","Guoqiang Xu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06221v1.pdf","comment":"Codes available at: https://github.com/skyriver-2000/TRAD-Official"},{"id":"http://arxiv.org/abs/2305.13860v2","updated":"2024-03-10T13:58:08Z","published":"2023-05-23T09:33:38Z","title":"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study","summary":"  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.\n","authors":["Yi Liu","Gelei Deng","Zhengzi Xu","Yuekang Li","Yaowen Zheng","Ying Zhang","Lida Zhao","Tianwei Zhang","Kailong Wang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2305.13860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11078v2","updated":"2024-03-10T13:53:45Z","published":"2024-02-16T21:10:33Z","title":"Model Editing by Pure Fine-Tuning","summary":"  Fine-tuning is dismissed as not effective for model editing due to its poor\nperformance compared to more specialized methods. However, fine-tuning is\nsimple, agnostic to the architectural details of the model being edited, and\nable to leverage ongoing advances in standard training methods (e.g., PEFT),\nmaking it an appealing choice for a model editor. In this work, we show that\npure fine-tuning can be a viable approach to model editing. We propose a slight\nmodification of naive fine-tuning with two key ingredients. First, we optimize\nthe conditional likelihood rather than the full likelihood. Second, we augment\nthe data with random paraphrases and facts to encourage generalization and\nlocality. Our experiments on ZsRE and CounterFact show that this simple\nmodification allows fine-tuning to often match or outperform specialized\neditors in the edit score.\n","authors":["Govind Gangadhar","Karl Stratos"],"pdf_url":"https://arxiv.org/pdf/2402.11078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04559v2","updated":"2024-03-10T13:48:43Z","published":"2024-02-07T03:37:19Z","title":"Can Large Language Model Agents Simulate Human Trust Behaviors?","summary":"  Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in applications such as social science.\nHowever, one fundamental question remains: can LLM agents really simulate human\nbehaviors? In this paper, we focus on one of the most critical behaviors in\nhuman interactions, trust, and aim to investigate whether or not LLM agents can\nsimulate human trust behaviors. We first find that LLM agents generally exhibit\ntrust behaviors, referred to as agent trust, under the framework of Trust\nGames, which are widely recognized in behavioral economics. Then, we discover\nthat LLM agents can have high behavioral alignment with humans regarding trust\nbehaviors, particularly for GPT-4, indicating the feasibility to simulate human\ntrust behaviors with LLM agents. In addition, we probe into the biases in agent\ntrust and the differences in agent trust towards agents and humans. We also\nexplore the intrinsic properties of agent trust under conditions including\nadvanced reasoning strategies and external manipulations. We further offer\nimportant implications of our discoveries for various scenarios where trust is\nparamount. Our study provides new insights into the behaviors of LLM agents and\nthe fundamental analogy between LLMs and humans.\n","authors":["Chengxing Xie","Canyu Chen","Feiran Jia","Ziyu Ye","Kai Shu","Adel Bibi","Ziniu Hu","Philip Torr","Bernard Ghanem","Guohao Li"],"pdf_url":"https://arxiv.org/pdf/2402.04559v2.pdf","comment":"The first two authors contributed equally. Project website:\n  https://www.camel-ai.org/research/agent-trust"},{"id":"http://arxiv.org/abs/2309.10444v4","updated":"2024-03-10T13:48:41Z","published":"2023-09-19T09:04:15Z","title":"Exploring Iterative Enhancement for Improving Learnersourced\n  Multiple-Choice Question Explanations with Large Language Models","summary":"  Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications.\n","authors":["Qiming Bao","Juho Leinonen","Alex Yuxuan Peng","Wanjun Zhong","Gaël Gendron","Timothy Pistotti","Alice Huang","Paul Denny","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2309.10444v4.pdf","comment":"The short version (v4) was accepted as a non-archival workshop paper\n  at AGI@ICLR 2024; the full version is under review"},{"id":"http://arxiv.org/abs/2403.06208v1","updated":"2024-03-10T13:04:54Z","published":"2024-03-10T13:04:54Z","title":"Personalized LoRA for Human-Centered Text Understanding","summary":"  Effectively and efficiently adapting a pre-trained language model (PLM) for\nhuman-centered text understanding (HCTU) is challenging since user tokens are\nmillion-level in most personalized applications and do not have concrete\nexplicit semantics. A standard and parameter-efficient approach (e.g., LoRA)\nnecessitates memorizing numerous suits of adapters for each user. In this work,\nwe introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework\nfor the HCTU task. PLoRA is effective, parameter-efficient, and dynamically\ndeploying in PLMs. Moreover, a personalized dropout and a mutual information\nmaximizing strategies are adopted and hence the proposed PLoRA can be well\nadapted to few/zero-shot learning scenarios for the cold-start issue.\nExperiments conducted on four benchmark datasets show that the proposed method\noutperforms existing methods in full/few/zero-shot learning scenarios for the\nHCTU task, even though it has fewer trainable parameters. For reproducibility,\nthe code for this paper is available at: https://github.com/yoyo-yun/PLoRA.\n","authors":["You Zhang","Jin Wang","Liang-Chih Yu","Dan Xu","Xuejie Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06208v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2403.06204v1","updated":"2024-03-10T13:02:27Z","published":"2024-03-10T13:02:27Z","title":"Identifying and interpreting non-aligned human conceptual\n  representations using language modeling","summary":"  The question of whether people's experience in the world shapes conceptual\nrepresentation and lexical semantics is longstanding. Word-association,\nfeature-listing and similarity rating tasks aim to address this question but\nrequire a subjective interpretation of the latent dimensions identified. In\nthis study, we introduce a supervised representational-alignment method that\n(i) determines whether two groups of individuals share the same basis of a\ncertain category, and (ii) explains in what respects they differ. In applying\nthis method, we show that congenital blindness induces conceptual\nreorganization in both a-modal and sensory-related verbal domains, and we\nidentify the associated semantic shifts. We first apply supervised\nfeature-pruning to a language model (GloVe) to optimize prediction accuracy of\nhuman similarity judgments from word embeddings. Pruning identifies one subset\nof retained GloVe features that optimizes prediction of judgments made by\nsighted individuals and another subset that optimizes judgments made by blind.\nA linear probing analysis then interprets the latent semantics of these\nfeature-subsets by learning a mapping from the retained GloVe features to 65\ninterpretable semantic dimensions. We applied this approach to seven semantic\ndomains, including verbs related to motion, sight, touch, and amodal verbs\nrelated to knowledge acquisition. We find that blind individuals more strongly\nassociate social and cognitive meanings to verbs related to motion or those\ncommunicating non-speech vocal utterances (e.g., whimper, moan). Conversely,\nfor amodal verbs, they demonstrate much sparser information. Finally, for some\nverbs, representations of blind and sighted are highly similar. The study\npresents a formal approach for studying interindividual differences in word\nmeaning, and the first demonstration of how blindness impacts conceptual\nrepresentation of everyday verbs.\n","authors":["Wanqian Bao","Uri Hasson"],"pdf_url":"https://arxiv.org/pdf/2403.06204v1.pdf","comment":"To appear at the ICLR 2024 Workshop on Representational Alignment\n  (Re-Align)"},{"id":"http://arxiv.org/abs/2403.06201v1","updated":"2024-03-10T12:50:35Z","published":"2024-03-10T12:50:35Z","title":"Are You Being Tracked? Discover the Power of Zero-Shot Trajectory\n  Tracing with LLMs!","summary":"  There is a burgeoning discussion around the capabilities of Large Language\nModels (LLMs) in acting as fundamental components that can be seamlessly\nincorporated into Artificial Intelligence of Things (AIoT) to interpret complex\ntrajectories. This study introduces LLMTrack, a model that illustrates how LLMs\ncan be leveraged for Zero-Shot Trajectory Recognition by employing a novel\nsingle-prompt technique that combines role-play and think step-by-step\nmethodologies with unprocessed Inertial Measurement Unit (IMU) data. We\nevaluate the model using real-world datasets designed to challenge it with\ndistinct trajectories characterized by indoor and outdoor scenarios. In both\ntest scenarios, LLMTrack not only meets but exceeds the performance benchmarks\nset by traditional machine learning approaches and even contemporary\nstate-of-the-art deep learning models, all without the requirement of training\non specialized datasets. The results of our research suggest that, with\nstrategically designed prompts, LLMs can tap into their extensive knowledge\nbase and are well-equipped to analyze raw sensor data with remarkable\neffectiveness.\n","authors":["Huanqi Yang","Sijie Ji","Rucheng Wu","Weitao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.06201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v1","updated":"2024-03-10T12:43:27Z","published":"2024-03-10T12:43:27Z","title":"A Comprehensive Overhaul of Multimodal Assistant with Small Language\n  Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18603v2","updated":"2024-03-10T11:17:58Z","published":"2024-02-28T08:29:42Z","title":"MMSR: Symbolic Regression is a Multimodal Task","summary":"  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n","authors":["Yanjie Li","Jingyi Liu","Weijun Li","Lina Yu","Min Wu","Wenqiang Li","Meilan Hao","Su Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2402.18603v2.pdf","comment":"12 page"},{"id":"http://arxiv.org/abs/2402.16278v3","updated":"2024-03-10T10:04:41Z","published":"2024-02-26T03:46:01Z","title":"A Self-matching Training Method with Annotation Embedding Models for\n  Ontology Subsumption Prediction","summary":"  Recently, ontology embeddings representing entities in a low-dimensional\nspace have been proposed for ontology completion. However, the ontology\nembeddings for concept subsumption prediction do not address the difficulties\nof similar and isolated entities and fail to extract the global information of\nannotation axioms from an ontology. In this paper, we propose a self-matching\ntraining method for the two ontology embedding models: Inverted-index Matrix\nEmbedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings\ncapture the global and local information in annotation axioms by means of the\noccurring locations of each word in a set of axioms and the co-occurrences of\nwords in each axiom. The self-matching training method increases the robustness\nof the concept subsumption prediction when predicted superclasses are similar\nto subclasses and are isolated to other entities in an ontology. Our evaluation\nexperiments show that the self-matching training method with InME outperforms\nthe existing ontology embeddings for the GO and FoodOn ontologies and that the\nmethod with the concatenation of CoME and OWL2Vec* outperforms them for the\nHeLiS ontology.\n","authors":["Yukihiro Shiraishi","Ken Kaneiwa"],"pdf_url":"https://arxiv.org/pdf/2402.16278v3.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.06149v1","updated":"2024-03-10T09:39:00Z","published":"2024-03-10T09:39:00Z","title":"Can Large Language Models Automatically Score Proficiency of Written\n  Essays?","summary":"  Although several methods were proposed to address the problem of automated\nessay scoring (AES) in the last 50 years, there is still much to desire in\nterms of effectiveness. Large Language Models (LLMs) are transformer-based\nmodels that demonstrate extraordinary capabilities on various tasks. In this\npaper, we test the ability of LLMs, given their powerful linguistic knowledge,\nto analyze and effectively score written essays. We experimented with two\npopular LLMs, namely ChatGPT and Llama. We aim to check if these models can do\nthis task and, if so, how their performance is positioned among the\nstate-of-the-art (SOTA) models across two levels, holistically and per\nindividual writing trait. We utilized prompt-engineering tactics in designing\nfour different prompts to bring their maximum potential to this task. Our\nexperiments conducted on the ASAP dataset revealed several interesting\nobservations. First, choosing the right prompt depends highly on the model and\nnature of the task. Second, the two LLMs exhibited comparable average\nperformance in AES, with a slight advantage for ChatGPT. Finally, despite the\nperformance gap between the two LLMs and SOTA models in terms of predictions,\nthey provide feedback to enhance the quality of the essays, which can\npotentially help both teachers and students.\n","authors":["Watheq Mansour","Salam Albatarni","Sohaila Eltanbouly","Tamer Elsayed"],"pdf_url":"https://arxiv.org/pdf/2403.06149v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.06139v1","updated":"2024-03-10T08:59:04Z","published":"2024-03-10T08:59:04Z","title":"Fine-grainedly Synthesize Streaming Data Based On Large Language Models\n  With Graph Structure Understanding For Data Sparsity","summary":"  Due to the sparsity of user data, sentiment analysis on user reviews in\ne-commerce platforms often suffers from poor performance, especially when faced\nwith extremely sparse user data or long-tail labels. Recently, the emergence of\nLLMs has introduced new solutions to such problems by leveraging graph\nstructures to generate supplementary user profiles. However, previous\napproaches have not fully utilized the graph understanding capabilities of LLMs\nand have struggled to adapt to complex streaming data environments. In this\nwork, we propose a fine-grained streaming data synthesis framework that\ncategorizes sparse users into three categories: Mid-tail, Long-tail, and\nExtreme. Specifically, we design LLMs to comprehensively understand three key\ngraph elements in streaming data, including Local-global Graph Understanding,\nSecond-Order Relationship Extraction, and Product Attribute Understanding,\nwhich enables the generation of high-quality synthetic data to effectively\naddress sparsity across different categories. Experimental results on three\nreal datasets demonstrate significant performance improvements, with\nsynthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%,\nrespectively.\n","authors":["Xin Zhang","Linhai Zhang","Deyu Zhou","Guoqiang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.06139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06115v1","updated":"2024-03-10T07:21:31Z","published":"2024-03-10T07:21:31Z","title":"FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained\n  Monetary Policy Analysis Framework on Their Language","summary":"  The effectiveness of central bank communication is a crucial aspect of\nmonetary policy transmission. While recent research has examined the influence\nof policy communication by the chairs of the Federal Reserve on various\nfinancial variables, much of the literature relies on rule-based or\ndictionary-based methods in parsing the language of the chairs, leaving nuanced\ninformation about policy stance contained in nonverbal emotion out of the\nanalysis. In the current study, we propose the Fine-Grained Monetary Policy\nAnalysis Framework (FMPAF), a novel approach that integrates large language\nmodels (LLMs) with regression analysis to provide a comprehensive analysis of\nthe impact of the press-conference communications of chairs of the Federal\nReserve on financial markets. We conduct extensive comparisons of model\nperformance under different levels of granularity, modalities, and\ncommunication scenarios. Based on our preferred specification, a one-unit\nincrease in the sentiment score is associated with an increase of the price of\nS\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a\n15-basis-point decrease in the policy interest rate, while not leading to a\nsignificant response in exchange rates.\n","authors":["Yayue Deng","Mohan Xu","Yao Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06115v1.pdf","comment":"accepted by AAAI 2024 Workshop: AI in Finance for Social Impact"},{"id":"http://arxiv.org/abs/2403.06108v1","updated":"2024-03-10T06:30:54Z","published":"2024-03-10T06:30:54Z","title":"Large Language Models on Fine-grained Emotion Detection Dataset with\n  Data Augmentation and Transfer Learning","summary":"  This paper delves into enhancing the classification performance on the\nGoEmotions dataset, a large, manually annotated dataset for emotion detection\nin text. The primary goal of this paper is to address the challenges of\ndetecting subtle emotions in text, a complex issue in Natural Language\nProcessing (NLP) with significant practical applications. The findings offer\nvaluable insights into addressing the challenges of emotion detection in text\nand suggest directions for future research, including the potential for a\nsurvey paper that synthesizes methods and performances across various datasets\nin this domain.\n","authors":["Kaipeng Wang","Zhi Jing","Yongye Su","Yikun Han"],"pdf_url":"https://arxiv.org/pdf/2403.06108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16906v3","updated":"2024-03-10T06:16:01Z","published":"2024-02-25T00:56:27Z","title":"LDB: A Large Language Model Debugger via Verifying Runtime Execution\n  Step-by-step","summary":"  Large language models (LLMs) are leading significant progress in code\ngeneration. Beyond one-pass code generation, recent works further integrate\nunit tests and program verifiers into LLMs to iteratively refine the generated\nprograms. However, these works consider the generated programs as an\nindivisible entity, which falls short for LLMs in debugging the programs,\nespecially when the programs contain complex logic flows and data operations.\nIn contrast, when human developers debug programs, they typically set\nbreakpoints and selectively examine runtime execution information. The\nexecution flow and the intermediate variables play a crucial role in the\ndebugging process, yet they are underutilized in the existing literature on\ncode generation. In this study, we introduce Large Language Model Debugger\n(LDB), a novel debugging framework that enables LLMs to refine their generated\nprograms with the runtime execution information. Specifically, LDB segments the\nprograms into basic blocks and tracks the values of intermediate variables\nafter each block throughout the runtime execution. This allows LLMs to\nconcentrate on simpler code units within the overall execution flow, verify\ntheir correctness against the task description block by block, and efficiently\npinpoint any potential errors. Experiments demonstrate that LDB consistently\nenhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and\nTransCoder benchmarks, archiving new state-of-the-art performance in code\ndebugging for various LLM selections.\n","authors":["Lily Zhong","Zilong Wang","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2402.16906v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.06100v1","updated":"2024-03-10T05:55:00Z","published":"2024-03-10T05:55:00Z","title":"Automatic design optimization of preference-based subjective evaluation\n  with online learning in crowdsourcing environment","summary":"  A preference-based subjective evaluation is a key method for evaluating\ngenerative media reliably. However, its huge combinations of pairs prohibit it\nfrom being applied to large-scale evaluation using crowdsourcing. To address\nthis issue, we propose an automatic optimization method for preference-based\nsubjective evaluation in terms of pair combination selections and allocation of\nevaluation volumes with online learning in a crowdsourcing environment. We use\na preference-based online learning method based on a sorting algorithm to\nidentify the total order of evaluation targets with minimum sample volumes. Our\nonline learning algorithm supports parallel and asynchronous execution under\nfixed-budget conditions required for crowdsourcing. Our experiment on\npreference-based subjective evaluation of synthetic speech shows that our\nmethod successfully optimizes the test by reducing pair combinations from 351\nto 83 and allocating optimal evaluation volumes for each pair ranging from 30\nto 663 without compromising evaluation accuracies and wasting budget\nallocations.\n","authors":["Yusuke Yasuda","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2403.06100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06098v1","updated":"2024-03-10T05:40:12Z","published":"2024-03-10T05:40:12Z","title":"VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video\n  Diffusion Models","summary":"  The arrival of Sora marks a new era for text-to-video diffusion models,\nbringing significant advancements in video generation and potential\napplications. However, Sora, as well as other text-to-video diffusion models,\nhighly relies on the prompts, and there is no publicly available dataset\nfeaturing a study of text-to-video prompts. In this paper, we introduce\nVidProM, the first large-scale dataset comprising 1.67 million unique\ntext-to-video prompts from real users. Additionally, the dataset includes 6.69\nmillion videos generated by four state-of-the-art diffusion models and some\nrelated data. We initially demonstrate the curation of this large-scale\ndataset, which is a time-consuming and costly process. Subsequently, we show\nhow the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery\ndataset for image generation. Based on the analysis of these prompts, we\nidentify the necessity for a new prompt dataset specifically designed for\ntext-to-video generation and gain insights into the preferences of real users\nwhen creating videos. Our large-scale and diverse dataset also inspires many\nexciting new research areas. For instance, to develop better, more efficient,\nand safer text-to-video diffusion models, we suggest exploring text-to-video\nprompt engineering, efficient video generation, and video copy detection for\ndiffusion models. We make the collected dataset VidProM publicly available at\nGitHub and Hugging Face under the CC-BY- NC 4.0 License.\n","authors":["Wenhao Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.06098v1.pdf","comment":"Please download the collected dataset from\n  https://github.com/WangWenhao0716/VidProM and\n  https://huggingface.co/datasets/WenhaoWang/VidProM"},{"id":"http://arxiv.org/abs/2402.11895v3","updated":"2024-03-10T05:38:20Z","published":"2024-02-19T07:21:09Z","title":"Bridging or Breaking: Impact of Intergroup Interactions on Religious\n  Polarization","summary":"  While exposure to diverse viewpoints may reduce polarization, it can also\nhave a backfire effect and exacerbate polarization when the discussion is\nadversarial. Here, we examine the question whether intergroup interactions\naround important events affect polarization between majority and minority\ngroups in social networks. We compile data on the religious identity of nearly\n700,000 Indian Twitter users engaging in COVID-19-related discourse during\n2020. We introduce a new measure for an individual's group conformity based on\ncontextualized embeddings of tweet text, which helps us assess polarization\nbetween religious groups. We then use a meta-learning framework to examine\nheterogeneous treatment effects of intergroup interactions on an individual's\ngroup conformity in the light of communal, political, and socio-economic\nevents. We find that for political and social events, intergroup interactions\nreduce polarization. This decline is weaker for individuals at the extreme who\nalready exhibit high conformity to their group. In contrast, during communal\nevents, intergroup interactions can increase group conformity. Finally, we\ndecompose the differential effects across religious groups in terms of emotions\nand topics of discussion. The results show that the dynamics of religious\npolarization are sensitive to the context and have important implications for\nunderstanding the role of intergroup interactions.\n","authors":["Rochana Chaturvedi","Sugat Chaturvedi","Elena Zheleva"],"pdf_url":"https://arxiv.org/pdf/2402.11895v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06097v1","updated":"2024-03-10T05:12:16Z","published":"2024-03-10T05:12:16Z","title":"Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese\n  Address Entity Recognition Dataset for UAV Delivery","summary":"  We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.\n","authors":["Yuxuan Yao","Sichun Luo","Haohan Zhao","Guanzhi Deng","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2403.06097v1.pdf","comment":"Accepted by TheWebConf'24 (WWW'24) as a Resource Paper"},{"id":"http://arxiv.org/abs/2310.19181v2","updated":"2024-03-10T04:12:27Z","published":"2023-10-29T22:52:40Z","title":"From Chatbots to PhishBots? -- Preventing Phishing scams created using\n  ChatGPT, Google Bard and Claude","summary":"  The advanced capabilities of Large Language Models (LLMs) have made them\ninvaluable across various applications, from conversational agents and content\ncreation to data analysis, research, and innovation. However, their\neffectiveness and accessibility also render them susceptible to abuse for\ngenerating malicious content, including phishing attacks. This study explores\nthe potential of using four popular commercially available LLMs, i.e., ChatGPT\n(GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishing\nattacks using a series of malicious prompts. We discover that these LLMs can\ngenerate both phishing websites and emails that can convincingly imitate\nwell-known brands and also deploy a range of evasive tactics that are used to\nelude detection mechanisms employed by anti-phishing systems. These attacks can\nbe generated using unmodified or \"vanilla\" versions of these LLMs without\nrequiring any prior adversarial exploits such as jailbreaking. We evaluate the\nperformance of the LLMs towards generating these attacks and find that they can\nalso be utilized to create malicious prompts that, in turn, can be fed back to\nthe model to generate phishing scams - thus massively reducing the\nprompt-engineering effort required by attackers to scale these threats. As a\ncountermeasure, we build a BERT-based automated detection tool that can be used\nfor the early detection of malicious prompts to prevent LLMs from generating\nphishing content. Our model is transferable across all four commercial LLMs,\nattaining an average accuracy of 96% for phishing website prompts and 94% for\nphishing email prompts. We also disclose the vulnerabilities to the concerned\nLLMs, with Google acknowledging it as a severe issue. Our detection model is\navailable for use at Hugging Face, as well as a ChatGPT Actions plugin.\n","authors":["Sayak Saha Roy","Poojitha Thota","Krishna Vamsi Naragam","Shirin Nilizadeh"],"pdf_url":"https://arxiv.org/pdf/2310.19181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06082v1","updated":"2024-03-10T04:01:49Z","published":"2024-03-10T04:01:49Z","title":"FrameQuant: Flexible Low-Bit Quantization for Transformers","summary":"  Transformers are the backbone of powerful foundation models for many Vision\nand Natural Language Processing tasks. But their compute and memory/storage\nfootprint is large, and so, serving such models is expensive often requiring\nhigh-end hardware. To mitigate this difficulty, Post-Training Quantization\nseeks to modify a pre-trained model and quantize it to eight bits or lower,\nsignificantly boosting compute/memory/latency efficiency. Such models have been\nsuccessfully quantized to four bits with some performance loss. In this work,\nwe outline a simple scheme to quantize Transformer-based models to just two\nbits (plus some overhead) with only a small drop in accuracy. Key to our\nformulation is a concept borrowed from Harmonic analysis called Fusion Frames.\nOur main finding is that the quantization must take place not in the original\nweight space, but instead in the Fusion Frame representations. If quantization\nis interpreted as the addition of noise, our casting of the problem allows\ninvoking an extensive body of known consistent recovery and noise robustness\nguarantees. Further, if desired, de-noising filters are known in closed form.\nWe show empirically, via a variety of experiments, that (almost) two-bit\nquantization for Transformer models promises sizable efficiency gains.\n","authors":["Harshavardhan Adepu","Zhanpeng Zeng","Li Zhang","Vikas Singh"],"pdf_url":"https://arxiv.org/pdf/2403.06082v1.pdf","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.06064v1","updated":"2024-03-10T02:16:13Z","published":"2024-03-10T02:16:13Z","title":"L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node\n  Classification","summary":"  Linear Graph Convolutional Networks (GCNs) are used to classify the node in\nthe graph data. However, we note that most existing linear GCN models perform\nneural network operations in Euclidean space, which do not explicitly capture\nthe tree-like hierarchical structure exhibited in real-world datasets that\nmodeled as graphs. In this paper, we attempt to introduce hyperbolic space into\nlinear GCN and propose a novel framework for Lorentzian linear GCN.\nSpecifically, we map the learned features of graph nodes into hyperbolic space,\nand then perform a Lorentzian linear feature transformation to capture the\nunderlying tree-like structure of data. Experimental results on standard\ncitation networks datasets with semi-supervised learning show that our approach\nyields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and\n81.3$\\%$ on PubMed datasets. Furthermore, we observe that our approach can be\ntrained up to two orders of magnitude faster than other nonlinear GCN models on\nPubMed dataset. Our code is publicly available at\nhttps://github.com/llqy123/LLGC-master.\n","authors":["Qiuyu Liang","Weihua Wang","Feilong Bao","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2403.06064v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.06063v1","updated":"2024-03-10T02:14:24Z","published":"2024-03-10T02:14:24Z","title":"Target-constrained Bidirectional Planning for Generation of\n  Target-oriented Proactive Dialogue","summary":"  Target-oriented proactive dialogue systems aim to lead conversations from a\ndialogue context toward a pre-determined target, such as making recommendations\non designated items or introducing new specific topics. To this end, it is\ncritical for such dialogue systems to plan reasonable actions to drive the\nconversation proactively, and meanwhile, to plan appropriate topics to move the\nconversation forward to the target topic smoothly. In this work, we mainly\nfocus on effective dialogue planning for target-oriented dialogue generation.\nInspired by decision-making theories in cognitive science, we propose a novel\ntarget-constrained bidirectional planning (TRIP) approach, which plans an\nappropriate dialogue path by looking ahead and looking back. By formulating the\nplanning as a generation task, our TRIP bidirectionally generates a dialogue\npath consisting of a sequence of <action, topic> pairs using two Transformer\ndecoders. They are expected to supervise each other and converge on consistent\nactions and topics by minimizing the decision gap and contrastive generation of\ntargets. Moreover, we propose a target-constrained decoding algorithm with a\nbidirectional agreement to better control the planning process. Subsequently,\nwe adopt the planned dialogue paths to guide dialogue generation in a pipeline\nmanner, where we explore two variants: prompt-based generation and\nplan-controlled generation. Extensive experiments are conducted on two\nchallenging dialogue datasets, which are re-purposed for exploring\ntarget-oriented dialogue. Our automatic and human evaluations demonstrate that\nthe proposed methods significantly outperform various baseline models.\n","authors":["Jian Wang","Dongding Lin","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2403.06063v1.pdf","comment":"Accepted by ACM Transactions on Information Systems (TOIS)"},{"id":"http://arxiv.org/abs/2403.06060v1","updated":"2024-03-10T01:39:10Z","published":"2024-03-10T01:39:10Z","title":"Ensemble Language Models for Multilingual Sentiment Analysis","summary":"  The rapid advancement of social media enables us to analyze user opinions. In\nrecent times, sentiment analysis has shown a prominent research gap in\nunderstanding human sentiment based on the content shared on social media.\nAlthough sentiment analysis for commonly spoken languages has advanced\nsignificantly, low-resource languages like Arabic continue to get little\nresearch due to resource limitations. In this study, we explore sentiment\nanalysis on tweet texts from SemEval-17 and the Arabic Sentiment Tweet dataset.\nMoreover, We investigated four pretrained language models and proposed two\nensemble language models. Our findings include monolingual models exhibiting\nsuperior performance and ensemble models outperforming the baseline while the\nmajority voting ensemble outperforms the English language.\n","authors":["Md Arid Hasan"],"pdf_url":"https://arxiv.org/pdf/2403.06060v1.pdf","comment":"This is one of my graduate course project reports and currently, I'm\n  not planning to submit to any conferences"},{"id":"http://arxiv.org/abs/2310.03976v3","updated":"2024-03-10T01:19:41Z","published":"2023-10-06T02:19:10Z","title":"From Text to Self: Users' Perceptions of Potential of AI on\n  Interpersonal Communication and Self","summary":"  In the rapidly evolving landscape of AI-mediated communication (AIMC), tools\npowered by Large Language Models (LLMs) are becoming integral to interpersonal\ncommunication. Employing a mixed-methods approach, we conducted a one-week\ndiary and interview study to explore users' perceptions of these tools' ability\nto: 1) support interpersonal communication in the short-term, and 2) lead to\npotential long-term effects. Our findings indicate that participants view AIMC\nsupport favorably, citing benefits such as increased communication confidence,\nand finding precise language to express their thoughts, navigating linguistic\nand cultural barriers. However, the study also uncovers current limitations of\nAIMC tools, including verbosity, unnatural responses, and excessive emotional\nintensity. These shortcomings are further exacerbated by user concerns about\ninauthenticity and potential overreliance on the technology. Furthermore, we\nidentified four key communication spaces delineated by communication stakes\n(high or low) and relationship dynamics (formal or informal) that\ndifferentially predict users' attitudes toward AIMC tools. Specifically,\nparticipants found the tool is more suitable for communicating in formal\nrelationships than informal ones and more beneficial in high-stakes than\nlow-stakes communication.\n","authors":["Yue Fu","Sami Foell","Xuhai Xu","Alexis Hiniker"],"pdf_url":"https://arxiv.org/pdf/2310.03976v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13729v2","updated":"2024-03-10T01:11:20Z","published":"2023-11-22T22:52:00Z","title":"Comparison of pipeline, sequence-to-sequence, and GPT models for\n  end-to-end relation extraction: experiments with the rare disease use-case","summary":"  End-to-end relation extraction (E2ERE) is an important and realistic\napplication of natural language processing (NLP) in biomedicine. In this paper,\nwe aim to compare three prevailing paradigms for E2ERE using a complex dataset\nfocused on rare diseases involving discontinuous and nested entities. We use\nthe RareDis information extraction dataset to evaluate three competing\napproaches (for E2ERE): NER $\\rightarrow$ RE pipelines, joint sequence to\nsequence models, and generative pre-trained transformer (GPT) models. We use\ncomparable state-of-the-art models and best practices for each of these\napproaches and conduct error analyses to assess their failure modes. Our\nfindings reveal that pipeline models are still the best, while\nsequence-to-sequence models are not far behind; GPT models with eight times as\nmany parameters are worse than even sequence-to-sequence models and lose to\npipeline models by over 10 F1 points. Partial matches and discontinuous\nentities caused many NER errors contributing to lower overall E2E performances.\nWe also verify these findings on a second E2ERE dataset for chemical-protein\ninteractions. Although generative LM-based methods are more suitable for\nzero-shot settings, when training data is available, our results show that it\nis better to work with more conventional models trained and tailored for E2ERE.\nMore innovative methods are needed to marry the best of the both worlds from\nsmaller encoder-decoder pipeline models and the larger GPT models to improve\nE2ERE. As of now, we see that well designed pipeline models offer substantial\nperformance gains at a lower cost and carbon footprint for E2ERE. Our\ncontribution is also the first to conduct E2ERE for the RareDis dataset.\n","authors":["Shashank Gupta","Xuguang Ai","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2311.13729v2.pdf","comment":"In V2 we added new experiments with T5 models. The dataset and code\n  for all our experiments are publicly available:\n  https://github.com/shashank140195/Raredis"},{"id":"http://arxiv.org/abs/2305.14902v2","updated":"2024-03-10T01:04:48Z","published":"2023-05-24T08:55:11Z","title":"M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box\n  Machine-Generated Text Detection","summary":"  Large language models (LLMs) have demonstrated remarkable capability to\ngenerate fluent responses to a wide variety of user queries. However, this has\nalso raised concerns about the potential misuse of such texts in journalism,\neducation, and academia. In this study, we strive to create automated systems\nthat can detect machine-generated texts and pinpoint potential misuse. We first\nintroduce a large-scale benchmark \\textbf{M4}, which is a multi-generator,\nmulti-domain, and multi-lingual corpus for machine-generated text detection.\nThrough an extensive empirical study of this dataset, we show that it is\nchallenging for detectors to generalize well on instances from unseen domains\nor LLMs. In such cases, detectors tend to misclassify machine-generated text as\nhuman-written. These results show that the problem is far from solved and that\nthere is a lot of room for improvement. We believe that our dataset will enable\nfuture research towards more robust approaches to this pressing societal\nproblem. The dataset is available at https://github.com/mbzuai-nlp/M4.\n","authors":["Yuxia Wang","Jonibek Mansurov","Petar Ivanov","Jinyan Su","Artem Shelmanov","Akim Tsvigun","Chenxi Whitehouse","Osama Mohammed Afzal","Tarek Mahmoud","Toru Sasaki","Thomas Arnold","Alham Fikri Aji","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2305.14902v2.pdf","comment":"41 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.06259v1","updated":"2024-03-10T16:57:10Z","published":"2024-03-10T16:57:10Z","title":"Editing Conceptual Knowledge for Large Language Models","summary":"  Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.\n","authors":["Xiaohan Wang","Shengyu Mao","Ningyu Zhang","Shumin Deng","Yunzhi Yao","Yue Shen","Lei Liang","Jinjie Gu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.06259v1.pdf","comment":"Work in progress. Code: https://github.com/zjunlp/EasyEdit Dataset:\n  https://huggingface.co/datasets/zjunlp/ConceptEdit"},{"id":"http://arxiv.org/abs/2403.06221v1","updated":"2024-03-10T13:58:38Z","published":"2024-03-10T13:58:38Z","title":"TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned\n  Decision","summary":"  Numerous large language model (LLM) agents have been built for different\ntasks like web navigation and online shopping due to LLM's wide knowledge and\ntext-understanding ability. Among these works, many of them utilize in-context\nexamples to achieve generalization without the need for fine-tuning, while few\nof them have considered the problem of how to select and effectively utilize\nthese examples. Recently, methods based on trajectory-level retrieval with task\nmeta-data and using trajectories as in-context examples have been proposed to\nimprove the agent's overall performance in some sequential decision making\ntasks. However, these methods can be problematic due to plausible examples\nretrieved without task-specific state transition dynamics and long input with\nplenty of irrelevant context. In this paper, we propose a novel framework\n(TRAD) to address these issues. TRAD first conducts Thought Retrieval,\nachieving step-level demonstration selection via thought matching, leading to\nmore helpful demonstrations and less irrelevant input noise. Then, TRAD\nintroduces Aligned Decision, complementing retrieved demonstration steps with\ntheir previous or subsequent steps, which enables tolerance for imperfect\nthought and provides a choice for balance between more context and less noise.\nExtensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not\nonly outperforms state-of-the-art models but also effectively helps in reducing\nnoise and promoting generalization. Furthermore, TRAD has been deployed in\nreal-world scenarios of a global business insurance company and improves the\nsuccess rate of robotic process automation.\n","authors":["Ruiwen Zhou","Yingxuan Yang","Muning Wen","Ying Wen","Wenhao Wang","Chunling Xi","Guoqiang Xu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06221v1.pdf","comment":"Codes available at: https://github.com/skyriver-2000/TRAD-Official"},{"id":"http://arxiv.org/abs/2402.17188v3","updated":"2024-03-10T10:51:36Z","published":"2024-02-27T03:58:39Z","title":"PromptMM: Multi-Modal Knowledge Distillation for Recommendation with\n  Prompt-Tuning","summary":"  Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited\nfrom the incorporation of multimedia (e.g., visual, textual, and acoustic)\ncontent into their personal recommender systems. These modalities provide\nintuitive semantics that facilitate modality-aware user preference modeling.\nHowever, two key challenges in multi-modal recommenders remain unresolved: i)\nThe introduction of multi-modal encoders with a large number of additional\nparameters causes overfitting, given high-dimensional multi-modal features\nprovided by extractors (e.g., ViT, BERT). ii) Side information inevitably\nintroduces inaccuracies and redundancies, which skew the modality-interaction\ndependency from reflecting true user preference. To tackle these problems, we\npropose to simplify and empower recommenders through Multi-modal Knowledge\nDistillation (PromptMM) with the prompt-tuning that enables adaptive quality\ndistillation. Specifically, PromptMM conducts model compression through\ndistilling u-i edge relationship and multi-modal node content from cumbersome\nteachers to relieve students from the additional feature reduction parameters.\nTo bridge the semantic gap between multi-modal context and collaborative\nsignals for empowering the overfitting teacher, soft prompt-tuning is\nintroduced to perform student task-adaptive. Additionally, to adjust the impact\nof inaccuracies in multimedia data, a disentangled multi-modal list-wise\ndistillation is developed with modality-aware re-weighting mechanism.\nExperiments on real-world data demonstrate PromptMM's superiority over existing\ntechniques. Ablation tests confirm the effectiveness of key components.\nAdditional tests show the efficiency and effectiveness.\n","authors":["Wei Wei","Jiabin Tang","Yangqin Jiang","Lianghao Xia","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.17188v3.pdf","comment":"WWW 2024"},{"id":"http://arxiv.org/abs/2310.03813v3","updated":"2024-03-10T05:13:41Z","published":"2023-10-05T18:02:03Z","title":"Cold-start Bundle Recommendation via Popularity-based Coalescence and\n  Curriculum Heating","summary":"  How can we recommend cold-start bundles to users? The cold-start problem in\nbundle recommendation is crucial because new bundles are continuously created\non the Web for various marketing purposes. Despite its importance, existing\nmethods for cold-start item recommendation are not readily applicable to\nbundles. They depend overly on historical information, even for less popular\nbundles, failing to address the primary challenge of the highly skewed\ndistribution of bundle interactions. In this work, we propose CoHeat\n(Popularity-based Coalescence and Curriculum Heating), an accurate approach for\ncold-start bundle recommendation. CoHeat first represents users and bundles\nthrough graph-based views, capturing collaborative information effectively. To\nestimate the user-bundle relationship more accurately, CoHeat addresses the\nhighly skewed distribution of bundle interactions through a popularity-based\ncoalescence approach, which incorporates historical and affiliation information\nbased on the bundle's popularity. Furthermore, it effectively learns latent\nrepresentations by exploiting curriculum learning and contrastive learning.\nCoHeat demonstrates superior performance in cold-start bundle recommendation,\nachieving up to 193% higher nDCG@20 compared to the best competitor.\n","authors":["Hyunsik Jeon","Jong-eun Lee","Jeongin Yun","U Kang"],"pdf_url":"https://arxiv.org/pdf/2310.03813v3.pdf","comment":"8 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.06097v1","updated":"2024-03-10T05:12:16Z","published":"2024-03-10T05:12:16Z","title":"Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese\n  Address Entity Recognition Dataset for UAV Delivery","summary":"  We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.\n","authors":["Yuxuan Yao","Sichun Luo","Haohan Zhao","Guanzhi Deng","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2403.06097v1.pdf","comment":"Accepted by TheWebConf'24 (WWW'24) as a Resource Paper"},{"id":"http://arxiv.org/abs/2403.06071v1","updated":"2024-03-10T03:33:59Z","published":"2024-03-10T03:33:59Z","title":"Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised\n  Semantic Hashing","summary":"  Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.\n","authors":["Liyang He","Zhenya Huang","Jiayu Liu","Enhong Chen","Fei Wang","Jing Sha","Shijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06071v1.pdf","comment":"12 pages, 19 figures, Proceedings of the ACM Web Conference 2024 (WWW\n  '24)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.06324v1","updated":"2024-03-10T21:53:42Z","published":"2024-03-10T21:53:42Z","title":"ACM MMSys 2024 Bandwidth Estimation in Real Time Communications\n  Challenge","summary":"  The quality of experience (QoE) delivered by video conferencing systems to\nend users depends in part on correctly estimating the capacity of the\nbottleneck link between the sender and the receiver over time. Bandwidth\nestimation for real-time communications (RTC) remains a significant challenge,\nprimarily due to the continuously evolving heterogeneous network architectures\nand technologies. From the first bandwidth estimation challenge which was\nhosted at ACM MMSys 2021, we learnt that bandwidth estimation models trained\nwith reinforcement learning (RL) in simulations to maximize network-based\nreward functions may not be optimal in reality due to the sim-to-real gap and\nthe difficulty of aligning network-based rewards with user-perceived QoE. This\ngrand challenge aims to advance bandwidth estimation model design by aligning\nreward maximization with user-perceived QoE optimization using offline RL and a\nreal-world dataset with objective rewards which have high correlations with\nsubjective user-perceived audio/video quality in Microsoft Teams. All models\nsubmitted to the grand challenge underwent initial evaluation on our emulation\nplatform. For a comprehensive evaluation under diverse network conditions with\ntemporal fluctuations, top models were further evaluated on our geographically\ndistributed testbed by using each model to conduct 600 calls within a 12-day\nperiod. The winning model is shown to deliver comparable performance to the top\nbehavior policy in the released dataset. By leveraging real-world data and\nintegrating objective audio/video quality scores as rewards, offline RL can\ntherefore facilitate the development of competitive bandwidth estimators for\nRTC.\n","authors":["Sami Khairy","Gabriel Mittag","Scott Inglis","Vishak Gopal","Mehrsa Golestaneh","Ross Cutler","Francis Yan","Zhixiong Niu"],"pdf_url":"https://arxiv.org/pdf/2403.06324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01596v4","updated":"2024-03-10T21:41:51Z","published":"2023-10-02T19:41:42Z","title":"ImagenHub: Standardizing the evaluation of conditional image generation\n  models","summary":"  Recently, a myriad of conditional image generation and editing models have\nbeen developed to serve different downstream tasks, including text-to-image\ngeneration, text-guided image editing, subject-driven image generation,\ncontrol-guided image generation, etc. However, we observe huge inconsistencies\nin experimental conditions: datasets, inference, and evaluation metrics -\nrender fair comparisons difficult. This paper proposes ImagenHub, which is a\none-stop library to standardize the inference and evaluation of all the\nconditional image generation models. Firstly, we define seven prominent tasks\nand curate high-quality evaluation datasets for them. Secondly, we built a\nunified inference pipeline to ensure fair comparison. Thirdly, we design two\nhuman evaluation scores, i.e. Semantic Consistency and Perceptual Quality,\nalong with comprehensive guidelines to evaluate generated images. We train\nexpert raters to evaluate the model outputs based on the proposed metrics. Our\nhuman evaluation achieves a high inter-worker agreement of Krippendorff's alpha\non 76% models with a value higher than 0.4. We comprehensively evaluated a\ntotal of around 30 models and observed three key takeaways: (1) the existing\nmodels' performance is generally unsatisfying except for Text-guided Image\nGeneration and Subject-driven Image Generation, with 74% models achieving an\noverall score lower than 0.5. (2) we examined the claims from published papers\nand found 83% of them hold with a few exceptions. (3) None of the existing\nautomatic metrics has a Spearman's correlation higher than 0.2 except\nsubject-driven image generation. Moving forward, we will continue our efforts\nto evaluate newly published models and update our leaderboard to keep track of\nthe progress in conditional image generation.\n","authors":["Max Ku","Tianle Li","Kai Zhang","Yujie Lu","Xingyu Fu","Wenwen Zhuang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.01596v4.pdf","comment":"Accepted to ICLR2024 Camera Ready"},{"id":"http://arxiv.org/abs/2312.06197v2","updated":"2024-03-10T15:22:04Z","published":"2023-12-11T08:17:58Z","title":"MART: Learning Hierarchical Music Audio Representations with Part-Whole\n  Transformer","summary":"  Recent research in self-supervised contrastive learning of music\nrepresentations has demonstrated remarkable results across diverse downstream\ntasks. However, a prevailing trend in existing methods involves representing\nequally-sized music clips in either waveform or spectrogram formats, often\noverlooking the intrinsic part-whole hierarchies within music. In our quest to\ncomprehend the bottom-up structure of music, we introduce MART, a hierarchical\nmusic representation learning approach that facilitates feature interactions\namong cropped music clips while considering their part-whole hierarchies.\nSpecifically, we propose a hierarchical part-whole transformer to capture the\nstructural relationships between music clips in a part-whole hierarchy.\nFurthermore, a hierarchical contrastive learning objective is crafted to align\npart-whole music representations at adjacent levels, progressively establishing\na multi-hierarchy representation space. The effectiveness of our music\nrepresentation learning from part-whole hierarchies has been empirically\nvalidated across multiple downstream tasks, including music classification and\ncover song identification.\n","authors":["Dong Yao","Jieming Zhu","Jiahao Xun","Shengyu Zhang","Zhou Zhao","Liqun Deng","Wenqiao Zhang","Zhenhua Dong","Xin Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.06197v2.pdf","comment":null}]},"2024-03-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.07225v2","updated":"2024-03-09T23:26:34Z","published":"2023-10-11T06:26:19Z","title":"Exploring the landscape of large language models in medical question\n  answering","summary":"  With the rapid development of new large language models (LLMs), each claiming\nto surpass previous models, an overall picture of medical LLM research can be\nelusive. To address this challenge, we benchmark a range of top LLMs and\nidentify consistent patterns which appear across models. We test $8$ well-known\nLLMs on $874$ newly collected questions from Polish medical licensing exams.\nFor each question, we score each model on the top-1 accuracy and the\ndistribution of probabilities assigned. We then compare with factors including\nquestion difficulty for humans, question length, and the scores of the other\nmodels. LLM accuracies were positively correlated pairwise ($0.29$ to $0.62$).\nModel performance was also correlated with human performance ($0.07$ to\n$0.16$), but negatively correlated to the difference between the question-level\naccuracy of top-scoring and bottom-scoring humans ($-0.16$ to $-0.23$). The top\noutput probability and question length were positive and negative predictors of\naccuracy respectively (p $< 0.05$). The top scoring LLM, GPT-4 Turbo, scored\n$82\\%$, followed by Med42, PaLM 2, Mixtral and GPT-3.5 around $63\\%$. We found\nevidence of similarities between models in which questions they answer\ncorrectly, as well as similarities with human test takers. Larger models\ntypically performed better, but differences in training methods were also\nhighly impactful. Model accuracy was positively correlated with confidence, but\nnegatively correlated with question length. We expect that similar training\nmethods will lead these patterns to persist across future models. These\npatterns can therefore aid medical experts in forming expectations about LLMs\nas a category to support application research.\n","authors":["Andrew M. Bean","Karolina Korgul","Felix Krones","Robert McCraith","Adam Mahdi"],"pdf_url":"https://arxiv.org/pdf/2310.07225v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.16789v3","updated":"2024-03-09T22:26:06Z","published":"2023-10-25T17:21:23Z","title":"Detecting Pretraining Data from Large Language Models","summary":"  Although large language models (LLMs) are widely deployed, the data used to\ntrain them is rarely disclosed. Given the incredible scale of this data, up to\ntrillions of tokens, it is all but certain that it includes potentially\nproblematic text such as copyrighted materials, personally identifiable\ninformation, and test data for widely reported reference benchmarks. However,\nwe currently have no way to know which data of these types is included or in\nwhat proportions. In this paper, we study the pretraining data detection\nproblem: given a piece of text and black-box access to an LLM without knowing\nthe pretraining data, can we determine if the model was trained on the provided\ntext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that\nuses data created before and after model training to support gold truth\ndetection. We also introduce a new detection method Min-K% Prob based on a\nsimple hypothesis: an unseen example is likely to contain a few outlier words\nwith low probabilities under the LLM, while a seen example is less likely to\nhave words with such low probabilities. Min-K% Prob can be applied without any\nknowledge about the pretraining corpus or any additional training, departing\nfrom previous detection methods that require training a reference model on data\nthat is similar to the pretraining data. Moreover, our experiments demonstrate\nthat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous\nmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted book\ndetection, contaminated downstream example detection and privacy auditing of\nmachine unlearning, and find it a consistently effective solution.\n","authors":["Weijia Shi","Anirudh Ajith","Mengzhou Xia","Yangsibo Huang","Daogao Liu","Terra Blevins","Danqi Chen","Luke Zettlemoyer"],"pdf_url":"https://arxiv.org/pdf/2310.16789v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10638v5","updated":"2024-03-09T22:22:48Z","published":"2023-10-16T17:57:12Z","title":"In-Context Pretraining: Language Modeling Beyond Document Boundaries","summary":"  Large language models (LMs) are currently trained to predict tokens given\ndocument prefixes, enabling them to directly perform long-form generation and\nprompting-style tasks which can be reduced to document completion. Existing\npretraining pipelines train LMs by concatenating random sets of short documents\nto create input contexts but the prior documents provide no signal for\npredicting the next document. We instead present In-Context Pretraining, a new\napproach where language models are pretrained on a sequence of related\ndocuments, thereby explicitly encouraging them to read and reason across\ndocument boundaries. We can do In-Context Pretraining by simply changing the\ndocument ordering so that each context contains related documents, and directly\napplying existing pretraining pipelines. However, this document sorting problem\nis challenging. There are billions of documents and we would like the sort to\nmaximize contextual similarity for every document without repeating any data.\nTo do this, we introduce approximate algorithms for finding related documents\nwith efficient nearest neighbor search and constructing coherent input contexts\nwith a graph traversal algorithm. Our experiments show In-Context Pretraining\noffers a simple and scalable approach to significantly enhance LMs'performance:\nwe see notable improvements in tasks that require more complex contextual\nreasoning, including in-context learning (+8%), reading comprehension (+15%),\nfaithfulness to previous contexts (+16%), long-context reasoning (+5%), and\nretrieval augmentation (+9%).\n","authors":["Weijia Shi","Sewon Min","Maria Lomeli","Chunting Zhou","Margaret Li","Gergely Szilvasy","Rich James","Xi Victoria Lin","Noah A. Smith","Luke Zettlemoyer","Scott Yih","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2310.10638v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06023v1","updated":"2024-03-09T22:18:26Z","published":"2024-03-09T22:18:26Z","title":"Persian Slang Text Conversion to Formal and Deep Learning of Persian\n  Short Texts on Social Media for Sentiment Classification","summary":"  The lack of a suitable tool for the analysis of conversational texts in the\nPersian language has made various analyses of these texts, including Sentiment\nAnalysis, difficult. In this research, we tried to make the understanding of\nthese texts easier for the machine by providing PSC, Persian Slang Converter, a\ntool for converting conversational texts into formal ones, and by using the\nmost up-to-date and best deep learning methods along with the PSC, the\nsentiment learning of short Persian language texts for the machine in a better\nway. be made More than 10 million unlabeled texts from various social networks\nand movie subtitles (as Conversational texts) and about 10 million news texts\n(as formal texts) have been used for training unsupervised models and formal\nimplementation of the tool. 60,000 texts from the comments of Instagram social\nnetwork users with positive, negative, and neutral labels are considered\nsupervised data for training the emotion classification model of short texts.\nUsing the formal tool, 57% of the words of the corpus of conversation were\nconverted. Finally, by using the formalizer, FastText model, and deep LSTM\nnetwork, an accuracy of 81.91 was obtained on the test data.\n","authors":["Mohsen Khazeni","Mohammad Heydari","Amir Albadvi"],"pdf_url":"https://arxiv.org/pdf/2403.06023v1.pdf","comment":"10 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.06018v1","updated":"2024-03-09T21:36:13Z","published":"2024-03-09T21:36:13Z","title":"Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in\n  Low-Resource Languages","summary":"  Large pre-trained language models (PLMs) are at the forefront of advances in\nNatural Language Processing. One widespread use case of PLMs is \"prompting\" -\nor in-context learning - where a user provides a description of a task and some\ncompleted examples of the task to a PLM as context before prompting the PLM to\nperform the task on a new example. Only the largest, most capable PLMs are able\nto perform in-context learning effectively, and these models are typically\ntrained with a predominantly English corpus, leaving all other languages\nbehind. The data limitations in most languages preclude the training of\nlanguage-specific PLMs capable of prompting. Albeit the surge in work of\nprompting settings, it is still unclear how PLMs should be adapted\ncross-lingually specifically for prompting. We evaluate the possible methods to\nadapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for\nprompting in low-resource languages, namely for Kinyarwanda, Hausa, and\nLuganda. We consider three methods: few-shot prompting (prompt),\nlanguage-adaptive fine-tuning (LAFT), and neural machine translation\n(translate), and evaluate on abstractive summarization, multi-class topic\nclassification, and named-entity recognition. Although LAFT carries the\ngreatest compute cost and intuitively should lead to the best results, our\nexperiments exhibit that LAFT is only occasionally the optimal choice for\nadapting PLMs for prompting. Rather, the translate and prompt settings are a\ncompute-efficient and cost-effective method of few-shot prompting for the\nselected low-resource languages. We find that the results are task and language\ndependent but find that the prompting method is the best on average across all\ntasks and languages. Results show that the prompt setting performs better than\nboth translating and LAFT with statistical significance for all shots when\naggregated across all tasks and languages.\n","authors":["Christopher Toukmaji"],"pdf_url":"https://arxiv.org/pdf/2403.06018v1.pdf","comment":"47 pages, 26 figures; a thesis submitted in partial satisfaction of\n  the requirements for the degree of Bachelor of Science in Computer Science at\n  the University of California - Santa Cruz"},{"id":"http://arxiv.org/abs/2403.06016v1","updated":"2024-03-09T21:29:40Z","published":"2024-03-09T21:29:40Z","title":"End-to-end solution for linked open data query logs analytics","summary":"  Important advances in pillar domains are derived from exploiting query-logs\nwhich represents users interest and preferences. Deep understanding of users\nprovides useful knowledge which can influence strongly decision-making. In this\nwork, we want to extract valuable information from Linked Open Data (LOD)\nquery-logs. LOD logs have experienced significant growth due to the large\nexploitation of LOD datasets. However, exploiting these logs is a difficult\ntask because of their complex structure. Moreover, these logs suffer from many\nrisks related to their Quality and Provenance, impacting their trust. To tackle\nthese issues, we start by clearly defining the ecosystem of LOD query-logs.\nThen, we provide an end-to-end solution to exploit these logs. At the end, real\nLOD logs are used and a set of experiments are conducted to validate the\nproposed solution.\n","authors":["Dihia Lanasri"],"pdf_url":"https://arxiv.org/pdf/2403.06016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14356v3","updated":"2024-03-09T20:47:30Z","published":"2023-10-22T16:51:42Z","title":"Computer Vision Datasets and Models Exhibit Cultural and Linguistic\n  Diversity in Perception","summary":"  Computer vision often treats human perception as homogeneous: an implicit\nassumption that visual stimuli are perceived similarly by everyone. This\nassumption is reflected in the way researchers collect datasets and train\nvision models. By contrast, literature in cross-cultural psychology and\nlinguistics has provided evidence that people from different cultural\nbackgrounds observe vastly different concepts even when viewing the same visual\nstimuli. In this paper, we study how these differences manifest themselves in\nvision-language datasets and models, using language as a proxy for culture. By\ncomparing textual descriptions generated across 7 languages for the same\nimages, we find significant differences in the semantic content and linguistic\nexpression. When datasets are multilingual as opposed to monolingual,\ndescriptions have higher semantic coverage on average, where coverage is\nmeasured using scene graphs, model embeddings, and linguistic taxonomies. For\nexample, multilingual descriptions have on average 29.9% more objects, 24.5%\nmore relations, and 46.0% more attributes than a set of monolingual captions.\nWhen prompted to describe images in different languages, popular models (e.g.\nLLaVA) inherit this bias and describe different parts of the image. Moreover,\nfinetuning models on captions from one language performs best on corresponding\ntest data from that language, while finetuning on multilingual data performs\nconsistently well across all test data compositions. Our work points towards\nthe need to account for and embrace the diversity of human perception in the\ncomputer vision community.\n","authors":["Andre Ye","Sebastin Santy","Jena D. Hwang","Amy X. Zhang","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2310.14356v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16137v6","updated":"2024-03-09T19:14:07Z","published":"2023-08-30T16:47:51Z","title":"LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language\n  Models","summary":"  Today's large language models (LLMs) typically train on short text segments\n(e.g., <4K tokens) due to the quadratic complexity of their Transformer\narchitectures. As a result, their performance suffers drastically on inputs\nlonger than those encountered during training, substantially limiting their\napplications in real-world tasks involving long contexts such as encoding\nscientific articles, code repositories, or long dialogues. Through theoretical\nanalysis and empirical investigation, this work identifies three major factors\ncontributing to this length generalization failure. Our theoretical analysis\nfurther reveals that commonly used techniques like truncating the attention\nwindow or relative positional encodings are inadequate to address them.\nAnswering these challenges, we propose LM-Infinite, a simple and effective\nmethod for enhancing LLMs' capabilities of handling long contexts. LM-Infinite\nis highly flexible and can be used with most modern LLMs off-the-shelf. Without\nany parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments\nto generalize to up to 200M length inputs while retaining perplexity. It also\nimproves performance on downstream tasks such as Passkey Retrieval and Qasper\nin the zero-shot setting. LM-Infinite brings substantial efficiency\nimprovements: it achieves 2.7x decoding speed up and 7.5x memory saving over\nthe original model. Our code will be publicly available upon publication.\n","authors":["Chi Han","Qifan Wang","Hao Peng","Wenhan Xiong","Yu Chen","Heng Ji","Sinong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.16137v6.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.14702v2","updated":"2024-03-09T19:13:54Z","published":"2024-02-22T16:59:09Z","title":"InfFeed: Influence Functions as a Feedback to Improve the Performance of\n  Subjective Tasks","summary":"  Recently, influence functions present an apparatus for achieving\nexplainability for deep neural models by quantifying the perturbation of\nindividual train instances that might impact a test prediction. Our objectives\nin this paper are twofold. First we incorporate influence functions as a\nfeedback into the model to improve its performance. Second, in a dataset\nextension exercise, using influence functions to automatically identify data\npoints that have been initially `silver' annotated by some existing method and\nneed to be cross-checked (and corrected) by annotators to improve the model\nperformance. To meet these objectives, in this paper, we introduce InfFeed,\nwhich uses influence functions to compute the influential instances for a\ntarget instance. Toward the first objective, we adjust the label of the target\ninstance based on its influencer(s) label. In doing this, InfFeed outperforms\nthe state-of-the-art baselines (including LLMs) by a maximum macro F1-score\nmargin of almost 4% for hate speech classification, 3.5% for stance\nclassification, and 3% for irony and 2% for sarcasm detection. Toward the\nsecond objective we show that manually re-annotating only those silver\nannotated data points in the extension set that have a negative influence can\nimmensely improve the model performance bringing it very close to the scenario\nwhere all the data points in the extension set have gold labels. This allows\nfor huge reduction of the number of data points that need to be manually\nannotated since out of the silver annotated extension dataset, the influence\nfunction scheme picks up ~1/1000 points that need manual correction.\n","authors":["Somnath Banerjee","Maulindu Sarkar","Punyajoy Saha","Binny Mathew","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.14702v2.pdf","comment":"Accepted at LREC-COLING 2024 (Long Paper)"},{"id":"http://arxiv.org/abs/2310.02107v3","updated":"2024-03-09T19:07:00Z","published":"2023-10-03T14:51:34Z","title":"Instances Need More Care: Rewriting Prompts for Instances with LLMs in\n  the Loop Yields Better Zero-Shot Performance","summary":"  Large language models (LLMs) have revolutionized zero-shot task performance,\nmitigating the need for task-specific annotations while enhancing task\ngeneralizability. Despite its advancements, current methods using trigger\nphrases such as ``Let's think step by step'' remain limited. This study\nintroduces PRomPTed, an approach that optimizes the zero-shot prompts for\nindividual task instances following an innovative manner of ``LLMs in the\nloop''. Our comprehensive evaluation across 13 datasets and 10 task types based\non GPT-4 reveals that PRomPTed significantly outperforms both the naive\nzero-shot approaches and a strong baseline (i.e., ``Output Refinement'') which\nrefines the task output instead of the input prompt. Our experimental results\nalso confirmed the generalization of this advantage to the relatively weaker\nGPT-3.5. Even more intriguingly, we found that leveraging GPT-3.5 to rewrite\nprompts for the stronger GPT-4 not only matches but occasionally exceeds the\nefficacy of using GPT-4 as the prompt rewriter. Our research thus presents a\nhuge value in not only enhancing zero-shot LLM performance but also potentially\nenabling supervising LLMs with their weaker counterparts, a capability\nattracting much interest recently.\n","authors":["Saurabh Srivastava","Chengyue Huang","Weiguo Fan","Ziyu Yao"],"pdf_url":"https://arxiv.org/pdf/2310.02107v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.05982v1","updated":"2024-03-09T18:43:48Z","published":"2024-03-09T18:43:48Z","title":"Enhanced Auto Language Prediction with Dictionary Capsule -- A Novel\n  Approach","summary":"  The paper presents a novel Auto Language Prediction Dictionary Capsule\n(ALPDC) framework for language prediction and machine translation. The model\nuses a combination of neural networks and symbolic representations to predict\nthe language of a given input text and then translate it to a target language\nusing pre-built dictionaries. This research work also aims to translate the\ntext of various languages to its literal meaning in English. The proposed model\nachieves state-of-the-art results on several benchmark datasets and\nsignificantly improves translation accuracy compared to existing methods. The\nresults show the potential of the proposed method for practical use in\nmultilingual communication and natural language processing tasks.\n","authors":["Pinni Venkata Abhiram","Ananya Rathore","Abhir Mirikar","Hari Krishna S","Sheena Christabel Pravin","Vishwanath Kamath Pethri","Manjunath Lokanath Belgod","Reetika Gupta","K Muthukumaran"],"pdf_url":"https://arxiv.org/pdf/2403.05982v1.pdf","comment":"21 Pages"},{"id":"http://arxiv.org/abs/2403.05975v1","updated":"2024-03-09T18:24:58Z","published":"2024-03-09T18:24:58Z","title":"Measuring Bias in a Ranked List using Term-based Representations","summary":"  In most recent studies, gender bias in document ranking is evaluated with the\nNFaiRR metric, which measures bias in a ranked list based on an aggregation\nover the unbiasedness scores of each ranked document. This perspective in\nmeasuring the bias of a ranked list has a key limitation: individual documents\nof a ranked list might be biased while the ranked list as a whole balances the\ngroups' representations. To address this issue, we propose a novel metric\ncalled TExFAIR (term exposure-based fairness), which is based on two new\nextensions to a generic fairness evaluation framework, attention-weighted\nranking fairness (AWRF). TExFAIR assesses fairness based on the term-based\nrepresentation of groups in a ranked list: (i) an explicit definition of\nassociating documents to groups based on probabilistic term-level associations,\nand (ii) a rank-biased discounting factor (RBDF) for counting\nnon-representative documents towards the measurement of the fairness of a\nranked list. We assess TExFAIR on the task of measuring gender bias in passage\nranking, and study the relationship between TExFAIR and NFaiRR. Our experiments\nshow that there is no strong correlation between TExFAIR and NFaiRR, which\nindicates that TExFAIR measures a different dimension of fairness than NFaiRR.\nWith TExFAIR, we extend the AWRF framework to allow for the evaluation of\nfairness in settings with term-based representations of groups in documents in\na ranked list.\n","authors":["Amin Abolghasemi","Leif Azzopardi","Arian Askari","Maarten de Rijke","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2403.05975v1.pdf","comment":"Accepted at the 46th European Conference on Information Retrieval\n  (ECIR 2024)"},{"id":"http://arxiv.org/abs/2403.05973v1","updated":"2024-03-09T17:46:24Z","published":"2024-03-09T17:46:24Z","title":"Calibrating Large Language Models Using Their Generations Only","summary":"  As large language models (LLMs) are increasingly deployed in user-facing\napplications, building trust and maintaining safety by accurately quantifying a\nmodel's confidence in its prediction becomes even more important. However,\nfinding effective ways to calibrate LLMs - especially when the only interface\nto the models is their generated text - remains a challenge. We propose APRICOT\n(auxiliary prediction of confidence targets): A method to set confidence\ntargets and train an additional model that predicts an LLM's confidence based\non its textual input and output alone. This approach has several advantages: It\nis conceptually simple, does not require access to the target model beyond its\noutput, does not interfere with the language generation, and has a multitude of\npotential usages, for instance by verbalizing the predicted confidence or\nadjusting the given answer based on the confidence. We show how our approach\nperforms competitively in terms of calibration error for white-box and\nblack-box LLMs on closed-book question-answering to detect incorrect LLM\nanswers.\n","authors":["Dennis Ulmer","Martin Gubri","Hwaran Lee","Sangdoo Yun","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2403.05973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09282v3","updated":"2024-03-09T16:45:40Z","published":"2024-02-14T16:10:45Z","title":"Distilling Large Language Models into Tiny Models for Named Entity\n  Recognition","summary":"  Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural\nLanguage Processing (NLP), showing potential in traditional tasks such as Named\nEntity Recognition (NER). Our study explores a three-phase training strategy\nthat harnesses GPT-4's capabilities to enhance the BERT model's performance on\nNER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC\ndataset without fine-tuning. We then train BERT using a mix of original and\nLLM-annotated data, analyzing the efficacy of LLM annotations against\ntraditional methods. The second phase involves comparative experiments with\ndifferent training regimens, assessing the synergy between distilled and\noriginal data. We observe that sequential strategies, particularly a simple mix\nof training first with distilled data followed by original data, significantly\nboost performance. In the third phase, we investigate various data blending\ntechniques, including sigmoid and power decay functions, to optimize the\ntraining process further. Our results indicate that a strategic mix of\ndistilled and original data markedly elevates the NER capabilities of BERT. Our\napproach presents a scalable methodology that reduces manual annotation costs\nand increases efficiency, making it especially pertinent in resource-limited\nand closed-network environments. The study concludes that while the 'Simple\nMix' strategy yields the best results, understanding its underlying mechanisms\nrequires further research. Future work will also focus on refining prompt\ndesigns and enhancing annotation selection processes, aiming to extend our\nmethodology to diverse NLP tasks.\n","authors":["Yining Huang"],"pdf_url":"https://arxiv.org/pdf/2402.09282v3.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.18152v4","updated":"2024-03-09T16:08:16Z","published":"2023-10-27T14:00:04Z","title":"Disentangled Representation Learning with Large Language Models for\n  Text-Attributed Graphs","summary":"  Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs\nsuch as citation networks, e-commerce networks and social networks has\nattracted considerable attention in the web community. Recently, large language\nmodels (LLMs) have demonstrated exceptional capabilities across a wide range of\ntasks. However, the existing works focus on harnessing the potential of LLMs\nsolely relying on prompts to convey graph structure information to LLMs, thus\nsuffering from insufficient understanding of the complex structural\nrelationships within TAGs. To address this problem, in this paper we present\nthe Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the\nreasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model\nincorporates graph structure information through tailored disentangled graph\nneural network (GNN) layers, enabling LLMs to capture the intricate\nrelationships hidden in text-attributed graphs from multiple structural\nfactors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing\ncomputational costs and allowing much more flexibility in combining with\ndifferent LLM models. Experimental evaluations demonstrate the effectiveness of\nthe proposed DGTL model on achieving superior or comparable performance over\nstate-of-the-art baselines. Additionally, we also demonstrate that our DGTL\nmodel can offer natural language explanations for predictions, thereby\nsignificantly enhancing model interpretability.\n","authors":["Yijian Qin","Xin Wang","Ziwei Zhang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.18152v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05961v4","updated":"2024-03-09T15:50:17Z","published":"2023-09-12T05:03:28Z","title":"Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering\n  Trends across Diverse Platforms","summary":"  Community Question Answering (CQA) platforms steadily gain popularity as they\nprovide users with fast responses to their queries. The swiftness of these\nresponses is contingent on a mixture of query-specific and user-related\nelements. This paper scrutinizes these contributing factors within the context\nof six highly popular CQA platforms, identified through their standout\nanswering speed. Our investigation reveals a correlation between the time taken\nto yield the first response to a question and several variables: the metadata,\nthe formulation of the questions, and the level of interaction among users.\nAdditionally, by employing conventional machine learning models to analyze\nthese metadata and patterns of user interaction, we endeavor to predict which\nqueries will receive their initial responses promptly.\n","authors":["Rima Hazra","Agnik Saha","Somnath Banerjee","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2309.05961v4.pdf","comment":"Accepted as POSTER"},{"id":"http://arxiv.org/abs/2403.04656v2","updated":"2024-03-09T15:37:36Z","published":"2024-03-07T16:59:55Z","title":"Chain of Thought Explanation for Dialogue State Tracking","summary":"  Dialogue state tracking (DST) aims to record user queries and goals during a\nconversational interaction achieved by maintaining a predefined set of slots\nand their corresponding values. Current approaches decide slot values opaquely,\nwhile humans usually adopt a more deliberate approach by collecting information\nfrom relevant dialogue turns and then reasoning the appropriate values. In this\nwork, we focus on the steps needed to figure out slot values by proposing a\nmodel named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, which\nis built on the generative DST framework, is designed to create detailed\nexplanations step by step after determining the slot values. This process leads\nto more accurate and reliable slot values. More-over, to improve the reasoning\nability of the CoTE, we further construct more fluent and high-quality\nexplanations with automatic paraphrasing, leading the method CoTE-refined.\nExperimental results on three widely recognized DST benchmarks-MultiWOZ 2.2,\nWoZ 2.0, and M2M-demonstrate the remarkable effectiveness of the CoTE.\nFurthermore, through a meticulous fine-grained analysis, we observe significant\nbenefits of our CoTE on samples characterized by longer dialogue turns, user\nresponses, and reasoning steps.\n","authors":["Lin Xu","Ningxin Peng","Daquan Zhou","See-Kiong Ng","Jinlan Fu"],"pdf_url":"https://arxiv.org/pdf/2403.04656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01665v3","updated":"2024-03-09T15:27:33Z","published":"2023-04-04T09:50:07Z","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled\n  Neural Networks","summary":"  Language models' (LMs) proficiency in handling deterministic symbolic\nreasoning and rule-based tasks remains limited due to their dependency implicit\nlearning on textual data. To endow LMs with genuine rule comprehension\nabilities, we propose \"Neural Comprehension\" - a framework that synergistically\nintegrates compiled neural networks (CoNNs) into the standard transformer\narchitecture. CoNNs are neural modules designed to explicitly encode rules\nthrough artificially generated attention weights. By incorporating CoNN\nmodules, the Neural Comprehension framework enables LMs to accurately and\nrobustly execute rule-intensive symbolic tasks. Extensive experiments\ndemonstrate the superiority of our approach over existing techniques in terms\nof length generalization, efficiency, and interpretability for symbolic\noperations. Furthermore, it can be applied to LMs across different model\nscales, outperforming tool-calling methods in arithmetic reasoning tasks while\nmaintaining superior inference efficiency. Our work highlights the potential of\nseamlessly unifying explicit rule learning via CoNNs and implicit pattern\nlearning in LMs, paving the way for true symbolic comprehension capabilities.\n","authors":["Yixuan Weng","Minjun Zhu","Fei Xia","Bin Li","Shizhu He","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.01665v3.pdf","comment":"Accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2403.05931v1","updated":"2024-03-09T14:50:20Z","published":"2024-03-09T14:50:20Z","title":"Thread Detection and Response Generation using Transformers with Prompt\n  Optimisation","summary":"  Conversational systems are crucial for human-computer interaction, managing\ncomplex dialogues by identifying threads and prioritising responses. This is\nespecially vital in multi-party conversations, where precise identification of\nthreads and strategic response prioritisation ensure efficient dialogue\nmanagement. To address these challenges an end-to-end model that identifies\nthreads and prioritises their response generation based on the importance was\ndeveloped, involving a systematic decomposition of the problem into discrete\ncomponents - thread detection, prioritisation, and performance optimisation\nwhich was meticulously analysed and optimised. These refined components\nseamlessly integrate into a unified framework, in conversational systems.\nLlama2 7b is used due to its high level of generalisation but the system can be\nupdated with any open source Large Language Model(LLM). The computational\ncapabilities of the Llama2 model was augmented by using fine tuning methods and\nstrategic prompting techniques to optimise the model's performance, reducing\ncomputational time and increasing the accuracy of the model. The model achieves\nup to 10x speed improvement, while generating more coherent results compared to\nexisting models.\n","authors":["Kevin Joshua T","Arnav Agarwal","Shriya Sanjay","Yash Sarda","John Sahaya Rani Alex","Saurav Gupta","Sushant Kumar","Vishwanath Kamath"],"pdf_url":"https://arxiv.org/pdf/2403.05931v1.pdf","comment":"6 pages, 4 figures, submitted to 2024 IEEE International Conference\n  on Signal Processing and Communications (SPCOM)"},{"id":"http://arxiv.org/abs/2307.08153v4","updated":"2024-03-09T14:18:41Z","published":"2023-07-16T21:22:40Z","title":"Analyzing Dataset Annotation Quality Management in the Wild","summary":"  Data quality is crucial for training accurate, unbiased, and trustworthy\nmachine learning models as well as for their correct evaluation. Recent works,\nhowever, have shown that even popular datasets used to train and evaluate\nstate-of-the-art models contain a non-negligible amount of erroneous\nannotations, biases, or artifacts. While practices and guidelines regarding\ndataset creation projects exist, to our knowledge, large-scale analysis has yet\nto be performed on how quality management is conducted when creating natural\nlanguage datasets and whether these recommendations are followed. Therefore, we\nfirst survey and summarize recommended quality management practices for dataset\ncreation as described in the literature and provide suggestions for applying\nthem. Then, we compile a corpus of 591 scientific publications introducing text\ndatasets and annotate it for quality-related aspects, such as annotator\nmanagement, agreement, adjudication, or data validation. Using these\nannotations, we then analyze how quality management is conducted in practice. A\nmajority of the annotated publications apply good or excellent quality\nmanagement. However, we deem the effort of 30\\% of the works as only subpar.\nOur analysis also shows common errors, especially when using inter-annotator\nagreement and computing annotation error rates.\n","authors":["Jan-Christoph Klie","Richard Eckart de Castilho","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2307.08153v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05920v1","updated":"2024-03-09T14:02:59Z","published":"2024-03-09T14:02:59Z","title":"High Throughput Phenotyping of Physician Notes with Large Language and\n  Hybrid NLP Models","summary":"  Deep phenotyping is the detailed description of patient signs and symptoms\nusing concepts from an ontology. The deep phenotyping of the numerous physician\nnotes in electronic health records requires high throughput methods. Over the\npast thirty years, progress toward making high throughput phenotyping feasible.\nIn this study, we demonstrate that a large language model and a hybrid NLP\nmodel (combining word vectors with a machine learning classifier) can perform\nhigh throughput phenotyping on physician notes with high accuracy. Large\nlanguage models will likely emerge as the preferred method for high throughput\ndeep phenotyping of physician notes.\n","authors":["Syed I. Munzir","Daniel B. Hier","Michael D. Carrithers"],"pdf_url":"https://arxiv.org/pdf/2403.05920v1.pdf","comment":"Submitted to IEEE EMBS Summer conference 2024"},{"id":"http://arxiv.org/abs/2211.09935v3","updated":"2024-03-09T13:53:47Z","published":"2022-11-17T23:14:51Z","title":"CAPE: Corrective Actions from Precondition Errors using Large Language\n  Models","summary":"  Extracting commonsense knowledge from a large language model (LLM) offers a\npath to designing intelligent robots. Existing approaches that leverage LLMs\nfor planning are unable to recover when an action fails and often resort to\nretrying failed actions, without resolving the error's underlying cause. We\npropose a novel approach (CAPE) that attempts to propose corrective actions to\nresolve precondition errors during planning. CAPE improves the quality of\ngenerated plans by leveraging few-shot reasoning from action preconditions. Our\napproach enables embodied agents to execute more tasks than baseline methods\nwhile ensuring semantic correctness and minimizing re-prompting. In\nVirtualHome, CAPE generates executable plans while improving a human-annotated\nplan correctness metric from 28.89% to 49.63% over SayCan. Our improvements\ntransfer to a Boston Dynamics Spot robot initialized with a set of skills\n(specified in language) and associated preconditions, where CAPE improves the\ncorrectness metric of the executed task plans by 76.49% compared to SayCan. Our\napproach enables the robot to follow natural language commands and robustly\nrecover from failures, which baseline approaches largely cannot resolve or\naddress inefficiently.\n","authors":["Shreyas Sundara Raman","Vanya Cohen","Ifrah Idrees","Eric Rosen","Ray Mooney","Stefanie Tellex","David Paulius"],"pdf_url":"https://arxiv.org/pdf/2211.09935v3.pdf","comment":"17 pages, 6 figures, accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2403.03640v2","updated":"2024-03-09T13:02:11Z","published":"2024-03-06T11:56:02Z","title":"Apollo: An Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People","summary":"  Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.\n","authors":["Xidong Wang","Nuo Chen","Junyin Chen","Yan Hu","Yidong Wang","Xiangbo Wu","Anningzhe Gao","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2403.03640v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.05902v1","updated":"2024-03-09T12:46:53Z","published":"2024-03-09T12:46:53Z","title":"MaiBaam Annotation Guidelines","summary":"  This document provides the annotation guidelines for MaiBaam, a Bavarian\ncorpus annotated with part-of-speech (POS) tags and syntactic dependencies.\nMaiBaam belongs to the Universal Dependencies (UD) project, and our annotations\nelaborate on the general and German UD version 2 guidelines. In this document,\nwe detail how to preprocess and tokenize Bavarian data, provide an overview of\nthe POS tags and dependencies we use, explain annotation decisions that would\nalso apply to closely related languages like German, and lastly we introduce\nand motivate decisions that are specific to Bavarian grammar.\n","authors":["Verena Blaschke","Barbara Kovačić","Siyao Peng","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2403.05902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11941v2","updated":"2024-03-09T12:15:34Z","published":"2024-02-19T08:29:03Z","title":"Comprehensive Cognitive LLM Agent for Smartphone GUI Automation","summary":"  Large language models (LLMs) have shown remarkable potential as human-like\nautonomous language agents to interact with real-world environments, especially\nfor graphical user interface (GUI) automation. However, those GUI agents\nrequire comprehensive cognition ability including exhaustive perception and\nreliable action response. We propose \\underline{Co}mprehensive\n\\underline{Co}gnitive LLM \\underline{Agent}, CoCo-Agent, with two novel\napproaches, comprehensive environment perception (CEP) and conditional action\nprediction (CAP), to systematically improve the GUI automation performance.\nFirst, CEP facilitates the GUI perception through different aspects and\ngranularity, including screenshots and complementary detailed layouts for the\nvisual channel and historical actions for the textual channel. Second, CAP\ndecomposes the action prediction into sub-problems: action type prediction and\naction target conditioned on the action type. With our technical design, our\nagent achieves new state-of-the-art performance on AITW and META-GUI\nbenchmarks, showing promising abilities in realistic scenarios. Code is\navailable at https://github.com/xbmxb/AAgent.\n","authors":["Xinbei Ma","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.11941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05881v1","updated":"2024-03-09T11:23:38Z","published":"2024-03-09T11:23:38Z","title":"KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge\n  Graphs and Ranking Techniques","summary":"  Large Language Models (LLMs) have significantly advanced healthcare\ninnovation on generation capabilities. However, their application in real\nclinical settings is challenging due to potential deviations from medical facts\nand inherent biases. In this work, we develop an augmented LLM framework,\nKG-Rank, which leverages a medical knowledge graph (KG) with ranking and\nre-ranking techniques, aiming to improve free-text question-answering (QA) in\nthe medical domain. Specifically, upon receiving a question, we initially\nretrieve triplets from a medical KG to gather factual information.\nSubsequently, we innovatively apply ranking methods to refine the ordering of\nthese triplets, aiming to yield more precise answers. To the best of our\nknowledge, KG-Rank is the first application of ranking models combined with KG\nin medical QA specifically for generating long answers. Evaluation of four\nselected medical QA datasets shows that KG-Rank achieves an improvement of over\n18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it\nrealizes a 14% improvement in ROUGE-L, showing the effectiveness and potential\nof KG-Rank.\n","authors":["Rui Yang","Haoran Liu","Qingcheng Zeng","Yu He Ke","Wanxin Li","Lechao Cheng","Qingyu Chen","James Caverlee","Yutaka Matsuo","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2403.05881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08491v2","updated":"2024-03-09T10:44:58Z","published":"2023-10-12T16:50:08Z","title":"Prometheus: Inducing Fine-grained Evaluation Capability in Language\n  Models","summary":"  Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,\nGPT-4) as an evaluator for long-form responses has become the de facto\nstandard. However, for practitioners with large-scale evaluation tasks and\ncustom criteria in consideration (e.g., child-readability), using proprietary\nLLMs as an evaluator is unreliable due to the closed-source nature,\nuncontrolled versioning, and prohibitive costs. In this work, we propose\nPrometheus, a fully open-source LLM that is on par with GPT-4's evaluation\ncapabilities when the appropriate reference materials (reference answer, score\nrubric) are accompanied. We first construct the Feedback Collection, a new\ndataset that consists of 1K fine-grained score rubrics, 20K instructions, and\n100K responses and language feedback generated by GPT-4. Using the Feedback\nCollection, we train Prometheus, a 13B evaluator LLM that can assess any given\nlong-form text based on customized score rubric provided by the user.\nExperimental results show that Prometheus scores a Pearson correlation of 0.897\nwith human evaluators when evaluating with 45 customized score rubrics, which\nis on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).\nFurthermore, measuring correlation with GPT-4 with 1222 customized score\nrubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask\nEval) shows similar trends, bolstering Prometheus's capability as an evaluator\nLLM. Lastly, Prometheus achieves the highest accuracy on two human preference\nbenchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced\nreward models explicitly trained on human preference datasets, highlighting its\npotential as an universal reward model. We open-source our code, dataset, and\nmodel at https://kaistai.github.io/prometheus/.\n","authors":["Seungone Kim","Jamin Shin","Yejin Cho","Joel Jang","Shayne Longpre","Hwaran Lee","Sangdoo Yun","Seongjin Shin","Sungdong Kim","James Thorne","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2310.08491v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2306.14580v2","updated":"2024-03-09T09:15:41Z","published":"2023-06-26T10:45:16Z","title":"TransERR: Translation-based Knowledge Graph Embedding via Efficient\n  Relation Rotation","summary":"  This paper presents a translation-based knowledge geraph embedding method via\nefficient relation rotation (TransERR), a straightforward yet effective\nalternative to traditional translation-based knowledge graph embedding models.\nDifferent from the previous translation-based models, TransERR encodes\nknowledge graphs in the hypercomplex-valued space, thus enabling it to possess\na higher degree of translation freedom in mining latent information between the\nhead and tail entities. To further minimize the translation distance, TransERR\nadaptively rotates the head entity and the tail entity with their corresponding\nunit quaternions, which are learnable in model training. We also provide\nmathematical proofs to demonstrate the ability of TransERR in modeling various\nrelation patterns, including symmetry, antisymmetry, inversion, composition,\nand subrelation patterns. The experiments on 10 benchmark datasets validate the\neffectiveness and the generalization of TransERR. The results also indicate\nthat TransERR can better encode large-scale datasets with fewer parameters than\nthe previous translation-based models. Our code and datasets are available\nat~\\url{https://github.com/dellixx/TransERR}.\n","authors":["Jiang Li","Xiangdong Su","Fujun Zhang","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2306.14580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05846v1","updated":"2024-03-09T09:11:49Z","published":"2024-03-09T09:11:49Z","title":"Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines","summary":"  Text-to-image diffusion models (T2I) use a latent representation of a text\nprompt to guide the image generation process. However, the process by which the\nencoder produces the text representation is unknown. We propose the Diffusion\nLens, a method for analyzing the text encoder of T2I models by generating\nimages from its intermediate representations. Using the Diffusion Lens, we\nperform an extensive analysis of two recent T2I models. Exploring compound\nprompts, we find that complex scenes describing multiple objects are composed\nprogressively and more slowly compared to simple scenes; Exploring knowledge\nretrieval, we find that representation of uncommon concepts requires further\ncomputation compared to common concepts, and that knowledge retrieval is\ngradual across layers. Overall, our findings provide valuable insights into the\ntext encoder component in T2I pipelines.\n","authors":["Michael Toker","Hadas Orgad","Mor Ventura","Dana Arad","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.05846v1.pdf","comment":"Project webpage: tokeron.github.io/DiffusionLensWeb"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.06021v1","updated":"2024-03-09T21:55:55Z","published":"2024-03-09T21:55:55Z","title":"Hierarchical Query Classification in E-commerce Search","summary":"  E-commerce platforms typically store and structure product information and\nsearch data in a hierarchy. Efficiently categorizing user search queries into a\nsimilar hierarchical structure is paramount in enhancing user experience on\ne-commerce platforms as well as news curation and academic research. The\nsignificance of this task is amplified when dealing with sensitive query\ncategorization or critical information dissemination, where inaccuracies can\nlead to considerable negative impacts. The inherent complexity of hierarchical\nquery classification is compounded by two primary challenges: (1) the\npronounced class imbalance that skews towards dominant categories, and (2) the\ninherent brevity and ambiguity of search queries that hinder accurate\nclassification.\n  To address these challenges, we introduce a novel framework that leverages\nhierarchical information through (i) enhanced representation learning that\nutilizes the contrastive loss to discern fine-grained instance relationships\nwithin the hierarchy, called ''instance hierarchy'', and (ii) a nuanced\nhierarchical classification loss that attends to the intrinsic label taxonomy,\nnamed ''label hierarchy''. Additionally, based on our observation that certain\nunlabeled queries share typographical similarities with labeled queries, we\npropose a neighborhood-aware sampling technique to intelligently select these\nunlabeled queries to boost the classification performance. Extensive\nexperiments demonstrate that our proposed method is better than\nstate-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to\nSOTA on the public datasets of Web of Science and RCV1-V2. These results\nunderscore the efficacy of our proposed solution, and pave the path toward the\nnext generation of hierarchy-aware query classification systems.\n","authors":["Bing He","Sreyashi Nag","Limeng Cui","Suhang Wang","Zheng Li","Rahul Goutam","Zhen Li","Haiyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06021v1.pdf","comment":"Published at: the ACM Web Conference 2024 in the industry track\n  (WWW'24)"},{"id":"http://arxiv.org/abs/2109.12512v2","updated":"2024-03-09T19:53:03Z","published":"2021-09-26T07:10:45Z","title":"DemiNet: Dependency-Aware Multi-Interest Network with Self-Supervised\n  Graph Learning for Click-Through Rate Prediction","summary":"  In this paper, we propose a novel model named DemiNet (short for\nDEpendency-Aware Multi-Interest Network) to address the above two issues. To be\nspecific, we first consider various dependency types between item nodes and\nperform dependency-aware heterogeneous attention for denoising and obtaining\naccurate sequence item representations. Secondly, for multiple interests\nextraction, multi-head attention is conducted on top of the graph embedding. To\nfilter out noisy inter-item correlations and enhance the robustness of\nextracted interests, self-supervised interest learning is introduced to the\nabove two steps. Thirdly, to aggregate the multiple interests, interest experts\ncorresponding to different interest routes give rating scores respectively,\nwhile a specialized network assigns the confidence of each score. Experimental\nresults on three real-world datasets demonstrate that the proposed DemiNet\nsignificantly improves the overall recommendation performance over several\nstate-of-the-art baselines. Further studies verify the efficacy and\ninterpretability benefits brought by the fine-grained user interest modeling.\n","authors":["Yule Wang","Qiang Luo","Yue Ding","Yunzhe Li","Dong Wang","Hongbo Deng"],"pdf_url":"https://arxiv.org/pdf/2109.12512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06282v4","updated":"2024-03-09T18:28:34Z","published":"2023-10-10T03:32:33Z","title":"MuseChat: A Conversational Music Recommendation System for Videos","summary":"  Music recommendation for videos attracts growing interest in multi-modal\nresearch. However, existing systems focus primarily on content compatibility,\noften ignoring the users' preferences. Their inability to interact with users\nfor further refinements or to provide explanations leads to a less satisfying\nexperience. We address these issues with MuseChat, a first-of-its-kind\ndialogue-based recommendation system that personalizes music suggestions for\nvideos. Our system consists of two key functionalities with associated modules:\nrecommendation and reasoning. The recommendation module takes a video along\nwith optional information including previous suggested music and user's\npreference as inputs and retrieves an appropriate music matching the context.\nThe reasoning module, equipped with the power of Large Language Model\n(Vicuna-7B) and extended to multi-modal inputs, is able to provide reasonable\nexplanation for the recommended music. To evaluate the effectiveness of\nMuseChat, we build a large-scale dataset, conversational music recommendation\nfor videos, that simulates a two-turn interaction between a user and a\nrecommender based on accurate music track information. Experiment results show\nthat MuseChat achieves significant improvements over existing video-based music\nretrieval methods as well as offers strong interpretability and\ninteractability.\n","authors":["Zhikang Dong","Bin Chen","Xiulong Liu","Pawel Polak","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06282v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05961v4","updated":"2024-03-09T15:50:17Z","published":"2023-09-12T05:03:28Z","title":"Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering\n  Trends across Diverse Platforms","summary":"  Community Question Answering (CQA) platforms steadily gain popularity as they\nprovide users with fast responses to their queries. The swiftness of these\nresponses is contingent on a mixture of query-specific and user-related\nelements. This paper scrutinizes these contributing factors within the context\nof six highly popular CQA platforms, identified through their standout\nanswering speed. Our investigation reveals a correlation between the time taken\nto yield the first response to a question and several variables: the metadata,\nthe formulation of the questions, and the level of interaction among users.\nAdditionally, by employing conventional machine learning models to analyze\nthese metadata and patterns of user interaction, we endeavor to predict which\nqueries will receive their initial responses promptly.\n","authors":["Rima Hazra","Agnik Saha","Somnath Banerjee","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2309.05961v4.pdf","comment":"Accepted as POSTER"},{"id":"http://arxiv.org/abs/2403.05873v1","updated":"2024-03-09T10:49:31Z","published":"2024-03-09T10:49:31Z","title":"LEGION: Harnessing Pre-trained Language Models for GitHub Topic\n  Recommendations with Distribution-Balance Loss","summary":"  Open-source development has revolutionized the software industry by promoting\ncollaboration, transparency, and community-driven innovation. Today, a vast\namount of various kinds of open-source software, which form networks of\nrepositories, is often hosted on GitHub - a popular software development\nplatform. To enhance the discoverability of the repository networks, i.e.,\ngroups of similar repositories, GitHub introduced repository topics in 2017\nthat enable users to more easily explore relevant projects by type, technology,\nand more. It is thus crucial to accurately assign topics for each GitHub\nrepository. Current methods for automatic topic recommendation rely heavily on\nTF-IDF for encoding textual data, presenting challenges in understanding\nsemantic nuances. This paper addresses the limitations of existing techniques\nby proposing Legion, a novel approach that leverages Pre-trained Language\nModels (PTMs) for recommending topics for GitHub repositories. The key novelty\nof Legion is three-fold. First, Legion leverages the extensive capabilities of\nPTMs in language understanding to capture contextual information and semantic\nmeaning in GitHub repositories. Second, Legion overcomes the challenge of\nlong-tailed distribution, which results in a bias toward popular topics in\nPTMs, by proposing a Distribution-Balanced Loss (DB Loss) to better train the\nPTMs. Third, Legion employs a filter to eliminate vague recommendations,\nthereby improving the precision of PTMs. Our empirical evaluation on a\nbenchmark dataset of real-world GitHub repositories shows that Legion can\nimprove vanilla PTMs by up to 26% on recommending GitHubs topics. Legion also\ncan suggest GitHub topics more precisely and effectively than the\nstate-of-the-art baseline with an average improvement of 20% and 5% in terms of\nPrecision and F1-score, respectively.\n","authors":["Yen-Trang Dang","Thanh-Le Cong","Phuc-Thanh Nguyen","Anh M. T. Bui","Phuong T. Nguyen","Bach Le","Quyet-Thang Huynh"],"pdf_url":"https://arxiv.org/pdf/2403.05873v1.pdf","comment":"Accepted to EASE'24"},{"id":"http://arxiv.org/abs/2402.11480v3","updated":"2024-03-09T09:37:53Z","published":"2024-02-18T07:06:17Z","title":"Pattern-wise Transparent Sequential Recommendation","summary":"  A transparent decision-making process is essential for developing reliable\nand trustworthy recommender systems. For sequential recommendation, it means\nthat the model can identify critical items asthe justifications for its\nrecommendation results. However, achieving both model transparency and\nrecommendation performance simultaneously is challenging, especially for models\nthat take the entire sequence of items as input without screening. In this\npaper,we propose an interpretable framework (named PTSR) that enables a\npattern-wise transparent decision-making process. It breaks the sequence of\nitems into multi-level patterns that serve as atomic units for the entire\nrecommendation process. The contribution of each pattern to the outcome is\nquantified in the probability space. With a carefully designed pattern\nweighting correction, the pattern contribution can be learned in the absence of\nground-truth critical patterns. The final recommended items are those items\nthat most critical patterns strongly endorse. Extensive experiments on four\npublic datasets demonstrate remarkable recommendation performance, while case\nstudies validate the model transparency. Our code is available at\nhttps://anonymous.4open.science/r/PTSR-2237.\n","authors":["Kun Ma","Cong Xu","Zeyuan Chen","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.11480v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03765v2","updated":"2024-03-09T07:12:52Z","published":"2023-01-10T03:04:27Z","title":"Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language\n  Understanding","summary":"  Current natural language understanding (NLU) models have been continuously\nscaling up, both in terms of model size and input context, introducing more\nhidden and input neurons. While this generally improves performance on average,\nthe extra neurons do not yield a consistent improvement for all instances. This\nis because some hidden neurons are redundant, and the noise mixed in input\nneurons tends to distract the model. Previous work mainly focuses on\nextrinsically reducing low-utility neurons by additional post- or\npre-processing, such as network pruning and context selection, to avoid this\nproblem. Beyond that, can we make the model reduce redundant parameters and\nsuppress input noise by intrinsically enhancing the utility of each neuron? If\na model can efficiently utilize neurons, no matter which neurons are ablated\n(disabled), the ablated submodel should perform no better than the original\nfull model. Based on such a comparison principle between models, we propose a\ncross-model comparative loss for a broad range of tasks. Comparative loss is\nessentially a ranking loss on top of the task-specific losses of the full and\nablated models, with the expectation that the task-specific loss of the full\nmodel is minimal. We demonstrate the universal effectiveness of comparative\nloss through extensive experiments on 14 datasets from 3 distinct NLU tasks\nbased on 5 widely used pretrained language models and find it particularly\nsuperior for models with few parameters or long input.\n","authors":["Yunchang Zhu","Liang Pang","Kangxi Wu","Yanyan Lan","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2301.03765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10435v2","updated":"2024-03-09T04:04:09Z","published":"2023-09-19T08:54:47Z","title":"Reformulating Sequential Recommendation: Learning Dynamic User Interest\n  with Content-enriched Language Modeling","summary":"  Recommender systems are essential for online applications, and sequential\nrecommendation has enjoyed significant prevalence due to its expressive ability\nto capture dynamic user interests. However, previous sequential modeling\nmethods still have limitations in capturing contextual information. The primary\nreason for this issue is that language models often lack an understanding of\ndomain-specific knowledge and item-related textual content. To address this\nissue, we adopt a new sequential recommendation paradigm and propose LANCER,\nwhich leverages the semantic understanding capabilities of pre-trained language\nmodels to generate personalized recommendations. Our approach bridges the gap\nbetween language models and recommender systems, resulting in more human-like\nrecommendations. We demonstrate the effectiveness of our approach through\nexperiments on several benchmark datasets, showing promising results and\nproviding valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.\n","authors":["Junzhe Jiang","Shang Qu","Mingyue Cheng","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2309.10435v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.05851v1","updated":"2024-03-09T09:33:43Z","published":"2024-03-09T09:33:43Z","title":"Interest-Aware Joint Caching, Computing, and Communication Optimization\n  for Mobile VR Delivery in MEC Networks","summary":"  In the upcoming B5G/6G era, virtual reality (VR) over wireless has become a\ntypical application, which is an inevitable trend in the development of video.\nHowever, in immersive and interactive VR experiences, VR services typically\nexhibit high delay, while simultaneously posing challenges for the energy\nconsumption of local devices. To address these issues, this paper aims to\nimprove the performance of the VR service in the edge-terminal cooperative\nsystem. Specifically, we formulate a problem of joint caching, computing, and\ncommunication VR service policy, by optimizing the weighted sum of overall VR\ndelivery delay and energy consumption of local devices. For the purpose of\ndesigning the optimal VR service policy, the optimization problem is decoupled\ninto three independent subproblems to be solved separately. To enhance the\ncaching efficiency within the network, a bidirectional encoder representations\nfrom transformers (Bert)-based user interest analysis method is first proposed\nto characterize the content requesting behavior accurately. On the basis of\nthis, a service cost minimum-maximization problem is formulated with\nconsideration of performance fairness among users. Thereafter, the joint\ncaching and computing scheme is derived for each user with given allocation of\ncommunication resources while a bisection-based communication scheme is\nacquired with the given information on joint caching and computing policy. With\nalternative optimization, an optimal policy for joint caching, computing and\ncommunication based on user interest can be finally obtained. Simulation\nresults are presented to demonstrate the superiority of the proposed user\ninterest-aware caching scheme and the effective of the joint caching, computing\nand communication optimization policy with consideration of user fairness.\n","authors":["Baojie Fu","Tong Tang","Dapeng Wu","Ruyan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05834v1","updated":"2024-03-09T08:36:28Z","published":"2024-03-09T08:36:28Z","title":"Enhancing Expressiveness in Dance Generation via Integrating Frequency\n  and Music Style Information","summary":"  Dance generation, as a branch of human motion generation, has attracted\nincreasing attention. Recently, a few works attempt to enhance dance\nexpressiveness, which includes genre matching, beat alignment, and dance\ndynamics, from certain aspects. However, the enhancement is quite limited as\nthey lack comprehensive consideration of the aforementioned three factors. In\nthis paper, we propose ExpressiveBailando, a novel dance generation method\ndesigned to generate expressive dances, concurrently taking all three factors\ninto account. Specifically, we mitigate the issue of speed homogenization by\nincorporating frequency information into VQ-VAE, thus improving dance dynamics.\nAdditionally, we integrate music style information by extracting genre- and\nbeat-related features with a pre-trained music model, hence achieving\nimprovements in the other two factors. Extensive experimental results\ndemonstrate that our proposed method can generate dances with high\nexpressiveness and outperforms existing methods both qualitatively and\nquantitatively.\n","authors":["Qiaochu Huang","Xu He","Boshi Tang","Haolin Zhuang","Liyang Chen","Shuochen Gao","Zhiyong Wu","Haozhi Huang","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2403.05834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05768v1","updated":"2024-03-09T02:33:38Z","published":"2024-03-09T02:33:38Z","title":"Deep Contrastive Multi-view Clustering under Semantic Feature Guidance","summary":"  Contrastive learning has achieved promising performance in the field of\nmulti-view clustering recently. However, the positive and negative sample\nconstruction mechanisms ignoring semantic consistency lead to false negative\npairs, limiting the performance of existing algorithms from further\nimprovement. To solve this problem, we propose a multi-view clustering\nframework named Deep Contrastive Multi-view Clustering under Semantic feature\nguidance (DCMCS) to alleviate the influence of false negative pairs.\nSpecifically, view-specific features are firstly extracted from raw features\nand fused to obtain fusion view features according to view importance. To\nmitigate the interference of view-private information, specific view and fusion\nview semantic features are learned by cluster-level contrastive learning and\nconcatenated to measure the semantic similarity of instances. By minimizing\ninstance-level contrastive loss weighted by semantic similarity, DCMCS\nadaptively weakens contrastive leaning between false negative pairs.\nExperimental results on several public datasets demonstrate the proposed\nframework outperforms the state-of-the-art methods.\n","authors":["Siwen Liu","Jinyan Liu","Hanning Yuan","Qi Li","Jing Geng","Ziqiang Yuan","Huaxu Han"],"pdf_url":"https://arxiv.org/pdf/2403.05768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08824v1","updated":"2024-03-09T11:16:09Z","published":"2024-03-09T11:16:09Z","title":"Measuring Non-Typical Emotions for Mental Health: A Survey of\n  Computational Approaches","summary":"  Analysis of non-typical emotions, such as stress, depression and engagement\nis less common and more complex compared to that of frequently discussed\nemotions like happiness, sadness, fear, and anger. The importance of these\nnon-typical emotions has been increasingly recognized due to their implications\non mental health and well-being. Stress and depression impact the engagement in\ndaily tasks, highlighting the need to understand their interplay. This survey\nis the first to simultaneously explore computational methods for analyzing\nstress, depression, and engagement. We discuss the most commonly used datasets,\ninput modalities, data processing techniques, and information fusion methods\nused for the computational analysis of stress, depression and engagement. A\ntimeline and taxonomy of non-typical emotion analysis approaches along with\ntheir generic pipeline and categories are presented. Subsequently, we describe\nstate-of-the-art computational approaches for non-typical emotion analysis,\nincluding a performance summary on the most commonly used datasets. Following\nthis, we explore the applications, along with the associated challenges,\nlimitations, and future research directions.\n","authors":["Puneet Kumar","Alexander Vedernikov","Xiaobai Li"],"pdf_url":"https://arxiv.org/pdf/2403.08824v1.pdf","comment":"Under review in IEEE Transactions on Affective Computing"}]},"2024-03-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.13254v2","updated":"2024-03-12T17:59:56Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v2.pdf","comment":"13 pages, 6 figures, 8 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2403.07872v1","updated":"2024-03-12T17:59:48Z","published":"2024-03-12T17:59:48Z","title":"Rethinking Generative Large Language Model Evaluation for Semantic\n  Comprehension","summary":"  Despite their sophisticated capabilities, large language models (LLMs)\nencounter a major hurdle in effective assessment. This paper first revisits the\nprevalent evaluation method-multiple choice question answering (MCQA), which\nallows for straightforward accuracy measurement. Through a comprehensive\nevaluation of 24 models across 11 benchmarks, we highlight several potential\ndrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation\nand the generation of open-ended responses in practical scenarios. In response,\nwe introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,\nGoogle-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with\nGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This\nsystem is designed to mirror real-world usage, and for this purpose, we have\ncompiled a new benchmark called ``Real-world questions'' (RWQ), comprising\n20,772 authentic user inquiries. Additionally, we thoroughly analyze the\ncharacteristics of our system and compare it with prior leaderboards like\nAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo\nsystem, the feasibility of registering new models, and its potential to reshape\nLLM leaderboards.\n","authors":["Fangyun Wei","Xi Chen","Lin Luo"],"pdf_url":"https://arxiv.org/pdf/2403.07872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07865v1","updated":"2024-03-12T17:55:38Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80\\% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13165v3","updated":"2024-03-12T17:41:13Z","published":"2024-01-24T00:58:30Z","title":"Misgendering and Assuming Gender in Machine Translation when Working\n  with Low-Resource Languages","summary":"  This chapter focuses on gender-related errors in machine translation (MT) in\nthe context of low-resource languages. We begin by explaining what low-resource\nlanguages are, examining the inseparable social and computational factors that\ncreate such linguistic hierarchies. We demonstrate through a case study of our\nmother tongue Bengali, a global language spoken by almost 300 million people\nbut still classified as low-resource, how gender is assumed and inferred in\ntranslations to and from the high(est)-resource English when no such\ninformation is provided in source texts. We discuss the postcolonial and\nsocietal impacts of such errors leading to linguistic erasure and\nrepresentational harms, and conclude by discussing potential solutions towards\nuplifting languages by providing them more agency in MT conversations.\n","authors":["Sourojit Ghosh","Srishti Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2401.13165v3.pdf","comment":"Upcoming Publication, Gendered Technology in Translation and\n  Interpreting Centering Rights in the Development of Language Technology,\n  Routledge 2024"},{"id":"http://arxiv.org/abs/2403.07825v1","updated":"2024-03-12T17:04:28Z","published":"2024-03-12T17:04:28Z","title":"The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage\n  Brought By Model Editing","summary":"  Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy.However, the editing of these models, crucial for\nrectifying outdated or erroneous information, often leads to a complex issue\nknown as the ripple effect in the hidden space. This effect, while difficult to\ndetect, can significantly impede the efficacy of model editing tasks and\ndeteriorate model performance.This paper addresses this scientific challenge by\nproposing a novel evaluation methodology, Graphical Outlier Relation based\nAssessment(GORA), which quantitatively evaluates the adaptations of the model\nand the subsequent impact of editing. Furthermore, we introduce the Selective\nOutlier Re-Editing Approach(SORA), a model editing method designed to mitigate\nthis ripple effect. Our comprehensive evaluations reveal that the ripple effect\nin the hidden space is a significant issue in all current model editing\nmethods. However, our proposed methods, GORA and SORA, effectively identify and\nalleviate this issue, respectively, contributing to the advancement of LLM\nediting techniques.\n","authors":["Jianchen Wang","Zhouhong Gu","Zhuozhi Xiong","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.07825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02129v3","updated":"2024-03-12T16:58:53Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07816v1","updated":"2024-03-12T16:54:58Z","published":"2024-03-12T16:54:58Z","title":"Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM","summary":"  We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff.\n","authors":["Sainbayar Sukhbaatar","Olga Golovneva","Vasu Sharma","Hu Xu","Xi Victoria Lin","Baptiste Rozière","Jacob Kahn","Daniel Li","Wen-tau Yih","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2403.07816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06602v2","updated":"2024-03-12T16:54:57Z","published":"2023-11-11T16:16:11Z","title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance","summary":"  Answering questions within business and finance requires reasoning,\nprecision, and a wide-breadth of technical knowledge. Together, these\nrequirements make this domain difficult for large language models (LLMs). We\nintroduce BizBench, a benchmark for evaluating models' ability to reason about\nrealistic financial problems. BizBench comprises eight quantitative reasoning\ntasks, focusing on question-answering (QA) over financial data via program\nsynthesis. We include three financially-themed code-generation tasks from newly\ncollected and augmented QA data. Additionally, we isolate the reasoning\ncapabilities required for financial QA: reading comprehension of financial text\nand tables for extracting intermediate values, and understanding financial\nconcepts and formulas needed to calculate complex solutions. Collectively,\nthese tasks evaluate a model's financial background knowledge, ability to parse\nfinancial documents, and capacity to solve problems with code. We conduct an\nin-depth evaluation of open-source and commercial LLMs, comparing and\ncontrasting the behavior of code-focused and language-focused models. We\ndemonstrate that the current bottleneck in performance is due to LLMs' limited\nbusiness and financial understanding, highlighting the value of a challenging\nbenchmark for quantitative reasoning within this domain.\n","authors":["Rik Koncel-Kedziorski","Michael Krumdick","Viet Lai","Varshini Reddy","Charles Lovering","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2311.06602v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.07809v1","updated":"2024-03-12T16:46:54Z","published":"2024-03-12T16:46:54Z","title":"pyvene: A Library for Understanding and Improving PyTorch Models via\n  Interventions","summary":"  Interventions on model-internal states are fundamental operations in many\nareas of AI, including model editing, steering, robustness, and\ninterpretability. To facilitate such research, we introduce $\\textbf{pyvene}$,\nan open-source Python library that supports customizable interventions on a\nrange of different PyTorch modules. $\\textbf{pyvene}$ supports complex\nintervention schemes with an intuitive configuration format, and its\ninterventions can be static or include trainable parameters. We show how\n$\\textbf{pyvene}$ provides a unified and extensible framework for performing\ninterventions on neural models and sharing the intervened upon models with\nothers. We illustrate the power of the library via interpretability analyses\nusing causal abstraction and knowledge localization. We publish our library\nthrough Python Package Index (PyPI) and provide code, documentation, and\ntutorials at https://github.com/stanfordnlp/pyvene.\n","authors":["Zhengxuan Wu","Atticus Geiger","Aryaman Arora","Jing Huang","Zheng Wang","Noah D. Goodman","Christopher D. Manning","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2403.07809v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.07805v1","updated":"2024-03-12T16:42:44Z","published":"2024-03-12T16:42:44Z","title":"Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models","summary":"  Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.\ncom/sail-sg/lm-random-memory-access.\n","authors":["Tongyao Zhu","Qian Liu","Liang Pang","Zhengbao Jiang","Min-Yen Kan","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2403.07805v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.18603v3","updated":"2024-03-12T16:35:25Z","published":"2024-02-28T08:29:42Z","title":"MMSR: Symbolic Regression is a Multimodal Task","summary":"  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n","authors":["Yanjie Li","Jingyi Liu","Weijun Li","Lina Yu","Min Wu","Wenqiang Li","Meilan Hao","Su Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2402.18603v3.pdf","comment":"12 page"},{"id":"http://arxiv.org/abs/2403.07794v1","updated":"2024-03-12T16:33:30Z","published":"2024-03-12T16:33:30Z","title":"Fine-tuning Large Language Models with Sequential Instructions","summary":"  Large language models (LLMs) struggle to follow a sequence of instructions in\na single query as they may ignore or misinterpret part of it. This impairs\ntheir performance in complex problems whose solution requires multiple\nintermediate steps, such as multilingual (translate then answer) and multimodal\n(caption then answer) tasks. We empirically verify this with open-source LLMs\nas large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential\ninstructions in present-day data, we propose sequential instruction tuning, a\nsimple yet effective strategy to automatically augment instruction tuning data\nand equip LLMs with the ability to execute multiple sequential instructions.\nAfter exploring interleaving instructions in existing datasets, such as Alpaca,\nwith a wide range of intermediate tasks, we find that sequential\ninstruction-tuned models consistently outperform the conventional\ninstruction-tuned baselines in downstream tasks involving reasoning,\nmultilingual, and multimodal abilities. To shed further light on our technique,\nwe analyse how adversarial intermediate texts, unseen tasks, prompt\nverbalization, number of tasks, and prompt length affect SIT. We hope that this\nmethod will open new research avenues on instruction tuning for complex tasks.\n","authors":["Hanxu Hu","Pinzhen Chen","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2403.07794v1.pdf","comment":"11pages, 3 figures"},{"id":"http://arxiv.org/abs/2306.08543v3","updated":"2024-03-12T16:15:19Z","published":"2023-06-14T14:44:03Z","title":"Knowledge Distillation of Large Language Models","summary":"  Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge of white-box LLMs into small models is still\nunder-explored, which becomes more important with the prosperity of open-source\nLLMs. In this work, we propose a KD approach that distills LLMs into smaller\nlanguage models. We first replace the forward Kullback-Leibler divergence (KLD)\nobjective in the standard KD approaches with reverse KLD, which is more\nsuitable for KD on generative language models, to prevent the student model\nfrom overestimating the low-probability regions of the teacher distribution.\nThen, we derive an effective optimization approach to learn this objective. The\nstudent models are named MiniLLM. Extensive experiments in the\ninstruction-following setting show that MiniLLM generates more precise\nresponses with higher overall quality, lower exposure bias, better calibration,\nand higher long-text generation performance than the baselines. Our method is\nscalable for different model families with 120M to 13B parameters. Our code,\ndata, and model checkpoints can be found in\nhttps://github.com/microsoft/LMOps/tree/main/minillm.\n","authors":["Yuxian Gu","Li Dong","Furu Wei","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2306.08543v3.pdf","comment":"Published as a conference paper in ICLR 2024"},{"id":"http://arxiv.org/abs/2402.12177v4","updated":"2024-03-12T16:04:23Z","published":"2024-02-19T14:33:24Z","title":"Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning","summary":"  Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.\n","authors":["Mingtian Zhang","Shawn Lan","Peter Hayes","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.12177v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07769v1","updated":"2024-03-12T15:56:10Z","published":"2024-03-12T15:56:10Z","title":"Transforming Competition into Collaboration: The Revolutionary Role of\n  Multi-Agent Systems and Language Models in Modern Organizations","summary":"  This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.\n","authors":["Carlos Jose Xavier Cruz"],"pdf_url":"https://arxiv.org/pdf/2403.07769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10691v3","updated":"2024-03-12T15:53:06Z","published":"2023-09-19T15:25:42Z","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\n  Feedback","summary":"  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n","authors":["Xingyao Wang","Zihan Wang","Jiateng Liu","Yangyi Chen","Lifan Yuan","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.10691v3.pdf","comment":"ICLR 2024. Code is available on our project website:\n  https://xingyaoww.github.io/mint-bench"},{"id":"http://arxiv.org/abs/2403.06914v2","updated":"2024-03-12T15:52:14Z","published":"2024-03-11T17:03:04Z","title":"MEND: Meta dEmonstratioN Distillation for Efficient and Effective\n  In-Context Learning","summary":"  Large Language models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities, where a LLM makes predictions for a given test input\ntogether with a few input-output pairs (demonstrations). Nevertheless, the\ninclusion of demonstrations leads to a quadratic increase in the computational\noverhead of the self-attention mechanism. Existing solutions attempt to distill\nlengthy demonstrations into compact vectors. However, they often require\ntask-specific retraining or compromise LLM's in-context learning performance.\nTo mitigate these challenges, we present Meta dEmonstratioN Distillation\n(MEND), where a language model learns to distill any lengthy demonstrations\ninto vectors without retraining for a new downstream task. We exploit the\nknowledge distillation to enhance alignment between MEND and LLM, achieving\nboth efficiency and effectiveness simultaneously. MEND is endowed with the\nmeta-knowledge of distilling demonstrations through a two-stage training\nprocess, which includes meta-distillation pretraining and fine-tuning.\nComprehensive evaluations across seven diverse ICL task partitions using\ndecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not\nonly matches but often outperforms the Vanilla ICL as well as other\nstate-of-the-art distillation models, while significantly reducing the\ncomputational demands. This innovation promises enhanced scalability and\nefficiency for the practical deployment of large language models\n","authors":["Yichuan Li","Xiyao Ma","Sixing Lu","Kyumin Lee","Xiaohu Liu","Chenlei Guo"],"pdf_url":"https://arxiv.org/pdf/2403.06914v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.11725v2","updated":"2024-03-12T15:48:17Z","published":"2024-01-22T07:07:06Z","title":"Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language\n  Conversion for Language Models","summary":"  Symbols (or more broadly, non-natural language textual representations) such\nas numerical sequences, molecular formulas, and table delimiters widely exist,\nplaying important roles in various tasks such as abstract reasoning, chemical\nproperty prediction, and table question answering. Despite the impressive\nnatural language comprehension capabilities of large language models (LLMs),\ntheir reasoning abilities for symbols remain inadequate, which could attributed\nto the difference between symbol representations and general natural languages.\nWe propose symbol-to-language (S2L), a tuning-free method that enables large\nlanguage models to solve symbol-related problems with information expressed in\nnatural language. Specifically, S2L first converts the symbols involved to\nlanguage-based representations, which can be implemented by prompting LLMs or\nleveraging external tools, then these language-based representations are\nintegrated into the original problem via direct substitution or concatenation,\nserving as useful input information for LLMs. We evaluate the S2L method using\nboth API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight\nsymbol-related tasks, ranging from symbol-only abstract reasoning to sentiment\nanalysis in social media. Experimental results show that S2L consistently leads\nto superior performance. For example, by employing S2L for GPT-4, there can be\naverage significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and\nDyck language, respectively. Codes and data are available at\nhttps://github.com/THUNLP-MT/symbol2language.\n","authors":["Yile Wang","Sijie Cheng","Zixin Sun","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.11725v2.pdf","comment":"ICLR AGI Workshop 2024"},{"id":"http://arxiv.org/abs/2403.07747v1","updated":"2024-03-12T15:32:39Z","published":"2024-03-12T15:32:39Z","title":"FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models","summary":"  To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon.\n","authors":["Yan Liu","Renren Jin","Lin Shi","Zheng Yao","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.07747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07726v1","updated":"2024-03-12T15:06:22Z","published":"2024-03-12T15:06:22Z","title":"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and\n  Related Observable Overgeneration Mistakes","summary":"  This paper presents the results of the SHROOM, a shared task focused on\ndetecting hallucinations: outputs from natural language generation (NLG)\nsystems that are fluent, yet inaccurate. Such cases of overgeneration put in\njeopardy many NLG applications, where correctness is often mission-critical.\nThe shared task was conducted with a newly constructed dataset of 4000 model\noutputs labeled by 5 annotators each, spanning 3 NLP tasks: machine\ntranslation, paraphrase generation and definition modeling.\n  The shared task was tackled by a total of 58 different users grouped in 42\nteams, out of which 27 elected to write a system description paper;\ncollectively, they submitted over 300 prediction sets on both tracks of the\nshared task. We observe a number of key trends in how this approach was tackled\n-- many participants rely on a handful of model, and often rely either on\nsynthetic data for fine-tuning or zero-shot prompting strategies. While a\nmajority of the teams did outperform our proposed baseline system, the\nperformances of top-scoring systems are still consistent with a random handling\nof the more challenging items.\n","authors":["Timothee Mickus","Elaine Zosa","Raúl Vázquez","Teemu Vahtola","Jörg Tiedemann","Vincent Segonne","Alessandro Raganato","Marianna Apidianaki"],"pdf_url":"https://arxiv.org/pdf/2403.07726v1.pdf","comment":"SemEval 2024 shared task. Pre-review version"},{"id":"http://arxiv.org/abs/2403.07714v1","updated":"2024-03-12T14:57:40Z","published":"2024-03-12T14:57:40Z","title":"StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models","summary":"  Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.\n","authors":["Zhicheng Guo","Sijie Cheng","Hao Wang","Shihao Liang","Yujia Qin","Peng Li","Zhiyuan Liu","Maosong Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09582v5","updated":"2024-03-12T14:55:29Z","published":"2023-02-19T14:21:33Z","title":"Language-Specific Representation of Emotion-Concept Knowledge Causally\n  Supports Emotion Inference","summary":"  Humans no doubt use language to communicate about their emotional\nexperiences, but does language in turn help humans understand emotions, or is\nlanguage just a vehicle of communication? This study used a form of artificial\nintelligence (AI) known as large language models (LLMs) to assess whether\nlanguage-based representations of emotion causally contribute to the AI's\nability to generate inferences about the emotional meaning of novel situations.\nFourteen attributes of human emotion concept representation were found to be\nrepresented by the LLM's distinct artificial neuron populations. By\nmanipulating these attribute-related neurons, we in turn demonstrated the role\nof emotion concept knowledge in generative emotion inference. The\nattribute-specific performance deterioration was related to the importance of\ndifferent attributes in human mental space. Our findings provide a\nproof-in-concept that even a LLM can learn about emotions in the absence of\nsensory-motor representations and highlight the contribution of\nlanguage-derived emotion-concept knowledge for emotion inference.\n","authors":["Ming Li","Yusheng Su","Hsiu-Yuan Huang","Jiali Cheng","Xin Hu","Xinmiao Zhang","Huadong Wang","Yujia Qin","Xiaozhi Wang","Kristen A. Lindquist","Zhiyuan Liu","Dan Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.09582v5.pdf","comment":"44 pages, 14 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.07708v1","updated":"2024-03-12T14:51:57Z","published":"2024-03-12T14:51:57Z","title":"Improving Reinforcement Learning from Human Feedback Using Contrastive\n  Rewards","summary":"  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm\nused to align large language models (LLMs) with human preferences. Yet existing\nRLHF heavily relies on accurate and informative reward models, which are\nvulnerable and sensitive to noise from various sources, e.g. human labeling\nerrors, making the pipeline fragile. In this work, we improve the effectiveness\nof the reward model by introducing a penalty term on the reward, named as\n\\textit{contrastive rewards}. %Contrastive rewards Our approach involves two\nsteps: (1) an offline sampling step to obtain responses to prompts that serve\nas baseline calculation and (2) a contrastive reward calculated using the\nbaseline responses and used in the Proximal Policy Optimization (PPO) step. We\nshow that contrastive rewards enable the LLM to penalize reward uncertainty,\nimprove robustness, encourage improvement over baselines, calibrate according\nto task difficulty, and reduce variance in PPO. We show empirically contrastive\nrewards can improve RLHF substantially, evaluated by both GPTs and humans, and\nour method consistently outperforms strong baselines.\n","authors":["Wei Shen","Xiaoying Zhang","Yuanshun Yao","Rui Zheng","Hongyi Guo","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01070v3","updated":"2024-03-12T14:50:30Z","published":"2023-11-02T08:37:30Z","title":"Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech\n  Models via Language-Specific Experts","summary":"  Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we propose DistilWhisper, an approach able to bridge\nthe performance gap in ASR for these languages while retaining the advantages\nof multitask and multilingual capabilities. Our approach involves two key\nstrategies: lightweight modular ASR fine-tuning of whisper-small using\nlanguage-specific experts, and knowledge distillation from whisper-large-v2.\nThis dual approach allows us to effectively boost ASR performance while keeping\nthe robustness inherited from the multitask and multilingual pre-training.\nResults demonstrate that our approach is more effective than standard\nfine-tuning or LoRA adapters, boosting performance in the targeted languages\nfor both in- and out-of-domain test sets, while introducing only a negligible\nparameter overhead at inference.\n","authors":["Thomas Palmeira Ferraz","Marcely Zanon Boito","Caroline Brun","Vassilina Nikoulina"],"pdf_url":"https://arxiv.org/pdf/2311.01070v3.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.07693v1","updated":"2024-03-12T14:37:03Z","published":"2024-03-12T14:37:03Z","title":"Large, Small or Both: A Novel Data Augmentation Framework Based on\n  Language Models for Debiasing Opinion Summarization","summary":"  As more than 70$\\%$ of reviews in the existing opinion summary data set are\npositive, current opinion summarization approaches are reluctant to generate\nnegative summaries given the input of negative texts. To address such sentiment\nbias, a direct approach without the over-reliance on a specific framework is to\ngenerate additional data based on large language models to balance the\nemotional distribution of the dataset. However, data augmentation based on\nlarge language models faces two disadvantages: 1) the potential issues or\ntoxicity in the augmented data; 2) the expensive costs. Therefore, in this\npaper, we propose a novel data augmentation framework based on both large and\nsmall language models for debiasing opinion summarization. In specific, a small\nsize of synthesized negative reviews is obtained by rewriting the positive text\nvia a large language model. Then, a disentangle reconstruction model is trained\nbased on the generated data. After training, a large amount of synthetic data\ncan be obtained by decoding the new representation obtained from the\ncombination of different sample representations and filtering based on\nconfusion degree and sentiment classification. Experiments have proved that our\nframework can effectively alleviate emotional bias same as using only large\nmodels, but more economically.\n","authors":["Yanyue Zhang","Pengfei Li","Yilong Lai","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.07693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07691v1","updated":"2024-03-12T14:34:08Z","published":"2024-03-12T14:34:08Z","title":"Reference-free Monolithic Preference Optimization with Odds Ratio","summary":"  While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.\nWe release code and model checkpoints for Mistral-ORPO-$\\alpha$ (7B) and\nMistral-ORPO-$\\beta$ (7B).\n","authors":["Jiwoo Hong","Noah Lee","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2403.07691v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.07690v1","updated":"2024-03-12T14:33:53Z","published":"2024-03-12T14:33:53Z","title":"SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted\n  Technical Debt","summary":"  Self-admitted technical debt (SATD) refers to a form of technical debt in\nwhich developers explicitly acknowledge and document the existence of technical\nshortcuts, workarounds, or temporary solutions within the codebase. Over recent\nyears, researchers have manually labeled datasets derived from various software\ndevelopment artifacts: source code comments, messages from the issue tracker\nand pull request sections, and commit messages. These datasets are designed for\ntraining, evaluation, performance validation, and improvement of machine\nlearning and deep learning models to accurately identify SATD instances.\nHowever, class imbalance poses a serious challenge across all the existing\ndatasets, particularly when researchers are interested in categorizing the\nspecific types of SATD. In order to address the scarcity of labeled data for\nSATD \\textit{identification} (i.e., whether an instance is SATD or not) and\n\\textit{categorization} (i.e., which type of SATD is being classified) in\nexisting datasets, we share the \\textit{SATDAUG} dataset, an augmented version\nof existing SATD datasets, including source code comments, issue tracker, pull\nrequests, and commit messages. These augmented datasets have been balanced in\nrelation to the available artifacts and provide a much richer source of labeled\ndata for training machine learning or deep learning models.\n","authors":["Edi Sutoyo","Andrea Capiluppi"],"pdf_url":"https://arxiv.org/pdf/2403.07690v1.pdf","comment":"Accepted to be published at the 21st IEEE/ACM International\n  Conference on Mining Software Repositories (MSR 2024)"},{"id":"http://arxiv.org/abs/2403.07687v1","updated":"2024-03-12T14:27:17Z","published":"2024-03-12T14:27:17Z","title":"Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost","summary":"  Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most of this data\ncomes from Western countries, leading to poor results for underrepresented\ncountries. To address this issue, more data needs to be collected from these\ncountries, but the cost of annotation can be a significant bottleneck. In this\npaper, we propose methods to identify the data to be annotated to balance model\nperformance and annotation costs. Our approach first involves finding the\ncountries with images of topics (objects and actions) most visually distinct\nfrom those already in the training datasets used by current large\nvision-language foundation models. Next, we identify countries with higher\nvisual similarity for these topics and show that using data from these\ncountries to supplement the training data improves model performance and\nreduces annotation costs. The resulting lists of countries and corresponding\ntopics are made available at\nhttps://github.com/MichiganNLP/visual_diversity_budget.\n","authors":["Oana Ignat","Longju Bai","Joan Nwatu","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2403.07687v1.pdf","comment":"accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2403.07678v1","updated":"2024-03-12T14:12:59Z","published":"2024-03-12T14:12:59Z","title":"MoralBERT: Detecting Moral Values in Social Discourse","summary":"  Morality plays a fundamental role in how we perceive information while\ngreatly influencing our decisions and judgements. Controversial topics,\nincluding vaccination, abortion, racism, and sexuality, often elicit opinions\nand attitudes that are not solely based on evidence but rather reflect moral\nworldviews. Recent advances in natural language processing have demonstrated\nthat moral values can be gauged in human-generated textual content. Here, we\ndesign a range of language representation models fine-tuned to capture exactly\nthe moral nuances in text, called MoralBERT. We leverage annotated moral data\nfrom three distinct sources: Twitter, Reddit, and Facebook user-generated\ncontent covering various socially relevant topics. This approach broadens\nlinguistic diversity and potentially enhances the models' ability to comprehend\nmorality in various contexts. We also explore a domain adaptation technique and\ncompare it to the standard fine-tuned BERT model, using two different\nframeworks for moral prediction: single-label and multi-label. We compare\nin-domain approaches with conventional models relying on lexicon-based\ntechniques, as well as a Machine Learning classifier with Word2Vec\nrepresentation. Our results showed that in-domain prediction models\nsignificantly outperformed traditional models. While the single-label setting\nreaches a higher accuracy than previously achieved for the task when using BERT\npretrained models. Experiments in an out-of-domain setting, instead, suggest\nthat further work is needed for existing domain adaptation techniques to\ngeneralise between different social media platforms, especially for the\nmulti-label task. The investigations and outcomes from this study pave the way\nfor further exploration, enabling a more profound comprehension of moral\nnarratives about controversial social issues.\n","authors":["Vjosa Preniqi","Iacopo Ghinassi","Kyriaki Kalimeri","Charalampos Saitis"],"pdf_url":"https://arxiv.org/pdf/2403.07678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12243v3","updated":"2024-03-12T13:52:13Z","published":"2024-02-19T15:58:15Z","title":"Understanding the Effects of Noise in Text-to-SQL: An Examination of the\n  BIRD-Bench Benchmark","summary":"  Text-to-SQL, which involves translating natural language into Structured\nQuery Language (SQL), is crucial for enabling broad access to structured\ndatabases without expert knowledge. However, designing models for such tasks is\nchallenging due to numerous factors, including the presence of 'noise,' such as\nambiguous questions and syntactical errors. This study provides an in-depth\nanalysis of the distribution and types of noise in the widely used BIRD-Bench\nbenchmark and the impact of noise on models. While BIRD-Bench was created to\nmodel dirty and noisy database values, it was not created to contain noise and\nerrors in the questions and gold queries. We found that noise in questions and\ngold queries are prevalent in the dataset, with varying amounts across domains,\nand with an uneven distribution between noise types. The presence of incorrect\ngold SQL queries, which then generate incorrect gold answers, has a significant\nimpact on the benchmark's reliability. Surprisingly, when evaluating models on\ncorrected SQL queries, zero-shot baselines surpassed the performance of\nstate-of-the-art prompting methods. We conclude that informative noise labels\nand reliable benchmarks are crucial to developing new Text-to-SQL methods that\ncan handle varying types of noise.\n","authors":["Niklas Wretblad","Fredrik Gordh Riseby","Rahul Biswas","Amin Ahmadi","Oskar Holmström"],"pdf_url":"https://arxiv.org/pdf/2402.12243v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07652v1","updated":"2024-03-12T13:41:15Z","published":"2024-03-12T13:41:15Z","title":"Harder Tasks Need More Experts: Dynamic Routing in MoE Models","summary":"  In this paper, we introduce a novel dynamic expert selection framework for\nMixture of Experts (MoE) models, aiming to enhance computational efficiency and\nmodel performance by adjusting the number of activated experts based on input\ndifficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,\nwhich activates a predetermined number of experts regardless of the input's\ncomplexity, our method dynamically selects experts based on the confidence\nlevel in expert selection for each input. This allows for a more efficient\nutilization of computational resources, activating more experts for complex\ntasks requiring advanced reasoning and fewer for simpler tasks. Through\nextensive evaluations, our dynamic routing method demonstrates substantial\nimprovements over conventional Top-2 routing across various benchmarks,\nachieving an average improvement of 0.7% with less than 90% activated\nparameters. Further analysis shows our model dispatches more experts to tasks\nrequiring complex reasoning skills, like BBH, confirming its ability to\ndynamically allocate computational resources in alignment with the input's\ncomplexity. Our findings also highlight a variation in the number of experts\nneeded across different layers of the transformer model, offering insights into\nthe potential for designing heterogeneous MoE frameworks. The code and models\nare available at https://github.com/ZhenweiAn/Dynamic_MoE.\n","authors":["Quzhe Huang","Zhenwei An","Nan Zhuang","Mingxu Tao","Chen Zhang","Yang Jin","Kun Xu","Kun Xu","Liwei Chen","Songfang Huang","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03646v2","updated":"2024-03-12T13:38:31Z","published":"2023-10-05T16:21:36Z","title":"TRAM: Bridging Trust Regions and Sharpness Aware Minimization","summary":"  Sharpness-aware minimization (SAM) reports improving domain generalization by\nreducing the loss surface curvature in the parameter space. However,\ngeneralization during fine-tuning is often more dependent on the\ntransferability of representations in the function space. Trust-region methods\n(TR) target this goal by regularizing representation curvature to reduce\ncatastrophic forgetting of pre-trained task-agnostic information while adopting\ntask-specific skills. We consider unifying these strategies for low curvature\nin both parameter space and function space to improve out-of-domain (OOD)\ngeneralization. We propose Trust Region Aware Minimization (TRAM), a SAM\nalgorithm fine-tuning for low parameter sharpness and smooth, informative\nrepresentations preserving pre-trained structure. TRAM uses a trust region\nbound to inform the SAM adversarial neighborhood, introducing an awareness of\nfunction curvature within optimization for flatter minima. We empirically\nvalidate TRAM in vision (cross-dataset adaptation) and text (OOD language\nmodeling, zero-shot cross-lingual transfer) tasks where robust domain transfer\nand representation generality are critical. TRAM outperforms SAM- and TR-based\noptimization across all tasks, notably surpassing competing methods for hard\ntransfer between anticorrelated domains. TRAM establishes a novel standard in\nfine-tuning for domain-generalizable models with minimal additional computation\nover previous sharpness-aware methods.\n","authors":["Tom Sherborne","Naomi Saphra","Pradeep Dasigi","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2310.03646v2.pdf","comment":"Camera Ready for ICLR 2024 (Accepted as Spotlight). 21 pages, 14\n  tables, 2 figures"},{"id":"http://arxiv.org/abs/2212.07249v3","updated":"2024-03-12T13:30:16Z","published":"2022-12-14T14:34:15Z","title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning","summary":"  Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.\n","authors":["Jiashuo Sun","Hang Zhang","Chen Lin","Xiangdong Su","Yeyun Gong","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2212.07249v3.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2309.11911v4","updated":"2024-03-12T12:54:36Z","published":"2023-09-21T09:22:07Z","title":"InstructERC: Reforming Emotion Recognition in Conversation with a\n  Retrieval Multi-task LLMs Framework","summary":"  The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach,\n  \\textbf{InstructERC}, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\n  InstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios. Our code and aligned\nunified dataset (UIME) can be found in the Github link.\\footnote{You can find\nthe offical realization in the Github link:\nhttps://github.com/LIN-SHANG/InstructERC}\n","authors":["Shanglin Lei","Guanting Dong","Xiaoping Wang","Keheng Wang","Sirui Wang"],"pdf_url":"https://arxiv.org/pdf/2309.11911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19282v5","updated":"2024-03-12T12:27:52Z","published":"2024-02-29T15:49:15Z","title":"WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset","summary":"  This paper presents WanJuan-CC, a safe and high-quality open-sourced English\nwebtext dataset derived from Common Crawl data. The study addresses the\nchallenges of constructing large-scale pre-training datasets for language\nmodels, which require vast amounts of high-quality data. A comprehensive\nprocess was designed to handle Common Crawl data, including extraction,\nheuristic rule filtering, fuzzy deduplication, content safety filtering, and\ndata quality filtering. From approximately 68 billion original English\ndocuments, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of\nhigh-quality data as part of WanJuan-CC. We have open-sourced 100B Tokens from\nthis dataset. The paper also provides statistical information related to data\nquality, enabling users to select appropriate data according to their needs. To\nevaluate the quality and utility of the dataset, we trained 1B-parameter and\n3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Results\nshow that WanJuan-CC performs better on validation datasets and downstream\ntasks.\n","authors":["Jiantao Qiu","Haijun Lv","Zhenjiang Jin","Rui Wang","Wenchang Ning","Jia Yu","ChaoBin Zhang","Zhenxiang Li","Pei Chu","Yuan Qu","Jin Shi","Lindong Lu","Runyu Peng","Zhiyuan Zeng","Huanze Tang","Zhikai Lei","Jiawei Hong","Keyu Chen","Zhaoye Fei","Ruiliang Xu","Wei Li","Zhongying Tu","Hang Yan","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2402.19282v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05326v2","updated":"2024-03-12T12:12:36Z","published":"2024-03-08T14:05:36Z","title":"ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in\n  Dialogues","summary":"  Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,\nQuestion-Answering and Dialogue) has attracted ever-more interest in recent\nyears and achieved important progresses. However, existing studies on\ninteractive ASU largely ignore the coreference issue for opinion targets (i.e.,\naspects), while this phenomenon is ubiquitous in interactive scenarios\nespecially dialogues, limiting the ASU performance. Recently, large language\nmodels (LLMs) shows the powerful ability to integrate various NLP tasks with\nthe chat paradigm. In this way, this paper proposes a new Chat-based Aspect\nSentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in\nunderstanding aspect sentiments in dialogue scenarios. Particularly, this\nChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to\naddress the aspect coreference issue. On this basis, we propose a Trusted\nSelf-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.\nSpecifically, this TSA treats the ACR task as an auxiliary task to boost the\nperformance of the primary ASU task, and further integrates trusted learning\ninto reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination\nproblem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to\nevaluate TSA, and extensive experiments show that our proposed TSA can\nsignificantly outperform several state-of-the-art baselines, justifying the\neffectiveness of TSA to ChatASU and the importance of considering the\ncoreference and hallucination issues in ChatASU.\n","authors":["Yiding Liu","Jingjing Wang","Jiamin Luo","Tao Zeng","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07581v1","updated":"2024-03-12T12:10:18Z","published":"2024-03-12T12:10:18Z","title":"LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced\n  Personality Detection Model","summary":"  Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.\n","authors":["Linmei Hu","Hongyu He","Duokang Wang","Ziwang Zhao","Yingxia Shao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2403.07581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16480v2","updated":"2024-03-12T12:07:39Z","published":"2023-11-27T05:05:41Z","title":"WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images","summary":"  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text\npairs for visual-language models by recognizing and cleaning pathology reports\nwhich narrate diagnostic slides in TCGA. On the model end, we propose the\nmultiple instance generative model (MI-Gen) which can produce pathology reports\nfor gigapixel WSIs. We benchmark our model on the largest subset of\nTCGA-PathoText. Experimental results show our model can generate pathology\nreports which contain multiple clinical clues. Furthermore, WSI-text prediction\ncan be seen as an approach of visual-language pre-training, which enables our\nmodel to be transferred to downstream diagnostic tasks like carcinoma grading\nand phenotyping. We observe that simple semantic extraction from the pathology\nreports can achieve the best performance (0.838 of F1 score) on BRCA subtyping\nwithout adding extra parameters or tricky fine-tuning. Our collected dataset\nand related code are available.\n","authors":["Pingyi Chen","Honglin Li","Chenglu Zhu","Sunyi Zheng","Zhongyi Shui","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.16480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07567v1","updated":"2024-03-12T11:53:27Z","published":"2024-03-12T11:53:27Z","title":"Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource\n  Agglutinative Data-to-Text Generation","summary":"  Most data-to-text datasets are for English, so the difficulties of modelling\ndata-to-text for low-resource languages are largely unexplored. In this paper\nwe tackle data-to-text for isiXhosa, which is low-resource and agglutinative.\nWe introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of\nWebNLG, which presents a new linguistic context that shifts modelling demands\nto subword-driven techniques. We also develop an evaluation framework for T2X\nthat measures how accurately generated text describes the data. This enables\nfuture users of T2X to go beyond surface-level metrics in evaluation. On the\nmodelling side we explore two classes of methods - dedicated data-to-text\nmodels trained from scratch and pretrained language models (PLMs). We propose a\nnew dedicated architecture aimed at agglutinative data-to-text, the Subword\nSegmental Pointer Generator (SSPG). It jointly learns to segment words and copy\nentities, and outperforms existing dedicated models for 2 agglutinative\nlanguages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,\nwhich reveals that standard PLMs come up short. Fine-tuning machine translation\nmodels emerges as the best method overall. These findings underscore the\ndistinct challenge presented by T2X: neither well-established data-to-text\narchitectures nor customary pretrained methodologies prove optimal. We conclude\nwith a qualitative analysis of generation errors and an ablation study.\n","authors":["Francois Meyer","Jan Buys"],"pdf_url":"https://arxiv.org/pdf/2403.07567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07557v1","updated":"2024-03-12T11:41:51Z","published":"2024-03-12T11:41:51Z","title":"SIFiD: Reassess Summary Factual Inconsistency Detection with LLM","summary":"  Ensuring factual consistency between the summary and the original document is\nparamount in summarization tasks. Consequently, considerable effort has been\ndedicated to detecting inconsistencies. With the advent of Large Language\nModels (LLMs), recent studies have begun to leverage their advanced language\nunderstanding capabilities for inconsistency detection. However, early attempts\nhave shown that LLMs underperform traditional models due to their limited\nability to follow instructions and the absence of an effective detection\nmethodology. In this study, we reassess summary inconsistency detection with\nLLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in\nLLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency\nDetection with Filtered Document) that identify key sentences within documents\nby either employing natural language inference or measuring semantic similarity\nbetween summaries and documents.\n","authors":["Jiuding Yang","Hui Liu","Weidong Guo","Zhuwei Rao","Yu Xu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2403.07557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07556v1","updated":"2024-03-12T11:40:44Z","published":"2024-03-12T11:40:44Z","title":"Truth-Aware Context Selection: Mitigating the Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts","summary":"  Although large language models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by the untruthful context\nprovided by users or knowledge argumentation tools, thereby producing\nhallucinations. To alleviate the LLMs from being misled by untruthful\ninformation and take advantage of knowledge argumentation, we propose\nTruth-Aware Context Selection (TACS), a lightweight method to shield untruthful\ncontext from the inputs. TACS begins by performing truth detection on the input\ncontext, leveraging the parameterized knowledge within the LLM. Subsequently,\nit constructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results show that TACS can effectively\nfilter information in context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.\n","authors":["Tian Yu","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07556v1.pdf","comment":"Code is available at: https://github.com/ictnlp/TACS"},{"id":"http://arxiv.org/abs/2403.07544v1","updated":"2024-03-12T11:32:30Z","published":"2024-03-12T11:32:30Z","title":"MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki","summary":"  NLP in the age of monolithic large language models is approaching its limits\nin terms of size and information that can be handled. The trend goes to\nmodularization, a necessary step into the direction of designing smaller\nsub-networks and components with specialized functionality. In this paper, we\npresent the MAMMOTH toolkit: a framework designed for training massively\nmultilingual modular machine translation systems at scale, initially derived\nfrom OpenNMT-py and then adapted to ensure efficient training across\ncomputation clusters. We showcase its efficiency across clusters of A100 and\nV100 NVIDIA GPUs, and discuss our design philosophy and plans for future\ninformation. The toolkit is publicly available online.\n","authors":["Timothee Mickus","Stig-Arne Grönroos","Joseph Attieh","Michele Boggia","Ona De Gibert","Shaoxiong Ji","Niki Andreas Lopi","Alessandro Raganato","Raúl Vázquez","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2403.07544v1.pdf","comment":"Presented as a demo at EACL 2024"},{"id":"http://arxiv.org/abs/2403.05820v2","updated":"2024-03-12T11:26:07Z","published":"2024-03-09T06:59:47Z","title":"An Audio-textual Diffusion Model For Converting Speech Signals Into\n  Ultrasound Tongue Imaging Data","summary":"  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator\nmovements, such as ultrasound tongue imaging (UTI) data. An issue of existing\nAAI methods is only using the personalized acoustic information to derive the\ngeneral patterns of tongue motions, and thus the quality of generated UTI data\nis limited. To address this issue, this paper proposes an audio-textual\ndiffusion model for the UTI data generation task. In this model, the inherent\nacoustic characteristics of individuals related to the tongue motion details\nare encoded by using wav2vec 2.0, while the ASR transcriptions related to the\nuniversality of tongue motions are encoded by using BERT. UTI data are then\ngenerated by using a diffusion module. Experimental results showed that the\nproposed diffusion model could generate high-quality UTI data with clear tongue\ncontour that is crucial for the linguistic analysis and clinical assessment.\nThe project can be found on the\nwebsite\\footnote{https://yangyudong2020.github.io/wav2uti/\n","authors":["Yudong Yang","Rongfeng Su","Xiaokang Liu","Nan Yan","Lan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05820v2.pdf","comment":"ICASSP2024 Accept"},{"id":"http://arxiv.org/abs/2403.06412v2","updated":"2024-03-12T10:33:06Z","published":"2024-03-11T03:54:33Z","title":"CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in\n  Korean","summary":"  Despite the rapid development of large language models (LLMs) for the Korean\nlanguage, there remains an obvious lack of benchmark datasets that test the\nrequisite Korean cultural and linguistic knowledge. Because many existing\nKorean benchmark datasets are derived from the English counterparts through\ntranslation, they often overlook the different cultural contexts. For the few\nbenchmark datasets that are sourced from Korean data capturing cultural\nknowledge, only narrow tasks such as bias and hate speech detection are\noffered. To address this gap, we introduce a benchmark of Cultural and\nLinguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.\nCLIcK sources its data from official Korean exams and textbooks, partitioning\nthe questions into eleven categories under the two main categories of language\nand culture. For each instance in CLIcK, we provide fine-grained annotation of\nwhich cultural and linguistic knowledge is required to answer the question\ncorrectly. Using CLIcK, we test 13 language models to assess their performance.\nOur evaluation uncovers insights into their performances across the categories,\nas well as the diverse factors affecting their comprehension. CLIcK offers the\nfirst large-scale comprehensive Korean-centric analysis of LLMs' proficiency in\nKorean culture and language.\n","authors":["Eunsu Kim","Juyoung Suk","Philhoon Oh","Haneul Yoo","James Thorne","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2403.06412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11432v3","updated":"2024-03-12T09:51:35Z","published":"2023-08-22T13:30:37Z","title":"A Survey on Large Language Model based Autonomous Agents","summary":"  Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n","authors":["Lei Wang","Chen Ma","Xueyang Feng","Zeyu Zhang","Hao Yang","Jingsen Zhang","Zhiyuan Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.11432v3.pdf","comment":"35 pages, 5 figures, 3 tables, has been accepted by frontiers of\n  computer science (FCS), doi={10.1007/s11704-024-40231-1}"},{"id":"http://arxiv.org/abs/2403.01548v3","updated":"2024-03-12T09:49:28Z","published":"2024-03-03T15:53:41Z","title":"In-Context Sharpness as Alerts: An Inner Representation Perspective for\n  Hallucination Mitigation","summary":"  Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.\n","authors":["Shiqi Chen","Miao Xiong","Junteng Liu","Zhengxuan Wu","Teng Xiao","Siyang Gao","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2403.01548v3.pdf","comment":"code repo is available at:\n  https://github.com/hkust-nlp/Activation_decoding.git"},{"id":"http://arxiv.org/abs/2403.07440v1","updated":"2024-03-12T09:32:25Z","published":"2024-03-12T09:32:25Z","title":"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\n  Brain-Inspired Method for Parameter-Efficient Fine-Tuning","summary":"  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\nbeen proven to significantly enhance model performance on a variety of\ndownstream tasks and effectively control the output behaviors of LPLMs. Recent\nstudies have proposed numerous methods for fine-tuning a small number of\nparameters based on open-source LPLMs, reducing the demand for computational\nand storage resources. Among these, reparameterization fine-tuning methods\nrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\nalthough these methods perform well in many aspects, there is still\nconsiderable room for improvement in terms of complex task adaptability,\nperformance, stability, and algorithm complexity. In response to this, inspired\nby the idea that the functions of the brain are shaped by its geometric\nstructure, this paper integrates this idea into LoRA technology and proposes a\nnew matrix transformation-based reparameterization method for efficient\nfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\nMTLoRA aims to dynamically alter its spatial geometric structure by applying a\ntransformation-matrix T to perform linear transformations, such as rotation,\nscaling, and translation, on the task-specific parameter matrix, generating new\nmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\ncomplex geometric structure feature patterns in the brain on functions, thereby\nenhancing the model's performance in downstream tasks. In Natural Language\nUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\nthe results reveal that MTLoRA achieves an overall performance increase of\nabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\nMTLoRA improves performance by an average of 0.95% and 0.31% in the DART and\nWebNLG tasks, respectively.\n","authors":["Yao Liang","Yuwei Wang","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.07440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13040v5","updated":"2024-03-12T08:52:02Z","published":"2023-05-22T13:47:51Z","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\n  Dialogue Agents","summary":"  Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/.\n","authors":["Shuzheng Si","Wentao Ma","Haoyu Gao","Yuchuan Wu","Ting-En Lin","Yinpei Dai","Hangyu Li","Rui Yan","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2305.13040v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2211.04118v3","updated":"2024-03-12T08:29:41Z","published":"2022-11-08T09:29:45Z","title":"ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning","summary":"  The prompt has become an effective linguistic tool for utilizing pre-trained\nlanguage models. However, in few-shot scenarios, subtle changes in the prompt\ndesign always make the result widely different, and the prompt learning methods\nalso make it easy to overfit the limited samples. To alleviate this, we explore\nutilizing suitable contrastive samples and multi-degree contrastive learning\nmethods to improve the robustness of the prompt representation. Therefore, the\nproposed Consprompt combined with the prompt encoding network, contrastive\nsampling modules, and contrastive scoring modules, is introduced to realize\ndifferential contrastive learning. Our results exhibit state-of-the-art\nperformance in different few-shot settings, and the ablation experiments also\ncertify the effectiveness of utilizing multi-degree contrastive learning in the\nprompt-based fine-tuning process.\n","authors":["Jinta Weng","Yifan Deng","d Donghao Li","Hao You","Yue Hu","Heyan Huang"],"pdf_url":"https://arxiv.org/pdf/2211.04118v3.pdf","comment":"2 figures"},{"id":"http://arxiv.org/abs/2403.07398v1","updated":"2024-03-12T08:13:52Z","published":"2024-03-12T08:13:52Z","title":"Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs","summary":"  Event commonsense reasoning requires the ability to reason about the\nrelationship between events, as well as infer implicit context underlying that\nrelationship. However, data scarcity makes it challenging for language models\nto learn to generate commonsense inferences for contexts and questions\ninvolving interactions between complex events. To address this demand, we\npresent COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop\nlogical queries (e.g., the joint effect or cause of both event A and B, or the\neffect of the effect of event C) from an existing commonsense knowledge graph\n(CSKG), and verbalizing them using handcrafted rules and large language models\ninto multiple-choice and text generation questions. Our experiments show that\nlanguage models trained on COM2 exhibit significant improvements in complex\nreasoning ability, resulting in enhanced zero-shot performance in both\nin-domain and out-of-domain tasks for question answering and generative\ncommonsense reasoning, without expensive human annotations.\n","authors":["Tianqing Fang","Zeming Chen","Yangqiu Song","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2403.07398v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2403.07384v1","updated":"2024-03-12T07:45:33Z","published":"2024-03-12T07:45:33Z","title":"SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models","summary":"  Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.\n","authors":["Yu Yang","Siddhartha Mishra","Jeffrey N Chiang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2403.07384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01841v2","updated":"2024-03-12T07:34:28Z","published":"2024-03-04T08:38:56Z","title":"Making Pre-trained Language Models Great on Tabular Prediction","summary":"  The transferability of deep neural networks (DNNs) has made significant\nprogress in image and language processing. However, due to the heterogeneity\namong tables, such DNN bonus is still far from being well exploited on tabular\ndata prediction (e.g., regression or classification tasks). Condensing\nknowledge from diverse domains, language models (LMs) possess the capability to\ncomprehend feature names from various tables, potentially serving as versatile\nlearners in transferring knowledge across distinct tables and diverse\nprediction tasks, but their discrete text representation space is inherently\nincompatible with numerical feature values in tables. In this paper, we present\nTP-BERTa, a specifically pre-trained LM for tabular data prediction.\nConcretely, a novel relative magnitude tokenization converts scalar numerical\nfeature values to finely discrete, high-dimensional tokens, and an\nintra-feature attention approach integrates feature values with the\ncorresponding feature names. Comprehensive experiments demonstrate that our\npre-trained TP-BERTa leads the performance among tabular DNNs and is\ncompetitive with Gradient Boosted Decision Tree models in typical tabular data\nregime.\n","authors":["Jiahuan Yan","Bo Zheng","Hongxia Xu","Yiheng Zhu","Danny Z. Chen","Jimeng Sun","Jian Wu","Jintai Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01841v2.pdf","comment":"Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).\n  OpenReview link is https://openreview.net/forum?id=anzIzGZuLi, codes will be\n  available at https://github.com/jyansir/tp-berta"},{"id":"http://arxiv.org/abs/2403.07379v1","updated":"2024-03-12T07:32:47Z","published":"2024-03-12T07:32:47Z","title":"Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The\n  Lengths, Bends, and Dead Ends","summary":"  We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich structure of parameters contained within their optimization\ntrajectories. Towards this end, we introduce some natural notions of the\ncomplexity of optimization trajectories, both qualitative and quantitative,\nwhich reveal the inherent nuance and interplay involved between various\noptimization choices, such as momentum, weight decay, and batch size. We use\nthem to provide key hallmarks about the nature of optimization in deep neural\nnetworks: when it goes right, and when it finds itself in a dead end. Further,\nthanks to our trajectory perspective, we uncover an intertwined behaviour of\nmomentum and weight decay that promotes directional exploration, as well as a\ndirectional regularization behaviour of some others. We perform experiments\nover large-scale vision and language settings, including large language models\n(LLMs) with up to 12 billion parameters, to demonstrate the value of our\napproach.\n","authors":["Sidak Pal Singh","Bobby He","Thomas Hofmann","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.07379v1.pdf","comment":"Preprint, 51 pages"},{"id":"http://arxiv.org/abs/2403.07378v1","updated":"2024-03-12T07:31:18Z","published":"2024-03-12T07:31:18Z","title":"SVD-LLM: Truncation-aware Singular Value Decomposition for Large\n  Language Model Compression","summary":"  The advancements in Large Language Models (LLMs) have been hindered by their\nsubstantial sizes, which necessitate LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression methods\nhave two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the remaining model parameters\nafter SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM\ncompression method that addresses the limitations of existing methods. SVD-LLM\nincorporates a truncation-aware data whitening strategy to ensure a direct\nmapping between singular values and compression loss. Moreover, SVD-LLM adopts\na layer-wise closed-form model parameter update strategy to compensate for\naccuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total\nof 11 datasets and seven models from three different LLM families at four\ndifferent scales. Our results demonstrate the superiority of SVD-LLM over\nstate-of-the-arts, especially at high model compression ratios. The source code\nis available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.\n","authors":["Xin Wang","Yu Zheng","Zhongwei Wan","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07378v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2403.07376v1","updated":"2024-03-12T07:27:02Z","published":"2024-03-12T07:27:02Z","title":"NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning\n  Disentangled Reasoning","summary":"  Vision-and-Language Navigation (VLN), as a crucial research problem of\nEmbodied AI, requires an embodied agent to navigate through complex 3D\nenvironments following natural language instructions. Recent research has\nhighlighted the promising capacity of large language models (LLMs) in VLN by\nimproving navigational reasoning accuracy and interpretability. However, their\npredominant use in an offline manner usually suffers from substantial domain\ngap between the VLN task and the LLM training corpus. This paper introduces a\nnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\nparameter-efficient in-domain training to enable self-guided navigational\ndecision, leading to a significant mitigation of the domain gap in a\ncost-effective manner. Specifically, at each timestep, the LLM is prompted to\nforecast the navigational chain-of-thought by: 1) acting as a world model to\nimagine the next observation according to the instruction, 2) selecting the\ncandidate observation that best aligns with the imagination, and 3) determining\nthe action based on the reasoning from the prior steps. Through constructing\nformalized labels for training, the LLM can learn to generate desired and\nreasonable chain-of-thought outputs for improving the action decision.\nExperimental results across various training settings and popular VLN\nbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\n(R4R)) show the significant superiority of NavCoT over the direct action\nprediction variants. Through simple parameter-efficient finetuning, our NavCoT\noutperforms a recent GPT4-based approach with ~7% relative improvement on the\nR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\nscalable LLM-based embodied agents, which are helpful for developing real-world\nrobotics applications. Code is available at\nhttps://github.com/expectorlin/NavCoT.\n","authors":["Bingqian Lin","Yunshuang Nie","Ziming Wei","Jiaqi Chen","Shikui Ma","Jianhua Han","Hang Xu","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2403.07376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09476v3","updated":"2024-03-12T07:00:02Z","published":"2023-07-18T17:56:50Z","title":"Overthinking the Truth: Understanding how Language Models Process False\n  Demonstrations","summary":"  Modern language models can imitate complex patterns through few-shot\nlearning, enabling them to complete challenging tasks without fine-tuning.\nHowever, imitation can also lead models to reproduce inaccuracies or harmful\ncontent if present in the context. We study harmful imitation through the lens\nof a model's internal representations, and identify two related phenomena:\n\"overthinking\" and \"false induction heads\". The first phenomenon, overthinking,\nappears when we decode predictions from intermediate layers, given correct vs.\nincorrect few-shot demonstrations. At early layers, both demonstrations induce\nsimilar model behavior, but the behavior diverges sharply at some \"critical\nlayer\", after which the accuracy given incorrect demonstrations progressively\ndecreases. The second phenomenon, false induction heads, are a possible\nmechanistic cause of overthinking: these are heads in late layers that attend\nto and copy false information from previous demonstrations, and whose ablation\nreduces overthinking. Beyond scientific understanding, our results suggest that\nstudying intermediate model computations could be a promising avenue for\nunderstanding and guarding against harmful model behaviors.\n","authors":["Danny Halawi","Jean-Stanislas Denain","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2307.09476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07350v1","updated":"2024-03-12T06:16:33Z","published":"2024-03-12T06:16:33Z","title":"KEBench: A Benchmark on Knowledge Editing for Large Vision-Language\n  Models","summary":"  Currently, little research has been done on knowledge editing for Large\nVision-Language Models (LVLMs). Editing LVLMs faces the challenge of\neffectively integrating diverse modalities (image and text) while ensuring\ncoherent and contextually relevant modifications. An existing benchmark has\nthree metrics (Reliability, Locality and Generality) to measure knowledge\nediting for LVLMs. However, the benchmark falls short in the quality of\ngenerated images used in evaluation and cannot assess whether models\neffectively utilize edited knowledge in relation to the associated content. We\nadopt different data collection methods to construct a new benchmark,\n$\\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive\nevaluation. Leveraging a multimodal knowledge graph, our image data exhibits\nclear directionality towards entities. This directional aspect can be further\nutilized to extract entity-related knowledge and form editing data. We\nconducted experiments of different editing methods on five LVLMs, and\nthoroughly analyze how these methods impact the models. The results reveal\nstrengths and deficiencies of these methods and, hopefully, provide insights\ninto potential avenues for future research.\n","authors":["Han Huang","Haitian Zhong","Qiang Liu","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.07350v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.07342v1","updated":"2024-03-12T06:01:04Z","published":"2024-03-12T06:01:04Z","title":"Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive\n  Learning","summary":"  Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of\nfine-grained sentiment analysis, aiming to extract structured sentiment\ntriplets from unstructured textual data. Existing approaches to ASTE often\ncomplicate the task with additional structures or external data. In this\nresearch, we propose a novel tagging scheme and employ a contrastive learning\napproach to mitigate these challenges. The proposed approach demonstrates\ncomparable or superior performance in comparison to state-of-the-art\ntechniques, while featuring a more compact design and reduced computational\noverhead. Notably, even in the era of Large Language Models (LLMs), our method\nexhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning\nscenarios. This study also provides valuable insights for the advancement of\nASTE techniques within the paradigm of large language models.\n","authors":["Qiao Sun","Liujia Yang","Minghao Ma","Nanyang Ye","Qinying Gu"],"pdf_url":"https://arxiv.org/pdf/2403.07342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07339v1","updated":"2024-03-12T05:44:27Z","published":"2024-03-12T05:44:27Z","title":"IM-Unpack: Training and Inference with Arbitrarily Low Precision\n  Integers","summary":"  GEneral Matrix Multiply (GEMM) is a central operation in deep learning and\ncorresponds to the largest chunk of the compute footprint. Therefore, improving\nits efficiency is an active topic of ongoing research. A popular strategy is\nthe use of low bit-width integers to approximate the original entries in a\nmatrix. This allows efficiency gains, but often requires sophisticated\ntechniques to control the rounding error incurred. In this work, we first\nverify/check that when the low bit-width restriction is removed, for a variety\nof Transformer-based models, whether integers are sufficient for all GEMMs need\n-- for {\\em both} training and inference stages, and can achieve parity with\nfloating point counterparts. No sophisticated techniques are needed. We find\nthat while a large majority of entries in matrices (encountered in such models)\ncan be easily represented by {\\em low} bit-width integers, the existence of a\nfew heavy hitter entries make it difficult to achieve efficiency gains via the\nexclusive use of low bit-width GEMMs alone. To address this issue, we develop a\nsimple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\em unpack} a\nmatrix with large integer entries into a larger matrix whose entries all lie\nwithin the representable range of arbitrarily low bit-width integers. This\nallows {\\em equivalence} with the original GEMM, i.e., the exact result can be\nobtained using purely low bit-width integer GEMMs. This comes at the cost of\nadditional operations -- we show that for many popular models, this overhead is\nquite small.\n","authors":["Zhanpeng Zeng","Karthikeyan Sankaralingam","Vikas Singh"],"pdf_url":"https://arxiv.org/pdf/2403.07339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03102v3","updated":"2024-03-12T05:33:16Z","published":"2024-03-05T16:43:03Z","title":"\"In Dialogues We Learn\": Towards Personalized Dialogue Without\n  Pre-defined Profiles through In-Dialogue Learning","summary":"  Personalized dialogue systems have gained significant attention in recent\nyears for their ability to generate responses in alignment with different\npersonas. However, most existing approaches rely on pre-defined personal\nprofiles, which are not only time-consuming and labor-intensive to create but\nalso lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning\nframework that enhances the ability of pre-trained large language models to\nleverage dialogue history to characterize persona for completing personalized\ndialogue generation tasks without pre-defined profiles. Our experiments on\nthree datasets demonstrate that IDL brings substantial improvements, with BLEU\nand ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,\nthe results of human evaluations further validate the efficacy of our proposed\nmethod.\n","authors":["Chuanqi Cheng","Quan Tu","Wei Wu","Shuo Shang","Cunli Mao","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.03102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06259v3","updated":"2024-03-12T05:22:46Z","published":"2023-08-11T17:47:54Z","title":"Self-Alignment with Instruction Backtranslation","summary":"  We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.\n","authors":["Xian Li","Ping Yu","Chunting Zhou","Timo Schick","Omer Levy","Luke Zettlemoyer","Jason Weston","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2308.06259v3.pdf","comment":"ICLR2024 camera ready"},{"id":"http://arxiv.org/abs/2403.07321v1","updated":"2024-03-12T05:15:21Z","published":"2024-03-12T05:15:21Z","title":"GPT-generated Text Detection: Benchmark Dataset and Tensor-based\n  Detection Method","summary":"  As natural language models like ChatGPT become increasingly prevalent in\napplications and services, the need for robust and accurate methods to detect\ntheir output is of paramount importance. In this paper, we present GPT Reddit\nDataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text\ndetection dataset designed to assess the performance of detection models in\nidentifying generated responses from ChatGPT. The dataset consists of a diverse\ncollection of context-prompt pairs based on Reddit, with human-generated and\nChatGPT-generated responses. We provide an analysis of the dataset's\ncharacteristics, including linguistic diversity, context complexity, and\nresponse quality. To showcase the dataset's utility, we benchmark several\ndetection methods on it, demonstrating their efficacy in distinguishing between\nhuman and ChatGPT-generated responses. This dataset serves as a resource for\nevaluating and advancing detection techniques in the context of ChatGPT and\ncontributes to the ongoing efforts to ensure responsible and trustworthy\nAI-driven communication on the internet. Finally, we propose GpTen, a novel\ntensor-based GPT text detection method that is semi-supervised in nature since\nit only has access to human-generated text and performs on par with\nfully-supervised baselines.\n","authors":["Zubair Qazi","William Shiao","Evangelos E. Papalexakis"],"pdf_url":"https://arxiv.org/pdf/2403.07321v1.pdf","comment":"4 pages, 2 figures, published in the WWW 2024 Short Papers Track"},{"id":"http://arxiv.org/abs/2403.07311v1","updated":"2024-03-12T04:47:29Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of predicting multiple links within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, a challenge increasingly\nresolvable due to advancements in natural language processing (NLP) and KG\nembedding techniques. This paper introduces a novel methodology, the Knowledge\nGraph Large Language Model Framework (KG-LLM), which leverages pivotal NLP\nparadigms, including chain-of-thought (CoT) prompting and in-context learning\n(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a\nCoT prompt, our framework is designed to discern and learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)\nwithin this framework, employing both non-ICL and ICL tasks for a comprehensive\nevaluation. Further, we explore the framework's potential to provide LLMs with\nzero-shot capabilities for handling previously unseen prompts. Our experimental\nfindings discover that integrating ICL and CoT not only augments the\nperformance of our approach but also significantly boosts the models'\ngeneralization capacity, thereby ensuring more precise predictions in\nunfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Yiting Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v1.pdf","comment":"24 pages, 3 figures, submit to ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2401.11624v4","updated":"2024-03-12T04:38:53Z","published":"2024-01-21T23:34:42Z","title":"In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey","summary":"  Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.\n","authors":["Man Luo","Xin Xu","Yue Liu","Panupong Pasupat","Mehran Kazemi"],"pdf_url":"https://arxiv.org/pdf/2401.11624v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06117v2","updated":"2024-03-12T04:38:27Z","published":"2023-10-09T19:48:55Z","title":"Take a Step Back: Evoking Reasoning via Abstraction in Large Language\n  Models","summary":"  We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide reasoning, LLMs significantly improve their abilities in following a\ncorrect reasoning path towards the solution. We conduct experiments of\nStep-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe\nsubstantial performance gains on various challenging reasoning-intensive tasks\nincluding STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back\nPrompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.\n","authors":["Huaixiu Steven Zheng","Swaroop Mishra","Xinyun Chen","Heng-Tze Cheng","Ed H. Chi","Quoc V Le","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06117v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2311.09618v3","updated":"2024-03-12T04:14:18Z","published":"2023-11-16T07:01:48Z","title":"Simulating Opinion Dynamics with Networks of LLM-based Agents","summary":"  Accurately simulating human opinion dynamics is crucial for understanding a\nvariety of societal phenomena, including polarization and the spread of\nmisinformation. However, the agent-based models (ABMs) commonly used for such\nsimulations often over-simplify human behavior. We propose a new approach to\nsimulating opinion dynamics based on populations of Large Language Models\n(LLMs). Our findings reveal a strong inherent bias in LLM agents towards\nproducing accurate information, leading simulated agents to consensus in line\nwith scientific reality. This bias limits their utility for understanding\nresistance to consensus views on issues like climate change. After inducing\nconfirmation bias through prompt engineering, however, we observed opinion\nfragmentation in line with existing agent-based modeling and opinion dynamics\nresearch. These insights highlight the promise and limitations of LLM agents in\nthis domain and suggest a path forward: refining LLMs with real-world discourse\nto better simulate the evolution of human beliefs.\n","authors":["Yun-Shiuan Chuang","Agam Goyal","Nikunj Harlalka","Siddharth Suresh","Robert Hawkins","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2311.09618v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07300v1","updated":"2024-03-12T04:04:38Z","published":"2024-03-12T04:04:38Z","title":"Taming Pre-trained LLMs for Generalised Time Series Forecasting via\n  Cross-modal Knowledge Distillation","summary":"  Multivariate time series forecasting has recently gained great success with\nthe rapid growth of deep learning models. However, existing approaches usually\ntrain models from scratch using limited temporal data, preventing their\ngeneralization. Recently, with the surge of the Large Language Models (LLMs),\nseveral works have attempted to introduce LLMs into time series forecasting.\nDespite promising results, these methods directly take time series as the input\nto LLMs, ignoring the inherent modality gap between temporal and text data. In\nthis work, we propose a novel Large Language Models and time series alignment\nframework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time\nseries forecasting challenge. Based on cross-modal knowledge distillation, the\nproposed method exploits both input-agnostic static knowledge and\ninput-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers\nthe forecasting model with favorable performance as well as strong\ngeneralization abilities. Extensive experiments demonstrate the proposed method\nestablishes a new state of the art for both long- and short-term forecasting.\nCode is available at \\url{https://github.com/Hank0626/LLaTA}.\n","authors":["Peiyuan Liu","Hang Guo","Tao Dai","Naiqi Li","Jigang Bao","Xudong Ren","Yong Jiang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2403.07300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16319v2","updated":"2024-03-12T03:43:57Z","published":"2023-09-28T10:24:39Z","title":"Augmenting Transformers with Recursively Composed Multi-grained\n  Representations","summary":"  We present ReCAT, a recursive composition augmented Transformer that is able\nto explicitly model hierarchical syntactic structures of raw texts without\nrelying on gold trees during both learning and inference. Existing research\nalong this line restricts data to follow a hierarchical tree structure and thus\nlacks inter-span communications. To overcome the problem, we propose a novel\ncontextual inside-outside (CIO) layer that learns contextualized\nrepresentations of spans through bottom-up and top-down passes, where a\nbottom-up pass forms representations of high-level spans by composing low-level\nspans, while a top-down pass combines information inside and outside a span. By\nstacking several CIO layers between the embedding layer and the attention\nlayers in Transformer, the ReCAT model can perform both deep intra-span and\ndeep inter-span interactions, and thus generate multi-grained representations\nfully contextualized with other spans. Moreover, the CIO layers can be jointly\npre-trained with Transformers, making ReCAT enjoy scaling ability, strong\nperformance, and interpretability at the same time. We conduct experiments on\nvarious sentence-level and span-level tasks. Evaluation results indicate that\nReCAT can significantly outperform vanilla Transformer models on all span-level\ntasks and baselines that combine recursive networks with Transformers on\nnatural language inference tasks. More interestingly, the hierarchical\nstructures induced by ReCAT exhibit strong consistency with human-annotated\nsyntactic trees, indicating good interpretability brought by the CIO layers.\n","authors":["Xiang Hu","Qingyang Zhu","Kewei Tu","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2309.16319v2.pdf","comment":"ICLR 2024 poster"},{"id":"http://arxiv.org/abs/2403.07283v1","updated":"2024-03-12T03:30:04Z","published":"2024-03-12T03:30:04Z","title":"A Framework for Cost-Effective and Self-Adaptive LLM Shaking and\n  Recovery Mechanism","summary":"  As Large Language Models (LLMs) gain great success in real-world\napplications, an increasing number of users are seeking to develop and deploy\ntheir customized LLMs through cloud services. Nonetheless, in some specific\ndomains, there are still concerns regarding cost and trade-offs between privacy\nissues and accuracy. In this study, we introduce a cost-effective and\nself-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With\ncarefully designed horizontal and vertical shaking operators, we can achieve\ncomparable accuracy results with SOTA privacy-preserving LLM schemes using\nCryptography-based or Differential Privacy-based methods. Experiments also show\nthat with the CypherTalk framework, users can achieve reliable accuracy when\nusing optimized shaking operator settings. To our best knowledge, this is the\nfirst work that considers cost, and trade-off between model utility and privacy\nin LLM scenarios.\n","authors":["Zhiyu Chen","Yu Li","Suochao Zhang","Jingbo Zhou","Jiwen Zhou","Chenfu Bao","Dianhai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.07283v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2403.07279v1","updated":"2024-03-12T03:17:59Z","published":"2024-03-12T03:17:59Z","title":"A Survey of Explainable Knowledge Tracing","summary":"  With the long term accumulation of high quality educational data, artificial\nintelligence has shown excellent performance in knowledge tracing. However, due\nto the lack of interpretability and transparency of some algorithms, this\napproach will result in reduced stakeholder trust and a decreased acceptance of\nintelligent decisions. Therefore, algorithms need to achieve high accuracy, and\nusers need to understand the internal operating mechanism and provide reliable\nexplanations for decisions. This paper thoroughly analyzes the interpretability\nof KT algorithms. First, the concepts and common methods of explainable\nartificial intelligence and knowledge tracing are introduced. Next, explainable\nknowledge tracing models are classified into two categories: transparent models\nand black box models. Then, the interpretable methods used are reviewed from\nthree stages: ante hoc interpretable methods, post hoc interpretable methods,\nand other dimensions. It is worth noting that current evaluation methods for\nexplainable knowledge tracing are lacking. Hence, contrast and deletion\nexperiments are conducted to explain the prediction results of the deep\nknowledge tracing model on the ASSISTment2009 by using three XAI methods.\nMoreover, this paper offers some insights into evaluation methods from the\nperspective of educational stakeholders. This paper provides a detailed and\ncomprehensive review of the research on explainable knowledge tracing, aiming\nto offer some basis and inspiration for researchers interested in the\ninterpretability of knowledge tracing.\n","authors":["Yanhong Bai","Jiabao Zhao","Tingjiang Wei","Qing Cai","Liang He"],"pdf_url":"https://arxiv.org/pdf/2403.07279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16421v2","updated":"2024-03-12T03:14:18Z","published":"2023-03-29T03:05:43Z","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of\n  Commonsense Problem in Large Language Models","summary":"  Large language models (LLMs) have made significant progress in NLP. However,\ntheir ability to memorize, represent, and leverage commonsense knowledge has\nbeen a well-known pain point. In this paper, we specifically focus on ChatGPT,\na widely used and easily accessible LLM, and ask the following questions: (1)\nCan ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of\nthe underlying commonsense knowledge for answering a specific question? (3) Is\nChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage\ncommonsense for answering questions? We conduct a series of experiments on 11\ndatasets to evaluate ChatGPT's commonsense abilities, including answering\ncommonsense questions, identifying necessary knowledge, generating knowledge\ndescriptions, and using knowledge descriptions to answer questions again.\nExperimental results show that: (1) ChatGPT can achieve good QA accuracies in\ncommonsense tasks, while still struggling with certain domains of datasets. (2)\nChatGPT is knowledgeable, and can accurately generate most of the commonsense\nknowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an\ninexperienced commonsense problem solver, which cannot precisely identify the\nneeded commonsense for answering a specific question. These findings raise the\nneed to explore improved mechanisms for effectively incorporating commonsense\ninto LLMs like ChatGPT, such as better instruction following and commonsense\nguidance.\n","authors":["Ning Bian","Xianpei Han","Le Sun","Hongyu Lin","Yaojie Lu","Ben He","Shanshan Jiang","Bin Dong"],"pdf_url":"https://arxiv.org/pdf/2303.16421v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.07260v1","updated":"2024-03-12T02:37:11Z","published":"2024-03-12T02:37:11Z","title":"CKERC : Joint Large Language Models with Commonsense Knowledge for\n  Emotion Recognition in Conversation","summary":"  Emotion recognition in conversation (ERC) is a task which predicts the\nemotion of an utterance in the context of a conversation. It tightly depends on\ndialogue context, speaker identity information, multiparty dialogue scenario\nand so on. However, the state-of-the-art method (instructERC) solely\nidentifying speaker, and ignores commonsense knowledge(i.e., reaction of the\nlisteners and intention of the speaker, etc.) behind speakers during a\nconversation, which can deeply mine speaker information. To this end, we\npropose a novel joint large language models with commonsense knowledge\nframework for emotion recognition in conversation, namely CKERC.We design\nprompts to generate interlocutors' commonsense based on historical utterances\nwith large language model. And we use the interlocutor commonsense\nidentification task for LLM pre-training to fine-tune speaker implicit clues\ninformation.By solving above challenge, our method achieve state-of-the-art.We\nextensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD,\nEmoryNLP, demonstrate our method superiority. Also, we conduct in-depth\nanalysis and further demonstrate the effectiveness of commonsense knowledge in\nERC task in large language model.\n","authors":["Yumeng Fu"],"pdf_url":"https://arxiv.org/pdf/2403.07260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07230v1","updated":"2024-03-12T00:58:19Z","published":"2024-03-12T00:58:19Z","title":"Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\n  Preferences","summary":"  Direct Preference Optimization (DPO) is an effective technique that leverages\npairwise preference data (usually one chosen and rejected response pair per\nuser prompt) to align LLMs to human preferences. In practice, multiple\nresponses can exist for a given prompt with varying quality relative to each\nother. With availability of such quality ratings for multiple responses, we\npropose utilizing these responses to create multiple preference pairs for a\ngiven prompt. Our work focuses on systematically using the constructed multiple\npreference pair in DPO training via curriculum learning methodology. In\nparticular, we order these multiple pairs of preference data from easy to hard\n(emulating curriculum training) according to various criteria. We show detailed\ncomparisons of our proposed approach to the standard single-pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance\ngains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\nhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\nof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\nwith similar parameter size. Curry-DPO also achieves the highest adjusted win\nrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\n87.9% respectively) in our experiments, with notable gains of upto 7.5% when\ncompared to standard DPO technique.\n","authors":["Pulkit Pattnaik","Rishabh Maheshwary","Kelechi Ogueji","Vikas Yadav","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2403.07230v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2309.00770v2","updated":"2024-03-12T00:50:00Z","published":"2023-09-02T00:32:55Z","title":"Bias and Fairness in Large Language Models: A Survey","summary":"  Rapid advancements of large language models (LLMs) have enabled the\nprocessing, understanding, and generation of human-like text, with increasing\nintegration into systems that touch our social sphere. Despite this success,\nthese models can learn, perpetuate, and amplify harmful social biases. In this\npaper, we present a comprehensive survey of bias evaluation and mitigation\ntechniques for LLMs. We first consolidate, formalize, and expand notions of\nsocial bias and fairness in natural language processing, defining distinct\nfacets of harm and introducing several desiderata to operationalize fairness\nfor LLMs. We then unify the literature by proposing three intuitive taxonomies,\ntwo for bias evaluation, namely metrics and datasets, and one for mitigation.\nOur first taxonomy of metrics for bias evaluation disambiguates the\nrelationship between metrics and evaluation datasets, and organizes metrics by\nthe different levels at which they operate in a model: embeddings,\nprobabilities, and generated text. Our second taxonomy of datasets for bias\nevaluation categorizes datasets by their structure as counterfactual inputs or\nprompts, and identifies the targeted harms and social groups; we also release a\nconsolidation of publicly-available datasets for improved access. Our third\ntaxonomy of techniques for bias mitigation classifies methods by their\nintervention during pre-processing, in-training, intra-processing, and\npost-processing, with granular subcategories that elucidate research trends.\nFinally, we identify open problems and challenges for future work. Synthesizing\na wide range of recent research, we aim to provide a clear guide of the\nexisting literature that empowers researchers and practitioners to better\nunderstand and prevent the propagation of bias in LLMs.\n","authors":["Isabel O. Gallegos","Ryan A. Rossi","Joe Barrow","Md Mehrab Tanjim","Sungchul Kim","Franck Dernoncourt","Tong Yu","Ruiyi Zhang","Nesreen K. Ahmed"],"pdf_url":"https://arxiv.org/pdf/2309.00770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00746v5","updated":"2024-03-12T00:16:10Z","published":"2024-02-01T16:40:32Z","title":"Health-LLM: Personalized Retrieval-Augmented Disease Prediction System","summary":"  Artificial intelligence (AI) in healthcare has significantly advanced\nintelligent medical treatment. However, traditional intelligent healthcare is\nlimited by static data and unified standards, preventing full integration with\nindividual situations and other challenges. Hence, a more professional and\ndetailed intelligent healthcare method is needed for development. To this end,\nwe propose an innovative framework named Heath-LLM, which combines large-scale\nfeature extraction and medical knowledge trade-off scoring. Compared to\ntraditional health management methods, our system has three main advantages.\nFirst, our system integrates health reports into a large model to provide\ndetailed task information. Second, professional medical expertise is used to\nadjust the weighted scores of health characteristics. Third, we use a\nsemi-automated feature extraction framework to enhance the analytical power of\nlanguage models and incorporate expert insights to improve the accuracy of\ndisease prediction. We have conducted disease prediction experiments on a large\nnumber of health reports to assess the effectiveness of Health-LLM. The results\nof the experiments indicate that the proposed system surpasses traditional\nmethods and has the potential to revolutionize disease prediction and\npersonalized health management. The code is available at\nhttps://github.com/jmyissb/HealthLLM.\n","authors":["Mingyu Jin","Qinkai Yu","Dong Shu","Chong Zhang","Suiyuan Zhu","Mengnan Du","Yanda Meng","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.00746v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08140v1","updated":"2024-03-12T23:59:15Z","published":"2024-03-12T23:59:15Z","title":"BAGEL: Bootstrapping Agents by Guiding Exploration with Language","summary":"  Following natural language instructions by executing actions in digital\nenvironments (e.g. web-browsers and REST APIs) is a challenging task for\nlanguage model (LM) agents. Unfortunately, LM agents often fail to generalize\nto new environments without human demonstrations. This work presents BAGEL, a\nmethod for bootstrapping LM agents without human supervision. BAGEL converts a\nseed set of randomly explored trajectories or synthetic instructions, into\ndemonstrations, via round-trips between two noisy LM components: an LM labeler\nwhich converts a trajectory into a synthetic instruction, and a zero-shot LM\nagent which maps the synthetic instruction into a refined trajectory. By\nperforming these round-trips iteratively, BAGEL quickly converts the initial\ndistribution of trajectories towards those that are well-described by natural\nlanguage. We use BAGEL demonstrations to adapt a zero shot LM agent at test\ntime via in-context learning over retrieved demonstrations, and find\nimprovements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x\nreduction in execution failures.\n","authors":["Shikhar Murty","Christopher Manning","Peter Shaw","Mandar Joshi","Kenton Lee"],"pdf_url":"https://arxiv.org/pdf/2403.08140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08137v1","updated":"2024-03-12T23:47:28Z","published":"2024-03-12T23:47:28Z","title":"From Paper to Card: Transforming Design Implications with Generative AI","summary":"  Communicating design implications is common within the HCI community when\npublishing academic papers, yet these papers are rarely read and used by\ndesigners. One solution is to use design cards as a form of translational\nresource that communicates valuable insights from papers in a more digestible\nand accessible format to assist in design processes. However, creating design\ncards can be time-consuming, and authors may lack the resources/know-how to\nproduce cards. Through an iterative design process, we built a system that\nhelps create design cards from academic papers using an LLM and text-to-image\nmodel. Our evaluation with designers (N=21) and authors of selected papers\n(N=12) revealed that designers perceived the design implications from our\ndesign cards as more inspiring and generative, compared to reading original\npaper texts, and the authors viewed our system as an effective way of\ncommunicating their design implications. We also propose future enhancements\nfor AI-generated design cards.\n","authors":["Donghoon Shin","Lucy Lu Wang","Gary Hsieh"],"pdf_url":"https://arxiv.org/pdf/2403.08137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01614v2","updated":"2024-03-12T23:14:33Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents -- it\ncan successfully complete 51.1 of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out to be not effective for web agents, and the best grounding strategy\nwe develop in this paper leverages both the HTML structure and visuals. Yet,\nthere is still a substantial gap with oracle grounding, leaving ample room for\nfurther improvement. All code, data, and evaluation tools are available at\nhttps://github.com/OSU-NLP-Group/SeeAct.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08115v1","updated":"2024-03-12T22:53:32Z","published":"2024-03-12T22:53:32Z","title":"Legally Binding but Unfair? Towards Assessing Fairness of Privacy\n  Policies","summary":"  Privacy policies are expected to inform data subjects about their data\nprotection rights. They should explain the data controller's data management\npractices, and make facts such as retention periods or data transfers to third\nparties transparent. Privacy policies only fulfill their purpose, if they are\ncorrectly perceived, interpreted, understood, and trusted by the data subject.\nAmongst others, this requires that a privacy policy is written in a fair way,\ne.g., it does not use polarizing terms, does not require a certain education,\nor does not assume a particular social background. In this work-in-progress\npaper, we outline our approach to assessing fairness in privacy policies. To\nthis end, we identify from fundamental legal sources and fairness research, how\nthe dimensions informational fairness, representational fairness and\nethics/morality are related to privacy policies. We propose options to\nautomatically assess policies in these fairness dimensions, based on text\nstatistics, linguistic methods and artificial intelligence. Finally, we conduct\ninitial experiments with German privacy policies to provide evidence that our\napproach is applicable. Our experiments indicate that there are indeed issues\nin all three dimensions of fairness. For example, our approach finds out if a\npolicy discriminates against individuals with impaired reading skills or\ncertain demographics, and identifies questionable ethics. This is important, as\nfuture privacy policies may be used in a corpus for legal artificial\nintelligence models.\n","authors":["Vincent Freiberger","Erik Buchmann"],"pdf_url":"https://arxiv.org/pdf/2403.08115v1.pdf","comment":"Submitted to IWSPA 2024 and under review"},{"id":"http://arxiv.org/abs/2403.08111v1","updated":"2024-03-12T22:36:27Z","published":"2024-03-12T22:36:27Z","title":"AI-Assisted Causal Pathway Diagram for Human-Centered Design","summary":"  This paper explores the integration of causal pathway diagrams (CPD) into\nhuman-centered design (HCD), investigating how these diagrams can enhance the\nearly stages of the design process. A dedicated CPD plugin for the online\ncollaborative whiteboard platform Miro was developed to streamline diagram\ncreation and offer real-time AI-driven guidance. Through a user study with\ndesigners (N=20), we found that CPD's branching and its emphasis on causal\nconnections supported both divergent and convergent processes during design.\nCPD can also facilitate communication among stakeholders. Additionally, we\nfound our plugin significantly reduces designers' cognitive workload and\nincreases their creativity during brainstorming, highlighting the implications\nof AI-assisted tools in supporting creative work and evidence-based designs.\n","authors":["Ruican Zhong","Donghoon Shin","Rosemary Meza","Predrag Klasnja","Lucas Colusso","Gary Hsieh"],"pdf_url":"https://arxiv.org/pdf/2403.08111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08103v1","updated":"2024-03-12T22:23:08Z","published":"2024-03-12T22:23:08Z","title":"Contextual Clarity: Generating Sentences with Transformer Models using\n  Context-Reverso Data","summary":"  In the age of information abundance, the ability to provide users with\ncontextually relevant and concise information is crucial. Keyword in Context\n(KIC) generation is a task that plays a vital role in and generation\napplications, such as search engines, personal assistants, and content\nsummarization. In this paper, we present a novel approach to generating\nunambiguous and brief sentence-contexts for given keywords using the T5\ntransformer model, leveraging data obtained from the Context-Reverso API. The\ncode is available at https://github.com/Rusamus/word2context/tree/main .\n","authors":["Ruslan Musaev"],"pdf_url":"https://arxiv.org/pdf/2403.08103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13405v2","updated":"2024-03-12T22:12:25Z","published":"2024-02-20T22:19:56Z","title":"A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set\n  Expansion and Taxonomy Expansion","summary":"  Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy\nConstruction are three representative tasks that can be used to automatically\npopulate an existing taxonomy with new entities. However, previous approaches\noften address these tasks separately with heterogeneous techniques, lacking a\nunified perspective. To tackle this issue, in this paper, we identify the\ncommon key skills needed for these tasks from the view of taxonomy structures\n-- finding 'siblings' and finding 'parents' -- and propose a unified\ntaxonomy-guided instruction tuning framework to jointly solve the three tasks.\nTo be specific, by leveraging the existing taxonomy as a rich source of entity\nrelationships, we utilize instruction tuning to fine-tune a large language\nmodel to generate parent and sibling entities. Extensive experiments on\nmultiple benchmark datasets demonstrate the effectiveness of TaxoInstruct,\nwhich outperforms task-specific baselines across all three tasks.\n","authors":["Yanzhen Shen","Yu Zhang","Yunyi Zhang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2402.13405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00635v3","updated":"2024-03-12T22:05:53Z","published":"2022-11-01T17:56:57Z","title":"Two-stage LLM Fine-tuning with Less Specialization and More\n  Generalization","summary":"  Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n","authors":["Yihan Wang","Si Si","Daliang Li","Michal Lukasik","Felix Yu","Cho-Jui Hsieh","Inderjit S Dhillon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2211.00635v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08081v1","updated":"2024-03-12T21:15:38Z","published":"2024-03-12T21:15:38Z","title":"Mechanics of Next Token Prediction with Self-Attention","summary":"  Transformer-based language models are trained on large datasets to predict\nthe next token given an input sequence. Despite this simple training objective,\nthey have led to revolutionary advances in natural language processing.\nUnderlying this success is the self-attention mechanism. In this work, we ask:\n$\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$\n$\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$\n$\\textit{next-token}$ $\\textit{prediction?}$ We show that training\nself-attention with gradient descent learns an automaton which generates the\nnext token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$\n$\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects\nthe $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with\nthe last input token. $\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It\nthen creates a convex combination of the high-priority tokens from which the\nnext token can be sampled. Under suitable conditions, we rigorously\ncharacterize these mechanics through a directed graph over tokens extracted\nfrom the training data. We prove that gradient descent implicitly discovers the\nstrongly-connected components (SCC) of this graph and self-attention learns to\nretrieve the tokens that belong to the highest-priority SCC available in the\ncontext window. Our theory relies on decomposing the model weights into a\ndirectional component and a finite component that correspond to hard retrieval\nand soft composition steps respectively. This also formalizes a related\nimplicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that\nthese findings shed light on how self-attention processes sequential data and\npave the path toward demystifying more complex architectures.\n","authors":["Yingcong Li","Yixiao Huang","M. Emrullah Ildiz","Ankit Singh Rawat","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2403.08081v1.pdf","comment":"Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.08059v1","updated":"2024-03-12T20:11:38Z","published":"2024-03-12T20:11:38Z","title":"FluoroSAM: A Language-aligned Foundation Model for X-ray Image\n  Segmentation","summary":"  Automated X-ray image segmentation would accelerate research and development\nin diagnostic and interventional precision medicine. Prior efforts have\ncontributed task-specific models capable of solving specific image analysis\nproblems, but the utility of these models is restricted to their particular\ntask domain, and expanding to broader use requires additional data, labels, and\nretraining efforts. Recently, foundation models (FMs) -- machine learning\nmodels trained on large amounts of highly variable data thus enabling broad\napplicability -- have emerged as promising tools for automated image analysis.\nExisting FMs for medical image analysis focus on scenarios and modalities where\nobjects are clearly defined by visually apparent boundaries, such as surgical\ntool segmentation in endoscopy. X-ray imaging, by contrast, does not generally\noffer such clearly delineated boundaries or structure priors. During X-ray\nimage formation, complex 3D structures are projected in transmission onto the\nimaging plane, resulting in overlapping features of varying opacity and shape.\nTo pave the way toward an FM for comprehensive and automated analysis of\narbitrary medical X-ray images, we develop FluoroSAM, a language-aligned\nvariant of the Segment-Anything Model, trained from scratch on 1.6M synthetic\nX-ray images. FluoroSAM is trained on data including masks for 128 organ types\nand 464 non-anatomical objects, such as tools and implants. In real X-ray\nimages of cadaveric specimens, FluoroSAM is able to segment bony anatomical\nstructures based on text-only prompting with 0.51 and 0.79 DICE with\npoint-based refinement, outperforming competing SAM variants for all\nstructures. FluoroSAM is also capable of zero-shot generalization to segmenting\nclasses beyond the training set thanks to its language alignment, which we\ndemonstrate for full lung segmentation on real chest X-rays.\n","authors":["Benjamin D. Killeen","Liam J. Wang","Han Zhang","Mehran Armand","Russell H. Taylor","Greg Osgood","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2403.08059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11767v3","updated":"2024-03-12T20:10:05Z","published":"2023-08-15T23:22:37Z","title":"Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm","summary":"  ChatGPT and generative AI tools are becoming the new reality. This work is\nmotivated by the premise that ``ChatGPT content may exhibit a distinctive\nbehavior that can be separated from scientific articles''. In this study, we\ndemonstrate how we tested this premise in two phases and prove its validity.\nSubsequently, we introduce xFakeSci, a novel learning algorithm, that is\ncapable of distinguishing ChatGPT-generated articles from publications produced\nby scientists. The algorithm is trained using network models driven from\nmultiple types of data sources, such as ChatGPT-generated documents achieved by\nmeans of prompt-engineering, and PubMed articles. To mitigate over-fitting\nissues, we incorporate a calibration step that is built upon data-driven\nheuristics, including ratios. We evaluate the algorithm across multiple\ndatasets covering publication periods and diseases (cancer, depression, and\nAlzheimer's). Further, we show how the algorithm is benchmarked against the\nstate-of-the-art (SOTA) algorithms. While the xFakeSci algorithm achieve F1\nscore ranging from 80% - 94%, SOTA algorithms score F1 values between 38% -\n52%. We attribute the noticeable difference to the introduction of calibration\nand a proximity distance heuristic, which we underscore this promising\nperformance. Indeed, the prediction of fake science generated by ChatGPT\npresents a considerable challenge. Nonetheless, the introduction of xFakeSci\nalgorithm is a significant step on the way to combating fake science.\n","authors":["Ahmed Abdeen Hamed","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2308.11767v3.pdf","comment":"14 pages, 6 figures, 6 tables, 5 algorithms"},{"id":"http://arxiv.org/abs/2403.08058v1","updated":"2024-03-12T20:10:04Z","published":"2024-03-12T20:10:04Z","title":"CHAI: Clustered Head Attention for Efficient LLM Inference","summary":"  Large Language Models (LLMs) with hundreds of billions of parameters have\ntransformed the field of machine learning. However, serving these models at\ninference time is both compute and memory intensive, where a single request can\nrequire multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is\none of the key components of LLMs, which can account for over 50% of LLMs\nmemory and compute requirement. We observe that there is a high amount of\nredundancy across heads on which tokens they pay attention to. Based on this\ninsight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a\nhigh amount of correlation for self-attention at runtime, thus reducing both\nmemory and compute. In our experiments, we show that CHAI is able to reduce the\nmemory requirements for storing K,V cache by up to 21.4% and inference time\nlatency by up to 1.73x without any fine-tuning required. CHAI achieves this\nwith a maximum 3.2% deviation in accuracy across 3 different models (i.e.\nOPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.\n","authors":["Saurabh Agarwal","Bilge Acun","Basil Homer","Mostafa Elhoushi","Yejin Lee","Shivaram Venkataraman","Dimitris Papailiopoulos","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08053v1","updated":"2024-03-12T19:57:39Z","published":"2024-03-12T19:57:39Z","title":"Generating Clarification Questions for Disambiguating Contracts","summary":"  Enterprises frequently enter into commercial contracts that can serve as\nvital sources of project-specific requirements. Contractual clauses are\nobligatory, and the requirements derived from contracts can detail the\ndownstream implementation activities that non-legal stakeholders, including\nrequirement analysts, engineers, and delivery personnel, need to conduct.\nHowever, comprehending contracts is cognitively demanding and error-prone for\nsuch stakeholders due to the extensive use of Legalese and the inherent\ncomplexity of contract language. Furthermore, contracts often contain\nambiguously worded clauses to ensure comprehensive coverage. In contrast,\nnon-legal stakeholders require a detailed and unambiguous comprehension of\ncontractual clauses to craft actionable requirements. In this work, we\nintroduce a novel legal NLP task that involves generating clarification\nquestions for contracts. These questions aim to identify contract ambiguities\non a document level, thereby assisting non-legal stakeholders in obtaining the\nnecessary details for eliciting requirements. This task is challenged by three\ncore issues: (1) data availability, (2) the length and unstructured nature of\ncontracts, and (3) the complexity of legal text. To address these issues, we\npropose ConRAP, a retrieval-augmented prompting framework for generating\nclarification questions to disambiguate contractual text. Experiments conducted\non contracts sourced from the publicly available CUAD dataset show that ConRAP\nwith ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the\ngenerated clarification questions are deemed useful by human evaluators.\n","authors":["Anmol Singhal","Chirag Jain","Preethu Rose Anish","Arkajyoti Chakraborty","Smita Ghaisas"],"pdf_url":"https://arxiv.org/pdf/2403.08053v1.pdf","comment":"9 pages, 3 figures, accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08046v1","updated":"2024-03-12T19:40:18Z","published":"2024-03-12T19:40:18Z","title":"Big City Bias: Evaluating the Impact of Metropolitan Size on\n  Computational Job Market Abilities of Language Models","summary":"  Large language models (LLMs) have emerged as a useful technology for job\nmatching, for both candidates and employers. Job matching is often based on a\nparticular geographic location, such as a city or region. However, LLMs have\nknown biases, commonly derived from their training data. In this work, we aim\nto quantify the metropolitan size bias encoded within large language models,\nevaluating zero-shot salary, employer presence, and commute duration\npredictions in 384 of the United States' metropolitan regions. Across all\nbenchmarks, we observe negative correlations between the metropolitan size and\nthe performance of the LLMS, indicating that smaller regions are indeed\nunderrepresented. More concretely, the smallest 10 metropolitan regions show\nupwards of 300% worse benchmark performance than the largest 10.\n","authors":["Charlie Campanella","Rob van der Goot"],"pdf_url":"https://arxiv.org/pdf/2403.08046v1.pdf","comment":"5 pages, 3 figures, 2 tables, NLP4HR Workshop @ EACL 2024"},{"id":"http://arxiv.org/abs/2403.08043v1","updated":"2024-03-12T19:34:54Z","published":"2024-03-12T19:34:54Z","title":"Authorship Style Transfer with Policy Optimization","summary":"  Authorship style transfer aims to rewrite a given text into a specified\ntarget while preserving the original meaning in the source. Existing approaches\nrely on the availability of a large number of target style exemplars for model\ntraining. However, these overlook cases where a limited number of target style\nexamples are available. The development of parameter-efficient transfer\nlearning techniques and policy optimization (PO) approaches suggest lightweight\nPO is a feasible approach to low-resource style transfer. In this work, we\npropose a simple two step tune-and-optimize technique for low-resource textual\nstyle transfer. We apply our technique to authorship transfer as well as a\nlarger-data native language style task and in both cases find it outperforms\nstate-of-the-art baseline models.\n","authors":["Shuai Liu","Shantanu Agarwal","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2403.08043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10659v2","updated":"2024-03-12T19:12:55Z","published":"2024-02-16T13:10:14Z","title":"Network Formation and Dynamics Among Multi-LLMs","summary":"  Social networks shape opinions, behaviors, and information dissemination in\nhuman societies. As large language models (LLMs) increasingly integrate into\nsocial and professional environments, understanding their behavior within the\ncontext of social interactions and networks becomes essential. Our study\nanalyzes LLMs' network formation behavior to examine whether the dynamics of\nmultiple LLMs are similar to or different from human social dynamics. We\nobserve that LLMs exhibit key social network principles, including preferential\nattachment, triadic closure, homophily, community structure, and the\nsmall-world phenomenon, when asked about their preferences in network\nformation. We also investigate LLMs' decision-making based on real-world\nnetworks, revealing that triadic closure and homophily have a stronger\ninfluence than preferential attachment and that LLMs perform well in network\nformation predictions. Overall, our study opens up new possibilities for using\nLLMs in network science research and helps develop socially aware LLMs by\nshedding light on their network formation behaviors and exploring their impacts\non social dynamics.\n","authors":["Marios Papachristou","Yuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.10659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08035v1","updated":"2024-03-12T19:12:28Z","published":"2024-03-12T19:12:28Z","title":"Harnessing Artificial Intelligence to Combat Online Hate: Exploring the\n  Challenges and Opportunities of Large Language Models in Hate Speech\n  Detection","summary":"  Large language models (LLMs) excel in many diverse applications beyond\nlanguage generation, e.g., translation, summarization, and sentiment analysis.\nOne intriguing application is in text classification. This becomes pertinent in\nthe realm of identifying hateful or toxic speech -- a domain fraught with\nchallenges and ethical dilemmas. In our study, we have two objectives: firstly,\nto offer a literature review revolving around LLMs as classifiers, emphasizing\ntheir role in detecting and classifying hateful or toxic content. Subsequently,\nwe explore the efficacy of several LLMs in classifying hate speech: identifying\nwhich LLMs excel in this task as well as their underlying attributes and\ntraining. Providing insight into the factors that contribute to an LLM\nproficiency (or lack thereof) in discerning hateful content. By combining a\ncomprehensive literature review with an empirical analysis, our paper strives\nto shed light on the capabilities and constraints of LLMs in the crucial domain\nof hate speech detection.\n","authors":["Tharindu Kumarage","Amrita Bhattacharjee","Joshua Garland"],"pdf_url":"https://arxiv.org/pdf/2403.08035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14336v3","updated":"2024-03-12T18:54:12Z","published":"2023-05-23T17:58:10Z","title":"Schema-Driven Information Extraction from Heterogeneous Tables","summary":"  In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we present a benchmark comprised of tables\nfrom four diverse domains: machine learning papers, chemistry literature,\nmaterial science journals, and webpages. We use this collection of annotated\ntables to evaluate the ability of open-source and API-based language models to\nextract information from tables covering diverse domains and data formats. Our\nexperiments demonstrate that surprisingly competitive performance can be\nachieved without requiring task-specific pipelines or labels, achieving F1\nscores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover,\nthrough detailed ablation studies and analyses, we investigate the factors\ncontributing to model success and validate the practicality of distilling\ncompact models to reduce API reliance.\n","authors":["Fan Bai","Junmo Kang","Gabriel Stanovsky","Dayne Freitag","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2305.14336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07950v2","updated":"2024-03-12T18:34:05Z","published":"2024-01-15T20:22:21Z","title":"SciGLM: Training Scientific Language Models with Self-Reflective\n  Instruction Annotation and Tuning","summary":"  Large Language Models (LLMs) have shown promise in assisting scientific\ndiscovery. However, such applications are currently limited by LLMs'\ndeficiencies in understanding intricate scientific concepts, deriving symbolic\nequations, and solving advanced numerical calculations. To bridge these gaps,\nwe introduce SciGLM, a suite of scientific language models able to conduct\ncollege-level scientific reasoning. Central to our approach is a novel\nself-reflective instruction annotation framework to address the data scarcity\nchallenge in the science domain. This framework leverages existing LLMs to\ngenerate step-by-step reasoning for unlabelled scientific questions, followed\nby a process of self-reflective critic-and-revise. Applying this framework, we\ncurated SciInstruct, a diverse and high-quality dataset encompassing physics,\nchemistry, math, and formal proofs. We fine-tuned the ChatGLM family of\nlanguage models with SciInstruct, enhancing their scientific and mathematical\nreasoning capabilities. Remarkably, the SciGLM consistently improves both the\nbase model (ChatGLM3-6B-Base) by 4.87% and larger-scale models (32B) by 2.67%,\nwithout sacrificing the language understanding capabilities of the base model.\nThis makes SciGLM a suitable foundational model to facilitate diverse\nscientific discovery tasks. For the benefit of the wider research community, we\nrelease SciInstruct, and SciGLM, alongside a self-reflective framework and\nfine-tuning code at https://github.com/THUDM/SciGLM.\n","authors":["Dan Zhang","Ziniu Hu","Sining Zhoubian","Zhengxiao Du","Kaiyu Yang","Zihan Wang","Yisong Yue","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2401.07950v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2403.08011v1","updated":"2024-03-12T18:21:20Z","published":"2024-03-12T18:21:20Z","title":"Gujarati-English Code-Switching Speech Recognition using ensemble\n  prediction of spoken language","summary":"  An important and difficult task in code-switched speech recognition is to\nrecognize the language, as lots of words in two languages can sound similar,\nespecially in some accents. We focus on improving performance of end-to-end\nAutomatic Speech Recognition models by conditioning transformer layers on\nlanguage ID of words and character in the output in an per layer supervised\nmanner. To this end, we propose two methods of introducing language specific\nparameters and explainability in the multi-head attention mechanism, and\nimplement a Temporal Loss that helps maintain continuity in input alignment.\nDespite being unable to reduce WER significantly, our method shows promise in\npredicting the correct language from just spoken data. We introduce\nregularization in the language prediction by dropping LID in the sequence,\nwhich helps align long repeated output sequences.\n","authors":["Yash Sharma","Basil Abraham","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2403.08011v1.pdf","comment":"Bachelor's thesis, 28 pages, includes appendix"},{"id":"http://arxiv.org/abs/2403.08010v1","updated":"2024-03-12T18:19:47Z","published":"2024-03-12T18:19:47Z","title":"Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological\n  Analysis Based on LLM","summary":"  How can we construct an automated debate judge to evaluate an extensive,\nvibrant, multi-turn debate? This task is challenging, as judging a debate\ninvolves grappling with lengthy texts, intricate argument relationships, and\nmulti-dimensional assessments. At the same time, current research mainly\nfocuses on short dialogues, rarely touching upon the evaluation of an entire\ndebate. In this paper, by leveraging Large Language Models (LLMs), we propose\nDebatrix, which makes the analysis and assessment of multi-turn debates more\naligned with majority preferences. Specifically, Debatrix features a vertical,\niterative chronological analysis and a horizontal, multi-dimensional evaluation\ncollaboration. To align with real-world debate scenarios, we introduced the\nPanelBench benchmark, comparing our system's performance to actual debate\noutcomes. The findings indicate a notable enhancement over directly using LLMs\nfor debate evaluation. Source code and benchmark data are available online at\nhttps://github.com/ljcleo/Debatrix .\n","authors":["Jingcong Liang","Rong Ye","Meng Han","Ruofei Lai","Xinyu Zhang","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2403.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08004v1","updated":"2024-03-12T18:12:50Z","published":"2024-03-12T18:12:50Z","title":"Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing","summary":"  The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset.\n","authors":["Rodrigo Santos","João Silva","António Branco"],"pdf_url":"https://arxiv.org/pdf/2403.08004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08002v1","updated":"2024-03-12T18:12:02Z","published":"2024-03-12T18:12:02Z","title":"Training Small Multimodal Models to Bridge Biomedical Competency Gap: A\n  Case Study in Radiology Imaging","summary":"  The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such large models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world applications. Frontier models such as GPT-4V still have\nmajor competency gaps in multimodal capabilities for biomedical applications.\nMoreover, pragmatic issues such as access, cost, latency, and compliance make\nit hard for clinicians to use privately-hosted state-of-the-art large models\ndirectly on private patient data. In this paper, we explore training\nopen-source small multimodal models (SMMs) to bridge biomedical competency gaps\nfor unmet clinical needs. To maximize data efficiency, we adopt a modular\napproach by incorporating state-of-the-art pre-trained models for image and\ntext modalities, and focusing on training a lightweight adapter to ground each\nmodality to the text embedding space. We conduct a comprehensive study of this\napproach on radiology imaging. For training, we assemble a large dataset with\nover 1 million image-text pairs. For evaluation, we propose a clinically driven\nnovel approach using GPT-4 and demonstrate its parity with expert evaluation.\nWe also study grounding qualitatively using attention. For best practice, we\nconduct a systematic ablation study on various choices in data engineering and\nmultimodal training. The resulting LLaVA-Rad (7B) model attains\nstate-of-the-art results on radiology tasks such as report generation and\ncross-modal retrieval, even outperforming much larger models such as GPT-4V and\nMed-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in\nprivate settings, offering a promising state-of-the-art tool for real-world\nclinical applications.\n","authors":["Juan Manuel Zambrano Chaves","Shih-Cheng Huang","Yanbo Xu","Hanwen Xu","Naoto Usuyama","Sheng Zhang","Fei Wang","Yujia Xie","Mahmoud Khademi","Ziyi Yang","Hany Awadalla","Julia Gong","Houdong Hu","Jianwei Yang","Chunyuan Li","Jianfeng Gao","Yu Gu","Cliff Wong","Mu Wei","Tristan Naumann","Muhao Chen","Matthew P. Lungren","Serena Yeung-Levy","Curtis P. Langlotz","Sheng Wang","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2403.08002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07974v1","updated":"2024-03-12T17:58:04Z","published":"2024-03-12T17:58:04Z","title":"LiveCodeBench: Holistic and Contamination Free Evaluation of Large\n  Language Models for Code","summary":"  Large Language Models (LLMs) applied to code-related applications have\nemerged as a prominent field, attracting significant interest from both\nacademia and industry. However, as new and improved LLMs are developed,\nexisting evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient\nfor assessing their capabilities. In this work, we propose LiveCodeBench, a\ncomprehensive and contamination-free evaluation of LLMs for code, which\ncontinuously collects new problems over time from contests across three\ncompetition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our\nbenchmark also focuses on a broader range of code related capabilities, such as\nself-repair, code execution, and test output prediction, beyond just code\ngeneration. Currently, LiveCodeBench hosts four hundred high-quality coding\nproblems that were published between May 2023 and February 2024. We have\nevaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We\npresent empirical findings on contamination, holistic performance comparisons,\npotential overfitting in existing benchmarks as well as individual model\ncomparisons. We will release all prompts and model completions for further\ncommunity analysis, along with a general toolkit for adding new scenarios and\nmodel\n","authors":["Naman Jain","King Han","Alex Gu","Wen-Ding Li","Fanjia Yan","Tianjun Zhang","Sida Wang","Armando Solar-Lezama","Koushik Sen","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2403.07974v1.pdf","comment":"Website - https://livecodebench.github.io/"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2402.13254v2","updated":"2024-03-12T17:59:56Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v2.pdf","comment":"13 pages, 6 figures, 8 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2403.07874v1","updated":"2024-03-12T17:59:51Z","published":"2024-03-12T17:59:51Z","title":"Beyond Text: Frozen Large Language Models in Visual Signal Comprehension","summary":"  In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.\n","authors":["Lei Zhu","Fangyun Wei","Yanye Lu"],"pdf_url":"https://arxiv.org/pdf/2403.07874v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07860v1","updated":"2024-03-12T17:50:11Z","published":"2024-03-12T17:50:11Z","title":"Bridging Different Language Models and Generative Vision Models for\n  Text-to-Image Generation","summary":"  Text-to-image generation has made significant advancements with the\nintroduction of text-to-image diffusion models. These models typically consist\nof a language model that interprets user prompts and a vision model that\ngenerates corresponding images. As language and vision models continue to\nprogress in their respective domains, there is a great potential in exploring\nthe replacement of components in text-to-image diffusion models with more\nadvanced counterparts. A broader research objective would therefore be to\ninvestigate the integration of any two unrelated language and generative vision\nmodels for text-to-image generation. In this paper, we explore this objective\nand propose LaVi-Bridge, a pipeline that enables the integration of diverse\npre-trained language models and generative vision models for text-to-image\ngeneration. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and\nplug-and-play approach without requiring modifications to the original weights\nof the language and vision models. Our pipeline is compatible with various\nlanguage models and generative vision models, accommodating different\nstructures. Within this framework, we demonstrate that incorporating superior\nmodules, such as more advanced language models or generative vision models,\nresults in notable improvements in capabilities like text alignment or image\nquality. Extensive evaluations have been conducted to verify the effectiveness\nof LaVi-Bridge. Code is available at\nhttps://github.com/ShihaoZhaoZSH/LaVi-Bridge.\n","authors":["Shihao Zhao","Shaozhe Hao","Bojia Zi","Huaizhe Xu","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2403.07860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04079v3","updated":"2024-03-12T17:47:59Z","published":"2024-01-08T18:31:38Z","title":"RudolfV: A Foundation Model by Pathologists for Pathologists","summary":"  Histopathology plays a central role in clinical medicine and biomedical\nresearch. While artificial intelligence shows promising results on many\npathological tasks, generalization and dealing with rare diseases, where\ntraining data is scarce, remains a challenge. Distilling knowledge from\nunlabelled data into a foundation model before learning from, potentially\nlimited, labelled data provides a viable path to address these challenges. In\nthis work, we extend the state of the art of foundation models for digital\npathology whole slide images by semi-automated data curation and incorporating\npathologist domain knowledge. Specifically, we combine computational and\npathologist domain knowledge (1) to curate a diverse dataset of 133k slides\ncorresponding to 1.2 billion image patches covering data from different\nfixation, staining, and scanning protocols as well as data from different\nindications and labs across the EU and US, (2) for grouping semantically\nsimilar slides and tissue patches, and (3) to augment the input images during\ntraining. We evaluate the resulting model on a set of public and internal\nbenchmarks and show that although our foundation model is trained with an order\nof magnitude less slides, it performs on par or better than competing models.\nWe expect that scaling our approach to more data and larger models will further\nincrease its performance and capacity to deal with increasingly complex real\nworld tasks in diagnostics and biomedical research.\n","authors":["Jonas Dippel","Barbara Feulner","Tobias Winterhoff","Simon Schallenberg","Gabriel Dernbach","Andreas Kunft","Stephan Tietz","Timo Milbich","Simon Heinke","Marie-Lisa Eich","Julika Ribbat-Idel","Rosemarie Krupar","Philipp Jurmeister","David Horst","Lukas Ruff","Klaus-Robert Müller","Frederick Klauschen","Maximilian Alber"],"pdf_url":"https://arxiv.org/pdf/2401.04079v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14865v3","updated":"2024-03-12T17:45:45Z","published":"2023-09-26T11:42:56Z","title":"Unsupervised Multi-Person 3D Human Pose Estimation From 2D Poses Alone","summary":"  Current unsupervised 2D-3D human pose estimation (HPE) methods do not work in\nmulti-person scenarios due to perspective ambiguity in monocular images.\nTherefore, we present one of the first studies investigating the feasibility of\nunsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing on\nreconstructing human interactions. To address the issue of perspective\nambiguity, we expand upon prior work by predicting the cameras' elevation angle\nrelative to the subjects' pelvis. This allows us to rotate the predicted poses\nto be level with the ground plane, while obtaining an estimate for the vertical\noffset in 3D between individuals. Our method involves independently lifting\neach subject's 2D pose to 3D, before combining them in a shared 3D coordinate\nsystem. The poses are then rotated and offset by the predicted elevation angle\nbefore being scaled. This by itself enables us to retrieve an accurate 3D\nreconstruction of their poses. We present our results on the CHI3D dataset,\nintroducing its use for unsupervised 2D-3D pose estimation with three new\nquantitative metrics, and establishing a benchmark for future research.\n","authors":["Peter Hardy","Hansung Kim"],"pdf_url":"https://arxiv.org/pdf/2309.14865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07854v1","updated":"2024-03-12T17:44:45Z","published":"2024-03-12T17:44:45Z","title":"Distilling the Knowledge in Data Pruning","summary":"  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n","authors":["Emanuel Ben-Baruch","Adam Botach","Igor Kviatkovsky","Manoj Aggarwal","Gérard Medioni"],"pdf_url":"https://arxiv.org/pdf/2403.07854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07851v1","updated":"2024-03-12T17:43:20Z","published":"2024-03-12T17:43:20Z","title":"12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems\nto expand their inference capabilities to new classes using only a few labeled\nexamples, without forgetting the previously learned classes. Classical\nbackpropagation-based learning and its variants are often unsuitable for\nbattery-powered, memory-constrained systems at the extreme edge. In this work,\nwe introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a\nlightweight model consisting of a pretrained and metalearned feature extractor\nand an expandable explicit memory storing the class prototypes. The\narchitecture is pretrained with a novel feature orthogonality regularization\nand metalearned with a multi-margin loss. For learning a new class, our\napproach extends the explicit memory with novel class prototypes, while the\nremaining architecture is kept frozen. This allows learning previously unseen\nclasses based on only a few examples with one single pass (hence online).\nO-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,\nachieving state-of-the-art results. Tailored for ultra-low-power platforms, we\nimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online\nlearning capabilities within just 12 mJ per new class.\n","authors":["Yoga Esa Wibowo","Cristian Cioflan","Thorir Mar Ingolfsson","Michael Hersche","Leo Zhao","Abbas Rahimi","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.07851v1.pdf","comment":"6 pages, 4 tables, 3 figures. Accepted at IEEE DATE 2024"},{"id":"http://arxiv.org/abs/2308.07652v2","updated":"2024-03-12T17:41:56Z","published":"2023-08-15T09:00:21Z","title":"Geometry of the Visual Cortex with Applications to Image Inpainting and\n  Enhancement","summary":"  Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structure\ninspired by the visual cortex V1, we propose algorithms for image inpainting\nand enhancement based on hypoelliptic diffusion. We innovate on previous\nimplementations of the methods by Citti, Sarti, and Boscain et al., by\nproposing an alternative that prevents fading and is capable of producing\nsharper results in a procedure that we call WaxOn-WaxOff. We also exploit the\nsub-Riemannian structure to define a completely new unsharp filter using\n$SE(2)$, analogous to the classical unsharp filter for 2D image processing. We\ndemonstrate our method on blood vessels enhancement in retinal scans.\n","authors":["Francesco Ballerin","Erlend Grong"],"pdf_url":"https://arxiv.org/pdf/2308.07652v2.pdf","comment":"Associated python package available at\n  https://github.com/ballerin/v1diffusion"},{"id":"http://arxiv.org/abs/2403.06025v2","updated":"2024-03-12T17:35:29Z","published":"2024-03-09T22:25:14Z","title":"CarbonNet: How Computer Vision Plays a Role in Climate Change?\n  Application: Learning Geomechanics from Subsurface Geometry of CCS to\n  Mitigate Global Warming","summary":"  We introduce a new approach using computer vision to predict the land surface\ndisplacement from subsurface geometry images for Carbon Capture and\nSequestration (CCS). CCS has been proved to be a key component for a carbon\nneutral society. However, scientists see there are challenges along the way\nincluding the high computational cost due to the large model scale and\nlimitations to generalize a pre-trained model with complex physics. We tackle\nthose challenges by training models directly from the subsurface geometry\nimages. The goal is to understand the respons of land surface displacement due\nto carbon injection and utilize our trained models to inform decision making in\nCCS projects.\n  We implement multiple models (CNN, ResNet, and ResNetUNet) for static\nmechanics problem, which is a image prediction problem. Next, we use the LSTM\nand transformer for transient mechanics scenario, which is a video prediction\nproblem. It shows ResNetUNet outperforms the others thanks to its architecture\nin static mechanics problem, and LSTM shows comparable performance to\ntransformer in transient problem. This report proceeds by outlining our dataset\nin detail followed by model descriptions in method section. Result and\ndiscussion state the key learning, observations, and conclusion with future\nwork rounds out the paper.\n","authors":["Wei Chen","Yunan Li","Yuan Tian"],"pdf_url":"https://arxiv.org/pdf/2403.06025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07839v1","updated":"2024-03-12T17:24:26Z","published":"2024-03-12T17:24:26Z","title":"MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with\n  Module-wise Pruning Error Metric","summary":"  Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.\n","authors":["Haokun Lin","Haoli Bai","Zhili Liu","Lu Hou","Muyi Sun","Linqi Song","Ying Wei","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2403.07839v1.pdf","comment":"18 pages, 8 figures, Published in CVPR2024"},{"id":"http://arxiv.org/abs/2311.02058v3","updated":"2024-03-12T17:23:55Z","published":"2023-11-03T17:38:35Z","title":"LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery","summary":"  We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.\n","authors":["Weikang Wan","Yifeng Zhu","Rutav Shah","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.02058v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.07834v1","updated":"2024-03-12T17:17:20Z","published":"2024-03-12T17:17:20Z","title":"When Eye-Tracking Meets Machine Learning: A Systematic Review on\n  Applications in Medical Image Analysis","summary":"  Eye-gaze tracking research offers significant promise in enhancing various\nhealthcare-related tasks, above all in medical image analysis and\ninterpretation. Eye tracking, a technology that monitors and records the\nmovement of the eyes, provides valuable insights into human visual attention\npatterns. This technology can transform how healthcare professionals and\nmedical specialists engage with and analyze diagnostic images, offering a more\ninsightful and efficient approach to medical diagnostics. Hence, extracting\nmeaningful features and insights from medical images by leveraging eye-gaze\ndata improves our understanding of how radiologists and other medical experts\nmonitor, interpret, and understand images for diagnostic purposes. Eye-tracking\ndata, with intricate human visual attention patterns embedded, provides a\nbridge to integrating artificial intelligence (AI) development and human\ncognition. This integration allows novel methods to incorporate domain\nknowledge into machine learning (ML) and deep learning (DL) approaches to\nenhance their alignment with human-like perception and decision-making.\nMoreover, extensive collections of eye-tracking data have also enabled novel\nML/DL methods to analyze human visual patterns, paving the way to a better\nunderstanding of human vision, attention, and cognition. This systematic review\ninvestigates eye-gaze tracking applications and methodologies for enhancing\nML/DL algorithms for medical image analysis in depth.\n","authors":["Sahar Moradizeyveh","Mehnaz Tabassum","Sidong Liu","Robert Ahadizad Newport","Amin Beheshti","Antonio Di Ieva"],"pdf_url":"https://arxiv.org/pdf/2403.07834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02129v3","updated":"2024-03-12T16:58:53Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07818v1","updated":"2024-03-12T16:57:56Z","published":"2024-03-12T16:57:56Z","title":"Label Dropout: Improved Deep Learning Echocardiography Segmentation\n  Using Multiple Datasets With Domain Shift and Partial Labelling","summary":"  Echocardiography (echo) is the first imaging modality used when assessing\ncardiac function. The measurement of functional biomarkers from echo relies\nupon the segmentation of cardiac structures and deep learning models have been\nproposed to automate the segmentation process. However, in order to translate\nthese tools to widespread clinical use it is important that the segmentation\nmodels are robust to a wide variety of images (e.g. acquired from different\nscanners, by operators with different levels of expertise etc.). To achieve\nthis level of robustness it is necessary that the models are trained with\nmultiple diverse datasets. A significant challenge faced when training with\nmultiple diverse datasets is the variation in label presence, i.e. the combined\ndata are often partially-labelled. Adaptations of the cross entropy loss\nfunction have been proposed to deal with partially labelled data. In this paper\nwe show that training naively with such a loss function and multiple diverse\ndatasets can lead to a form of shortcut learning, where the model associates\nlabel presence with domain characteristics, leading to a drop in performance.\nTo address this problem, we propose a novel label dropout scheme to break the\nlink between domain characteristics and the presence or absence of labels. We\ndemonstrate that label dropout improves echo segmentation Dice score by 62% and\n25% on two cardiac structures when training using multiple diverse partially\nlabelled datasets.\n","authors":["Iman Islam","Esther Puyol-Antón","Bram Ruijsink","Andrew J. Reader","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2403.07818v1.pdf","comment":"10 pages, 5 figures, submitted to MICCAI conference"},{"id":"http://arxiv.org/abs/2402.19344v3","updated":"2024-03-12T16:49:56Z","published":"2024-02-29T16:49:38Z","title":"The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition","summary":"  This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)\nCompetition, which is part of the respective Workshop held in conjunction with\nIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges in\nunderstanding human emotions and behaviors, crucial for the development of\nhuman-centered technologies. In more detail, the Competition focuses on affect\nrelated benchmarking tasks and comprises of five sub-challenges: i)\nValence-Arousal Estimation (the target is to estimate two continuous affect\ndimensions, valence and arousal), ii) Expression Recognition (the target is to\nrecognise between the mutually exclusive classes of the 7 basic expressions and\n'other'), iii) Action Unit Detection (the target is to detect 12 action units),\niv) Compound Expression Recognition (the target is to recognise between the 7\nmutually exclusive compound expression classes), and v) Emotional Mimicry\nIntensity Estimation (the target is to estimate six continuous emotion\ndimensions). In the paper, we present these Challenges, describe their\nrespective datasets and challenge protocols (we outline the evaluation metrics)\nand present the baseline systems as well as their obtained performance. More\ninformation for the Competition can be found in:\nhttps://affective-behavior-analysis-in-the-wild.github.io/6th.\n","authors":["Dimitrios Kollias","Panagiotis Tzirakis","Alan Cowen","Stefanos Zafeiriou","Irene Kotsia","Alice Baird","Chris Gagne","Chunchang Shao","Guanyu Hu"],"pdf_url":"https://arxiv.org/pdf/2402.19344v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07807v1","updated":"2024-03-12T16:44:52Z","published":"2024-03-12T16:44:52Z","title":"StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting","summary":"  We introduce StyleGaussian, a novel 3D style transfer technique that allows\ninstant transfer of any image's style to a 3D scene at 10 frames per second\n(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style\ntransfer without compromising its real-time rendering ability and multi-view\nconsistency. It achieves instant style transfer with three steps: embedding,\ntransfer, and decoding. Initially, 2D VGG scene features are embedded into\nreconstructed 3D Gaussians. Next, the embedded features are transformed\naccording to a reference style image. Finally, the transformed features are\ndecoded into the stylized RGB. StyleGaussian has two novel designs. The first\nis an efficient feature rendering strategy that first renders low-dimensional\nfeatures and then maps them into high-dimensional features while embedding VGG\nfeatures. It cuts the memory consumption significantly and enables 3DGS to\nrender the high-dimensional memory-intensive features. The second is a\nK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\nfeatures, it eliminates the 2D CNN operations that compromise strict multi-view\nconsistency. Extensive experiments show that StyleGaussian achieves instant 3D\nstylization with superior stylization quality while preserving real-time\nrendering and strict multi-view consistency. Project page:\nhttps://kunhao-liu.github.io/StyleGaussian/\n","authors":["Kunhao Liu","Fangneng Zhan","Muyu Xu","Christian Theobalt","Ling Shao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2403.07807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08359v2","updated":"2024-03-12T16:40:28Z","published":"2023-11-14T18:01:15Z","title":"Rotation-Agnostic Image Representation Learning for Digital Pathology","summary":"  This paper addresses complex challenges in histopathological image analysis\nthrough three key contributions. Firstly, it introduces a fast patch selection\nmethod, FPS, for whole-slide image (WSI) analysis, significantly reducing\ncomputational cost while maintaining accuracy. Secondly, it presents PathDino,\na lightweight histopathology feature extractor with a minimal configuration of\nfive Transformer blocks and only 9 million parameters, markedly fewer than\nalternatives. Thirdly, it introduces a rotation-agnostic representation\nlearning paradigm using self-supervised learning, effectively mitigating\noverfitting. We also show that our compact model outperforms existing\nstate-of-the-art histopathology-specific vision transformers on 12 diverse\ndatasets, including both internal datasets spanning four sites (breast, liver,\nskin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,\nDigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training\ndataset of 6 million histopathology patches from The Cancer Genome Atlas\n(TCGA), our approach demonstrates an average 8.5% improvement in patch-level\nmajority vote performance. These contributions provide a robust framework for\nenhancing image analysis in digital pathology, rigorously validated through\nextensive evaluation. Project Page:\nhttps://kimialabmayo.github.io/PathDino-Page/\n","authors":["Saghir Alfasly","Abubakr Shafique","Peyman Nejat","Jibran Khan","Areej Alsaafin","Ghazal Alabtah","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2311.08359v2.pdf","comment":"CVPR 2024 - 23 pages, 10 figures, and 18 tables"},{"id":"http://arxiv.org/abs/2403.07800v1","updated":"2024-03-12T16:36:27Z","published":"2024-03-12T16:36:27Z","title":"BraSyn 2023 challenge: Missing MRI synthesis and the effect of different\n  learning objectives","summary":"  This work is addressing the Brain Magnetic Resonance Image Synthesis for\nTumor Segmentation (BraSyn) challenge which was hosted as part of the Brain\nTumor Segmentation challenge (BraTS) 2023. In this challenge researchers are\ninvited to work on synthesizing a missing magnetic resonance image sequence\ngiven other available sequences to facilitate tumor segmentation pipelines\ntrained on complete sets of image sequences. This problem can be addressed\nusing deep learning in the framework of paired images-to-image translation. In\nthis work, we proposed to investigate the effectiveness of a commonly-used deep\nlearning framework such as Pix2Pix trained under supervision of different\nimage-quality loss functions. Our results indicate that using different loss\nfunctions significantly affects the synthesis quality. We systematically study\nthe impact of different loss functions in the multi-sequence MR image synthesis\nsetting of the BraSyn challenge. Furthermore, we show how image synthesis\nperformance can be optimized by beneficially combining different learning\nobjectives.\n","authors":["Ivo M. Baltruschat","Parvaneh Janbakhshi","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2403.07800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.14100v2","updated":"2024-03-12T16:36:06Z","published":"2022-04-29T13:47:39Z","title":"Adversarial Distortion Learning for Medical Image Denoising","summary":"  We present a novel adversarial distortion learning (ADL) for denoising two-\nand three-dimensional (2D/3D) biomedical image data. The proposed ADL consists\nof two auto-encoders: a denoiser and a discriminator. The denoiser removes\nnoise from input data and the discriminator compares the denoised result to its\nnoise-free counterpart. This process is repeated until the discriminator cannot\ndifferentiate the denoised data from the reference. Both the denoiser and the\ndiscriminator are built upon a proposed auto-encoder called Efficient-Unet.\nEfficient-Unet has a light architecture that uses the residual blocks and a\nnovel pyramidal approach in the backbone to efficiently extract and re-use\nfeature maps. During training, the textural information and contrast are\ncontrolled by two novel loss functions. The architecture of Efficient-Unet\nallows generalizing the proposed method to any sort of biomedical data. The 2D\nversion of our network was trained on ImageNet and tested on biomedical\ndatasets whose distribution is completely different from ImageNet; so, there is\nno need for re-training. Experimental results carried out on magnetic resonance\nimaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show that\nthe proposed method achieved the best on each benchmark. Our implementation and\npre-trained models are available at https://github.com/mogvision/ADL.\n","authors":["Morteza Ghahremani","Mohammad Khateri","Alejandra Sierra","Jussi Tohka"],"pdf_url":"https://arxiv.org/pdf/2204.14100v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07798v1","updated":"2024-03-12T16:35:32Z","published":"2024-03-12T16:35:32Z","title":"A Fourier Transform Framework for Domain Adaptation","summary":"  By using unsupervised domain adaptation (UDA), knowledge can be transferred\nfrom a label-rich source domain to a target domain that contains relevant\ninformation but lacks labels. Many existing UDA algorithms suffer from directly\nusing raw images as input, resulting in models that overly focus on redundant\ninformation and exhibit poor generalization capability. To address this issue,\nwe attempt to improve the performance of unsupervised domain adaptation by\nemploying the Fourier method (FTF).Specifically, FTF is inspired by the\namplitude of Fourier spectra, which primarily preserves low-level statistical\ninformation. In FTF, we effectively incorporate low-level information from the\ntarget domain into the source domain by fusing the amplitudes of both domains\nin the Fourier domain. Additionally, we observe that extracting features from\nbatches of images can eliminate redundant information while retaining\nclass-specific features relevant to the task. Building upon this observation,\nwe apply the Fourier Transform at the data stream level for the first time. To\nfurther align multiple sources of data, we introduce the concept of correlation\nalignment. To evaluate the effectiveness of our FTF method, we conducted\nevaluations on four benchmark datasets for domain adaptation, including\nOffice-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results\ndemonstrate superior performance.\n","authors":["Le Luo","Bingrong Xu","Qingyong Zhang","Cheng Lian","Jie Luo"],"pdf_url":"https://arxiv.org/pdf/2403.07798v1.pdf","comment":"9 pages,5 figures"},{"id":"http://arxiv.org/abs/2309.07096v3","updated":"2024-03-12T16:30:34Z","published":"2023-08-23T12:37:13Z","title":"Computational limits to the legibility of the imaged human brain","summary":"  Our knowledge of the organisation of the human brain at the population-level\nis yet to translate into power to predict functional differences at the\nindividual-level, limiting clinical applications, and casting doubt on the\ngeneralisability of inferred mechanisms. It remains unknown whether the\ndifficulty arises from the absence of individuating biological patterns within\nthe brain, or from limited power to access them with the models and compute at\nour disposal. Here we comprehensively investigate the resolvability of such\npatterns with data and compute at unprecedented scale. Across 23 810 unique\nparticipants from UK Biobank, we systematically evaluate the predictability of\n25 individual biological characteristics, from all available combinations of\nstructural and functional neuroimaging data. Over 4526 GPU hours of\ncomputation, we train, optimize, and evaluate out-of-sample 700 individual\npredictive models, including fully-connected feed-forward neural networks of\ndemographic, psychological, serological, chronic disease, and functional\nconnectivity characteristics, and both uni- and multi-modal 3D convolutional\nneural network models of macro- and micro-structural brain imaging. We find a\nmarked discrepancy between the high predictability of sex (balanced accuracy\n99.7%), age (mean absolute error 2.048 years, R2 0.859), and weight (mean\nabsolute error 2.609Kg, R2 0.625), for which we set new state-of-the-art\nperformance, and the surprisingly low predictability of other characteristics.\nNeither structural nor functional imaging predicted psychology better than the\ncoincidence of chronic disease (p<0.05). Serology predicted chronic disease\n(p<0.05) and was best predicted by it (p<0.001), followed by structural\nneuroimaging (p<0.05). Our findings suggest either more informative imaging or\nmore powerful models are needed to decipher individual level characteristics\nfrom the human brain.\n","authors":["James K Ruffle","Robert J Gray","Samia Mohinta","Guilherme Pombo","Chaitanya Kaul","Harpreet Hyare","Geraint Rees","Parashkev Nachev"],"pdf_url":"https://arxiv.org/pdf/2309.07096v3.pdf","comment":"38 pages, 6 figures, 1 table, 2 supplementary figures, 1\n  supplementary table"},{"id":"http://arxiv.org/abs/2205.10766v2","updated":"2024-03-12T16:29:18Z","published":"2022-05-22T06:54:33Z","title":"Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey","summary":"  Multi-object tracking (MOT) aims to associate target objects across video\nframes in order to obtain entire moving trajectories. With the advancement of\ndeep neural networks and the increasing demand for intelligent video analysis,\nMOT has gained significantly increased interest in the computer vision\ncommunity. Embedding methods play an essential role in object location\nestimation and temporal identity association in MOT. Unlike other computer\nvision tasks, such as image classification, object detection,\nre-identification, and segmentation, embedding methods in MOT have large\nvariations, and they have never been systematically analyzed and summarized. In\nthis survey, we first conduct a comprehensive overview with in-depth analysis\nfor embedding methods in MOT from seven different perspectives, including\npatch-level embedding, single-frame embedding, cross-frame joint embedding,\ncorrelation embedding, sequential embedding, tracklet embedding, and\ncross-track relational embedding. We further summarize the existing widely used\nMOT datasets and analyze the advantages of existing state-of-the-art methods\naccording to their embedding strategies. Finally, some critical yet\nunder-investigated areas and future research directions are discussed.\n","authors":["Gaoang Wang","Mingli Song","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2205.10766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05854v2","updated":"2024-03-12T16:26:39Z","published":"2024-03-09T09:52:15Z","title":"LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content","summary":"  Long-tail recognition is challenging because it requires the model to learn\ngood representations from tail categories and address imbalances across all\ncategories. In this paper, we propose a novel generative and fine-tuning\nframework, LTGC, to handle long-tail recognition via leveraging generated\ncontent. Firstly, inspired by the rich implicit knowledge in large-scale models\n(e.g., large language models, LLMs), LTGC leverages the power of these models\nto parse and reason over the original tail data to produce diverse tail-class\ncontent. We then propose several novel designs for LTGC to ensure the quality\nof the generated data and to efficiently fine-tune the model using both the\ngenerated and original data. The visualization demonstrates the effectiveness\nof the generation module in LTGC, which produces accurate and diverse tail\ndata. Additionally, the experimental results demonstrate that our LTGC\noutperforms existing state-of-the-art methods on popular long-tailed\nbenchmarks.\n","authors":["Qihao Zhao","Yalun Dai","Hao Li","Wei Hu","Fan Zhang","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05854v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07788v1","updated":"2024-03-12T16:23:49Z","published":"2024-03-12T16:23:49Z","title":"DexCap: Scalable and Portable Mocap Data Collection System for Dexterous\n  Manipulation","summary":"  Imitation learning from human hand motion data presents a promising avenue\nfor imbuing robots with human-like dexterity in real-world manipulation tasks.\nDespite this potential, substantial challenges persist, particularly with the\nportability of existing hand motion capture (mocap) systems and the difficulty\nof translating mocap data into effective control policies. To tackle these\nissues, we introduce DexCap, a portable hand motion capture system, alongside\nDexIL, a novel imitation algorithm for training dexterous robot skills directly\nfrom human hand mocap data. DexCap offers precise, occlusion-resistant tracking\nof wrist and finger motions based on SLAM and electromagnetic field together\nwith 3D observations of the environment. Utilizing this rich dataset, DexIL\nemploys inverse kinematics and point cloud-based imitation learning to\nreplicate human actions with robot hands. Beyond learning from human motion,\nDexCap also offers an optional human-in-the-loop correction mechanism to refine\nand further improve robot performance. Through extensive evaluation across six\ndexterous manipulation tasks, our approach not only demonstrates superior\nperformance but also showcases the system's capability to effectively learn\nfrom in-the-wild mocap data, paving the way for future data collection methods\nfor dexterous manipulation. More details can be found at\nhttps://dex-cap.github.io\n","authors":["Chen Wang","Haochen Shi","Weizhuo Wang","Ruohan Zhang","Li Fei-Fei","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07786v1","updated":"2024-03-12T16:20:27Z","published":"2024-03-12T16:20:27Z","title":"Generative deep learning-enabled ultra-large field-of-view lens-free\n  imaging","summary":"  Advancements in high-throughput biomedical applications necessitate\nreal-time, large field-of-view (FOV) imaging capabilities. Conventional\nlens-free imaging (LFI) systems, while addressing the limitations of physical\nlenses, have been constrained by dynamic, hard-to-model optical fields,\nresulting in a limited one-shot FOV of approximately 20 $mm^2$. This\nrestriction has been a major bottleneck in applications like live-cell imaging\nand automation of microfluidic systems for biomedical research. Here, we\npresent a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging\ngenerative artificial intelligence (AI) for holographic image reconstruction.\nWe demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$,\nsurpassing the current LFI system by more than 20-fold, and even larger than\nthe world's largest confocal microscope by 1.76 times. The resolution is at the\nsub-pixel level of 5.52 $\\mu m$, without the need for a shifting light source.\nThe unsupervised learning-based reconstruction does not require optical field\nmodeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics\nand 3D cell models) in complex optical fields possible. This GenLFI framework\nunlocks the potential of LFI systems, offering a robust tool to tackle new\nfrontiers in high-throughput biomedical applications such as drug discovery.\n","authors":["Ronald B. Liu","Zhe Liu","Max G. A. Wolf","Krishna P. Purohit","Gregor Fritz","Yi Feng","Carsten G. Hansen","Pierre O. Bagnaninchi","Xavier Casadevall i Solvas","Yunjie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.07786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.15411v2","updated":"2024-03-12T16:16:00Z","published":"2021-12-31T12:39:04Z","title":"Disjoint Contrastive Regression Learning for Multi-Sourced Annotations","summary":"  Large-scale datasets are important for the development of deep learning\nmodels. Such datasets usually require a heavy workload of annotations, which\nare extremely time-consuming and expensive. To accelerate the annotation\nprocedure, multiple annotators may be employed to label different subsets of\nthe data. However, the inconsistency and bias among different annotators are\nharmful to the model training, especially for qualitative and subjective\ntasks.To address this challenge, in this paper, we propose a novel contrastive\nregression framework to address the disjoint annotations problem, where each\nsample is labeled by only one annotator and multiple annotators work on\ndisjoint subsets of the data. To take account of both the intra-annotator\nconsistency and inter-annotator inconsistency, two strategies are\nemployed.Firstly, a contrastive-based loss is applied to learn the relative\nranking among different samples of the same annotator, with the assumption that\nthe ranking of samples from the same annotator is unanimous. Secondly, we apply\nthe gradient reversal layer to learn robust representations that are invariant\nto different annotators. Experiments on the facial expression prediction task,\nas well as the image quality assessment task, verify the effectiveness of our\nproposed framework.\n","authors":["Xiaoqian Ruan","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2112.15411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03293v3","updated":"2024-03-12T16:15:45Z","published":"2023-07-06T21:08:03Z","title":"CheXmask: a large-scale dataset of anatomical segmentation masks for\n  multi-center chest x-ray images","summary":"  The development of successful artificial intelligence models for chest X-ray\nanalysis relies on large, diverse datasets with high-quality annotations. While\nseveral databases of chest X-ray images have been released, most include\ndisease diagnosis labels but lack detailed pixel-level anatomical segmentation\nlabels. To address this gap, we introduce an extensive chest X-ray multi-center\nsegmentation dataset with uniform and fine-grain anatomical annotations for\nimages coming from five well-known publicly available databases: ChestX-ray8,\nChexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in 657,566\nsegmentation masks. Our methodology utilizes the HybridGNet model to ensure\nconsistent and high-quality segmentations across all datasets. Rigorous\nvalidation, including expert physician evaluation and automatic quality\ncontrol, was conducted to validate the resulting masks. Additionally, we\nprovide individualized quality indices per mask and an overall quality\nestimation per dataset. This dataset serves as a valuable resource for the\nbroader scientific community, streamlining the development and assessment of\ninnovative methodologies in chest X-ray analysis. The CheXmask dataset is\npublicly available at:\nhttps://physionet.org/content/chexmask-cxr-segmentation-data/\n","authors":["Nicolás Gaggion","Candelaria Mosquera","Lucas Mansilla","Julia Mariel Saidman","Martina Aineseder","Diego H. Milone","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2307.03293v3.pdf","comment":"The CheXmask dataset is publicly available at\n  https://physionet.org/content/chexmask-cxr-segmentation-data/"},{"id":"http://arxiv.org/abs/2403.07773v1","updated":"2024-03-12T15:59:08Z","published":"2024-03-12T15:59:08Z","title":"SemCity: Semantic Scene Generation with Triplane Diffusion","summary":"  We present \"SemCity,\" a 3D diffusion model for semantic scene generation in\nreal-world outdoor environments. Most 3D diffusion models focus on generating a\nsingle object, synthetic indoor scenes, or synthetic outdoor scenes, while the\ngeneration of real-world outdoor scenes is rarely addressed. In this paper, we\nconcentrate on generating a real-outdoor scene through learning a diffusion\nmodel on a real-world outdoor dataset. In contrast to synthetic data,\nreal-outdoor datasets often contain more empty spaces due to sensor\nlimitations, causing challenges in learning real-outdoor distributions. To\naddress this issue, we exploit a triplane representation as a proxy form of\nscene distributions to be learned by our diffusion model. Furthermore, we\npropose a triplane manipulation that integrates seamlessly with our triplane\ndiffusion model. The manipulation improves our diffusion model's applicability\nin a variety of downstream tasks related to outdoor scene generation such as\nscene inpainting, scene outpainting, and semantic scene completion refinements.\nIn experimental results, we demonstrate that our triplane diffusion model shows\nmeaningful generation results compared with existing work in a real-outdoor\ndataset, SemanticKITTI. We also show our triplane manipulation facilitates\nseamlessly adding, removing, or modifying objects within a scene. Further, it\nalso enables the expansion of scenes toward a city-level scale. Finally, we\nevaluate our method on semantic scene completion refinements where our\ndiffusion model enhances predictions of semantic scene completion networks by\nlearning scene distribution. Our code is available at\nhttps://github.com/zoomin-lee/SemCity.\n","authors":["Jumin Lee","Sebin Lee","Changho Jo","Woobin Im","Juhyeong Seon","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.07773v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07764v1","updated":"2024-03-12T15:53:14Z","published":"2024-03-12T15:53:14Z","title":"Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model","summary":"  Current makeup transfer methods are limited to simple makeup styles, making\nthem difficult to apply in real-world scenarios. In this paper, we introduce\nStable-Makeup, a novel diffusion-based makeup transfer method capable of\nrobustly transferring a wide range of real-world makeup, onto user-provided\nfaces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a\nDetail-Preserving (D-P) makeup encoder to encode makeup details. It also\nemploys content and structural control modules to preserve the content and\nstructural information of the source image. With the aid of our newly added\nmakeup cross-attention layers in U-Net, we can accurately transfer the detailed\nmakeup to the corresponding position in the source image. After\ncontent-structure decoupling training, Stable-Makeup can maintain content and\nthe facial structure of the source image. Moreover, our method has demonstrated\nstrong robustness and generalizability, making it applicable to varioustasks\nsuch as cross-domain makeup transfer, makeup-guided text-to-image generation\nand so on. Extensive experiments have demonstrated that our approach delivers\nstate-of-the-art (SOTA) results among existing makeup transfer methods and\nexhibits a highly promising with broad potential applications in various\nrelated fields.\n","authors":["Yuxuan Zhang","Lifu Wei","Qing Zhang","Yiren Song","Jiaming Liu","Huaxia Li","Xu Tang","Yao Hu","Haibo Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.07764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14207v2","updated":"2024-03-12T15:40:08Z","published":"2023-03-24T18:00:15Z","title":"DiffuScene: Denoising Diffusion Models for Generative Indoor Scene\n  Synthesis","summary":"  We present DiffuScene for indoor 3D scene synthesis based on a novel scene\nconfiguration denoising diffusion model. It generates 3D instance properties\nstored in an unordered object set and retrieves the most similar geometry for\neach object configuration, which is characterized as a concatenation of\ndifferent attributes, including location, size, orientation, semantics, and\ngeometry features. We introduce a diffusion network to synthesize a collection\nof 3D indoor objects by denoising a set of unordered object attributes.\nUnordered parametrization simplifies and eases the joint distribution\napproximation. The shape feature diffusion facilitates natural object\nplacements, including symmetries. Our method enables many downstream\napplications, including scene completion, scene arrangement, and\ntext-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that\nour method can synthesize more physically plausible and diverse indoor scenes\nthan state-of-the-art methods. Extensive ablation studies verify the\neffectiveness of our design choice in scene diffusion models.\n","authors":["Jiapeng Tang","Yinyu Nie","Lev Markhasin","Angela Dai","Justus Thies","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2303.14207v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07752v1","updated":"2024-03-12T15:39:56Z","published":"2024-03-12T15:39:56Z","title":"Vision-based Vehicle Re-identification in Bridge Scenario using Flock\n  Similarity","summary":"  Due to the needs of road traffic flow monitoring and public safety\nmanagement, video surveillance cameras are widely distributed in urban roads.\nHowever, the information captured directly by each camera is siloed, making it\ndifficult to use it effectively. Vehicle re-identification refers to finding a\nvehicle that appears under one camera in another camera, which can correlate\nthe information captured by multiple cameras. While license plate recognition\nplays an important role in some applications, there are some scenarios where\nre-identification method based on vehicle appearance are more suitable. The\nmain challenge is that the data of vehicle appearance has the characteristics\nof high inter-class similarity and large intra-class differences. Therefore, it\nis difficult to accurately distinguish between different vehicles by relying\nonly on vehicle appearance information. At this time, it is often necessary to\nintroduce some extra information, such as spatio-temporal information.\nNevertheless, the relative position of the vehicles rarely changes when passing\nthrough two adjacent cameras in the bridge scenario. In this paper, we present\na vehicle re-identification method based on flock similarity, which improves\nthe accuracy of vehicle re-identification by utilizing vehicle information\nadjacent to the target vehicle. When the relative position of the vehicles\nremains unchanged and flock size is appropriate, we obtain an average relative\nimprovement of 204% on VeRi dataset in our experiments. Then, the effect of the\nmagnitude of the relative position change of the vehicles as they pass through\ntwo cameras is discussed. We present two metrics that can be used to quantify\nthe difference and establish a connection between them. Although this\nassumption is based on the bridge scenario, it is often true in other scenarios\ndue to driving safety and camera location.\n","authors":["Chunfeng Zhang","Ping Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07752v1.pdf","comment":"6 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.07750v1","updated":"2024-03-12T15:36:42Z","published":"2024-03-12T15:36:42Z","title":"Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and\n  Image Embeddings","summary":"  The creation of high-quality human-labeled image-caption datasets presents a\nsignificant bottleneck in the development of Visual-Language Models (VLMs). We\npropose a novel approach that leverages the strengths of Large Language Models\n(LLMs) and image generation models to create synthetic image-text pairs for\nefficient and effective VLM training. Our method employs pretraining a\ntext-to-image model to synthesize image embeddings starting from captions\ngenerated by an LLM. These synthetic pairs are then used to train a VLM.\nExtensive experiments demonstrate that the VLM trained with synthetic data\nexhibits comparable performance on image captioning, while requiring a fraction\nof the data used by models trained solely on human-annotated data. In\nparticular, we outperform the baseline by 17% through augmentation with a\nsynthetic dataset. Furthermore, we show that synthesizing in the image\nembedding space is 25% faster than in the pixel space. This research introduces\na promising technique for generating large-scale, customizable image datasets,\nleading to enhanced VLM performance and wider applicability across various\ndomains, all with improved data efficiency and resource utilization.\n","authors":["Sahand Sharifzadeh","Christos Kaplanis","Shreya Pathak","Dharshan Kumaran","Anastasija Ilic","Jovana Mitrovic","Charles Blundell","Andrea Banino"],"pdf_url":"https://arxiv.org/pdf/2403.07750v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.09696v3","updated":"2024-03-12T15:29:56Z","published":"2023-07-19T00:41:39Z","title":"Towards Saner Deep Image Registration","summary":"  With recent advances in computing hardware and surges of deep-learning\narchitectures, learning-based deep image registration methods have surpassed\ntheir traditional counterparts, in terms of metric performance and inference\ntime. However, these methods focus on improving performance measurements such\nas Dice, resulting in less attention given to model behaviors that are equally\ndesirable for registrations, especially for medical imaging. This paper\ninvestigates these behaviors for popular learning-based deep registrations\nunder a sanity-checking microscope. We find that most existing registrations\nsuffer from low inverse consistency and nondiscrimination of identical pairs\ndue to overly optimized image similarities. To rectify these behaviors, we\npropose a novel regularization-based sanity-enforcer method that imposes two\nsanity checks on the deep model to reduce its inverse consistency errors and\nincrease its discriminative power simultaneously. Moreover, we derive a set of\ntheoretical guarantees for our sanity-checked image registration method, with\nexperimental results supporting our theoretical findings and their\neffectiveness in increasing the sanity of models without sacrificing any\nperformance. Our code and models are available at\nhttps://github.com/tuffr5/Saner-deep-registration.\n","authors":["Bin Duan","Ming Zhong","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2307.09696v3.pdf","comment":"ICCV 2023, fix typos"},{"id":"http://arxiv.org/abs/2403.07746v1","updated":"2024-03-12T15:28:51Z","published":"2024-03-12T15:28:51Z","title":"Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified\n  3D Perception","summary":"  Low-cost, vision-centric 3D perception systems for autonomous driving have\nmade significant progress in recent years, narrowing the gap to expensive\nLiDAR-based methods. The primary challenge in becoming a fully reliable\nalternative lies in robust depth prediction capabilities, as camera-based\nsystems struggle with long detection ranges and adverse lighting and weather\nconditions. In this work, we introduce HyDRa, a novel camera-radar fusion\narchitecture for diverse 3D perception tasks. Building upon the principles of\ndense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid\nfusion approach to combine the strengths of complementary camera and radar\nfeatures in two distinct representation spaces. Our Height Association\nTransformer module leverages radar features already in the perspective view to\nproduce more robust and accurate depth predictions. In the BEV, we refine the\ninitial sparse representation by a Radar-weighted Depth Consistency. HyDRa\nachieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and\n58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new\nsemantically rich and spatially accurate BEV features can be directly converted\ninto a powerful occupancy representation, beating all previous camera-based\nmethods on the Occ3D benchmark by an impressive 3.7 mIoU.\n","authors":["Philipp Wolters","Johannes Gilg","Torben Teepe","Fabian Herzog","Anouar Laouichi","Martin Hofmann","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2403.07746v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.06977v2","updated":"2024-03-12T15:22:52Z","published":"2024-03-11T17:59:34Z","title":"VideoMamba: State Space Model for Efficient Video Understanding","summary":"  Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba.\n","authors":["Kunchang Li","Xinhao Li","Yi Wang","Yinan He","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2403.06977v2.pdf","comment":"19 Pages, 7 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2403.07743v1","updated":"2024-03-12T15:22:05Z","published":"2024-03-12T15:22:05Z","title":"Equipping Computational Pathology Systems with Artifact Processing\n  Pipelines: A Showcase for Computation and Performance Trade-offs","summary":"  Histopathology is a gold standard for cancer diagnosis under a microscopic\nexamination. However, histological tissue processing procedures result in\nartifacts, which are ultimately transferred to the digitized version of glass\nslides, known as whole slide images (WSIs). Artifacts are diagnostically\nirrelevant areas and may result in wrong deep learning (DL) algorithms\npredictions. Therefore, detecting and excluding artifacts in the computational\npathology (CPATH) system is essential for reliable automated diagnosis. In this\npaper, we propose a mixture of experts (MoE) scheme for detecting five notable\nartifacts, including damaged tissue, blur, folded tissue, air bubbles, and\nhistologically irrelevant blood from WSIs. First, we train independent binary\nDL models as experts to capture particular artifact morphology. Then, we\nensemble their predictions using a fusion mechanism. We apply probabilistic\nthresholding over the final probability distribution to improve the sensitivity\nof the MoE. We developed DL pipelines using two MoEs and two multiclass models\nof state-of-the-art deep convolutional neural networks (DCNNs) and vision\ntransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed\nsimpler multiclass models and were tested on datasets from different hospitals\nand cancer types, where MoE using DCNNs yielded the best results. The proposed\nMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining\nless computational cost for inference than MoE using ViTs. This best\nperformance of MoEs comes with relatively higher computational trade-offs than\nmulticlass models. The proposed artifact detection pipeline will not only\nensure reliable CPATH predictions but may also provide quality control.\n","authors":["Neel Kanwal","Farbod Khoraminia","Umay Kiraz","Andres Mosquera-Zamudio","Carlos Monteagudo","Emiel A. M. Janssen","Tahlita C. M. Zuiverloon","Chunmig Rong","Kjersti Engan"],"pdf_url":"https://arxiv.org/pdf/2403.07743v1.pdf","comment":"Submitted to BMC Medical Informatics and Decision Making Journal"},{"id":"http://arxiv.org/abs/2403.07741v1","updated":"2024-03-12T15:19:25Z","published":"2024-03-12T15:19:25Z","title":"Uncertainty Quantification with Deep Ensembles for 6D Object Pose\n  Estimation","summary":"  The estimation of 6D object poses is a fundamental task in many computer\nvision applications. Particularly, in high risk scenarios such as human-robot\ninteraction, industrial inspection, and automation, reliable pose estimates are\ncrucial. In the last years, increasingly accurate and robust\ndeep-learning-based approaches for 6D object pose estimation have been\nproposed. Many top-performing methods are not end-to-end trainable but consist\nof multiple stages. In the context of deep uncertainty quantification, deep\nensembles are considered as state of the art since they have been proven to\nproduce well-calibrated and robust uncertainty estimates. However, deep\nensembles can only be applied to methods that can be trained end-to-end. In\nthis work, we propose a method to quantify the uncertainty of multi-stage 6D\nobject pose estimation approaches with deep ensembles. For the implementation,\nwe choose SurfEmb as representative, since it is one of the top-performing 6D\nobject pose estimation approaches in the BOP Challenge 2022. We apply\nestablished metrics and concepts for deep uncertainty quantification to\nevaluate the results. Furthermore, we propose a novel uncertainty calibration\nscore for regression tasks to quantify the quality of the estimated\nuncertainty.\n","authors":["Kira Wursthorn","Markus Hillemann","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2403.07741v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.07733v1","updated":"2024-03-12T15:13:12Z","published":"2024-03-12T15:13:12Z","title":"DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven\n  Segmentation","summary":"  Explainable Artificial Intelligence is critical in unraveling decision-making\nprocesses in complex machine learning models. LIME (Local Interpretable\nModel-agnostic Explanations) is a well-known XAI framework for image analysis.\nIt utilizes image segmentation to create features to identify relevant areas\nfor classification. Consequently, poor segmentation can compromise the\nconsistency of the explanation and undermine the importance of the segments,\naffecting the overall interpretability. Addressing these challenges, we\nintroduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a\ndata-driven segmentation for human-recognized feature generation, and ii) a\nhierarchical segmentation procedure through composition. We benchmark DSEG-LIME\non pre-trained models with images from the ImageNet dataset - scenarios without\ndomain-specific knowledge. The analysis includes a quantitative evaluation\nusing established XAI metrics, complemented by a qualitative assessment through\na user study. Our findings demonstrate that DSEG outperforms in most of the XAI\nmetrics and enhances the alignment of explanations with human-recognized\nconcepts, significantly improving interpretability. The code is available\nunder: https://github. com/patrick-knab/DSEG-LIME\n","authors":["Patrick Knab","Sascha Marton","Christian Bartelt"],"pdf_url":"https://arxiv.org/pdf/2403.07733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02695v2","updated":"2024-03-12T15:12:55Z","published":"2022-11-04T18:26:47Z","title":"WaveNets: Wavelet Channel Attention Networks","summary":"  Channel Attention reigns supreme as an effective technique in the field of\ncomputer vision. However, the proposed channel attention by SENet suffers from\ninformation loss in feature learning caused by the use of Global Average\nPooling (GAP) to represent channels as scalars. Thus, designing effective\nchannel attention mechanisms requires finding a solution to enhance features\npreservation in modeling channel inter-dependencies. In this work, we utilize\nWavelet transform compression as a solution to the channel representation\nproblem. We first test wavelet transform as an Auto-Encoder model equipped with\nconventional channel attention module. Next, we test wavelet transform as a\nstandalone channel compression method. We prove that global average pooling is\nequivalent to the recursive approximate Haar wavelet transform. With this\nproof, we generalize channel attention using Wavelet compression and name it\nWaveNet. Implementation of our method can be embedded within existing channel\nattention methods with a couple of lines of code. We test our proposed method\nusing ImageNet dataset for image classification task. Our method outperforms\nthe baseline SENet, and achieves the state-of-the-art results. Our code\nimplementation is publicly available at https://github.com/hady1011/WaveNet-C.\n","authors":["Hadi Salman","Caleb Parks","Shi Yin Hong","Justin Zhan"],"pdf_url":"https://arxiv.org/pdf/2211.02695v2.pdf","comment":"IEEE BigData2022 conference"},{"id":"http://arxiv.org/abs/2308.05525v3","updated":"2024-03-12T15:05:23Z","published":"2023-08-10T12:06:03Z","title":"Robustifying Point Cloud Networks by Refocusing","summary":"  The ability to cope with out-of-distribution (OOD) corruptions and\nadversarial attacks is crucial in real-world safety-demanding applications. In\nthis study, we develop a general mechanism to increase neural network\nrobustness based on focus analysis.\n  Recent studies have revealed the phenomenon of \\textit{Overfocusing}, which\nleads to a performance drop. When the network is primarily influenced by small\ninput regions, it becomes less robust and prone to misclassify under noise and\ncorruptions.\n  However, quantifying overfocusing is still vague and lacks clear definitions.\nHere, we provide a mathematical definition of \\textbf{focus},\n\\textbf{overfocusing} and \\textbf{underfocusing}. The notions are general, but\nin this study, we specifically investigate the case of 3D point clouds.\n  We observe that corrupted sets result in a biased focus distribution compared\nto the clean training set.\n  We show that as focus distribution deviates from the one learned in the\ntraining phase - classification performance deteriorates.\n  We thus propose a parameter-free \\textbf{refocusing} algorithm that aims to\nunify all corruptions under the same distribution.\n  We validate our findings on a 3D zero-shot classification task, achieving\nSOTA in robust 3D classification on ModelNet-C dataset, and in adversarial\ndefense against Shape-Invariant attack. Code is available in:\nhttps://github.com/yossilevii100/refocusing.\n","authors":["Meir Yossef Levi","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2308.05525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07720v1","updated":"2024-03-12T14:58:52Z","published":"2024-03-12T14:58:52Z","title":"Multi-modal Auto-regressive Modeling via Visual Words","summary":"  Large Language Models (LLMs), benefiting from the auto-regressive modelling\napproach performed on massive unannotated texts corpora, demonstrates powerful\nperceptual and reasoning capabilities. However, as for extending\nauto-regressive modelling to multi-modal scenarios to build Large Multi-modal\nModels (LMMs), there lies a great difficulty that the image information is\nprocessed in the LMM as continuous visual embeddings, which cannot obtain\ndiscrete supervised labels for classification. In this paper, we successfully\nperform multi-modal auto-regressive modeling with a unified objective for the\nfirst time. Specifically, we propose the concept of visual words, which maps\nthe visual features to probability distributions over LLM's vocabulary,\nproviding supervision information for visual modelling. We further explore the\ndistribution of visual features in the semantic space within LMM and the\npossibility of using text embeddings to represent visual information.\nExperimental results and ablation studies on 5 VQA tasks and 4 benchmark\ntoolkits validate the powerful performance of our proposed approach.\n","authors":["Tianshuo Peng","Zuchao Li","Lefei Zhang","Hai Zhao","Ping Wang","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2403.07720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07719v1","updated":"2024-03-12T14:58:51Z","published":"2024-03-12T14:58:51Z","title":"Dynamic Graph Representation with Knowledge-aware Attention for\n  Histopathology Whole Slide Image Analysis","summary":"  Histopathological whole slide images (WSIs) classification has become a\nfoundation task in medical microscopic imaging processing. Prevailing\napproaches involve learning WSIs as instance-bag representations, emphasizing\nsignificant instances but struggling to capture the interactions between\ninstances. Additionally, conventional graph representation methods utilize\nexplicit spatial positions to construct topological structures but restrict the\nflexible interaction capabilities between instances at arbitrary locations,\nparticularly when spatially distant. In response, we propose a novel dynamic\ngraph representation algorithm that conceptualizes WSIs as a form of the\nknowledge graph structure. Specifically, we dynamically construct neighbors and\ndirected edge embeddings based on the head and tail relationships between\ninstances. Then, we devise a knowledge-aware attention mechanism that can\nupdate the head node features by learning the joint attention score of each\nneighbor and edge. Finally, we obtain a graph-level embedding through the\nglobal pooling process of the updated head, serving as an implicit\nrepresentation for the WSI classification. Our end-to-end graph representation\nlearning approach has outperformed the state-of-the-art WSI analysis methods on\nthree TCGA benchmark datasets and in-house test sets. Our code is available at\nhttps://github.com/WonderLandxD/WiKG.\n","authors":["Jiawen Li","Yuxuan Chen","Hongbo Chu","Qiehe Sun","Tian Guan","Anjia Han","Yonghong He"],"pdf_url":"https://arxiv.org/pdf/2403.07719v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07715v1","updated":"2024-03-12T14:57:57Z","published":"2024-03-12T14:57:57Z","title":"Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound","summary":"  Self-supervised learning (SSL) is one strategy for addressing the paucity of\nlabelled data in medical imaging by learning representations from unlabelled\nimages. Contrastive and non-contrastive SSL methods produce learned\nrepresentations that are similar for pairs of related images. Such pairs are\ncommonly constructed by randomly distorting the same image twice. The\nvideographic nature of ultrasound offers flexibility for defining the\nsimilarity relationship between pairs of images. In this study, we investigated\nthe effect of utilizing proximal, distinct images from the same B-mode\nultrasound video as pairs for SSL. Additionally, we introduced a sample\nweighting scheme that increases the weight of closer image pairs and\ndemonstrated how it can be integrated into SSL objectives. Named Intra-Video\nPositive Pairs (IVPP), the method surpassed previous ultrasound-specific\ncontrastive learning methods' average test accuracy on COVID-19 classification\nwith the POCUS dataset by $\\ge 1.3\\%$. Detailed investigations of IVPP's\nhyperparameters revealed that some combinations of IVPP hyperparameters can\nlead to improved or worsened performance, depending on the downstream task.\nGuidelines for practitioners were synthesized based on the results, such as the\nmerit of IVPP with task-specific hyperparameters, and the improved performance\nof contrastive methods for ultrasound compared to non-contrastive counterparts.\n","authors":["Blake VanBerlo","Alexander Wong","Jesse Hoey","Robert Arntfield"],"pdf_url":"https://arxiv.org/pdf/2403.07715v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.07711v1","updated":"2024-03-12T14:53:56Z","published":"2024-03-12T14:53:56Z","title":"SSM Meets Video Diffusion Models: Efficient Video Generation with\n  Structured State Spaces","summary":"  Given the remarkable achievements in image generation through diffusion\nmodels, the research community has shown increasing interest in extending these\nmodels to video generation. Recent diffusion models for video generation have\npredominantly utilized attention layers to extract temporal features. However,\nattention layers are limited by their memory consumption, which increases\nquadratically with the length of the sequence. This limitation presents\nsignificant challenges when attempting to generate longer video sequences using\ndiffusion models. To overcome this challenge, we propose leveraging state-space\nmodels (SSMs). SSMs have recently gained attention as viable alternatives due\nto their linear memory consumption relative to sequence length. In the\nexperiments, we first evaluate our SSM-based model with UCF101, a standard\nbenchmark of video generation. In addition, to investigate the potential of\nSSMs for longer video generation, we perform an experiment using the MineRL\nNavigate dataset, varying the number of frames to 64 and 150. In these\nsettings, our SSM-based model can considerably save memory consumption for\nlonger sequences, while maintaining competitive FVD scores to the\nattention-based models. Our codes are available at\nhttps://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.\n","authors":["Yuta Oshima","Shohei Taniguchi","Masahiro Suzuki","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2403.07711v1.pdf","comment":"Accepted as workshop paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07706v1","updated":"2024-03-12T14:51:23Z","published":"2024-03-12T14:51:23Z","title":"Fast and Simple Explainability for Point Cloud Networks","summary":"  We propose a fast and simple explainable AI (XAI) method for point cloud\ndata. It computes pointwise importance with respect to a trained network\ndownstream task. This allows better understanding of the network properties,\nwhich is imperative for safety-critical applications. In addition to debugging\nand visualization, our low computational complexity facilitates online feedback\nto the network at inference. This can be used to reduce uncertainty and to\nincrease robustness. In this work, we introduce \\emph{Feature Based\nInterpretability} (FBI), where we compute the features' norm, per point, before\nthe bottleneck. We analyze the use of gradients and post- and pre-bottleneck\nstrategies, showing pre-bottleneck is preferred, in terms of smoothness and\nranking. We obtain at least three orders of magnitude speedup, compared to\ncurrent XAI methods, thus, scalable for big point clouds or large-scale\narchitectures. Our approach achieves SOTA results, in terms of classification\nexplainability. We demonstrate how the proposed measure is helpful in analyzing\nand characterizing various aspects of 3D learning, such as rotation invariance,\nrobustness to out-of-distribution (OOD) outliers or domain shift and dataset\nbias.\n","authors":["Meir Yossef Levi","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2403.07706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07705v1","updated":"2024-03-12T14:50:05Z","published":"2024-03-12T14:50:05Z","title":"Robust Synthetic-to-Real Transfer for Stereo Matching","summary":"  With advancements in domain generalized stereo matching networks, models\npre-trained on synthetic data demonstrate strong robustness to unseen domains.\nHowever, few studies have investigated the robustness after fine-tuning them in\nreal-world scenarios, during which the domain generalization ability can be\nseriously degraded. In this paper, we explore fine-tuning stereo matching\nnetworks without compromising their robustness to unseen domains. Our\nmotivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for\nfine-tuning: GT degrades, but PL preserves the domain generalization ability.\nEmpirically, we find the difference between GT and PL implies valuable\ninformation that can regularize networks during fine-tuning. We also propose a\nframework to utilize this difference for fine-tuning, consisting of a frozen\nTeacher, an exponential moving average (EMA) Teacher, and a Student network.\nThe core idea is to utilize the EMA Teacher to measure what the Student has\nlearned and dynamically improve GT and PL for fine-tuning. We integrate our\nframework with state-of-the-art networks and evaluate its effectiveness on\nseveral real-world datasets. Extensive experiments show that our method\neffectively preserves the domain generalization ability during fine-tuning.\n","authors":["Jiawei Zhang","Jiahe Li","Lei Huang","Xiaohan Yu","Lin Gu","Jin Zheng","Xiao Bai"],"pdf_url":"https://arxiv.org/pdf/2403.07705v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2112.12989v3","updated":"2024-03-12T14:47:47Z","published":"2021-12-24T08:17:18Z","title":"Domain-Aware Continual Zero-Shot Learning","summary":"  Modern visual systems have a wide range of potential applications in vision\ntasks for natural science research, such as aiding in species discovery,\nmonitoring animals in the wild, and so on. However, real-world vision tasks may\nexperience changes in environmental conditions, leading to shifts in how\ncaptured images are presented. To address this issue, we introduce Domain-Aware\nContinual Zero-Shot Learning (DACZSL), a task to recognize images of unseen\ncategories in continuously changing domains. Accordingly, we propose a\nDomain-Invariant Network (DIN) to learn factorized features for shifting\ndomains and improved textual representation for unseen classes. DIN continually\nlearns a global shared network for domain-invariant and task-invariant\nfeatures, and per-task private networks for task-specific features.\nFurthermore, we enhance the dual network with class-wise learnable prompts to\nimprove class-level text representation, thereby improving zero-shot prediction\nof future unseen classes. To evaluate DACZSL, we introduce two benchmarks,\nDomainNet-CZSL and iWildCam-CZSL. Our results show that DIN significantly\noutperforms existing baselines by over 5% in harmonic accuracy and over 1% in\nbackward transfer and achieves a new SoTA.\n","authors":["Kai Yi","Paul Janson","Wenxuan Zhang","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2112.12989v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07700v1","updated":"2024-03-12T14:46:03Z","published":"2024-03-12T14:46:03Z","title":"CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive\n  Self-Supervised Transformers","summary":"  In this paper, we introduce VoteCut, an innovative method for unsupervised\nobject discovery that leverages feature representations from multiple\nself-supervised models. VoteCut employs normalized-cut based graph\npartitioning, clustering and a pixel voting approach. Additionally, We present\nCuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels,\ngenerated by VoteCut, and a novel soft target loss to refine segmentation\naccuracy. Through rigorous evaluations across multiple datasets and several\nunsupervised setups, our methods demonstrate significant improvements in\ncomparison to previous state-of-the-art models. Our ablation studies further\nhighlight the contributions of each component, revealing the robustness and\nefficacy of our approach. Collectively, VoteCut and CuVLER pave the way for\nfuture advancements in image segmentation.\n","authors":["Shahaf Arica","Or Rubin","Sapir Gershov","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2403.07700v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2401.14168v3","updated":"2024-03-12T14:45:49Z","published":"2024-01-25T13:27:03Z","title":"Vivim: a Video Vision Mamba for Medical Video Object Segmentation","summary":"  Traditional convolutional neural networks have a limited receptive field\nwhile transformer-based networks are mediocre in constructing long-term\ndependency from the perspective of computational complexity. Such the\nbottleneck poses a significant challenge when processing long sequences in\nvideo analysis tasks. Very recently, the state space models (SSMs) with\nefficient hardware-aware designs, famous by Mamba, have exhibited impressive\nachievements in long sequence modeling, which facilitates the development of\ndeep neural networks on many vision tasks. To better capture available dynamic\ncues in video frames, this paper presents a generic Video Vision Mamba-based\nframework, dubbed as \\textbf{Vivim}, for medical video object segmentation\ntasks. Our Vivim can effectively compress the long-term spatiotemporal\nrepresentation into sequences at varying scales by our designed Temporal Mamba\nBlock. We also introduce a boundary-aware constraint to enhance the\ndiscriminative ability of Vivim on ambiguous lesions in medical images.\nExtensive experiments on thyroid segmentation in ultrasound videos and polyp\nsegmentation in colonoscopy videos demonstrate the effectiveness and efficiency\nof our Vivim, superior to existing methods. The code is available at:\nhttps://github.com/scott-yjyang/Vivim.\n","authors":["Yijun Yang","Zhaohu Xing","Chunwang Huang","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.14168v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14527v3","updated":"2024-03-12T14:45:02Z","published":"2023-05-23T21:00:42Z","title":"Slovo: Russian Sign Language Dataset","summary":"  One of the main challenges of the sign language recognition task is the\ndifficulty of collecting a suitable dataset due to the gap between\nhard-of-hearing and hearing societies. In addition, the sign language in each\ncountry differs significantly, which obliges the creation of new data for each\nof them. This paper presents the Russian Sign Language (RSL) video dataset\nSlovo, produced using crowdsourcing platforms. The dataset contains 20,000\nFullHD recordings, divided into 1,000 classes of isolated RSL gestures received\nby 194 signers. We also provide the entire dataset creation pipeline, from data\ncollection to video annotation, with the following demo application. Several\nneural networks are trained and evaluated on the Slovo to demonstrate its\nteaching ability. Proposed data and pre-trained models are publicly available.\n","authors":["Alexander Kapitanov","Karina Kvanchiani","Alexander Nagaev","Elizaveta Petrova"],"pdf_url":"https://arxiv.org/pdf/2305.14527v3.pdf","comment":"russian sign language recognition dataset, open-source, 11 pages"},{"id":"http://arxiv.org/abs/2311.16510v2","updated":"2024-03-12T14:41:31Z","published":"2023-11-27T12:58:02Z","title":"Source-Free Domain Adaptation with Frozen Multimodal Foundation Model","summary":"  Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a\ntarget domain, with only access to unlabeled target training data and the\nsource model pre-trained on a supervised source domain. Relying on pseudo\nlabeling and/or auxiliary supervision, conventional methods are inevitably\nerror-prone. To mitigate this limitation, in this work we for the first time\nexplore the potentials of off-the-shelf vision-language (ViL) multimodal models\n(e.g.,CLIP) with rich whilst heterogeneous knowledge. We find that directly\napplying the ViL model to the target domain in a zero-shot fashion is\nunsatisfactory, as it is not specialized for this particular task but largely\ngeneric. To make it task specific, we propose a novel Distilling multimodal\nFoundation model(DIFO)approach. Specifically, DIFO alternates between two steps\nduring adaptation: (i) Customizing the ViL model by maximizing the mutual\ninformation with the target model in a prompt learning manner, (ii) Distilling\nthe knowledge of this customized ViL model to the target model. For more\nfine-grained and reliable distillation, we further introduce two effective\nregularization terms, namely most-likely category encouragement and predictive\nconsistency. Extensive experiments show that DIFO significantly outperforms the\nstate-of-the-art alternatives. Code is here\n","authors":["Song Tang","Wenxin Su","Mao Ye","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.16510v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.02142v3","updated":"2024-03-12T14:39:31Z","published":"2023-12-04T18:58:40Z","title":"Object Recognition as Next Token Prediction","summary":"  We present an approach to pose object recognition as next token prediction.\nThe idea is to apply a language decoder that auto-regressively predicts the\ntext tokens from image embeddings to form labels. To ground this prediction\nprocess in auto-regression, we customize a non-causal attention mask for the\ndecoder, incorporating two key features: modeling tokens from different labels\nto be independent, and treating image tokens as a prefix. This masking\nmechanism inspires an efficient method - one-shot sampling - to simultaneously\nsample tokens of multiple labels in parallel and rank generated labels by their\nprobabilities during inference. To further enhance the efficiency, we propose a\nsimple strategy to construct a compact decoder by simply discarding the\nintermediate blocks of a pretrained language model. This approach yields a\ndecoder that matches the full model's performance while being notably more\nefficient. The code is available at https://github.com/kaiyuyue/nxtp\n","authors":["Kaiyu Yue","Bor-Chun Chen","Jonas Geiping","Hengduo Li","Tom Goldstein","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2312.02142v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07692v1","updated":"2024-03-12T14:36:52Z","published":"2024-03-12T14:36:52Z","title":"Masked AutoDecoder is Effective Multi-Task Vision Generalist","summary":"  Inspired by the success of general-purpose models in NLP, recent studies\nattempt to unify different vision tasks in the same sequence format and employ\nautoregressive Transformers for sequence prediction. They apply uni-directional\nattention to capture sequential dependencies and generate task sequences\nrecursively. However, such autoregressive Transformers may not fit vision tasks\nwell, as vision task sequences usually lack the sequential dependencies\ntypically observed in natural languages. In this work, we design Masked\nAutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of\ntwo core designs. First, we develop a parallel decoding framework that\nintroduces bi-directional attention to capture contextual dependencies\ncomprehensively and decode vision task sequences in parallel. Second, we design\na masked sequence modeling approach that learns rich task contexts by masking\nand reconstructing task sequences. In this way, MAD handles all the tasks by a\nsingle network branch and a simple cross-entropy loss with minimal\ntask-specific designs. Extensive experiments demonstrate the great potential of\nMAD as a new paradigm for unifying various vision tasks. MAD achieves superior\nperformance and inference efficiency compared to autoregressive counterparts\nwhile obtaining competitive accuracy with task-specific models. Code will be\nreleased.\n","authors":["Han Qiu","Jiaxing Huang","Peng Gao","Lewei Lu","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2403.07692v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07687v1","updated":"2024-03-12T14:27:17Z","published":"2024-03-12T14:27:17Z","title":"Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost","summary":"  Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most of this data\ncomes from Western countries, leading to poor results for underrepresented\ncountries. To address this issue, more data needs to be collected from these\ncountries, but the cost of annotation can be a significant bottleneck. In this\npaper, we propose methods to identify the data to be annotated to balance model\nperformance and annotation costs. Our approach first involves finding the\ncountries with images of topics (objects and actions) most visually distinct\nfrom those already in the training datasets used by current large\nvision-language foundation models. Next, we identify countries with higher\nvisual similarity for these topics and show that using data from these\ncountries to supplement the training data improves model performance and\nreduces annotation costs. The resulting lists of countries and corresponding\ntopics are made available at\nhttps://github.com/MichiganNLP/visual_diversity_budget.\n","authors":["Oana Ignat","Longju Bai","Joan Nwatu","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2403.07687v1.pdf","comment":"accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2403.07684v1","updated":"2024-03-12T14:21:30Z","published":"2024-03-12T14:21:30Z","title":"Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for\n  Video Adverse Weather Removal","summary":"  Real-world vision tasks frequently suffer from the appearance of unexpected\nadverse weather conditions, including rain, haze, snow, and raindrops. In the\nlast decade, convolutional neural networks and vision transformers have yielded\noutstanding results in single-weather video removal. However, due to the\nabsence of appropriate adaptation, most of them fail to generalize to other\nweather conditions. Although ViWS-Net is proposed to remove adverse weather\nconditions in videos with a single set of pre-trained weights, it is seriously\nblinded by seen weather at train-time and degenerates when coming to unseen\nweather during test-time. In this work, we introduce test-time adaptation into\nadverse weather removal in videos, and propose the first framework that\nintegrates test-time adaptation into the iterative diffusion reverse process.\nSpecifically, we devise a diffusion-based network with a novel temporal noise\nmodel to efficiently explore frame-correlated information in degraded video\nclips at training stage. During inference stage, we introduce a proxy task\nnamed Diffusion Tubelet Self-Calibration to learn the primer distribution of\ntest video stream and optimize the model by approximating the temporal noise\nmodel for online adaptation. Experimental results, on benchmark datasets,\ndemonstrate that our Test-Time Adaptation method with Diffusion-based\nnetwork(Diff-TTA) outperforms state-of-the-art methods in terms of restoring\nvideos degraded by seen weather conditions. Its generalizable capability is\nalso validated with unseen weather conditions in both synthesized and\nreal-world videos.\n","authors":["Yijun Yang","Hongtao Wu","Angelica I. Aviles-Rivero","Yulun Zhang","Jing Qin","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.07684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05930v2","updated":"2024-03-12T14:15:50Z","published":"2024-03-09T14:42:16Z","title":"Deep learning for multi-label classification of coral conditions in the\n  Indo-Pacific via underwater photogrammetry","summary":"  Since coral reef ecosystems face threats from human activities and climate\nchange, coral conservation programs are implemented worldwide. Monitoring coral\nhealth provides references for guiding conservation activities. However,\ncurrent labor-intensive methods result in a backlog of unsorted images,\nhighlighting the need for automated classification. Few studies have\nsimultaneously utilized accurate annotations along with updated algorithms and\ndatasets. This study aimed to create a dataset representing common coral\nconditions and associated stressors in the Indo-Pacific. Concurrently, it\nassessed existing classification algorithms and proposed a new multi-label\nmethod for automatically detecting coral conditions and extracting ecological\ninformation. A dataset containing over 20,000 high-resolution coral images of\ndifferent health conditions and stressors was constructed based on the field\nsurvey. Seven representative deep learning architectures were tested on this\ndataset, and their performance was quantitatively evaluated using the F1 metric\nand the match ratio. Based on this evaluation, a new method utilizing the\nensemble learning approach was proposed. The proposed method accurately\nclassified coral conditions as healthy, compromised, dead, and rubble; it also\nidentified corresponding stressors, including competition, disease, predation,\nand physical issues. This method can help develop the coral image archive,\nguide conservation activities, and provide references for decision-making for\nreef managers and conservationists. The proposed ensemble learning approach\noutperforms others on the dataset, showing State-Of-The-Art (SOTA) performance.\nFuture research should improve its generalizability and accuracy to support\nglobal coral conservation efforts.\n","authors":["Xinlei Shao","Hongruixuan Chen","Kirsty Magson","Jiaqi Wang","Jian Song","Jundong Chen","Jun Sasaki"],"pdf_url":"https://arxiv.org/pdf/2403.05930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.02158v3","updated":"2024-03-12T13:57:45Z","published":"2022-03-04T07:07:36Z","title":"Transformations in Learned Image Compression from a Modulation\n  Perspective","summary":"  In this paper, a unified transformation method in learned image\ncompression(LIC) is proposed from the perspective of modulation. Firstly, the\nquantization in LIC is considered as a generalized channel with additive\nuniform noise. Moreover, the LIC is interpreted as a particular communication\nsystem according to the consistency in structures and optimization objectives.\nThus, the technology of communication systems can be applied to guide the\ndesign of modules in LIC. Furthermore, a unified transform method based on\nsignal modulation (TSM) is defined. In the view of TSM, the existing\ntransformation methods are mathematically reduced to a linear modulation. A\nseries of transformation methods, e.g. TPM and TJM, are obtained by extending\nto nonlinear modulation. The experimental results on various datasets and\nbackbone architectures verify that the effectiveness and robustness of the\nproposed method. More importantly, it further confirms the feasibility of\nguiding LIC design from a communication perspective. For example, when backbone\narchitecture is hyperprior combining context model, our method achieves\n3.52$\\%$ BD-rate reduction over GDN on Kodak dataset without increasing\ncomplexity.\n","authors":["Youneng Bao","Fangyang Meng","Wen Tan","Chao Li","Yonghong Tian","Yongsheng Liang"],"pdf_url":"https://arxiv.org/pdf/2203.02158v3.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2402.09786v3","updated":"2024-03-12T13:36:23Z","published":"2024-02-15T08:34:21Z","title":"Examining Pathological Bias in a Generative Adversarial Network\n  Discriminator: A Case Study on a StyleGAN3 Model","summary":"  Generative adversarial networks (GANs) generate photorealistic faces that are\noften indistinguishable by humans from real faces. While biases in machine\nlearning models are often assumed to be due to biases in training data, we find\npathological internal color and luminance biases in the discriminator of a\npre-trained StyleGAN3-r model that are not explicable by the training data. We\nalso find that the discriminator systematically stratifies scores by both\nimage- and face-level qualities and that this disproportionately affects images\nacross gender, race, and other categories. We examine axes common in research\non stereotyping in social psychology.\n","authors":["Alvin Grissom II","Ryan F. Lei","Matt Gusdorff","Jeova Farias Sales Rocha Neto","Bailey Lin","Ryan Trotter"],"pdf_url":"https://arxiv.org/pdf/2402.09786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07636v1","updated":"2024-03-12T13:18:22Z","published":"2024-03-12T13:18:22Z","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Matching Framework","summary":"  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours outperforms recent methods by up to 8.07% and\n11.23% in AUC scores for seen and novel categories, respectively. Our code is\nreleased at\n\\href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}.\n","authors":["Minh Hieu Phan","Yutong Xie","Yuankai Qi","Lingqiao Liu","Liyang Liu","Bowen Zhang","Zhibin Liao","Qi Wu","Minh-Son To","Johan W. Verjans"],"pdf_url":"https://arxiv.org/pdf/2403.07636v1.pdf","comment":"Accepted at CVPR2024. Pre-print before final camera-ready version"},{"id":"http://arxiv.org/abs/2303.17189v2","updated":"2024-03-12T13:15:24Z","published":"2023-03-30T06:56:12Z","title":"LayoutDiffusion: Controllable Diffusion Model for Layout-to-image\n  Generation","summary":"  Recently, diffusion models have achieved great success in image synthesis.\nHowever, when it comes to the layout-to-image generation where an image often\nhas a complex scene of multiple objects, how to make strong control over both\nthe global layout map and each detailed object remains a challenging task. In\nthis paper, we propose a diffusion model named LayoutDiffusion that can obtain\nhigher generation quality and greater controllability than the previous works.\nTo overcome the difficult multimodal fusion of image and layout, we propose to\nconstruct a structural image patch with region information and transform the\npatched image into a special layout to fuse with the normal layout in a unified\nform. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention\n(OaCA) are proposed to model the relationship among multiple objects and\ndesigned to be object-aware and position-sensitive, allowing for precisely\ncontrolling the spatial related information. Extensive experiments show that\nour LayoutDiffusion outperforms the previous SOTA methods on FID, CAS by\nrelatively 46.35%, 26.70% on COCO-stuff and 44.29%, 41.82% on VG. Code is\navailable at https://github.com/ZGCTroy/LayoutDiffusion.\n","authors":["Guangcong Zheng","Xianpan Zhou","Xuewei Li","Zhongang Qi","Ying Shan","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2303.17189v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2403.07630v1","updated":"2024-03-12T13:11:58Z","published":"2024-03-12T13:11:58Z","title":"Hunting Attributes: Context Prototype-Aware Learning for Weakly\n  Supervised Semantic Segmentation","summary":"  Recent weakly supervised semantic segmentation (WSSS) methods strive to\nincorporate contextual knowledge to improve the completeness of class\nactivation maps (CAM). In this work, we argue that the knowledge bias between\ninstances and contexts affects the capability of the prototype to sufficiently\nunderstand instance semantics. Inspired by prototype learning theory, we\npropose leveraging prototype awareness to capture diverse and fine-grained\nfeature attributes of instances. The hypothesis is that contextual prototypes\nmight erroneously activate similar and frequently co-occurring object\ncategories due to this knowledge bias. Therefore, we propose to enhance the\nprototype representation ability by mitigating the bias to better capture\nspatial coverage in semantic object regions. With this goal, we present a\nContext Prototype-Aware Learning (CPAL) strategy, which leverages semantic\ncontext to enrich instance comprehension. The core of this method is to\naccurately capture intra-class variations in object features through\ncontext-aware prototypes, facilitating the adaptation to the semantic\nattributes of various instances. We design feature distribution alignment to\noptimize prototype awareness, aligning instance feature distributions with\ndense features. In addition, a unified training framework is proposed to\ncombine label-guided classification supervision and prototypes-guided\nself-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show\nthat CPAL significantly improves off-the-shelf methods and achieves\nstate-of-the-art performance. The project is available at\nhttps://github.com/Barrett-python/CPAL.\n","authors":["Feilong Tang","Zhongxing Xu","Zhaojun Qu","Wei Feng","Xingjian Jiang","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2403.07630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10601v2","updated":"2024-03-12T13:06:14Z","published":"2023-11-17T15:57:32Z","title":"Multimodal Indoor Localization Using Crowdsourced Radio Maps","summary":"  Indoor Positioning Systems (IPS) traditionally rely on odometry and building\ninfrastructures like WiFi, often supplemented by building floor plans for\nincreased accuracy. However, the limitation of floor plans in terms of\navailability and timeliness of updates challenges their wide applicability. In\ncontrast, the proliferation of smartphones and WiFi-enabled robots has made\ncrowdsourced radio maps - databases pairing locations with their corresponding\nReceived Signal Strengths (RSS) - increasingly accessible. These radio maps not\nonly provide WiFi fingerprint-location pairs but encode movement regularities\nakin to the constraints imposed by floor plans. This work investigates the\npossibility of leveraging these radio maps as a substitute for floor plans in\nmultimodal IPS. We introduce a new framework to address the challenges of radio\nmap inaccuracies and sparse coverage. Our proposed system integrates an\nuncertainty-aware neural network model for WiFi localization and a bespoken\nBayesian fusion technique for optimal fusion. Extensive evaluations on multiple\nreal-world sites indicate a significant performance enhancement, with results\nshowing ~ 25% improvement over the best baseline\n","authors":["Zhaoguang Yi","Xiangyu Wen","Qiyue Xia","Peize Li","Francisco Zampella","Firas Alsehly","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2311.10601v2.pdf","comment":"7 pages, 4 figures; ICRA'24 https://youtu.be/NTTKwJBFN5w"},{"id":"http://arxiv.org/abs/2403.07622v1","updated":"2024-03-12T13:05:51Z","published":"2024-03-12T13:05:51Z","title":"Multiple Latent Space Mapping for Compressed Dark Image Enhancement","summary":"  Dark image enhancement aims at converting dark images to normal-light images.\nExisting dark image enhancement methods take uncompressed dark images as inputs\nand achieve great performance. However, in practice, dark images are often\ncompressed before storage or transmission over the Internet. Current methods\nget poor performance when processing compressed dark images. Artifacts hidden\nin the dark regions are amplified by current methods, which results in\nuncomfortable visual effects for observers. Based on this observation, this\nstudy aims at enhancing compressed dark images while avoiding compression\nartifacts amplification. Since texture details intertwine with compression\nartifacts in compressed dark images, detail enhancement and blocking artifacts\nsuppression contradict each other in image space. Therefore, we handle the task\nin latent space. To this end, we propose a novel latent mapping network based\non variational auto-encoder (VAE). Firstly, different from previous VAE-based\nmethods with single-resolution features only, we exploit multiple latent spaces\nwith multi-resolution features, to reduce the detail blur and improve image\nfidelity. Specifically, we train two multi-level VAEs to project compressed\ndark images and normal-light images into their latent spaces respectively.\nSecondly, we leverage a latent mapping network to transform features from\ncompressed dark space to normal-light space. Specifically, since the\ndegradation models of darkness and compression are different from each other,\nthe latent mapping process is divided mapping into enlightening branch and\ndeblocking branch. Comprehensive experiments demonstrate that the proposed\nmethod achieves state-of-the-art performance in compressed dark image\nenhancement.\n","authors":["Yi Zeng","Zhengning Wang","Yuxuan Liu","Tianjiao Zeng","Xuhang Liu","Xinglong Luo","Shuaicheng Liu","Shuyuan Zhu","Bing Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.07622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07621v1","updated":"2024-03-12T13:04:37Z","published":"2024-03-12T13:04:37Z","title":"Smartphone region-wise image indoor localization using deep learning for\n  indoor tourist attraction","summary":"  Smart indoor tourist attractions, such as smart museums and aquariums,\nusually require a significant investment in indoor localization devices. The\nsmartphone Global Positional Systems use is unsuitable for scenarios where\ndense materials such as concrete and metal block weaken the GPS signals, which\nis the most common scenario in an indoor tourist attraction. Deep learning\nmakes it possible to perform region-wise indoor localization using smartphone\nimages. This approach does not require any investment in infrastructure,\nreducing the cost and time to turn museums and aquariums into smart museums or\nsmart aquariums. This paper proposes using deep learning algorithms to classify\nlocations using smartphone camera images for indoor tourism attractions. We\nevaluate our proposal in a real-world scenario in Brazil. We extensively\ncollect images from ten different smartphones to classify biome-themed fish\ntanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We\ntested seven state-of-the-art neural networks, three being transformer-based,\nachieving precision around 90% on average and recall and f-score around 89% on\naverage. The results indicate good feasibility of the proposal in a most indoor\ntourist attractions.\n","authors":["Gabriel Toshio Hirokawa Higa","Rodrigo Stuqui Monzani","Jorge Fernando da Silva Cecatto","Maria Fernanda Balestieri Mariano de Souza","Vanessa Aparecida de Moraes Weber","Hemerson Pistori","Edson Takashi Matsubara"],"pdf_url":"https://arxiv.org/pdf/2403.07621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.14347v2","updated":"2024-03-12T13:00:14Z","published":"2020-06-25T12:45:17Z","title":"Epoch-evolving Gaussian Process Guided Learning","summary":"  In this paper, we propose a novel learning scheme called epoch-evolving\nGaussian Process Guided Learning (GPGL), which aims at characterizing the\ncorrelation information between the batch-level distribution and the global\ndata distribution. Such correlation information is encoded as context labels\nand needs renewal every epoch. With the guidance of the context label and\nground truth label, GPGL scheme provides a more efficient optimization through\nupdating the model parameters with a triangle consistency loss. Furthermore,\nour GPGL scheme can be further generalized and naturally applied to the current\ndeep models, outperforming the existing batch-based state-of-the-art models on\nmainstream datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) remarkably.\n","authors":["Jiabao Cui","Xuewei Li","Bin Li","Hanbin Zhao","Bourahla Omar","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2006.14347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00759v2","updated":"2024-03-12T12:58:06Z","published":"2023-08-01T18:00:49Z","title":"Decomposition Ascribed Synergistic Learning for Unified Image\n  Restoration","summary":"  Learning to restore multiple image degradations within a single model is\nquite beneficial for real-world applications. Nevertheless, existing works\ntypically concentrate on regarding each degradation independently, while their\nrelationship has been less exploited to ensure the synergistic learning. To\nthis end, we revisit the diverse degradations through the lens of singular\nvalue decomposition, with the observation that the decomposed singular vectors\nand singular values naturally undertake the different types of degradation\ninformation, dividing various restoration tasks into two groups, \\ie, singular\nvector dominated and singular value dominated. The above analysis renders a\nmore unified perspective to ascribe the diverse degradations, compared to\nprevious task-level independent learning. The dedicated optimization of\ndegraded singular vectors and singular values inherently utilizes the potential\nrelationship among diverse restoration tasks, attributing to the Decomposition\nAscribed Synergistic Learning (DASL). Specifically, DASL comprises two\neffective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue\nOperator (SVAO), to favor the decomposed optimization, which can be lightly\nintegrated into existing image restoration backbone. Moreover, the congruous\ndecomposition loss has been devised for auxiliary. Extensive experiments on\nblended five image restoration tasks demonstrate the effectiveness of our\nmethod.\n","authors":["Jinghao Zhang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.00759v2.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2006.15524v4","updated":"2024-03-12T12:53:14Z","published":"2020-06-28T06:12:49Z","title":"MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot\n  Class-Incremental Learning","summary":"  As a challenging problem, few-shot class-incremental learning (FSCIL)\ncontinually learns a sequence of tasks, confronting the dilemma between slow\nforgetting of old knowledge and fast adaptation to new knowledge. In this\npaper, we concentrate on this \"slow vs. fast\" (SvF) dilemma to determine which\nknowledge components to be updated in a slow fashion or a fast fashion, and\nthereby balance old-knowledge preservation and new-knowledge adaptation. We\npropose a multi-grained SvF learning strategy to cope with the SvF dilemma from\ntwo different grains: intra-space (within the same feature space) and\ninter-space (between two different feature spaces). The proposed strategy\ndesigns a novel frequency-aware regularization to boost the intra-space SvF\ncapability, and meanwhile develops a new feature space composition operation to\nenhance the inter-space SvF learning performance. With the multi-grained SvF\nlearning strategy, our method outperforms the state-of-the-art approaches by a\nlarge margin.\n","authors":["Hanbin Zhao","Yongjian Fu","Mintong Kang","Qi Tian","Fei Wu","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2006.15524v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06495v2","updated":"2024-03-12T12:52:14Z","published":"2024-03-11T08:07:46Z","title":"Toward Generalist Anomaly Detection via In-context Residual Learning\n  with Few-shot Sample Prompts","summary":"  This paper explores the problem of Generalist Anomaly Detection (GAD), aiming\nto train one single detection model that can generalize to detect anomalies in\ndiverse datasets from different application domains without any further\ntraining on the target data. Some recent studies have shown that large\npre-trained Visual-Language Models (VLMs) like CLIP have strong generalization\ncapabilities on detecting industrial defects from various datasets, but their\nmethods rely heavily on handcrafted text prompts about defects, making them\ndifficult to generalize to anomalies in other applications, e.g., medical image\nanomalies or semantic anomalies in natural images. In this work, we propose to\ntrain a GAD model with few-shot normal images as sample prompts for AD on\ndiverse datasets on the fly. To this end, we introduce a novel approach that\nlearns an in-context residual learning model for GAD, termed InCTRL. It is\ntrained on an auxiliary dataset to discriminate anomalies from normal samples\nbased on a holistic evaluation of the residuals between query images and\nfew-shot normal sample prompts. Regardless of the datasets, per definition of\nanomaly, larger residuals are expected for anomalies than normal samples,\nthereby enabling InCTRL to generalize across different domains without further\ntraining. Comprehensive experiments on nine AD datasets are performed to\nestablish a GAD benchmark that encapsulate the detection of industrial defect\nanomalies, medical anomalies, and semantic anomalies in both one-vs-all and\nmulti-class setting, on which InCTRL is the best performer and significantly\noutperforms state-of-the-art competing methods.\n","authors":["Jiawen Zhu","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2403.06495v2.pdf","comment":"Accepted to CVPR 2024; 17 pages; 5 figures"},{"id":"http://arxiv.org/abs/2403.07605v1","updated":"2024-03-12T12:44:34Z","published":"2024-03-12T12:44:34Z","title":"Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in\n  Text-To-Image Generation","summary":"  In text-to-image generation, using negative prompts, which describe\nundesirable image characteristics, can significantly boost image quality.\nHowever, producing good negative prompts is manual and tedious. To address\nthis, we propose NegOpt, a novel method for optimizing negative prompt\ngeneration toward enhanced image generation, using supervised fine-tuning and\nreinforcement learning. Our combined approach results in a substantial increase\nof 25% in Inception Score compared to other approaches and surpasses\nground-truth negative prompts from the test set. Furthermore, with NegOpt we\ncan preferentially optimize the metrics most important to us. Finally, we\nconstruct Negative Prompts DB, a dataset of negative prompts.\n","authors":["Michael Ogezi","Ning Shi"],"pdf_url":"https://arxiv.org/pdf/2403.07605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03403v2","updated":"2024-03-12T12:41:04Z","published":"2023-06-06T04:49:51Z","title":"SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic\n  Segmentation","summary":"  As an important and challenging problem in computer vision, PAnoramic\nSemantic Segmentation (PASS) gives complete scene perception based on an\nultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic\nimage input focus on solving image distortions but lack consideration of the 3D\nproperties of original $360^{\\circ}$ data. Therefore, their performance will\ndrop a lot when inputting panoramic images with the 3D disturbance. To be more\nrobust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer\nfor PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical\ngeometry knowledge. Specifically, a spherical geometry-aware framework is\nproposed for PASS. It includes three modules, i.e., spherical geometry-aware\nimage projection, spherical deformable patch embedding, and a panorama-aware\nloss, which takes input images with 3D disturbance into account, adds a\nspherical geometry-aware constraint on the existing deformable patch embedding,\nand indicates the pixel density of original $360^{\\circ}$ data, respectively.\nExperimental results on Stanford2D3D Panoramic datasets show that SGAT4PASS\nsignificantly improves performance and robustness, with approximately a 2%\nincrease in mIoU, and when small 3D disturbances occur in the data, the\nstability of our performance is improved by an order of magnitude. Our code and\nsupplementary material are available at\nhttps://github.com/TencentARC/SGAT4PASS.\n","authors":["Xuewei Li","Tao Wu","Zhongang Qi","Gaoang Wang","Ying Shan","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2306.03403v2.pdf","comment":"Accepted by IJCAI 2023"},{"id":"http://arxiv.org/abs/2403.07601v1","updated":"2024-03-12T12:40:08Z","published":"2024-03-12T12:40:08Z","title":"Unified Source-Free Domain Adaptation","summary":"  In the pursuit of transferring a source model to a target domain without\naccess to the source training data, Source-Free Domain Adaptation (SFDA) has\nbeen extensively explored across various scenarios, including closed-set,\nopen-set, partial-set, and generalized settings. Existing methods, focusing on\nspecific scenarios, not only address only a subset of challenges but also\nnecessitate prior knowledge of the target domain, significantly limiting their\npractical utility and deployability. In light of these considerations, we\nintroduce a more practical yet challenging problem, termed unified SFDA, which\ncomprehensively incorporates all specific scenarios in a unified manner. To\ntackle this unified SFDA problem, we propose a novel approach called Latent\nCausal Factors Discovery (LCFD). In contrast to previous alternatives that\nemphasize learning the statistical description of reality, we formulate LCFD\nfrom a causality perspective. The objective is to uncover the causal\nrelationships between latent variables and model decisions, enhancing the\nreliability and robustness of the learned model against domain shifts. To\nintegrate extensive world knowledge, we leverage a pre-trained vision-language\nmodel such as CLIP. This aids in the formation and discovery of latent causal\nfactors in the absence of supervision in the variation of distribution and\nsemantics, coupled with a newly designed information bottleneck with\ntheoretical guarantees. Extensive experiments demonstrate that LCFD can achieve\nnew state-of-the-art results in distinct SFDA settings, as well as source-free\nout-of-distribution generalization.Our code and data are available at\nhttps://github.com/tntek/source-free-domain-adaptation.\n","authors":["Song Tang","Wenxin Su","Mao Ye","Jianwei Zhang","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.07601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07598v1","updated":"2024-03-12T12:35:12Z","published":"2024-03-12T12:35:12Z","title":"Mondrian: On-Device High-Performance Video Analytics with Compressive\n  Packed Inference","summary":"  In this paper, we present Mondrian, an edge system that enables\nhigh-performance object detection on high-resolution video streams. Many\nlightweight models and system optimization techniques have been proposed for\nresource-constrained devices, but they do not fully utilize the potential of\nthe accelerators over dynamic, high-resolution videos. To enable such\ncapability, we devise a novel Compressive Packed Inference to minimize\nper-pixel processing costs by selectively determining the necessary pixels to\nprocess and combining them to maximize processing parallelism. In particular,\nour system quickly extracts ROIs and dynamically shrinks them, reflecting the\neffect of the fast-changing characteristics of objects and scenes. It then\nintelligently combines such scaled ROIs into large canvases to maximize the\nutilization of inference accelerators such as GPU. Evaluation across various\ndatasets, models, and devices shows Mondrian outperforms state-of-the-art\nbaselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by\n15.0-19.7% higher accuracy, leading to $\\times$6.65 higher throughput than\nframe-wise inference for processing various 1080p video streams. We will\nrelease the code after the paper review.\n","authors":["Changmin Jeon","Seonjun Kim","Juheon Yi","Youngki Lee"],"pdf_url":"https://arxiv.org/pdf/2403.07598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07593v1","updated":"2024-03-12T12:25:54Z","published":"2024-03-12T12:25:54Z","title":"MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D\n  Sparse Convolutions","summary":"  This paper presents MinkUNeXt, an effective and efficient architecture for\nplace-recognition from point clouds entirely based on the new 3D MinkNeXt\nBlock, a residual block composed of 3D sparse convolutions that follows the\nphilosophy established by recent Transformers but purely using simple 3D\nconvolutions. Feature extraction is performed at different scales by a U-Net\nencoder-decoder network and the feature aggregation of those features into a\nsingle descriptor is carried out by a Generalized Mean Pooling (GeM). The\nproposed architecture demonstrates that it is possible to surpass the current\nstate-of-the-art by only relying on conventional 3D sparse convolutions without\nmaking use of more complex and sophisticated proposals such as Transformers,\nAttention-Layers or Deformable Convolutions. A thorough assessment of the\nproposal has been carried out using the Oxford RobotCar and the In-house\ndatasets. As a result, MinkUNeXt proves to outperform other methods in the\nstate-of-the-art.\n","authors":["J. J. Cabrera","A. Santo","A. Gil","C. Viegas","L. Payá"],"pdf_url":"https://arxiv.org/pdf/2403.07593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07592v1","updated":"2024-03-12T12:25:38Z","published":"2024-03-12T12:25:38Z","title":"Accurate Spatial Gene Expression Prediction by integrating\n  Multi-resolution features","summary":"  Recent advancements in Spatial Transcriptomics (ST) technology have\nfacilitated detailed gene expression analysis within tissue contexts. However,\nthe high costs and methodological limitations of ST necessitate a more robust\npredictive model. In response, this paper introduces TRIPLEX, a novel deep\nlearning framework designed to predict spatial gene expression from Whole Slide\nImages (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing\ncellular morphology at individual spots, the local context around these spots,\nand the global tissue organization. By integrating these features through an\neffective fusion strategy, TRIPLEX achieves accurate gene expression\nprediction. Our comprehensive benchmark study, conducted on three public ST\ndatasets and supplemented with Visium data from 10X Genomics, demonstrates that\nTRIPLEX outperforms current state-of-the-art models in Mean Squared Error\n(MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC).\nThe model's predictions align closely with ground truth gene expression\nprofiles and tumor annotations, underscoring TRIPLEX's potential in advancing\ncancer diagnosis and treatment.\n","authors":["Youngmin Chung","Ji Hun Ha","Kyeong Chan Im","Joo Sang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.07592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13325v4","updated":"2024-03-12T12:21:18Z","published":"2023-06-23T07:05:08Z","title":"Differentiable Display Photometric Stereo","summary":"  Photometric stereo leverages variations in illumination conditions to\nreconstruct surface normals. Display photometric stereo, which employs a\nconventional monitor as an illumination source, has the potential to overcome\nlimitations often encountered in bulky and difficult-to-use conventional\nsetups. In this paper, we present differentiable display photometric stereo\n(DDPS), addressing an often overlooked challenge in display photometric stereo:\nthe design of display patterns. Departing from using heuristic display\npatterns, DDPS learns the display patterns that yield accurate normal\nreconstruction for a target system in an end-to-end manner. To this end, we\npropose a differentiable framework that couples basis-illumination image\nformation with analytic photometric-stereo reconstruction. The differentiable\nframework facilitates the effective learning of display patterns via\nauto-differentiation. Also, for training supervision, we propose to use 3D\nprinting for creating a real-world training dataset, enabling accurate\nreconstruction on the target real-world setup. Finally, we exploit that\nconventional LCD monitors emit polarized light, which allows for the optical\nseparation of diffuse and specular reflections when combined with a\npolarization camera, leading to accurate normal reconstruction. Extensive\nevaluation of DDPS shows improved normal-reconstruction accuracy compared to\nheuristic patterns and demonstrates compelling properties such as robustness to\npattern initialization, calibration errors, and simplifications in image\nformation and reconstruction.\n","authors":["Seokjun Choi","Seungwoo Yoon","Giljoo Nam","Seungyong Lee","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2306.13325v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07589v1","updated":"2024-03-12T12:19:05Z","published":"2024-03-12T12:19:05Z","title":"PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral\n  Convolution","summary":"  Recently, some large kernel convnets strike back with appealing performance\nand efficiency. However, given the square complexity of convolution, scaling up\nkernels can bring about an enormous amount of parameters and the proliferated\nparameters can induce severe optimization problem. Due to these issues, current\nCNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e.,\n51x5 + 5x51) and start to saturate as the kernel size continues growing. In\nthis paper, we delve into addressing these vital issues and explore whether we\ncan continue scaling up kernels for more performance gains. Inspired by human\nvision, we propose a human-like peripheral convolution that efficiently reduces\nover 90% parameter count of dense grid convolution through parameter sharing,\nand manage to scale up kernel size to extremely large. Our peripheral\nconvolution behaves highly similar to human, reducing the complexity of\nconvolution from O(K^2) to O(logK) without backfiring performance. Built on\nthis, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK\noutperforms modern vision Transformers and ConvNet architectures like Swin,\nConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet\nclassification, semantic segmentation on ADE20K and object detection on MS\nCOCO. For the first time, we successfully scale up the kernel size of CNNs to\nan unprecedented 101x101 and demonstrate consistent improvements.\n","authors":["Honghao Chen","Xiangxiang Chu","Yongjian Ren","Xin Zhao","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2403.07589v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.14282v2","updated":"2024-03-12T12:14:51Z","published":"2023-11-24T05:11:35Z","title":"Image Super-Resolution with Text Prompt Diffusion","summary":"  Image super-resolution (SR) methods typically model degradation to improve\nreconstruction accuracy in complex and unknown degradation scenarios. However,\nextracting degradation information from low-resolution images is challenging,\nwhich limits the model performance. To boost image SR performance, one feasible\napproach is to introduce additional priors. Inspired by advancements in\nmulti-modal methods and text prompt image processing, we introduce text prompts\nto image SR to provide degradation priors. Specifically, we first design a\ntext-image generation pipeline to integrate text into the SR dataset through\nthe text degradation representation and degradation model. The text\nrepresentation applies a discretization manner based on the binning method to\ndescribe the degradation abstractly. This method maintains the flexibility of\nthe text and is user-friendly. Meanwhile, we propose the PromptSR to realize\nthe text prompt SR. The PromptSR utilizes the pre-trained language model (e.g.,\nT5 or CLIP) to enhance restoration. We train the model on the generated\ntext-image dataset. Extensive experiments indicate that introducing text\nprompts into SR, yields excellent results on both synthetic and real-world\nimages. Code is available at: https://github.com/zhengchen1999/PromptSR.\n","authors":["Zheng Chen","Yulun Zhang","Jinjin Gu","Xin Yuan","Linghe Kong","Guihai Chen","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2311.14282v2.pdf","comment":"Code is available at https://github.com/zhengchen1999/PromptSR"},{"id":"http://arxiv.org/abs/2311.16480v2","updated":"2024-03-12T12:07:39Z","published":"2023-11-27T05:05:41Z","title":"WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images","summary":"  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text\npairs for visual-language models by recognizing and cleaning pathology reports\nwhich narrate diagnostic slides in TCGA. On the model end, we propose the\nmultiple instance generative model (MI-Gen) which can produce pathology reports\nfor gigapixel WSIs. We benchmark our model on the largest subset of\nTCGA-PathoText. Experimental results show our model can generate pathology\nreports which contain multiple clinical clues. Furthermore, WSI-text prediction\ncan be seen as an approach of visual-language pre-training, which enables our\nmodel to be transferred to downstream diagnostic tasks like carcinoma grading\nand phenotyping. We observe that simple semantic extraction from the pathology\nreports can achieve the best performance (0.838 of F1 score) on BRCA subtyping\nwithout adding extra parameters or tricky fine-tuning. Our collected dataset\nand related code are available.\n","authors":["Pingyi Chen","Honglin Li","Chenglu Zhu","Sunyi Zheng","Zhongyi Shui","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.16480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07578v1","updated":"2024-03-12T12:07:00Z","published":"2024-03-12T12:07:00Z","title":"AACP: Aesthetics assessment of children's paintings based on\n  self-supervised learning","summary":"  The Aesthetics Assessment of Children's Paintings (AACP) is an important\nbranch of the image aesthetics assessment (IAA), playing a significant role in\nchildren's education. This task presents unique challenges, such as limited\navailable data and the requirement for evaluation metrics from multiple\nperspectives. However, previous approaches have relied on training large\ndatasets and subsequently providing an aesthetics score to the image, which is\nnot applicable to AACP. To solve this problem, we construct an aesthetics\nassessment dataset of children's paintings and a model based on self-supervised\nlearning. 1) We build a novel dataset composed of two parts: the first part\ncontains more than 20k unlabeled images of children's paintings; the second\npart contains 1.2k images of children's paintings, and each image contains\neight attributes labeled by multiple design experts. 2) We design a pipeline\nthat includes a feature extraction module, perception modules and a\ndisentangled evaluation module. 3) We conduct both qualitative and quantitative\nexperiments to compare our model's performance with five other methods using\nthe AACP dataset. Our experiments reveal that our method can accurately capture\naesthetic features and achieve state-of-the-art performance.\n","authors":["Shiqi Jiang","Ning Li","Chen Shi","Liping Guo","Changbo Wang","Chenhui Li"],"pdf_url":"https://arxiv.org/pdf/2403.07578v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2403.07576v1","updated":"2024-03-12T12:05:43Z","published":"2024-03-12T12:05:43Z","title":"FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine\n  Tuning in High-resolution Medical Image Classification","summary":"  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to\ntransfer pre-trained models to downstream tasks, avoiding the high cost of\nupdating entire large-scale pre-trained models (LPMs). In this work, we present\nFine-grained Prompt Tuning (FPT), a novel PEFT method for medical image\nclassification. FPT significantly reduces memory consumption compared to other\nPEFT methods, especially in high-resolution contexts. To achieve this, we first\nfreeze the weights of the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM takes high-resolution images as input to extract\nfine-grained features, while the side network is fed low-resolution images to\nreduce memory usage. To allow the side network to access pre-trained knowledge,\nwe introduce fine-grained prompts that summarize information from the LPM\nthrough a fusion module. Important tokens selection and preloading techniques\nare employed to further reduce training cost and memory requirements. We\nevaluate FPT on four medical datasets with varying sizes, modalities, and\ncomplexities. Experimental results demonstrate that FPT achieves comparable\nperformance to fine-tuning the entire LPM while using only 1.8% of the\nlearnable parameters and 13% of the memory costs of an encoder ViT-B model with\na 512 x 512 input resolution.\n","authors":["Yijin Huang","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10969v5","updated":"2024-03-12T11:59:39Z","published":"2022-10-20T02:35:26Z","title":"SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic\n  Retinopathy Grading","summary":"  Self-supervised Learning (SSL) has been widely applied to learn image\nrepresentations through exploiting unlabeled images. However, it has not been\nfully explored in the medical image analysis field. In this work,\nSaliency-guided Self-Supervised image Transformer (SSiT) is proposed for\nDiabetic Retinopathy (DR) grading from fundus images. We novelly introduce\nsaliency maps into SSL, with a goal of guiding self-supervised pre-training\nwith domain-specific prior knowledge. Specifically, two saliency-guided\nlearning tasks are employed in SSiT: (1) Saliency-guided contrastive learning\nis conducted based on the momentum contrast, wherein fundus images' saliency\nmaps are utilized to remove trivial patches from the input sequences of the\nmomentum-updated key encoder. Thus, the key encoder is constrained to provide\ntarget representations focusing on salient regions, guiding the query encoder\nto capture salient features. (2) The query encoder is trained to predict the\nsaliency segmentation, encouraging the preservation of fine-grained information\nin the learned representations. To assess our proposed method, four\npublicly-accessible fundus image datasets are adopted. One dataset is employed\nfor pre-training, while the three others are used to evaluate the pre-trained\nmodels' performance on downstream DR grading. The proposed SSiT significantly\noutperforms other representative state-of-the-art SSL methods on all downstream\ndatasets and under various evaluation settings. For example, SSiT achieves a\nKappa score of 81.88% on the DDR dataset under fine-tuning evaluation,\noutperforming all other ViT-based SSL methods by at least 9.48%.\n","authors":["Yijin Huang","Junyan Lyu","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2210.10969v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07570v1","updated":"2024-03-12T11:58:37Z","published":"2024-03-12T11:58:37Z","title":"An Active Contour Model Driven By the Hybrid Signed Pressure Function","summary":"  Due to the influence of imaging equipment and complex imaging environments,\nmost images in daily life have features of intensity inhomogeneity and noise.\nTherefore, many scholars have designed many image segmentation algorithms to\naddress these issues. Among them, the active contour model is one of the most\neffective image segmentation algorithms.This paper proposes an active contour\nmodel driven by the hybrid signed pressure function that combines global and\nlocal information construction. Firstly, a new global region-based signed\npressure function is introduced by combining the average intensity of the inner\nand outer regions of the curve with the median intensity of the inner region of\nthe evolution curve. Then, the paper uses the energy differences between the\ninner and outer regions of the curve in the local region to design the signed\npressure function of the local term. Combine the two SPF function to obtain a\nnew signed pressure function and get the evolution equation of the new model.\nFinally, experiments and numerical analysis show that the model has excellent\nsegmentation performance for both intensity inhomogeneous images and noisy\nimages.\n","authors":["Jing Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.07570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10535v2","updated":"2024-03-12T11:57:00Z","published":"2023-06-18T11:56:52Z","title":"ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging","summary":"  Multiple Instance Learning (MIL) is a weakly-supervised problem in which one\nlabel is assigned to the whole bag of instances. An important class of MIL\nmodels is instance-based, where we first classify instances and then aggregate\nthose predictions to obtain a bag label. The most common MIL model is when we\nconsider a bag as positive if at least one of its instances has a positive\nlabel. However, this reasoning does not hold in many real-life scenarios, where\nthe positive bag label is often a consequence of a certain percentage of\npositive instances. To address this issue, we introduce a dedicated\ninstance-based method called ProMIL, based on deep neural networks and\nBernstein polynomial estimation. An important advantage of ProMIL is that it\ncan automatically detect the optimal percentage level for decision-making. We\nshow that ProMIL outperforms standard instance-based MIL in real-world medical\napplications. We make the code available.\n","authors":["Łukasz Struski","Dawid Rymarczyk","Arkadiusz Lewicki","Robert Sabiniewicz","Jacek Tabor","Bartosz Zieliński"],"pdf_url":"https://arxiv.org/pdf/2306.10535v2.pdf","comment":"Accepted Paper to European Conference on Artificial Intelligence\n  (ECAI 2023)"},{"id":"http://arxiv.org/abs/2403.07569v1","updated":"2024-03-12T11:56:50Z","published":"2024-03-12T11:56:50Z","title":"Exploring Challenges in Deep Learning of Single-Station Ground Motion\n  Records","summary":"  Contemporary deep learning models have demonstrated promising results across\nvarious applications within seismology and earthquake engineering. These models\nrely primarily on utilizing ground motion records for tasks such as earthquake\nevent classification, localization, earthquake early warning systems, and\nstructural health monitoring. However, the extent to which these models\neffectively learn from these complex time-series signals has not been\nthoroughly analyzed. In this study, our objective is to evaluate the degree to\nwhich auxiliary information, such as seismic phase arrival times or seismic\nstation distribution within a network, dominates the process of deep learning\nfrom ground motion records, potentially hindering its effectiveness. We perform\na hyperparameter search on two deep learning models to assess their\neffectiveness in deep learning from ground motion records while also examining\nthe impact of auxiliary information on model performance. Experimental results\nreveal a strong reliance on the highly correlated P and S phase arrival\ninformation. Our observations highlight a potential gap in the field,\nindicating an absence of robust methodologies for deep learning of\nsingle-station ground motion recordings independent of any auxiliary\ninformation.\n","authors":["Ümit Mert Çağlar","Baris Yilmaz","Melek Türkmen","Erdem Akagündüz","Salih Tileylioglu"],"pdf_url":"https://arxiv.org/pdf/2403.07569v1.pdf","comment":"9 Pages, 12 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2310.18511v2","updated":"2024-03-12T11:52:42Z","published":"2023-10-27T22:01:43Z","title":"3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for\n  Compositional Recognition","summary":"  In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160\nmillion rendered views of more than 10 million stylized 3D shapes carefully\nannotated at the part-instance level, alongside matching RGB point clouds, 3D\ntextured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41\nshape categories, 275 fine-grained part categories, and 293 fine-grained\nmaterial classes that can be compositionally applied to parts of 3D objects. We\nrender a subset of one million stylized shapes from four equally spaced views\nas well as four randomized views, leading to a total of 160 million renderings.\nParts are segmented at the instance level, with coarse-grained and fine-grained\nsemantic levels. We introduce a new task, called Grounded CoMPaT Recognition\n(GCR), to collectively recognize and ground compositions of materials on parts\nof 3D objects. Additionally, we report the outcomes of a data challenge\norganized at CVPR2023, showcasing the winning method's utilization of a\nmodified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative\ntechniques for GCR enhancement. We hope our work will help ease future research\non compositional 3D Vision.\n","authors":["Habib Slim","Xiang Li","Yuchen Li","Mahmoud Ahmed","Mohamed Ayman","Ujjwal Upadhyay","Ahmed Abdelreheem","Arpit Prajapati","Suhail Pothigara","Peter Wonka","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2310.18511v2.pdf","comment":"https://3dcompat-dataset.org/v2/"},{"id":"http://arxiv.org/abs/2403.07564v1","updated":"2024-03-12T11:51:59Z","published":"2024-03-12T11:51:59Z","title":"RSBuilding: Towards General Remote Sensing Image Building Extraction and\n  Change Detection with Foundation Model","summary":"  The intelligent interpretation of buildings plays a significant role in urban\nplanning and management, macroeconomic analysis, population dynamics, etc.\nRemote sensing image building interpretation primarily encompasses building\nextraction and change detection. However, current methodologies often treat\nthese two tasks as separate entities, thereby failing to leverage shared\nknowledge. Moreover, the complexity and diversity of remote sensing image\nscenes pose additional challenges, as most algorithms are designed to model\nindividual small datasets, thus lacking cross-scene generalization. In this\npaper, we propose a comprehensive remote sensing image building understanding\nmodel, termed RSBuilding, developed from the perspective of the foundation\nmodel. RSBuilding is designed to enhance cross-scene generalization and task\nuniversality. Specifically, we extract image features based on the prior\nknowledge of the foundation model and devise a multi-level feature sampler to\naugment scale information. To unify task representation and integrate image\nspatiotemporal clues, we introduce a cross-attention decoder with task prompts.\nAddressing the current shortage of datasets that incorporate annotations for\nboth tasks, we have developed a federated training strategy to facilitate\nsmooth model convergence even when supervision for some tasks is missing,\nthereby bolstering the complementarity of different tasks. Our model was\ntrained on a dataset comprising up to 245,000 images and validated on multiple\nbuilding extraction and change detection datasets. The experimental results\nsubstantiate that RSBuilding can concurrently handle two structurally distinct\ntasks and exhibits robust zero-shot generalization capabilities.\n","authors":["Mingze Wang","Keyan Chen","Lili Su","Cilin Yan","Sheng Xu","Haotian Zhang","Pengcheng Yuan","Xiaolong Jiang","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07563v1","updated":"2024-03-12T11:51:55Z","published":"2024-03-12T11:51:55Z","title":"Learning Generalizable Feature Fields for Mobile Manipulation","summary":"  An open problem in mobile manipulation is how to represent objects and scenes\nin a unified manner, so that robots can use it both for navigating in the\nenvironment and manipulating objects. The latter requires capturing intricate\ngeometry while understanding fine-grained semantics, whereas the former\ninvolves capturing the complexity inherit to an expansive physical scale. In\nthis work, we present GeFF (Generalizable Feature Fields), a scene-level\ngeneralizable neural feature field that acts as a unified representation for\nboth navigation and manipulation that performs in real-time. To do so, we treat\ngenerative novel view synthesis as a pre-training task, and then align the\nresulting rich scene priors with natural language via CLIP feature\ndistillation. We demonstrate the effectiveness of this approach by deploying\nGeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's\nability to generalize to open-set objects as well as running time, when\nperforming open-vocabulary mobile manipulation in dynamic scenes.\n","authors":["Ri-Zhao Qiu","Yafei Hu","Ge Yang","Yuchen Song","Yang Fu","Jianglong Ye","Jiteng Mu","Ruihan Yang","Nikolay Atanasov","Sebastian Scherer","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07563v1.pdf","comment":"Preprint. Project website is at: https://geff-b1.github.io/"},{"id":"http://arxiv.org/abs/2403.07560v1","updated":"2024-03-12T11:48:49Z","published":"2024-03-12T11:48:49Z","title":"Unleashing Network Potentials for Semantic Scene Completion","summary":"  Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy\nand semantics from a single-view RGB-D image, and recent SSC methods commonly\nadopt multi-modal inputs. However, our investigation reveals two limitations:\nineffective feature learning from single modalities and overfitting to limited\ndatasets. To address these issues, this paper proposes a novel SSC framework -\nAdversarial Modality Modulation Network (AMMNet) - with a fresh perspective of\noptimizing gradient updates. The proposed AMMNet introduces two core modules: a\ncross-modal modulation enabling the interdependence of gradient flows between\nmodalities, and a customized adversarial training scheme leveraging dynamic\ngradient competition. Specifically, the cross-modal modulation adaptively\nre-calibrates the features to better excite representation potentials from each\nsingle modality. The adversarial training employs a minimax game of evolving\ngradients, with customized guidance to strengthen the generator's perception of\nvisual fidelity from both geometric completeness and semantic correctness.\nExtensive experimental results demonstrate that AMMNet outperforms\nstate-of-the-art SSC methods by a large margin, providing a promising direction\nfor improving the effectiveness and generalization of SSC methods.\n","authors":["Fengyun Wang","Qianru Sun","Dong Zhang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07560v1.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2402.09430v2","updated":"2024-03-12T11:48:02Z","published":"2024-01-24T16:10:14Z","title":"WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing","summary":"  WiFi-based human sensing has exhibited remarkable potential to analyze user\nbehaviors in a non-intrusive and device-free manner, benefiting applications as\ndiverse as smart homes and healthcare. However, most previous works focus on\nsingle-user sensing, which has limited practicability in scenarios involving\nmultiple users. Although recent studies have begun to investigate WiFi-based\nmulti-user sensing, there remains a lack of benchmark datasets to facilitate\nreproducible and comparable research. To bridge this gap, we present WiMANS, to\nour knowledge, the first dataset for multi-user sensing based on WiFi. WiMANS\ncontains over 9.4 hours of dual-band WiFi Channel State Information (CSI), as\nwell as synchronized videos, monitoring simultaneous activities of multiple\nusers. We exploit WiMANS to benchmark the performance of state-of-the-art\nWiFi-based human sensing models and video-based models, posing new challenges\nand opportunities for future work. We believe WiMANS can push the boundaries of\ncurrent studies and catalyze the research on WiFi-based multi-user sensing.\n","authors":["Shuokang Huang","Kaihan Li","Di You","Yichong Chen","Arvin Lin","Siying Liu","Xiaohui Li","Julie A. McCann"],"pdf_url":"https://arxiv.org/pdf/2402.09430v2.pdf","comment":"We present WiMANS, to our knowledge, the first dataset for multi-user\n  activity sensing based on WiFi"},{"id":"http://arxiv.org/abs/2403.04558v2","updated":"2024-03-12T11:42:06Z","published":"2024-03-07T14:56:06Z","title":"Reducing self-supervised learning complexity improves weakly-supervised\n  classification performance in computational pathology","summary":"  Deep Learning models have been successfully utilized to extract clinically\nactionable insights from routinely available histology data. Generally, these\nmodels require annotations performed by clinicians, which are scarce and costly\nto generate. The emergence of self-supervised learning (SSL) methods remove\nthis barrier, allowing for large-scale analyses on non-annotated data. However,\nrecent SSL approaches apply increasingly expansive model architectures and\nlarger datasets, causing the rapid escalation of data volumes, hardware\nprerequisites, and overall expenses, limiting access to these resources to few\ninstitutions. Therefore, we investigated the complexity of contrastive SSL in\ncomputational pathology in relation to classification performance with the\nutilization of consumer-grade hardware. Specifically, we analyzed the effects\nof adaptations in data volume, architecture, and algorithms on downstream\nclassification tasks, emphasizing their impact on computational resources. We\ntrained breast cancer foundation models on a large public patient cohort and\nvalidated them on various downstream classification tasks in a weakly\nsupervised manner on two external public patient cohorts. Our experiments\ndemonstrate that we can improve downstream classification performance whilst\nreducing SSL training duration by 90%. In summary, we propose a set of\nadaptations which enable the utilization of SSL in computational pathology in\nnon-resource abundant environments.\n","authors":["Tim Lenz","Omar S. M. El Nahhas","Marta Ligero","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2403.04558v2.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.07553v1","updated":"2024-03-12T11:39:18Z","published":"2024-03-12T11:39:18Z","title":"The future of document indexing: GPT and Donut revolutionize table of\n  content processing","summary":"  Industrial projects rely heavily on lengthy, complex specification documents,\nmaking tedious manual extraction of structured information a major bottleneck.\nThis paper introduces an innovative approach to automate this process,\nleveraging the capabilities of two cutting-edge AI models: Donut, a model that\nextracts information directly from scanned documents without OCR, and OpenAI\nGPT-3.5 Turbo, a robust large language model. The proposed methodology is\ninitiated by acquiring the table of contents (ToCs) from construction\nspecification documents and subsequently structuring the ToCs text into JSON\ndata. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5\nTurbo reaching 89% in effectively organizing the ToCs. This landmark\nachievement represents a significant leap forward in document indexing,\ndemonstrating the immense potential of AI to automate information extraction\ntasks across diverse document types, boosting efficiency and liberating\ncritical resources in various industries.\n","authors":["Degaga Wolde Feyisa","Haylemicheal Berihun","Amanuel Zewdu","Mahsa Najimoghadam","Marzieh Zare"],"pdf_url":"https://arxiv.org/pdf/2403.07553v1.pdf","comment":"Document AI, Document Classification, Information extraction, Large\n  Language Models, OCR Models, Visual Document Understanding"},{"id":"http://arxiv.org/abs/2403.07547v1","updated":"2024-03-12T11:32:57Z","published":"2024-03-12T11:32:57Z","title":"SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields","summary":"  Neural radiance fields (NeRF) has attracted considerable attention for their\nexceptional ability in synthesizing novel views with high fidelity. However,\nthe presence of motion blur, resulting from slight camera movements during\nextended shutter exposures, poses a significant challenge, potentially\ncompromising the quality of the reconstructed 3D scenes. While recent studies\nhave addressed this issue, they do not consider the continuous dynamics of\ncamera movements during image acquisition, leading to inaccurate scene\nreconstruction. Additionally, these methods are plagued by slow training and\nrendering speed. To effectively handle these issues, we propose sequential\nmotion understanding radiance fields (SMURF), a novel approach that employs\nneural ordinary differential equation (Neural-ODE) to model continuous camera\nmotion and leverages the explicit volumetric representation method for faster\ntraining and robustness to motion-blurred input images. The core idea of the\nSMURF is continuous motion blurring kernel (CMBK), a unique module designed to\nmodel a continuous camera movements for processing blurry inputs. Our model,\nrigorously evaluated against benchmark datasets, demonstrates state-of-the-art\nperformance both quantitatively and qualitatively.\n","authors":["Jungho Lee","Dogyoon Lee","Minhyeok Lee","Donghyung Kim","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.07547v1.pdf","comment":"25 pages, 10 figures, Code is available at\n  https://github.com/Jho-Yonsei/SMURF"},{"id":"http://arxiv.org/abs/2403.07542v1","updated":"2024-03-12T11:29:40Z","published":"2024-03-12T11:29:40Z","title":"A Survey of Vision Transformers in Autonomous Driving: Current Trends\n  and Future Directions","summary":"  This survey explores the adaptation of visual transformer models in\nAutonomous Driving, a transition inspired by their success in Natural Language\nProcessing. Surpassing traditional Recurrent Neural Networks in tasks like\nsequential image processing and outperforming Convolutional Neural Networks in\nglobal context capture, as evidenced in complex scene recognition, Transformers\nare gaining traction in computer vision. These capabilities are crucial in\nAutonomous Driving for real-time, dynamic visual scene processing. Our survey\nprovides a comprehensive overview of Vision Transformer applications in\nAutonomous Driving, focusing on foundational concepts such as self-attention,\nmulti-head attention, and encoder-decoder architecture. We cover applications\nin object detection, segmentation, pedestrian detection, lane detection, and\nmore, comparing their architectural merits and limitations. The survey\nconcludes with future research directions, highlighting the growing role of\nVision Transformers in Autonomous Driving.\n","authors":["Quoc-Vinh Lai-Dang"],"pdf_url":"https://arxiv.org/pdf/2403.07542v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.03227v3","updated":"2024-03-12T11:28:20Z","published":"2024-02-05T17:38:49Z","title":"IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of\n  brain MR images","summary":"  In MRI studies, the aggregation of imaging data from multiple acquisition\nsites enhances sample size but may introduce site-related variabilities that\nhinder consistency in subsequent analyses. Deep learning methods for image\ntranslation have emerged as a solution for harmonizing MR images across sites.\nIn this study, we introduce IGUANe (Image Generation with Unified Adversarial\nNetworks), an original 3D model that leverages the strengths of domain\ntranslation and straightforward application of style transfer methods for\nmulticenter brain MR image harmonization. IGUANe extends CycleGAN architecture\nby integrating an arbitrary number of domains for training through a\nmany-to-one strategy. During inference, the model can be applied to any image,\neven from an unknown acquisition site, making it a universal generator for\nharmonization. Trained on a dataset comprising T1-weighted images from 11\ndifferent scanners, IGUANe was evaluated on data from unseen sites. The\nassessments included the transformation of MR images with traveling subjects,\nthe preservation of pairwise distances between MR images within domains, the\nevolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease\n(AD), and the performance in age regression and patient classification tasks.\nComparisons with other harmonization and normalization methods suggest that\nIGUANe better preserves individual information in MR images and is more\nsuitable for maintaining and reinforcing variabilities related to age and AD.\nFuture studies may further assess IGUANe in other multicenter contexts, either\nusing the same model or retraining it for applications to different image\nmodalities.\n","authors":["Vincent Roca","Grégory Kuchcinski","Jean-Pierre Pruvo","Dorian Manouvriez","Renaud Lopes"],"pdf_url":"https://arxiv.org/pdf/2402.03227v3.pdf","comment":"23 pages, 8 figures; typos corrected"},{"id":"http://arxiv.org/abs/2309.06067v5","updated":"2024-03-12T11:24:51Z","published":"2023-09-12T09:07:03Z","title":"Implicit Neural Representation for MRI Parallel Imaging Reconstruction","summary":"  Magnetic resonance imaging (MRI) always suffers from long acquisition times.\nParallel imaging (PI) is one solution to reduce scan time by periodically\nskipping certain K-space lines and then reconstructing high-quality images from\nundersampled measurements. Recently, implicit neural representation (INR) has\nemerged as a new deep learning method that represents an object as a continuous\nfunction of spatial coordinates, and this function is normally parameterized by\na multilayer perceptron (MLP). In this paper, we propose a novel MRI PI\nreconstruction method based on INR, which represents the reconstructed\nfully-sampled images as the function of voxel coordinates and prior feature\nvectors of undersampled images to overcome the generalization problem of INR.\nSpecifically, we introduce a scale-embedded encoder to produce\nscale-independent voxel-specific features from MR images with different\nundersampling scales and then concatenate with coordinate vectors to recover\nfully-sampled MR images, thus achieving multiple scale reconstructions. The\nperformance of the proposed method was assessed by experimenting with publicly\navailable MRI datasets and was compared with other reconstruction methods. Our\nquantitative evaluation demonstrates the superiority of the proposed method\nover alternative reconstruction methods.\n","authors":["Hao Li","Yusheng Zhou","Jianan Liu","Xiling Liu","Tao Huang","Zhihan Lv"],"pdf_url":"https://arxiv.org/pdf/2309.06067v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07536v1","updated":"2024-03-12T11:19:46Z","published":"2024-03-12T11:19:46Z","title":"LaB-GATr: geometric algebra transformers for large biomedical surface\n  and volume meshes","summary":"  Many anatomical structures can be described by surface or volume meshes.\nMachine learning is a promising tool to extract information from these 3D\nmodels. However, high-fidelity meshes often contain hundreds of thousands of\nvertices, which creates unique challenges in building deep neural network\narchitectures. Furthermore, patient-specific meshes may not be canonically\naligned which limits the generalisation of machine learning algorithms. We\npropose LaB-GATr, a transfomer neural network with geometric tokenisation that\ncan effectively learn with large-scale (bio-)medical surface and volume meshes\nthrough sequence compression and interpolation. Our method extends the recently\nproposed geometric algebra transformer (GATr) and thus respects all Euclidean\nsymmetries, i.e. rotation, translation and reflection, effectively mitigating\nthe problem of canonical alignment between patients. LaB-GATr achieves\nstate-of-the-art results on three tasks in cardiovascular hemodynamics\nmodelling and neurodevelopmental phenotype prediction, featuring meshes of up\nto 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful\narchitecture for learning with high-fidelity meshes which has the potential to\nenable interesting downstream applications. Our implementation is publicly\navailable.\n","authors":["Julian Suk","Baris Imre","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2403.07536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07535v1","updated":"2024-03-12T11:18:35Z","published":"2024-03-12T11:18:35Z","title":"Adaptive Fusion of Single-View and Multi-View Depth for Autonomous\n  Driving","summary":"  Multi-view depth estimation has achieved impressive performance over various\nbenchmarks. However, almost all current multi-view systems rely on given ideal\ncamera poses, which are unavailable in many real-world scenarios, such as\nautonomous driving. In this work, we propose a new robustness benchmark to\nevaluate the depth estimation system under various noisy pose settings.\nSurprisingly, we find current multi-view depth estimation methods or\nsingle-view and multi-view fusion methods will fail when given noisy pose\nsettings. To address this challenge, we propose a single-view and multi-view\nfused depth estimation system, which adaptively integrates high-confident\nmulti-view and single-view results for both robust and accurate depth\nestimations. The adaptive fusion module performs fusion by dynamically\nselecting high-confidence regions between two branches based on a wrapping\nconfidence map. Thus, the system tends to choose the more reliable branch when\nfacing textureless scenes, inaccurate calibration, dynamic objects, and other\ndegradation or challenging conditions. Our method outperforms state-of-the-art\nmulti-view and fusion methods under robustness testing. Furthermore, we achieve\nstate-of-the-art performance on challenging benchmarks (KITTI and DDAD) when\ngiven accurate pose estimations. Project website:\nhttps://github.com/Junda24/AFNet/.\n","authors":["JunDa Cheng","Wei Yin","Kaixuan Wang","Xiaozhi Chen","Shijie Wang","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2403.07535v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07532v1","updated":"2024-03-12T11:11:19Z","published":"2024-03-12T11:11:19Z","title":"Open-World Semantic Segmentation Including Class Similarity","summary":"  Interpreting camera data is key for autonomously acting systems, such as\nautonomous vehicles. Vision systems that operate in real-world environments\nmust be able to understand their surroundings and need the ability to deal with\nnovel situations. This paper tackles open-world semantic segmentation, i.e.,\nthe variant of interpreting image data in which objects occur that have not\nbeen seen during training. We propose a novel approach that performs accurate\nclosed-world semantic segmentation and, at the same time, can identify new\ncategories without requiring any additional training data. Our approach\nadditionally provides a similarity measure for every newly discovered class in\nan image to a known category, which can be useful information in downstream\ntasks such as planning or mapping. Through extensive experiments, we show that\nour model achieves state-of-the-art results on classes known from training data\nas well as for anomaly segmentation and can distinguish between different\nunknown classes.\n","authors":["Matteo Sodano","Federico Magistri","Lucas Nunes","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2403.07532v1.pdf","comment":"Accepted at CVPR 2024. Code at: https://github.com/PRBonn/ContMAV"},{"id":"http://arxiv.org/abs/2311.11845v2","updated":"2024-03-12T10:57:53Z","published":"2023-11-20T15:35:00Z","title":"Entangled View-Epipolar Information Aggregation for Generalizable Neural\n  Radiance Fields","summary":"  Generalizable NeRF can directly synthesize novel views across new scenes,\neliminating the need for scene-specific retraining in vanilla NeRF. A critical\nenabling factor in these approaches is the extraction of a generalizable 3D\nrepresentation by aggregating source-view features. In this paper, we propose\nan Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF.\nDifferent from existing methods that consider cross-view and along-epipolar\ninformation independently, EVE-NeRF conducts the view-epipolar feature\naggregation in an entangled manner by injecting the scene-invariant appearance\ncontinuity and geometry consistency priors to the aggregation process. Our\napproach effectively mitigates the potential lack of inherent geometric and\nappearance constraint resulting from one-dimensional interactions, thus further\nboosting the 3D representation generalizablity. EVE-NeRF attains\nstate-of-the-art performance across various evaluation scenarios. Extensive\nexperiments demonstate that, compared to prevailing single-dimensional\naggregation, the entangled network excels in the accuracy of 3D scene geometry\nand appearance reconstruction. Our code is publicly available at\nhttps://github.com/tatakai1/EVENeRF.\n","authors":["Zhiyuan Min","Yawei Luo","Wei Yang","Yuesong Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2311.11845v2.pdf","comment":"Accepted by CVPR-2024"},{"id":"http://arxiv.org/abs/2403.06242v2","updated":"2024-03-12T10:54:57Z","published":"2024-03-10T15:54:45Z","title":"COVID-19 Computer-aided Diagnosis through AI-assisted CT Imaging\n  Analysis: Deploying a Medical AI System","summary":"  Computer-aided diagnosis (CAD) systems stand out as potent aids for\nphysicians in identifying the novel Coronavirus Disease 2019 (COVID-19) through\nmedical imaging modalities. In this paper, we showcase the integration and\nreliable and fast deployment of a state-of-the-art AI system designed to\nautomatically analyze CT images, offering infection probability for the swift\ndetection of COVID-19. The suggested system, comprising both classification and\nsegmentation components, is anticipated to reduce physicians' detection time\nand enhance the overall efficiency of COVID-19 detection. We successfully\nsurmounted various challenges, such as data discrepancy and anonymisation,\ntesting the time-effectiveness of the model, and data security, enabling\nreliable and scalable deployment of the system on both cloud and edge\nenvironments. Additionally, our AI system assigns a probability of infection to\neach 3D CT scan and enhances explainability through anchor set similarity,\nfacilitating timely confirmation and segregation of infected patients by\nphysicians.\n","authors":["Demetris Gerogiannis","Anastasios Arsenos","Dimitrios Kollias","Dimitris Nikitopoulos","Stefanos Kollias"],"pdf_url":"https://arxiv.org/pdf/2403.06242v2.pdf","comment":"accepted at IEEE ISBI 2024"},{"id":"http://arxiv.org/abs/2403.07518v1","updated":"2024-03-12T10:54:38Z","published":"2024-03-12T10:54:38Z","title":"Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and\n  Margin Loss","summary":"  Scene text recognition is an important and challenging task in computer\nvision. However, most prior works focus on recognizing pre-defined words, while\nthere are various out-of-vocabulary (OOV) words in real-world applications.\n  In this paper, we propose a novel open-vocabulary text recognition framework,\nPseudo-OCR, to recognize OOV words. The key challenge in this task is the lack\nof OOV training data. To solve this problem, we first propose a pseudo label\ngeneration module that leverages character detection and image inpainting to\nproduce substantial pseudo OOV training data from real-world images. Unlike\nprevious synthetic data, our pseudo OOV data contains real characters and\nbackgrounds to simulate real-world applications. Secondly, to reduce noises in\npseudo data, we present a semantic checking mechanism to filter semantically\nmeaningful data. Thirdly, we introduce a quality-aware margin loss to boost the\ntraining with pseudo data. Our loss includes a margin-based part to enhance the\nclassification ability, and a quality-aware part to penalize low-quality\nsamples in both real and pseudo data.\n  Extensive experiments demonstrate that our approach outperforms the\nstate-of-the-art on eight datasets and achieves the first rank in the ICDAR2022\nchallenge.\n","authors":["Xuhua Ren","Hengcan Shi","Jin Li"],"pdf_url":"https://arxiv.org/pdf/2403.07518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07516v1","updated":"2024-03-12T10:47:53Z","published":"2024-03-12T10:47:53Z","title":"D4D: An RGBD diffusion model to boost monocular depth estimation","summary":"  Ground-truth RGBD data are fundamental for a wide range of computer vision\napplications; however, those labeled samples are difficult to collect and\ntime-consuming to produce. A common solution to overcome this lack of data is\nto employ graphic engines to produce synthetic proxies; however, those data do\nnot often reflect real-world images, resulting in poor performance of the\ntrained models at the inference step. In this paper we propose a novel training\npipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion\nmodel able to generate realistic RGBD samples. We show the effectiveness of the\ndeveloped solution in improving the performances of deep learning models on the\nmonocular depth estimation task, where the correspondence between RGB and depth\nmap is crucial to achieving accurate measurements. Our supervised training\npipeline, enriched by the generated samples, outperforms synthetic and original\ndata performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%)\nrespectively on the indoor NYU Depth v2 and the outdoor KITTI dataset.\n","authors":["L. Papa","P. Russo","I. Amerini"],"pdf_url":"https://arxiv.org/pdf/2403.07516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07514v1","updated":"2024-03-12T10:47:45Z","published":"2024-03-12T10:47:45Z","title":"Uncertainty-guided Contrastive Learning for Single Source Domain\n  Generalisation","summary":"  In the context of single domain generalisation, the objective is for models\nthat have been exclusively trained on data from a single domain to demonstrate\nstrong performance when confronted with various unfamiliar domains. In this\npaper, we introduce a novel model referred to as Contrastive Uncertainty Domain\nGeneralisation Network (CUDGNet). The key idea is to augment the source\ncapacity in both input and label spaces through the fictitious domain generator\nand jointly learn the domain invariant representation of each class through\ncontrastive learning. Extensive experiments on two Single Source Domain\nGeneralisation (SSDG) datasets demonstrate the effectiveness of our approach,\nwhich surpasses the state-of-the-art single-DG methods by up to $7.08\\%$. Our\nmethod also provides efficient uncertainty estimation at inference time from a\nsingle forward pass through the generator subnetwork.\n","authors":["Anastasios Arsenos","Dimitrios Kollias","Evangelos Petrongonas","Christos Skliros","Stefanos Kollias"],"pdf_url":"https://arxiv.org/pdf/2403.07514v1.pdf","comment":"accepted at IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.07513v1","updated":"2024-03-12T10:47:29Z","published":"2024-03-12T10:47:29Z","title":"Spatiotemporal Representation Learning for Short and Long Medical Image\n  Time Series","summary":"  Analyzing temporal developments is crucial for the accurate prognosis of many\nmedical conditions. Temporal changes that occur over short time scales are key\nto assessing the health of physiological functions, such as the cardiac cycle.\nMoreover, tracking longer term developments that occur over months or years in\nevolving processes, such as age-related macular degeneration (AMD), is\nessential for accurate prognosis. Despite the importance of both short and long\nterm analysis to clinical decision making, they remain understudied in medical\ndeep learning. State of the art methods for spatiotemporal representation\nlearning, developed for short natural videos, prioritize the detection of\ntemporal constants rather than temporal developments. Moreover, they do not\naccount for varying time intervals between acquisitions, which are essential\nfor contextualizing observed changes. To address these issues, we propose two\napproaches. First, we combine clip-level contrastive learning with a novel\ntemporal embedding to adapt to irregular time series. Second, we propose\nmasking and predicting latent frame representations of the temporal sequence.\nOur two approaches outperform all prior methods on temporally-dependent tasks\nincluding cardiac output estimation and three prognostic AMD tasks. Overall,\nthis enables the automated analysis of temporal patterns which are typically\noverlooked in applications of deep learning to medicine.\n","authors":["Chengzhi Shen","Martin J. Menten","Hrvoje Bogunović","Ursula Schmidt-Erfurth","Hendrik Scholl","Sobha Sivaprasad","Andrew Lotery","Daniel Rueckert","Paul Hager","Robbie Holland"],"pdf_url":"https://arxiv.org/pdf/2403.07513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07537v2","updated":"2024-03-12T10:46:33Z","published":"2023-09-14T09:03:57Z","title":"Towards a universal mechanism for successful deep learning","summary":"  Recently, the underlying mechanism for successful deep learning (DL) was\npresented based on a quantitative method that measures the quality of a single\nfilter in each layer of a DL model, particularly VGG-16 trained on CIFAR-10.\nThis method exemplifies that each filter identifies small clusters of possible\noutput labels, with additional noise selected as labels outside the clusters.\nThis feature is progressively sharpened with each layer, resulting in an\nenhanced signal-to-noise ratio (SNR), which leads to an increase in the\naccuracy of the DL network. In this study, this mechanism is verified for\nVGG-16 and EfficientNet-B0 trained on the CIFAR-100 and ImageNet datasets, and\nthe main results are as follows. First, the accuracy and SNR progressively\nincrease with the layers. Second, for a given deep architecture, the maximal\nerror rate increases approximately linearly with the number of output labels.\nThird, similar trends were obtained for dataset labels in the range [3, 1,000],\nthus supporting the universality of this mechanism. Understanding the\nperformance of a single filter and its dominating features paves the way to\nhighly dilute the deep architecture without affecting its overall accuracy, and\nthis can be achieved by applying the filter's cluster connections (AFCC).\n","authors":["Yuval Meir","Yarden Tzach","Shiri Hodassman","Ofek Tevet","Ido Kanter"],"pdf_url":"https://arxiv.org/pdf/2309.07537v2.pdf","comment":"31 pages,7 figures, 9 tables. arXiv admin note: text overlap with\n  arXiv:2305.18078"},{"id":"http://arxiv.org/abs/2403.07508v1","updated":"2024-03-12T10:44:13Z","published":"2024-03-12T10:44:13Z","title":"MoAI: Mixture of All Intelligence for Large Language and Vision Models","summary":"  The rise of large language models (LLMs) and instruction tuning has led to\nthe current trend of instruction-tuned large language and vision models\n(LLVMs). This trend involves either meticulously curating numerous instruction\ntuning datasets tailored to specific objectives or enlarging LLVMs to manage\nvast amounts of vision language (VL) data. However, current LLVMs have\ndisregarded the detailed and comprehensive real-world scene understanding\navailable from specialized computer vision (CV) models in visual perception\ntasks such as segmentation, detection, scene graph generation (SGG), and\noptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\nthe large capacity and emergent capabilities of their LLM backbones. Therefore,\nwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\nauxiliary visual information obtained from the outputs of external\nsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\nintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\noutputs of the external CV models, the MoAI-Compressor aligns and condenses\nthem to efficiently use relevant auxiliary visual information for VL tasks.\nMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\nauxiliary features from the external CV models, and (3) language features by\nutilizing the concept of Mixture of Experts. Through this integration, MoAI\nsignificantly outperforms both open-source and closed-source LLVMs in numerous\nzero-shot VL tasks, particularly those related to real-world scene\nunderstanding such as object existence, positions, relations, and OCR without\nenlarging the model size or curating extra visual instruction tuning datasets.\n","authors":["Byung-Kwan Lee","Beomchan Park","Chae Won Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.07508v1.pdf","comment":"Code available: https://github.com/ByungKwanLee/MoAI"},{"id":"http://arxiv.org/abs/2403.07500v1","updated":"2024-03-12T10:38:03Z","published":"2024-03-12T10:38:03Z","title":"Block-wise LoRA: Revisiting Fine-grained LoRA for Effective\n  Personalization and Stylization in Text-to-Image Generation","summary":"  The objective of personalization and stylization in text-to-image is to\ninstruct a pre-trained diffusion model to analyze new concepts introduced by\nusers and incorporate them into expected styles. Recently, parameter-efficient\nfine-tuning (PEFT) approaches have been widely adopted to address this task and\nhave greatly propelled the development of this field. Despite their popularity,\nexisting efficient fine-tuning methods still struggle to achieve effective\npersonalization and stylization in T2I generation. To address this issue, we\npropose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained\nfine-tuning for different blocks of SD, which can generate images faithful to\ninput prompts and target identity and also with desired style. Extensive\nexperiments demonstrate the effectiveness of the proposed method.\n","authors":["Likun Li","Haoqi Zeng","Changpeng Yang","Haozhe Jia","Di Xu"],"pdf_url":"https://arxiv.org/pdf/2403.07500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04578v2","updated":"2024-03-12T10:35:56Z","published":"2024-01-09T14:32:24Z","title":"Effective pruning of web-scale datasets based on complexity of concept\n  clusters","summary":"  Utilizing massive web-scale datasets has led to unprecedented performance\ngains in machine learning models, but also imposes outlandish compute\nrequirements for their training. In order to improve training and data\nefficiency, we here push the limits of pruning large-scale multimodal datasets\nfor training CLIP-style models. Today's most effective pruning method on\nImageNet clusters data samples into separate concepts according to their\nembedding and prunes away the most prototypical samples. We scale this approach\nto LAION and improve it by noting that the pruning rate should be\nconcept-specific and adapted to the complexity of the concept. Using a simple\nand intuitive complexity measure, we are able to reduce the training cost to a\nquarter of regular training. By filtering from the LAION dataset, we find that\ntraining on a smaller set of high-quality data can lead to higher performance\nwith significantly lower training costs. More specifically, we are able to\noutperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot\naccuracy by 1.1p.p. while only using 27.7% of the data and training compute.\nDespite a strong reduction in training cost, we also see improvements on\nImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium\nbenchmark, we achieve a new state-of-the-art\nImagehttps://info.arxiv.org/help/prep#commentsNet zero-shot accuracy and a\ncompetitive average zero-shot accuracy on 38 evaluation tasks.\n","authors":["Amro Abbas","Evgenia Rusak","Kushal Tirumala","Wieland Brendel","Kamalika Chaudhuri","Ari S. Morcos"],"pdf_url":"https://arxiv.org/pdf/2401.04578v2.pdf","comment":"Accepted at ICLR 2024, code available at\n  https://github.com/amro-kamal/effective_pruning"},{"id":"http://arxiv.org/abs/2402.01313v3","updated":"2024-03-12T10:35:20Z","published":"2024-02-02T11:07:27Z","title":"AutoGCN -- Towards Generic Human Activity Recognition with Neural\n  Architecture Search","summary":"  This paper introduces AutoGCN, a generic Neural Architecture Search (NAS)\nalgorithm for Human Activity Recognition (HAR) using Graph Convolution Networks\n(GCNs). HAR has gained attention due to advances in deep learning, increased\ndata availability, and enhanced computational capabilities. At the same time,\nGCNs have shown promising results in modeling relationships between body key\npoints in a skeletal graph. While domain experts often craft dataset-specific\nGCN-based methods, their applicability beyond this specific context is severely\nlimited. AutoGCN seeks to address this limitation by simultaneously searching\nfor the ideal hyperparameters and architecture combination within a versatile\nsearch space using a reinforcement controller while balancing optimal\nexploration and exploitation behavior with a knowledge reservoir during the\nsearch process. We conduct extensive experiments on two large-scale datasets\nfocused on skeleton-based action recognition to assess the proposed algorithm's\nperformance. Our experimental results underscore the effectiveness of AutoGCN\nin constructing optimal GCN architectures for HAR, outperforming conventional\nNAS and GCN methods, as well as random search. These findings highlight the\nsignificance of a diverse search space and an expressive input representation\nto enhance the network performance and generalizability.\n","authors":["Felix Tempel","Inga Strümke","Espen Alexander F. Ihlen"],"pdf_url":"https://arxiv.org/pdf/2402.01313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07494v1","updated":"2024-03-12T10:33:26Z","published":"2024-03-12T10:33:26Z","title":"SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM","summary":"  We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D\nGaussian representation, that enables accurate 3D semantic mapping, robust\ncamera tracking, and high-quality rendering in real-time. In this system, we\nincorporate semantic feature embedding into 3D Gaussian representation, which\neffectively encodes semantic information within the spatial layout of the\nenvironment for precise semantic scene representation. Furthermore, we propose\nfeature-level loss for updating 3D Gaussian representation, enabling\nhigher-level guidance for 3D Gaussian optimization. In addition, to reduce\ncumulative drift and improve reconstruction accuracy, we introduce\nsemantic-informed bundle adjustment leveraging semantic associations for joint\noptimization of 3D Gaussian representation and camera poses, leading to more\nrobust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates\nsuperior performance over existing dense semantic SLAM methods in terms of\nmapping and tracking accuracy on Replica and ScanNet datasets, while also\nshowing excellent capabilities in novel-view semantic synthesis and 3D semantic\nmapping.\n","authors":["Siting Zhu","Renjie Qin","Guangming Wang","Jiuming Liu","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02031v2","updated":"2024-03-12T10:33:20Z","published":"2023-09-05T08:21:16Z","title":"A survey on efficient vision transformers: algorithms, techniques, and\n  performance benchmarking","summary":"  Vision Transformer (ViT) architectures are becoming increasingly popular and\nwidely employed to tackle computer vision applications. Their main feature is\nthe capacity to extract global information through the self-attention\nmechanism, outperforming earlier convolutional neural networks. However, ViT\ndeployment and performance have grown steadily with their size, number of\ntrainable parameters, and operations. Furthermore, self-attention's\ncomputational and memory cost quadratically increases with the image\nresolution. Generally speaking, it is challenging to employ these architectures\nin real-world applications due to many hardware and environmental restrictions,\nsuch as processing and computational capabilities. Therefore, this survey\ninvestigates the most efficient methodologies to ensure sub-optimal estimation\nperformances. More in detail, four efficient categories will be analyzed:\ncompact architecture, pruning, knowledge distillation, and quantization\nstrategies. Moreover, a new metric called Efficient Error Rate has been\nintroduced in order to normalize and compare models' features that affect\nhardware devices at inference time, such as the number of parameters, bits,\nFLOPs, and model size. Summarizing, this paper firstly mathematically defines\nthe strategies used to make Vision Transformer efficient, describes and\ndiscusses state-of-the-art methodologies, and analyzes their performances over\ndifferent application scenarios. Toward the end of this paper, we also discuss\nopen challenges and promising research directions.\n","authors":["Lorenzo Papa","Paolo Russo","Irene Amerini","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2309.02031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07487v1","updated":"2024-03-12T10:25:29Z","published":"2024-03-12T10:25:29Z","title":"Motion Mamba: Efficient and Long Sequence Motion Generation with\n  Hierarchical and Bidirectional Selective SSM","summary":"  Human motion generation stands as a significant pursuit in generative\ncomputer vision, while achieving long-sequence and efficient motion generation\nremains challenging. Recent advancements in state space models (SSMs), notably\nMamba, have showcased considerable promise in long sequence modeling with an\nefficient hardware-aware design, which appears to be a promising direction to\nbuild motion generation model upon it. Nevertheless, adapting SSMs to motion\ngeneration faces hurdles since the lack of a specialized design architecture to\nmodel motion sequence. To address these challenges, we propose Motion Mamba, a\nsimple and efficient approach that presents the pioneering motion generation\nmodel utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba\n(HTM) block to process temporal data by ensemble varying numbers of isolated\nSSM modules across a symmetric U-Net architecture aimed at preserving motion\nconsistency between frames. We also design a Bidirectional Spatial Mamba (BSM)\nblock to bidirectionally process latent poses, to enhance accurate motion\ngeneration within a temporal frame. Our proposed method achieves up to 50% FID\nimprovement and up to 4 times faster on the HumanML3D and KIT-ML datasets\ncompared to the previous best diffusion-based method, which demonstrates strong\ncapabilities of high-quality long sequence motion modeling and real-time human\nmotion generation. See project website\nhttps://steve-zeyu-zhang.github.io/MotionMamba/\n","authors":["Zeyu Zhang","Akide Liu","Ian Reid","Richard Hartley","Bohan Zhuang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07469v1","updated":"2024-03-12T10:04:08Z","published":"2024-03-12T10:04:08Z","title":"A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing\n  Objects in 3D Scenes","summary":"  Three-Dimensional (3D) dense captioning is an emerging vision-language\nbridging task that aims to generate multiple detailed and accurate descriptions\nfor 3D scenes. It presents significant potential and challenges due to its\ncloser representation of the real world compared to 2D visual captioning, as\nwell as complexities in data collection and processing of 3D point cloud\nsources. Despite the popularity and success of existing methods, there is a\nlack of comprehensive surveys summarizing the advancements in this field, which\nhinders its progress. In this paper, we provide a comprehensive review of 3D\ndense captioning, covering task definition, architecture classification,\ndataset analysis, evaluation metrics, and in-depth prosperity discussions.\nBased on a synthesis of previous literature, we refine a standard pipeline that\nserves as a common paradigm for existing methods. We also introduce a clear\ntaxonomy of existing models, summarize technologies involved in different\nmodules, and conduct detailed experiment analysis. Instead of a chronological\norder introduction, we categorize the methods into different classes to\nfacilitate exploration and analysis of the differences and connections among\nexisting techniques. We also provide a reading guideline to assist readers with\ndifferent backgrounds and purposes in reading efficiently. Furthermore, we\npropose a series of promising future directions for 3D dense captioning by\nidentifying challenges and aligning them with the development of related tasks,\noffering valuable insights and inspiring future research in this field. Our aim\nis to provide a comprehensive understanding of 3D dense captioning, foster\nfurther investigations, and contribute to the development of novel applications\nin multimedia and related domains.\n","authors":["Ting Yu","Xiaojun Lin","Shuhui Wang","Weiguo Sheng","Qingming Huang","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2403.07469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07463v1","updated":"2024-03-12T09:59:34Z","published":"2024-03-12T09:59:34Z","title":"Backdoor Attack with Mode Mixture Latent Modification","summary":"  Backdoor attacks become a significant security concern for deep neural\nnetworks in recent years. An image classification model can be compromised if\nmalicious backdoors are injected into it. This corruption will cause the model\nto function normally on clean images but predict a specific target label when\ntriggers are present. Previous research can be categorized into two genres:\npoisoning a portion of the dataset with triggered images for users to train the\nmodel from scratch, or training a backdoored model alongside a triggered image\ngenerator. Both approaches require significant amount of attackable parameters\nfor optimization to establish a connection between the trigger and the target\nlabel, which may raise suspicions as more people become aware of the existence\nof backdoor attacks. In this paper, we propose a backdoor attack paradigm that\nonly requires minimal alterations (specifically, the output layer) to a clean\nmodel in order to inject the backdoor under the guise of fine-tuning. To\nachieve this, we leverage mode mixture samples, which are located between\ndifferent modes in latent space, and introduce a novel method for conducting\nbackdoor attacks. We evaluate the effectiveness of our method on four popular\nbenchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet.\n","authors":["Hongwei Zhang","Xiaoyin Xu","Dongsheng An","Xianfeng Gu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10340v3","updated":"2024-03-12T09:53:46Z","published":"2022-12-20T15:25:38Z","title":"Weakly supervised training of universal visual concepts for multi-domain\n  semantic segmentation","summary":"  Deep supervised models have an unprecedented capacity to absorb large\nquantities of training data. Hence, training on multiple datasets becomes a\nmethod of choice towards strong generalization in usual scenes and graceful\nperformance degradation in edge cases. Unfortunately, different datasets often\nhave incompatible labels. For instance, the Cityscapes road class subsumes all\ndriving surfaces, while Vistas defines separate classes for road markings,\nmanholes etc. Furthermore, many datasets have overlapping labels. For instance,\npickups are labeled as trucks in VIPER, cars in Vistas, and vans in ADE20k. We\naddress this challenge by considering labels as unions of universal visual\nconcepts. This allows seamless and principled learning on multi-domain dataset\ncollections without requiring any relabeling effort. Our method achieves\ncompetitive within-dataset and cross-dataset generalization, as well as ability\nto learn visual concepts which are not separately labeled in any of the\ntraining datasets. Experiments reveal competitive or state-of-the-art\nperformance on two multi-domain dataset collections and on the WildDash 2\nbenchmark.\n","authors":["Petra Bevandić","Marin Oršić","Ivan Grubišić","Josip Šarić","Siniša Šegvić"],"pdf_url":"https://arxiv.org/pdf/2212.10340v3.pdf","comment":"27 pages, 16 figures, 10 tables, accepted to International Journal of\n  Computer Vision"},{"id":"http://arxiv.org/abs/2312.06439v2","updated":"2024-03-12T09:47:06Z","published":"2023-12-11T15:12:50Z","title":"DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior","summary":"  3D generation has raised great attention in recent years. With the success of\ntext-to-image diffusion models, the 2D-lifting technique becomes a promising\nroute to controllable 3D generation. However, these methods tend to present\ninconsistent geometry, which is also known as the Janus problem. We observe\nthat the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D\ndiffusion models and overfitting of the optimization objective. To address it,\nwe propose a two-stage 2D-lifting framework, namely DreamControl, which\noptimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained\nobjects with control-based score distillation. Specifically, adaptive viewpoint\nsampling and boundary integrity metric are proposed to ensure the consistency\nof generated priors. The priors are then regarded as input conditions to\nmaintain reasonable geometries, in which conditional LoRA and weighted score\nare further proposed to optimize detailed textures. DreamControl can generate\nhigh-quality 3D content in terms of both geometry consistency and texture\nfidelity. Moreover, our control-based optimization guidance is applicable to\nmore downstream tasks, including user-guided generation and 3D animation. The\nproject page is available at https://github.com/tyhuang0428/DreamControl.\n","authors":["Tianyu Huang","Yihan Zeng","Zhilu Zhang","Wan Xu","Hang Xu","Songcen Xu","Rynson W. H. Lau","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2312.06439v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2311.15637v2","updated":"2024-03-12T09:34:53Z","published":"2023-11-27T09:02:21Z","title":"Neural 3D Strokes: Creating Stylized 3D Scenes with Vectorized 3D\n  Strokes","summary":"  We present Neural 3D Strokes, a novel technique to generate stylized images\nof a 3D scene at arbitrary novel views from multi-view 2D images. Different\nfrom existing methods which apply stylization to trained neural radiance fields\nat the voxel level, our approach draws inspiration from image-to-painting\nmethods, simulating the progressive painting process of human artwork with\nvector strokes. We develop a palette of stylized 3D strokes from basic\nprimitives and splines, and consider the 3D scene stylization task as a\nmulti-view reconstruction process based on these 3D stroke primitives. Instead\nof directly searching for the parameters of these 3D strokes, which would be\ntoo costly, we introduce a differentiable renderer that allows optimizing\nstroke parameters using gradient descent, and propose a training scheme to\nalleviate the vanishing gradient issue. The extensive evaluation demonstrates\nthat our approach effectively synthesizes 3D scenes with significant geometric\nand aesthetic stylization while maintaining a consistent appearance across\ndifferent views. Our method can be further integrated with style loss and\nimage-text contrastive models to extend its applications, including color\ntransfer and text-driven 3D scene drawing. Results and code are available at\nhttp://buaavrcg.github.io/Neural3DStrokes.\n","authors":["Hao-Bin Duan","Miao Wang","Yan-Xun Li","Yong-Liang Yang"],"pdf_url":"https://arxiv.org/pdf/2311.15637v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2308.14286v2","updated":"2024-03-12T09:29:51Z","published":"2023-08-28T03:57:37Z","title":"Bridging Cross-task Protocol Inconsistency for Distillation in Dense\n  Object Detection","summary":"  Knowledge distillation (KD) has shown potential for learning compact models\nin dense object detection. However, the commonly used softmax-based\ndistillation ignores the absolute classification scores for individual\ncategories. Thus, the optimum of the distillation loss does not necessarily\nlead to the optimal student classification scores for dense object detectors.\nThis cross-task protocol inconsistency is critical, especially for dense object\ndetectors, since the foreground categories are extremely imbalanced. To address\nthe issue of protocol differences between distillation and classification, we\npropose a novel distillation method with cross-task consistent protocols,\ntailored for the dense object detection. For classification distillation, we\naddress the cross-task protocol inconsistency problem by formulating the\nclassification logit maps in both teacher and student models as multiple\nbinary-classification maps and applying a binary-classification distillation\nloss to each map. For localization distillation, we design an IoU-based\nLocalization Distillation Loss that is free from specific network structures\nand can be compared with existing localization distillation losses. Our\nproposed method is simple but effective, and experimental results demonstrate\nits superiority over existing methods. Code is available at\nhttps://github.com/TinyTigerPan/BCKD.\n","authors":["Longrong Yang","Xianpan Zhou","Xuewei Li","Liang Qiao","Zheyang Li","Ziwei Yang","Gaoang Wang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2308.14286v2.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2403.07437v1","updated":"2024-03-12T09:28:11Z","published":"2024-03-12T09:28:11Z","title":"Category-Agnostic Pose Estimation for Point Clouds","summary":"  The goal of object pose estimation is to visually determine the pose of a\nspecific object in the RGB-D input. Unfortunately, when faced with new\ncategories, both instance-based and category-based methods are unable to deal\nwith unseen objects of unseen categories, which is a challenge for pose\nestimation. To address this issue, this paper proposes a method to introduce\ngeometric features for pose estimation of point clouds without requiring\ncategory information. The method is based only on the patch feature of the\npoint cloud, a geometric feature with rotation invariance. After training\nwithout category information, our method achieves as good results as other\ncategory-based methods. Our method successfully achieved pose annotation of no\ncategory information instances on the CAMERA25 dataset and ModelNet40 dataset.\n","authors":["Bowen Liu","Wei Liu","Siang Chen","Pengwei Xie","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17336v3","updated":"2024-03-12T09:24:29Z","published":"2023-09-29T15:46:59Z","title":"Robust 3D Object Detection from LiDAR-Radar Point Clouds via Cross-Modal\n  Feature Augmentation","summary":"  This paper presents a novel framework for robust 3D object detection from\npoint clouds via cross-modal hallucination. Our proposed approach is agnostic\nto either hallucination direction between LiDAR and 4D radar. We introduce\nmultiple alignments on both spatial and feature levels to achieve simultaneous\nbackbone refinement and hallucination generation. Specifically, spatial\nalignment is proposed to deal with the geometry discrepancy for better instance\nmatching between LiDAR and radar. The feature alignment step further bridges\nthe intrinsic attribute gap between the sensing modalities and stabilizes the\ntraining. The trained object detection models can deal with difficult detection\ncases better, even though only single-modal data is used as the input during\nthe inference stage. Extensive experiments on the View-of-Delft (VoD) dataset\nshow that our proposed method outperforms the state-of-the-art (SOTA) methods\nfor both radar and LiDAR object detection while maintaining competitive\nefficiency in runtime. Code is available at\nhttps://github.com/DJNing/See_beyond_seeing.\n","authors":["Jianning Deng","Gabriel Chan","Hantao Zhong","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2309.17336v3.pdf","comment":"Accepted to ICRA 2024. 8 pages, 4 figures. Equal contribution for\n  Gabriel Chan and Hantao Zhong, listed randomly"},{"id":"http://arxiv.org/abs/2403.07436v1","updated":"2024-03-12T09:22:52Z","published":"2024-03-12T09:22:52Z","title":"JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object\n  Detection","summary":"  Event-based moving object detection is a challenging task, where static\nbackground and moving object are mixed together. Typically, existing methods\nmainly align the background events to the same spatial coordinate system via\nmotion compensation to distinguish the moving object. However, they neglect the\npotential spatial tailing effect of moving object events caused by excessive\nmotion, which may affect the structure integrity of the extracted moving\nobject. We discover that the moving object has a complete columnar structure in\nthe point cloud composed of motion-compensated events along the timestamp.\nMotivated by this, we propose a novel joint spatio-temporal reasoning method\nfor event-based moving object detection. Specifically, we first compensate the\nmotion of background events using inertial measurement unit. In spatial\nreasoning stage, we project the compensated events into the same image\ncoordinate, discretize the timestamp of events to obtain a time image that can\nreflect the motion confidence, and further segment the moving object through\nadaptive threshold on the time image. In temporal reasoning stage, we construct\nthe events into a point cloud along timestamp, and use RANSAC algorithm to\nextract the columnar shape in the cloud for peeling off the background.\nFinally, we fuse the results from the two reasoning stages to extract the final\nmoving object region. This joint spatio-temporal reasoning framework can\neffectively detect the moving object from motion confidence and geometric\nstructure. Moreover, we conduct extensive experiments on various datasets to\nverify that the proposed method can improve the moving object detection\naccuracy by 13\\%.\n","authors":["Hanyu Zhou","Zhiwei Shi","Hao Dong","Shihan Peng","Yi Chang","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2403.07436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07434v1","updated":"2024-03-12T09:17:21Z","published":"2024-03-12T09:17:21Z","title":"DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated\n  MR Images","summary":"  We propose a new method that employs transfer learning techniques to\neffectively correct sampling selection errors introduced by sparse annotations\nduring supervised learning for automated tumor segmentation. The practicality\nof current learning-based automated tissue classification approaches is\nseverely impeded by their dependency on manually segmented training databases\nthat need to be recreated for each scenario of application, site, or\nacquisition setup. The comprehensive annotation of reference datasets can be\nhighly labor-intensive, complex, and error-prone. The proposed method derives\nhigh-quality classifiers for the different tissue classes from sparse and\nunambiguous annotations and employs domain adaptation techniques for\neffectively correcting sampling selection errors introduced by the sparse\nsampling. The new approach is validated on labeled, multi-modal MR images of 19\npatients with malignant gliomas and by comparative analysis on the BraTS 2013\nchallenge data sets. Compared to training on fully labeled data, we reduced the\ntime for labeling and training by a factor greater than 70 and 180 respectively\nwithout sacrificing accuracy. This dramatically eases the establishment and\nconstant extension of large annotated databases in various scenarios and\nimaging setups and thus represents an important step towards practical\napplicability of learning-based approaches in tissue classification.\n","authors":["Michael Götz","Christian Weber","Franciszek Binczyk","Joanna Polanska","Rafal Tarnawski","Barbara Bobek-Billewicz","Ullrich Köthe","Jens Kleesiek","Bram Stieltjes","Klaus H. Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.07434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07432v1","updated":"2024-03-12T09:15:19Z","published":"2024-03-12T09:15:19Z","title":"Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for\n  Scene Flow","summary":"  Single RGB or LiDAR is the mainstream sensor for the challenging scene flow,\nwhich relies heavily on visual features to match motion features. Compared with\nsingle modality, existing methods adopt a fusion strategy to directly fuse the\ncross-modal complementary knowledge in motion space. However, these direct\nfusion methods may suffer the modality gap due to the visual intrinsic\nheterogeneous nature between RGB and LiDAR, thus deteriorating motion features.\nWe discover that event has the homogeneous nature with RGB and LiDAR in both\nvisual and motion spaces. In this work, we bring the event as a bridge between\nRGB and LiDAR, and propose a novel hierarchical visual-motion fusion framework\nfor scene flow, which explores a homogeneous space to fuse the cross-modal\ncomplementary knowledge for physical interpretation. In visual fusion, we\ndiscover that event has a complementarity (relative v.s. absolute) in luminance\nspace with RGB for high dynamic imaging, and has a complementarity (local\nboundary v.s. global shape) in scene structure space with LiDAR for structure\nintegrity. In motion fusion, we figure out that RGB, event and LiDAR are\ncomplementary (spatial-dense, temporal-dense v.s. spatiotemporal-sparse) to\neach other in correlation space, which motivates us to fuse their motion\ncorrelations for motion continuity. The proposed hierarchical fusion can\nexplicitly fuse the multimodal knowledge to progressively improve scene flow\nfrom visual space to motion space. Extensive experiments have been performed to\nverify the superiority of the proposed method.\n","authors":["Hanyu Zhou","Yi Chang","Zhiwei Shi","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2403.07432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07428v1","updated":"2024-03-12T09:11:02Z","published":"2024-03-12T09:11:02Z","title":"Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion\n  Segmentation","summary":"  In machine learning larger databases are usually associated with higher\nclassification accuracy due to better generalization. This generalization may\nlead to non-optimal classifiers in some medical applications with highly\nvariable expressions of pathologies. This paper presents a method for learning\nfrom a large training base by adaptively selecting optimal training samples for\ngiven input data. In this way heterogeneous databases are supported two-fold.\nFirst, by being able to deal with sparsely annotated data allows a quick\ninclusion of new data set and second, by training an input-dependent\nclassifier. The proposed approach is evaluated using the SISS challenge. The\nproposed algorithm leads to a significant improvement of the classification\naccuracy.\n","authors":["Michael Götz","Christian Weber","Christoph Kolb","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.07428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03267v2","updated":"2024-03-12T09:01:54Z","published":"2022-11-07T02:24:39Z","title":"Prompter: Utilizing Large Language Model Prompting for a Data Efficient\n  Embodied Instruction Following","summary":"  Embodied Instruction Following (EIF) studies how autonomous mobile\nmanipulation robots should be controlled to accomplish long-horizon tasks\ndescribed by natural language instructions. While much research on EIF is\nconducted in simulators, the ultimate goal of the field is to deploy the agents\nin real life. This is one of the reasons why recent methods have moved away\nfrom training models end-to-end and take modular approaches, which do not need\nthe costly expert operation data. However, as it is still in the early days of\nimporting modular ideas to EIF, a search for modules effective in the EIF task\nis still far from a conclusion. In this paper, we propose to extend the modular\ndesign using knowledge obtained from two external sources. First, we show that\nembedding the physical constraints of the deployed robots into the module\ndesign is highly effective. Our design also allows the same modular system to\nwork across robots of different configurations with minimal modifications.\nSecond, we show that the landmark-based object search, previously implemented\nby a trained model requiring a dedicated set of data, can be replaced by an\nimplementation that prompts pretrained large language models for\nlandmark-object relationships, eliminating the need for collecting dedicated\ntraining data. Our proposed Prompter achieves 41.53\\% and 45.32\\% on the ALFRED\nbenchmark with high-level instructions only and step-by-step instructions,\nrespectively, significantly outperforming the previous state of the art by\n5.46\\% and 9.91\\%.\n","authors":["Yuki Inoue","Hiroki Ohashi"],"pdf_url":"https://arxiv.org/pdf/2211.03267v2.pdf","comment":"8 pages, 3 figures, rejected by IROS2023"},{"id":"http://arxiv.org/abs/2403.07420v1","updated":"2024-03-12T08:57:29Z","published":"2024-03-12T08:57:29Z","title":"DragAnything: Motion Control for Anything using Entity Representation","summary":"  We introduce DragAnything, which utilizes a entity representation to achieve\nmotion control for any object in controllable video generation. Comparison to\nexisting motion control methods, DragAnything offers several advantages.\nFirstly, trajectory-based is more userfriendly for interaction, when acquiring\nother guidance signals (e.g., masks, depth maps) is labor-intensive. Users only\nneed to draw a line (trajectory) during interaction. Secondly, our entity\nrepresentation serves as an open-domain embedding capable of representing any\nobject, enabling the control of motion for diverse entities, including\nbackground. Lastly, our entity representation allows simultaneous and distinct\nmotion control for multiple objects. Extensive experiments demonstrate that our\nDragAnything achieves state-of-the-art performance for FVD, FID, and User\nStudy, particularly in terms of object motion control, where our method\nsurpasses the previous methods (e.g., DragNUWA) by 26% in human voting.\n","authors":["Wejia Wu","Zhuang Li","Yuchao Gu","Rui Zhao","Yefei He","David Junhao Zhang","Mike Zheng Shou","Yan Li","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07420v1.pdf","comment":"The project website is at:\n  https://weijiawu.github.io/draganything_page/ . The code is at:\n  https://github.com/showlab/DragAnything"},{"id":"http://arxiv.org/abs/2403.07408v1","updated":"2024-03-12T08:35:42Z","published":"2024-03-12T08:35:42Z","title":"NightHaze: Nighttime Image Dehazing via Self-Prior Learning","summary":"  Masked autoencoder (MAE) shows that severe augmentation during training\nproduces robust representations for high-level tasks. This paper brings the\nMAE-like framework to nighttime image enhancement, demonstrating that severe\naugmentation during training produces strong network priors that are resilient\nto real-world night haze degradations. We propose a novel nighttime image\ndehazing method with self-prior learning. Our main novelty lies in the design\nof severe augmentation, which allows our model to learn robust priors. Unlike\nMAE that uses masking, we leverage two key challenging factors of nighttime\nimages as augmentation: light effects and noise. During training, we\nintentionally degrade clear images by blending them with light effects as well\nas by adding noise, and subsequently restore the clear images. This enables our\nmodel to learn clear background priors. By increasing the noise values to\napproach as high as the pixel intensity values of the glow and light effect\nblended images, our augmentation becomes severe, resulting in stronger priors.\nWhile our self-prior learning is considerably effective in suppressing glow and\nrevealing details of background scenes, in some cases, there are still some\nundesired artifacts that remain, particularly in the forms of over-suppression.\nTo address these artifacts, we propose a self-refinement module based on the\nsemi-supervised teacher-student framework. Our NightHaze, especially our\nMAE-like self-prior learning, shows that models trained with severe\naugmentation effectively improve the visibility of input haze images,\napproaching the clarity of clear nighttime images. Extensive experiments\ndemonstrate that our NightHaze achieves state-of-the-art performance,\noutperforming existing nighttime image dehazing methods by a substantial margin\nof 15.5% for MUSIQ and 23.5% for ClipIQA.\n","authors":["Beibei Lin","Yeying Jin","Wending Yan","Wei Ye","Yuan Yuan","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2403.07408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07407v1","updated":"2024-03-12T08:34:34Z","published":"2024-03-12T08:34:34Z","title":"In-context learning enables multimodal large language models to classify\n  cancer pathology images","summary":"  Medical image classification requires labeled, task-specific datasets which\nare used to train deep learning networks de novo, or to fine-tune foundation\nmodels. However, this process is computationally and technically demanding. In\nlanguage processing, in-context learning provides an alternative, where models\nlearn from within prompts, bypassing the need for parameter updates. Yet,\nin-context learning remains underexplored in medical image analysis. Here, we\nsystematically evaluate the model Generative Pretrained Transformer 4 with\nVision capabilities (GPT-4V) on cancer image processing with in-context\nlearning on three cancer histopathology tasks of high importance:\nClassification of tissue subtypes in colorectal cancer, colon polyp subtyping\nand breast tumor detection in lymph node sections. Our results show that\nin-context learning is sufficient to match or even outperform specialized\nneural networks trained for particular tasks, while only requiring a minimal\nnumber of samples. In summary, this study demonstrates that large vision\nlanguage models trained on non-domain specific data can be applied out-of-the\nbox to solve medical image-processing tasks in histopathology. This\ndemocratizes access of generalist AI models to medical experts without\ntechnical background especially for areas where annotated data is scarce.\n","authors":["Dyke Ferber","Georg Wölflein","Isabella C. Wiest","Marta Ligero","Srividhya Sainath","Narmin Ghaffari Laleh","Omar S. M. El Nahhas","Gustav Müller-Franzes","Dirk Jäger","Daniel Truhn","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2403.07407v1.pdf","comment":"40 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.07406v1","updated":"2024-03-12T08:34:05Z","published":"2024-03-12T08:34:05Z","title":"FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental\n  Learning with Hill-Climbing","summary":"  Exemplar-free class-incremental learning (EFCIL) poses significant\nchallenges, primarily due to catastrophic forgetting, necessitating a delicate\nbalance between stability and plasticity to accurately recognize both new and\nprevious classes. Traditional EFCIL approaches typically skew towards either\nmodel plasticity through successive fine-tuning or stability by employing a\nfixed feature extractor beyond the initial incremental state. Building upon the\nfoundational FeTrIL framework, our research extends into novel experimental\ndomains to examine the efficacy of various oversampling techniques and dynamic\noptimization strategies across multiple challenging datasets and incremental\nsettings. We specifically explore how oversampling impacts accuracy relative to\nfeature availability and how different optimization methodologies, including\ndynamic recalibration and feature pool diversification, influence incremental\nlearning outcomes. The results from these comprehensive experiments, conducted\non CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior\nperformance of FeTrIL in balancing accuracy for both new and past classes\nagainst ten contemporary methods. Notably, our extensions reveal the nuanced\nimpacts of oversampling and optimization on EFCIL, contributing to a more\nrefined understanding of feature-space manipulation for class incremental\nlearning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way\nfor more adaptable and efficient EFCIL methodologies, promising significant\nimprovements in handling catastrophic forgetting without the need for\nexemplars.\n","authors":["Eduard Hogea","Adrian Popescu","Darian Onchis","Grégoire Petit"],"pdf_url":"https://arxiv.org/pdf/2403.07406v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2211.13131"},{"id":"http://arxiv.org/abs/2403.05369v2","updated":"2024-03-12T08:33:51Z","published":"2024-03-08T15:00:44Z","title":"Frequency-Adaptive Dilated Convolution for Semantic Segmentation","summary":"  Dilated convolution, which expands the receptive field by inserting gaps\nbetween its consecutive elements, is widely employed in computer vision. In\nthis study, we propose three strategies to improve individual phases of dilated\nconvolution from the view of spectrum analysis. Departing from the conventional\npractice of fixing a global dilation rate as a hyperparameter, we introduce\nFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts\ndilation rates spatially based on local frequency components. Subsequently, we\ndesign two plug-in modules to directly enhance effective bandwidth and\nreceptive field size. The Adaptive Kernel (AdaKern) module decomposes\nconvolution weights into low-frequency and high-frequency components,\ndynamically adjusting the ratio between these components on a per-channel\nbasis. By increasing the high-frequency part of convolution weights, AdaKern\ncaptures more high-frequency components, thereby improving effective bandwidth.\nThe Frequency Selection (FreqSelect) module optimally balances high- and\nlow-frequency components in feature representations through spatially variant\nreweighting. It suppresses high frequencies in the background to encourage FADC\nto learn a larger dilation, thereby increasing the receptive field for an\nexpanded scope. Extensive experiments on segmentation and object detection\nconsistently validate the efficacy of our approach. The code is publicly\navailable at \\url{https://github.com/Linwei-Chen/FADC}.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2403.05369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07403v1","updated":"2024-03-12T08:32:23Z","published":"2024-03-12T08:32:23Z","title":"From Canteen Food to Daily Meals: Generalizing Food Recognition to More\n  Practical Scenarios","summary":"  The precise recognition of food categories plays a pivotal role for\nintelligent health management, attracting significant research attention in\nrecent years. Prominent benchmarks, such as Food-101 and VIREO Food-172,\nprovide abundant food image resources that catalyze the prosperity of research\nin this field. Nevertheless, these datasets are well-curated from canteen\nscenarios and thus deviate from food appearances in daily life. This\ndiscrepancy poses great challenges in effectively transferring classifiers\ntrained on these canteen datasets to broader daily-life scenarios encountered\nby humans. Toward this end, we present two new benchmarks, namely DailyFood-172\nand DailyFood-16, specifically designed to curate food images from everyday\nmeals. These two datasets are used to evaluate the transferability of\napproaches from the well-curated food image domain to the everyday-life food\nimage domain. In addition, we also propose a simple yet effective baseline\nmethod named Multi-Cluster Reference Learning (MCRL) to tackle the\naforementioned domain gap. MCRL is motivated by the observation that food\nimages in daily-life scenarios exhibit greater intra-class appearance variance\ncompared with those in well-curated benchmarks. Notably, MCRL can be seamlessly\ncoupled with existing approaches, yielding non-trivial performance\nenhancements. We hope our new benchmarks can inspire the community to explore\nthe transferability of food recognition models trained on well-curated datasets\ntoward practical real-life applications.\n","authors":["Guoshan Liu","Yang Jiao","Jingjing Chen","Bin Zhu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.07403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06866v2","updated":"2024-03-12T08:28:39Z","published":"2024-03-11T16:21:50Z","title":"QUASAR: QUality and Aesthetics Scoring with Advanced Representations","summary":"  This paper introduces a new data-driven, non-parametric method for image\nquality and aesthetics assessment, surpassing existing approaches and requiring\nno prompt engineering or fine-tuning. We eliminate the need for expressive\ntextual embeddings by proposing efficient image anchors in the data. Through\nextensive evaluations of 7 state-of-the-art self-supervised models, our method\ndemonstrates superior performance and robustness across various datasets and\nbenchmarks. Notably, it achieves high agreement with human assessments even\nwith limited data and shows high robustness to the nature of data and their\npre-processing pipeline. Our contributions offer a streamlined solution for\nassessment of images while providing insights into the perception of visual\ninformation.\n","authors":["Sergey Kastryulin","Denis Prokopenko","Artem Babenko","Dmitry V. Dylov"],"pdf_url":"https://arxiv.org/pdf/2403.06866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.09957v3","updated":"2024-03-12T08:13:27Z","published":"2022-04-21T08:32:47Z","title":"Self-paced Multi-grained Cross-modal Interaction Modeling for Referring\n  Expression Comprehension","summary":"  As an important and challenging problem in vision-language tasks, referring\nexpression comprehension (REC) generally requires a large amount of\nmulti-grained information of visual and linguistic modalities to realize\naccurate reasoning. In addition, due to the diversity of visual scenes and the\nvariation of linguistic expressions, some hard examples have much more abundant\nmulti-grained information than others. How to aggregate multi-grained\ninformation from different modalities and extract abundant knowledge from hard\nexamples is crucial in the REC task. To address aforementioned challenges, in\nthis paper, we propose a Self-paced Multi-grained Cross-modal Interaction\nModeling framework, which improves the language-to-vision localization ability\nthrough innovations in network structure and learning mechanism. Concretely, we\ndesign a transformer-based multi-grained cross-modal attention, which\neffectively utilizes the inherent multi-grained information in visual and\nlinguistic encoders. Furthermore, considering the large variance of samples, we\npropose a self-paced sample informativeness learning to adaptively enhance the\nnetwork learning for samples containing abundant multi-grained information. The\nproposed framework significantly outperforms state-of-the-art methods on widely\nused datasets, such as RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame datasets,\ndemonstrating the effectiveness of our method.\n","authors":["Peihan Miao","Wei Su","Gaoang Wang","Xuewei Li","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2204.09957v3.pdf","comment":"Accepted by TIP"},{"id":"http://arxiv.org/abs/2312.03781v2","updated":"2024-03-12T08:13:01Z","published":"2023-12-06T09:39:38Z","title":"Lite-Mind: Towards Efficient and Robust Brain Representation Network","summary":"  Research in decoding visual information from the brain, particularly through\nthe non-invasive fMRI method, is rapidly progressing. The challenge arises from\nthe limited data availability and the low signal-to-noise ratio of fMRI\nsignals, leading to a low-precision task of fMRI-to-image retrieval.\nState-of-the-art MindEye remarkably improves fMRI-to-image retrieval\nperformance by leveraging a deep MLP with a high parameter count orders of\nmagnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to\nthe final hidden layer of CLIP's vision transformer. However, significant\nindividual variations exist among subjects, even within identical experimental\nsetups, mandating the training of subject-specific models. The substantial\nparameters pose significant challenges in deploying fMRI decoding on practical\ndevices, especially with the necessitating of specific models for each subject.\nTo this end, we propose Lite-Mind, a lightweight, efficient, and versatile\nbrain representation network based on discrete Fourier transform, that\nefficiently aligns fMRI voxels to fine-grained information of CLIP. Our\nexperiments demonstrate that Lite-Mind achieves an impressive 94.3%\nfMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7%\nfewer parameters than MindEye. Lite-Mind is also proven to be able to be\nmigrated to smaller brain datasets and establishes a new state-of-the-art for\nzero-shot classification on the GOD dataset. The code is available at\nhttps://github.com/gongzix/Lite-Mind.\n","authors":["Zixuan Gong","Qi Zhang","Duoqian Miao","Guangyin Bao","Liang Hu"],"pdf_url":"https://arxiv.org/pdf/2312.03781v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.07392v1","updated":"2024-03-12T07:59:41Z","published":"2024-03-12T07:59:41Z","title":"ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature\n  Interaction for Dense Predictions","summary":"  Although Vision Transformer (ViT) has achieved significant success in\ncomputer vision, it does not perform well in dense prediction tasks due to the\nlack of inner-patch information interaction and the limited diversity of\nfeature scale. Most existing studies are devoted to designing vision-specific\ntransformers to solve the above problems, which introduce additional\npre-training costs. Therefore, we present a plain, pre-training-free, and\nfeature-enhanced ViT backbone with Convolutional Multi-scale feature\ninteraction, named ViT-CoMer, which facilitates bidirectional interaction\nbetween CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has\nthe following advantages: (1) We inject spatial pyramid multi-receptive field\nconvolutional features into the ViT architecture, which effectively alleviates\nthe problems of limited local information interaction and single-feature\nrepresentation in ViT. (2) We propose a simple and efficient CNN-Transformer\nbidirectional fusion interaction module that performs multi-scale fusion across\nhierarchical features, which is beneficial for handling dense prediction tasks.\n(3) We evaluate the performance of ViT-CoMer across various dense prediction\ntasks, different frameworks, and multiple advanced pre-training. Notably, our\nViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and\n62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art\nmethods. We hope ViT-CoMer can serve as a new backbone for dense prediction\ntasks to facilitate future research. The code will be released at\nhttps://github.com/Traffic-X/ViT-CoMer.\n","authors":["Chunlong Xia","Xinliang Wang","Feng Lv","Xin Hao","Yifeng Shi"],"pdf_url":"https://arxiv.org/pdf/2403.07392v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.07390v1","updated":"2024-03-12T07:58:14Z","published":"2024-03-12T07:58:14Z","title":"Learning Correction Errors via Frequency-Self Attention for Blind Image\n  Super-Resolution","summary":"  Previous approaches for blind image super-resolution (SR) have relied on\ndegradation estimation to restore high-resolution (HR) images from their\nlow-resolution (LR) counterparts. However, accurate degradation estimation\nposes significant challenges. The SR model's incompatibility with degradation\nestimation methods, particularly the Correction Filter, may significantly\nimpair performance as a result of correction errors. In this paper, we\nintroduce a novel blind SR approach that focuses on Learning Correction Errors\n(LCE). Our method employs a lightweight Corrector to obtain a corrected\nlow-resolution (CLR) image. Subsequently, within an SR network, we jointly\noptimize SR performance by utilizing both the original LR image and the\nfrequency learning of the CLR image. Additionally, we propose a new\nFrequency-Self Attention block (FSAB) that enhances the global information\nutilization ability of Transformer. This block integrates both self-attention\nand frequency spatial attention mechanisms. Extensive ablation and comparison\nexperiments conducted across various settings demonstrate the superiority of\nour method in terms of visual quality and accuracy. Our approach effectively\naddresses the challenges associated with degradation estimation and correction\nerrors, paving the way for more accurate blind image SR.\n","authors":["Haochen Sun","Yan Yuan","Lijuan Su","Haotian Shao"],"pdf_url":"https://arxiv.org/pdf/2403.07390v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2403.07389v1","updated":"2024-03-12T07:57:33Z","published":"2024-03-12T07:57:33Z","title":"Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from\n  Duplex to Monoplex IHC Images","summary":"  Generative models enable the translation from a source image domain where\nreadily trained models are available to a target domain unseen during training.\nWhile Cycle Generative Adversarial Networks (GANs) are well established, the\nassociated cycle consistency constrain relies on that an invertible mapping\nexists between the two domains. This is, however, not the case for the\ntranslation between images stained with chromogenic monoplex and duplex\nimmunohistochemistry (IHC) assays. Focusing on the translation from the latter\nto the first, we propose - through the introduction of a novel training design,\nan alternative constrain leveraging a set of immunofluorescence (IF) images as\nan auxiliary unpaired image domain. Quantitative and qualitative results on a\ndownstream segmentation task show the benefit of the proposed method in\ncomparison to baseline approaches.\n","authors":["Nicolas Brieu","Nicolas Triltsch","Philipp Wortmann","Dominik Winter","Shashank Saran","Marlon Rebelatto","Günter Schmidt"],"pdf_url":"https://arxiv.org/pdf/2403.07389v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.07380v1","updated":"2024-03-12T07:41:51Z","published":"2024-03-12T07:41:51Z","title":"Gabor-guided transformer for single image deraining","summary":"  Image deraining have have gained a great deal of attention in order to\naddress the challenges posed by the effects of harsh weather conditions on\nvisual tasks. While convolutional neural networks (CNNs) are popular, their\nlimitations in capturing global information may result in ineffective rain\nremoval. Transformer-based methods with self-attention mechanisms have\nimproved, but they tend to distort high-frequency details that are crucial for\nimage fidelity. To solve this problem, we propose the Gabor-guided tranformer\n(Gabformer) for single image deraining. The focus on local texture features is\nenhanced by incorporating the information processed by the Gabor filter into\nthe query vector, which also improves the robustness of the model to noise due\nto the properties of the filter. Extensive experiments on the benchmarks\ndemonstrate that our method outperforms state-of-the-art approaches.\n","authors":["Sijin He","Guangfeng Lin"],"pdf_url":"https://arxiv.org/pdf/2403.07380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03634v2","updated":"2024-03-12T07:38:34Z","published":"2024-02-06T02:17:44Z","title":"Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view 3D\n  Object Detection","summary":"  Multi-view 3D object detection systems often struggle with generating precise\npredictions due to the challenges in estimating depth from images, increasing\nredundant and incorrect detections. Our paper presents Ray Denoising, an\ninnovative method that enhances detection accuracy by strategically sampling\nalong camera rays to construct hard negative examples. These examples, visually\nchallenging to differentiate from true positives, compel the model to learn\ndepth-aware features, thereby improving its capacity to distinguish between\ntrue and false positives. Ray Denoising is designed as a plug-and-play module,\ncompatible with any DETR-style multi-view 3D detectors, and it only minimally\nincreases training computational costs without affecting inference speed. Our\ncomprehensive experiments, including detailed ablation studies, consistently\ndemonstrate that Ray Denoising outperforms strong baselines across multiple\ndatasets. It achieves a 1.9\\% improvement in mean Average Precision (mAP) over\nthe state-of-the-art StreamPETR method on the NuScenes dataset. It shows\nsignificant performance gains on the Argoverse 2 dataset, highlighting its\ngeneralization capability. The code will be available at\nhttps://github.com/LiewFeng/RayDN.\n","authors":["Feng Liu","Tengteng Huang","Qianjing Zhang","Haotian Yao","Chi Zhang","Fang Wan","Qixiang Ye","Yanzhao Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.03634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07376v1","updated":"2024-03-12T07:27:02Z","published":"2024-03-12T07:27:02Z","title":"NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning\n  Disentangled Reasoning","summary":"  Vision-and-Language Navigation (VLN), as a crucial research problem of\nEmbodied AI, requires an embodied agent to navigate through complex 3D\nenvironments following natural language instructions. Recent research has\nhighlighted the promising capacity of large language models (LLMs) in VLN by\nimproving navigational reasoning accuracy and interpretability. However, their\npredominant use in an offline manner usually suffers from substantial domain\ngap between the VLN task and the LLM training corpus. This paper introduces a\nnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\nparameter-efficient in-domain training to enable self-guided navigational\ndecision, leading to a significant mitigation of the domain gap in a\ncost-effective manner. Specifically, at each timestep, the LLM is prompted to\nforecast the navigational chain-of-thought by: 1) acting as a world model to\nimagine the next observation according to the instruction, 2) selecting the\ncandidate observation that best aligns with the imagination, and 3) determining\nthe action based on the reasoning from the prior steps. Through constructing\nformalized labels for training, the LLM can learn to generate desired and\nreasonable chain-of-thought outputs for improving the action decision.\nExperimental results across various training settings and popular VLN\nbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\n(R4R)) show the significant superiority of NavCoT over the direct action\nprediction variants. Through simple parameter-efficient finetuning, our NavCoT\noutperforms a recent GPT4-based approach with ~7% relative improvement on the\nR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\nscalable LLM-based embodied agents, which are helpful for developing real-world\nrobotics applications. Code is available at\nhttps://github.com/expectorlin/NavCoT.\n","authors":["Bingqian Lin","Yunshuang Nie","Ziming Wei","Jiaqi Chen","Shikui Ma","Jianhua Han","Hang Xu","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2403.07376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16140v2","updated":"2024-03-12T07:23:51Z","published":"2023-07-30T06:24:03Z","title":"Fully $1\\times1$ Convolutional Network for Lightweight Image\n  Super-Resolution","summary":"  Deep models have achieved significant process on single image\nsuper-resolution (SISR) tasks, in particular large models with large kernel\n($3\\times3$ or more). However, the heavy computational footprint of such models\nprevents their deployment in real-time, resource-constrained environments.\nConversely, $1\\times1$ convolutions bring substantial computational efficiency,\nbut struggle with aggregating local spatial representations, an essential\ncapability to SISR models. In response to this dichotomy, we propose to\nharmonize the merits of both $3\\times3$ and $1\\times1$ kernels, and exploit a\ngreat potential for lightweight SISR tasks. Specifically, we propose a simple\nyet effective fully $1\\times1$ convolutional network, named Shift-Conv-based\nNetwork (SCNet). By incorporating a parameter-free spatial-shift operation, it\nequips the fully $1\\times1$ convolutional network with powerful representation\ncapability while impressive computational efficiency. Extensive experiments\ndemonstrate that SCNets, despite its fully $1\\times1$ convolutional structure,\nconsistently matches or even surpasses the performance of existing lightweight\nSR models that employ regular convolutions. The code and pre-trained models can\nbe found at https://github.com/Aitical/SCNet.\n","authors":["Gang Wu","Junjun Jiang","Kui Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2307.16140v2.pdf","comment":"Accepted by Machine Intelligence Research, DOI:\n  10.1007/s11633-024-1401-z"},{"id":"http://arxiv.org/abs/2403.07372v1","updated":"2024-03-12T07:16:20Z","published":"2024-03-12T07:16:20Z","title":"Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D\n  Object Detection","summary":"  Recent 3D object detectors typically utilize multi-sensor data and unify\nmulti-modal features in the shared bird's-eye view (BEV) representation space.\nHowever, our empirical findings indicate that previous methods have limitations\nin generating fusion BEV features free from cross-modal conflicts. These\nconflicts encompass extrinsic conflicts caused by BEV feature construction and\ninherent conflicts stemming from heterogeneous sensor signals. Therefore, we\npropose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly\neliminate the extrinsic/inherent conflicts in BEV space and produce improved\nmulti-modal BEV features. Specifically, we devise a Semantic-guided Flow-based\nAlignment (SFA) module to resolve extrinsic conflicts via unifying spatial\ndistribution in BEV space before fusion. Moreover, we design a Dissolved Query\nRecovering (DQR) mechanism to remedy inherent conflicts by preserving\nobjectness clues that are lost in the fusion BEV feature. In general, our\nmethod maximizes the effective information utilization of each modality and\nleverages inter-modal complementarity. Our method achieves state-of-the-art\nperformance in the highly competitive nuScenes 3D object detection dataset. The\ncode is released at https://github.com/fjhzhixi/ECFusion.\n","authors":["Jiahui Fu","Chen Gao","Zitian Wang","Lirong Yang","Xiaofei Wang","Beipeng Mu","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07372v1.pdf","comment":"Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.07371v1","updated":"2024-03-12T07:15:29Z","published":"2024-03-12T07:15:29Z","title":"Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of\n  Altered Diffusion Models","summary":"  This study discusses the critical issues of Virtual Try-On in contemporary\ne-commerce and the prospective metaverse, emphasizing the challenges of\npreserving intricate texture details and distinctive features of the target\nperson and the clothes in various scenarios, such as clothing texture and\nidentity characteristics like tattoos or accessories. In addition to the\nfidelity of the synthesized images, the efficiency of the synthesis process\npresents a significant hurdle. Various existing approaches are explored,\nhighlighting the limitations and unresolved aspects, e.g., identity information\nomission, uncontrollable artifacts, and low synthesis speed. It then proposes a\nnovel diffusion-based solution that addresses garment texture preservation and\nuser identity retention during virtual try-on. The proposed network comprises\ntwo primary modules - a warping module aligning clothing with individual\nfeatures and a try-on module refining the attire and generating missing parts\nintegrated with a mask-aware post-processing technique ensuring the integrity\nof the individual's identity. It demonstrates impressive results, surpassing\nthe state-of-the-art in speed by nearly 20 times during inference, with\nsuperior fidelity in qualitative assessments. Quantitative evaluations confirm\ncomparable performance with the recent SOTA method on the VITON-HD and\nDresscode datasets.\n","authors":["Phuong Dam","Jihoon Jeong","Anh Tran","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.07371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07369v1","updated":"2024-03-12T07:06:50Z","published":"2024-03-12T07:06:50Z","title":"Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized\n  Visual Class Discovery","summary":"  In this paper, we study the problem of Generalized Category Discovery (GCD),\nwhich aims to cluster unlabeled data from both known and unknown categories\nusing the knowledge of labeled data from known categories. Current GCD methods\nrely on only visual cues, which however neglect the multi-modality perceptive\nnature of human cognitive processes in discovering novel visual categories. To\naddress this, we propose a two-phase TextGCD framework to accomplish\nmulti-modality GCD by exploiting powerful Visual-Language Models. TextGCD\nmainly includes a retrieval-based text generation (RTG) phase and a\ncross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon\nusing category tags from diverse datasets and attributes from Large Language\nModels, generating descriptive texts for images in a retrieval manner. Second,\nCCT leverages disparities between textual and visual modalities to foster\nmutual learning, thereby enhancing visual GCD. In addition, we design an\nadaptive class aligning strategy to ensure the alignment of category\nperceptions between modalities as well as a soft-voting mechanism to integrate\nmulti-modality cues. Experiments on eight datasets show the large superiority\nof our approach over state-of-the-art methods. Notably, our approach\noutperforms the best competitor, by 7.7% and 10.8% in All accuracy on\nImageNet-1k and CUB, respectively.\n","authors":["Haiyang Zheng","Nan Pu","Wenjing Li","Nicu Sebe","Zhun Zhong"],"pdf_url":"https://arxiv.org/pdf/2403.07369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07366v1","updated":"2024-03-12T07:01:57Z","published":"2024-03-12T07:01:57Z","title":"Entropy is not Enough for Test-Time Adaptation: From the Perspective of\n  Disentangled Factors","summary":"  Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for\nunseen test data. The primary challenge of TTA is limited access to the entire\ntest dataset during online updates, causing error accumulation. To mitigate it,\nTTA methods have utilized the model output's entropy as a confidence metric\nthat aims to determine which samples have a lower likelihood of causing error.\nThrough experimental studies, however, we observed the unreliability of entropy\nas a confidence metric for TTA under biased scenarios and theoretically\nrevealed that it stems from the neglect of the influence of latent disentangled\nfactors of data on predictions. Building upon these findings, we introduce a\nnovel TTA method named Destroy Your Object (DeYO), which leverages a newly\nproposed confidence metric named Pseudo-Label Probability Difference (PLPD).\nPLPD quantifies the influence of the shape of an object on prediction by\nmeasuring the difference between predictions before and after applying an\nobject-destructive transformation. DeYO consists of sample selection and sample\nweighting, which employ entropy and PLPD concurrently. For robust adaptation,\nDeYO prioritizes samples that dominantly incorporate shape information when\nmaking predictions. Our extensive experiments demonstrate the consistent\nsuperiority of DeYO over baseline methods across various scenarios, including\nbiased and wild. Project page is publicly available at\nhttps://whitesnowdrop.github.io/DeYO/.\n","authors":["Jonghyun Lee","Dahuin Jung","Saehyung Lee","Junsung Park","Juhyeon Shin","Uiwon Hwang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.07366v1.pdf","comment":"ICLR 2024 Spotlight; 26 pages, 9 figures, 20 tables;"},{"id":"http://arxiv.org/abs/2301.01060v2","updated":"2024-03-12T06:55:20Z","published":"2023-01-03T12:02:19Z","title":"Knowledge-guided Causal Intervention for Weakly-supervised Object\n  Localization","summary":"  Previous weakly-supervised object localization (WSOL) methods aim to expand\nactivation map discriminative areas to cover the whole objects, yet neglect two\ninherent challenges when relying solely on image-level labels. First, the\n``entangled context'' issue arises from object-context co-occurrence (\\eg, fish\nand water), making the model inspection hard to distinguish object boundaries\nclearly. Second, the ``C-L dilemma'' issue results from the information decay\ncaused by the pooling layers, which struggle to retain both the semantic\ninformation for precise classification and those essential details for accurate\nlocalization, leading to a trade-off in performance. In this paper, we propose\na knowledge-guided causal intervention method, dubbed KG-CI-CAM, to address\nthese two under-explored issues in one go. More specifically, we tackle the\nco-occurrence context confounder problem via causal intervention, which\nexplores the causalities among image features, contexts, and categories to\neliminate the biased object-context entanglement in the class activation maps.\nBased on the disentangled object feature, we introduce a multi-source knowledge\nguidance framework to strike a balance between absorbing classification\nknowledge and localization knowledge during model training. Extensive\nexperiments conducted on several benchmark datasets demonstrate the\neffectiveness of KG-CI-CAM in learning distinct object boundaries amidst\nconfounding contexts and mitigating the dilemma between classification and\nlocalization performance.\n","authors":["Feifei Shao","Yawei Luo","Fei Gao","Yi Yang","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2301.01060v2.pdf","comment":"13 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.07362v1","updated":"2024-03-12T06:50:32Z","published":"2024-03-12T06:50:32Z","title":"Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine\n  Unlearning","summary":"  The trustworthy machine learning (ML) community is increasingly recognizing\nthe crucial need for models capable of selectively 'unlearning' data points\nafter training. This leads to the problem of machine unlearning (MU), aiming to\neliminate the influence of chosen data points on model performance, while still\nmaintaining the model's utility post-unlearning. Despite various MU methods for\ndata influence erasure, evaluations have largely focused on random data\nforgetting, ignoring the vital inquiry into which subset should be chosen to\ntruly gauge the authenticity of unlearning performance. To tackle this issue,\nwe introduce a new evaluative angle for MU from an adversarial viewpoint. We\npropose identifying the data subset that presents the most significant\nchallenge for influence erasure, i.e., pinpointing the worst-case forget set.\nUtilizing a bi-level optimization principle, we amplify unlearning challenges\nat the upper optimization level to emulate worst-case scenarios, while\nsimultaneously engaging in standard training and unlearning at the lower level,\nachieving a balance between data influence erasure and model utility. Our\nproposal offers a worst-case evaluation of MU's resilience and effectiveness.\nThrough extensive experiments across different datasets (including CIFAR-10,\n100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image\nclassifiers and generative models), we expose critical pros and cons in\nexisting (approximate) unlearning strategies. Our results illuminate the\ncomplex challenges of MU in practice, guiding the future development of more\naccurate and robust unlearning algorithms. The code is available at\nhttps://github.com/OPTML-Group/Unlearn-WorstCase.\n","authors":["Chongyu Fan","Jiancheng Liu","Alfred Hero","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07359v1","updated":"2024-03-12T06:45:34Z","published":"2024-03-12T06:45:34Z","title":"FSC: Few-point Shape Completion","summary":"  While previous studies have demonstrated successful 3D object shape\ncompletion with a sufficient number of points, they often fail in scenarios\nwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\nanalysis, we find that even a few points, e.g. 64 points, could retain\nsubstantial information to help recover the 3D shape of the object. To address\nthe challenge of shape completion with very sparse point clouds, we then\npropose Few-point Shape Completion (FSC) model, which contains a novel\ndual-branch feature extractor for handling extremely sparse inputs, coupled\nwith an extensive branch for maximal point utilization with a saliency branch\nfor dynamic importance assignment. This model is further bolstered by a\ntwo-stage revision network that refines both the extracted features and the\ndecoder output, enhancing the detail and authenticity of the completed point\ncloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\na few points. The proposed FSC (FSC) model outperforms previous methods on both\nfew-point inputs and many-point inputs, and shows good generalizability to\ndifferent object categories.\n","authors":["Xianzu Wu","Xianfeng Wu","Tianyu Luan","Yajing Bai","Zhongyuan Lai","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.07359v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07356v1","updated":"2024-03-12T06:29:54Z","published":"2024-03-12T06:29:54Z","title":"Premonition: Using Generative Models to Preempt Future Data Changes in\n  Continual Learning","summary":"  Continual learning requires a model to adapt to ongoing changes in the data\ndistribution, and often to the set of tasks to be performed. It is rare,\nhowever, that the data and task changes are completely unpredictable. Given a\ndescription of an overarching goal or data theme, which we call a realm, humans\ncan often guess what concepts are associated with it. We show here that the\ncombination of a large language model and an image generation model can\nsimilarly provide useful premonitions as to how a continual learning challenge\nmight develop over time. We use the large language model to generate text\ndescriptions of semantically related classes that might potentially appear in\nthe data stream in future. These descriptions are then rendered using Stable\nDiffusion to generate new labelled image samples. The resulting synthetic\ndataset is employed for supervised pre-training, but is discarded prior to\ncommencing continual learning, along with the pre-training classification head.\nWe find that the backbone of our pre-trained networks can learn representations\nuseful for the downstream continual learning problem, thus becoming a valuable\ninput to any existing continual learning method. Although there are\ncomplexities arising from the domain gap between real and synthetic images, we\nshow that pre-training models in this manner improves multiple Class Incremenal\nLearning (CIL) methods on fine-grained image classification benchmarks.\nSupporting code can be found at https://github.com/cl-premonition/premonition.\n","authors":["Mark D. McDonnell","Dong Gong","Ehsan Abbasnejad","Anton van den Hengel"],"pdf_url":"https://arxiv.org/pdf/2403.07356v1.pdf","comment":"31 pages total (14 main paper, 5 references, 12 appendices)"},{"id":"http://arxiv.org/abs/2403.07355v1","updated":"2024-03-12T06:28:41Z","published":"2024-03-12T06:28:41Z","title":"Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO\n  Systems","summary":"  This paper presents a finite-rate deep-learning (DL)-based channel state\ninformation (CSI) feedback method for massive multiple-input multiple-output\n(MIMO) systems. The presented method provides a finite-bit representation of\nthe latent vector based on a vector-quantized variational autoencoder (VQ-VAE)\nframework while reducing its computational complexity based on shape-gain\nvector quantization. In this method, the magnitude of the latent vector is\nquantized using a non-uniform scalar codebook with a proper transformation\nfunction, while the direction of the latent vector is quantized using a\ntrainable Grassmannian codebook. A multi-rate codebook design strategy is also\ndeveloped by introducing a codeword selection rule for a nested codebook along\nwith the design of a loss function. Simulation results demonstrate that the\nproposed method reduces the computational complexity associated with VQ-VAE\nwhile improving CSI reconstruction performance under a given feedback overhead.\n","authors":["Junyong Shin","Yujin Kang","Yo-Seb Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.07355v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2310.07990v2","updated":"2024-03-12T15:34:13Z","published":"2023-10-12T02:34:56Z","title":"Multi-View Variational Autoencoder for Missing Value Imputation in\n  Untargeted Metabolomics","summary":"  Background: Missing data is a common challenge in mass spectrometry-based\nmetabolomics, which can lead to biased and incomplete analyses. The integration\nof whole-genome sequencing (WGS) data with metabolomics data has emerged as a\npromising approach to enhance the accuracy of data imputation in metabolomics\nstudies. Method: In this study, we propose a novel method that leverages the\ninformation from WGS data and reference metabolites to impute unknown\nmetabolites. Our approach utilizes a multi-view variational autoencoder to\njointly model the burden score, polygenetic risk score (PGS), and linkage\ndisequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature\nextraction and missing metabolomics data imputation. By learning the latent\nrepresentations of both omics data, our method can effectively impute missing\nmetabolomics values based on genomic information. Results: We evaluate the\nperformance of our method on empirical metabolomics datasets with missing\nvalues and demonstrate its superiority compared to conventional imputation\ntechniques. Using 35 template metabolites derived burden scores, PGS and\nLD-pruned SNPs, the proposed methods achieved R^2-scores > 0.01 for 71.55% of\nmetabolites. Conclusion: The integration of WGS data in metabolomics imputation\nnot only improves data completeness but also enhances downstream analyses,\npaving the way for more comprehensive and accurate investigations of metabolic\npathways and disease associations. Our findings offer valuable insights into\nthe potential benefits of utilizing WGS data for metabolomics data imputation\nand underscore the importance of leveraging multi-modal data integration in\nprecision medicine research.\n","authors":["Chen Zhao","Kuan-Jui Su","Chong Wu","Xuewei Cao","Qiuying Sha","Wu Li","Zhe Luo","Tian Qin","Chuan Qiu","Lan Juan Zhao","Anqi Liu","Lindong Jiang","Xiao Zhang","Hui Shen","Weihua Zhou","Hong-Wen Deng"],"pdf_url":"https://arxiv.org/pdf/2310.07990v2.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.07732v1","updated":"2024-03-12T15:11:50Z","published":"2024-03-12T15:11:50Z","title":"DESERE: The 1st Workshop on Decentralised Search and Recommendation","summary":"  The DESERE Workshop, our First Workshop on Decentralised Search and\nRecommendation, offers a platform for researchers to explore and share\ninnovative ideas on decentralised web services, mainly focusing on three major\ntopics: (i) societal impact of decentralised systems: their effect on privacy,\npolicy, and regulation; (ii) decentralising applications: algorithmic and\nperformance challenges that arise from decentralisation; and (iii)\ninfrastructure to support decentralised systems and services: peer-to-peer\nnetworks, routing, and performance evaluation tools\n","authors":["Mohamed Ragab","Yury Savateev","Wenjie Wang","Reza Moosaei","Thanassis Tiropanis","Alexandra Poulovassilis","Adriane Chapman","Helen Oliver","George Roussos"],"pdf_url":"https://arxiv.org/pdf/2403.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07654v1","updated":"2024-03-12T13:45:20Z","published":"2024-03-12T13:45:20Z","title":"Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models","summary":"  Modern sequence-to-sequence relevance models like monoT5 can effectively\ncapture complex textual interactions between queries and documents through\ncross-encoding. However, the use of natural language tokens in prompts, such as\nQuery, Document, and Relevant for monoT5, opens an attack vector for malicious\ndocuments to manipulate their relevance score through prompt injection, e.g.,\nby adding target words such as true. Since such possibilities have not yet been\nconsidered in retrieval evaluation, we analyze the impact of query-independent\nprompt injection via manually constructed templates and LLM-based rewriting of\ndocuments on several existing relevance models. Our experiments on the TREC\nDeep Learning track show that adversarial documents can easily manipulate\ndifferent sequence-to-sequence relevance models, while BM25 (as a typical\nlexical model) is not affected. Remarkably, the attacks also affect\nencoder-only relevance models (which do not rely on natural language prompt\ntokens), albeit to a lesser extent.\n","authors":["Andrew Parry","Maik Fröbe","Sean MacAvaney","Martin Potthast","Matthias Hagen"],"pdf_url":"https://arxiv.org/pdf/2403.07654v1.pdf","comment":"13 pages, 3 figures, Accepted at ECIR 2024 as a Full Paper"},{"id":"http://arxiv.org/abs/2403.07623v1","updated":"2024-03-12T13:06:31Z","published":"2024-03-12T13:06:31Z","title":"Empowering Sequential Recommendation from Collaborative Signals and\n  Semantic Relatedness","summary":"  Sequential recommender systems (SRS) could capture dynamic user preferences\nby modeling historical behaviors ordered in time. Despite effectiveness,\nfocusing only on the \\textit{collaborative signals} from behaviors does not\nfully grasp user interests. It is also significant to model the\n\\textit{semantic relatedness} reflected in content features, e.g., images and\ntext. Towards that end, in this paper, we aim to enhance the SRS tasks by\neffectively unifying collaborative signals and semantic relatedness together.\nNotably, we empirically point out that it is nontrivial to achieve such a goal\ndue to semantic gap issues. Thus, we propose an end-to-end two-stream\narchitecture for sequential recommendation, named TSSR, to learn user\npreferences from ID-based and content-based sequence. Specifically, we first\npresent novel hierarchical contrasting module, including coarse user-grained\nand fine item-grained terms, to align the representations of inter-modality.\nFurthermore, we also design a two-stream architecture to learn the dependence\nof intra-modality sequence and the complex interactions of inter-modality\nsequence, which can yield more expressive capacity in understanding user\ninterests. We conduct extensive experiments on five public datasets. The\nexperimental results show that the TSSR could yield superior performance than\ncompetitive baselines. We also make our experimental codes publicly available\nat https://anonymous.4open.science/r/TSSR-2A27/.\n","authors":["Mingyue Cheng","Hao Zhang","Qi Liu","Fajie Yuan","Zhi Li","Zhenya Huang","Enhong Chen","Jun Zhou","Longfei Li"],"pdf_url":"https://arxiv.org/pdf/2403.07623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07571v1","updated":"2024-03-12T11:58:50Z","published":"2024-03-12T11:58:50Z","title":"Proactive Recommendation with Iterative Preference Guidance","summary":"  Recommender systems mainly tailor personalized recommendations according to\nuser interests learned from user feedback. However, such recommender systems\npassively cater to user interests and even reinforce existing interests in the\nfeedback loop, leading to problems like filter bubbles and opinion\npolarization. To counteract this, proactive recommendation actively steers\nusers towards developing new interests in a target item or topic by\nstrategically modulating recommendation sequences. Existing work for proactive\nrecommendation faces significant hurdles: 1) overlooking the user feedback in\nthe guidance process; 2) lacking explicit modeling of the guiding objective;\nand 3) insufficient flexibility for integration into existing industrial\nrecommender systems. To address these issues, we introduce an Iterative\nPreference Guidance (IPG) framework. IPG performs proactive recommendation in a\nflexible post-processing manner by ranking items according to their IPG scores\nthat consider both interaction probability and guiding value. These scores are\nexplicitly estimated with iteratively updated user representation that\nconsiders the most recent user interactions. Extensive experiments validate\nthat IPG can effectively guide user interests toward target interests with a\nreasonable trade-off in recommender accuracy. The code is available at\nhttps://github.com/GabyUSTC/IPG-Rec.\n","authors":["Shuxian Bi","Wenjie Wang","Hang Pan","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2403.07571v1.pdf","comment":"Accepted by WWW 2024 (Short)"},{"id":"http://arxiv.org/abs/2403.07553v1","updated":"2024-03-12T11:39:18Z","published":"2024-03-12T11:39:18Z","title":"The future of document indexing: GPT and Donut revolutionize table of\n  content processing","summary":"  Industrial projects rely heavily on lengthy, complex specification documents,\nmaking tedious manual extraction of structured information a major bottleneck.\nThis paper introduces an innovative approach to automate this process,\nleveraging the capabilities of two cutting-edge AI models: Donut, a model that\nextracts information directly from scanned documents without OCR, and OpenAI\nGPT-3.5 Turbo, a robust large language model. The proposed methodology is\ninitiated by acquiring the table of contents (ToCs) from construction\nspecification documents and subsequently structuring the ToCs text into JSON\ndata. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5\nTurbo reaching 89% in effectively organizing the ToCs. This landmark\nachievement represents a significant leap forward in document indexing,\ndemonstrating the immense potential of AI to automate information extraction\ntasks across diverse document types, boosting efficiency and liberating\ncritical resources in various industries.\n","authors":["Degaga Wolde Feyisa","Haylemicheal Berihun","Amanuel Zewdu","Mahsa Najimoghadam","Marzieh Zare"],"pdf_url":"https://arxiv.org/pdf/2403.07553v1.pdf","comment":"Document AI, Document Classification, Information extraction, Large\n  Language Models, OCR Models, Visual Document Understanding"},{"id":"http://arxiv.org/abs/2403.07478v1","updated":"2024-03-12T10:12:59Z","published":"2024-03-12T10:12:59Z","title":"Towards Graph Foundation Models for Personalization","summary":"  In the realm of personalization, integrating diverse information sources such\nas consumption signals and content-based representations is becoming\nincreasingly critical to build state-of-the-art solutions. In this regard, two\nof the biggest trends in research around this subject are Graph Neural Networks\n(GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in\nindustry for powering personalization at scale, FMs have only recently caught\nattention for their promising performance in personalization tasks like ranking\nand retrieval. In this paper, we present a graph-based foundation modeling\napproach tailored to personalization. Central to this approach is a\nHeterogeneous GNN (HGNN) designed to capture multi-hop content and consumption\nrelationships across a range of recommendable item types. To ensure the\ngenerality required from a Foundation Model, we employ a Large Language Model\n(LLM) text-based featurization of nodes that accommodates all item types, and\nconstruct the graph using co-interaction signals, which inherently transcend\ncontent specificity. To facilitate practical generalization, we further couple\nthe HGNN with an adaptation mechanism based on a two-tower (2T) architecture,\nwhich also operates agnostically to content type. This multi-stage approach\nensures high scalability; while the HGNN produces general purpose embeddings,\nthe 2T component models in a continuous space the sheer size of user-item\ninteraction data. Our comprehensive approach has been rigorously tested and\nproven effective in delivering recommendations across a diverse array of\nproducts within a real-world, industrial audio streaming platform.\n","authors":["Andreas Damianou","Francesco Fabbri","Paul Gigioli","Marco De Nadai","Alice Wang","Enrico Palumbo","Mounia Lalmas"],"pdf_url":"https://arxiv.org/pdf/2403.07478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07331v1","updated":"2024-03-12T05:32:33Z","published":"2024-03-12T05:32:33Z","title":"LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial\n  Keyword Queries","summary":"  With the proliferation of spatio-textual data, Top-k KNN spatial keyword\nqueries (TkQs), which return a list of objects based on a ranking function that\nevaluates both spatial and textual relevance, have found many real-life\napplications. Existing geo-textual indexes for TkQs use traditional retrieval\nmodels like BM25 to compute text relevance and usually exploit a simple linear\nfunction to compute spatial relevance, but its effectiveness is limited. To\nimprove effectiveness, several deep learning models have recently been\nproposed, but they suffer severe efficiency issues. To the best of our\nknowledge, there are no efficient indexes specifically designed to accelerate\nthe top-k search process for these deep learning models.\n  To tackle these issues, we propose a novel technique, which Learns to Index\nthe Spatio-Textual data for answering embedding based spatial keyword queries\n(called LIST). LIST is featured with two novel components. Firstly, we propose\na lightweight and effective relevance model that is capable of learning both\ntextual and spatial relevance. Secondly, we introduce a novel machine learning\nbased Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new\nlearning-to-cluster technique to group relevant queries and objects together\nwhile separating irrelevant queries and objects. Two key challenges in building\nan effective and efficient index are the absence of high-quality labels and\nunbalanced clustering results. We develop a novel pseudo-label generation\ntechnique to address the two challenges. Experimental results show that LIST\nsignificantly outperforms state-of-the-art methods on effectiveness, with\nimprovements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is\nthree orders of magnitude faster than the most effective baseline.\n","authors":["Ziqi Yin","Shanshan Feng","Shang Liu","Gao Cong","Yew Soon Ong","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2403.07331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11624v4","updated":"2024-03-12T04:38:53Z","published":"2024-01-21T23:34:42Z","title":"In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey","summary":"  Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.\n","authors":["Man Luo","Xin Xu","Yue Liu","Panupong Pasupat","Mehran Kazemi"],"pdf_url":"https://arxiv.org/pdf/2401.11624v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07265v1","updated":"2024-03-12T02:52:17Z","published":"2024-03-12T02:52:17Z","title":"Self-supervised Contrastive Learning for Implicit Collaborative\n  Filtering","summary":"  Contrastive learning-based recommendation algorithms have significantly\nadvanced the field of self-supervised recommendation, particularly with BPR as\na representative ranking prediction task that dominates implicit collaborative\nfiltering. However, the presence of false-positive and false-negative examples\nin recommendation systems hampers accurate preference learning. In this study,\nwe propose a simple self-supervised contrastive learning framework that\nleverages positive feature augmentation and negative label augmentation to\nimprove the self-supervisory signal. Theoretical analysis demonstrates that our\nlearning method is equivalent to maximizing the likelihood estimation with\nlatent variables representing user interest centers. Additionally, we establish\nan efficient negative label augmentation technique that samples unlabeled\nexamples with a probability linearly dependent on their relative ranking\npositions, enabling efficient augmentation in constant time complexity. Through\nvalidation on multiple datasets, we illustrate the significant improvements our\nmethod achieves over the widely used BPR optimization objective while\nmaintaining comparable runtime.\n","authors":["Shipeng Song","Bin Liu","Fei Teng","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2403.07265v1.pdf","comment":"3 figures"},{"id":"http://arxiv.org/abs/2403.06747v2","updated":"2024-03-12T02:13:07Z","published":"2024-03-11T14:13:41Z","title":"MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation","summary":"  Compared to business-to-consumer (B2C) e-commerce systems,\nconsumer-to-consumer (C2C) e-commerce platforms usually encounter the\nlimited-stock problem, that is, a product can only be sold one time in a C2C\nsystem. This poses several unique challenges for click-through rate (CTR)\nprediction. Due to limited user interactions for each product (i.e. item), the\ncorresponding item embedding in the CTR model may not easily converge. This\nmakes the conventional sequence modeling based approaches cannot effectively\nutilize user history information since historical user behaviors contain a\nmixture of items with different volume of stocks. Particularly, the attention\nmechanism in a sequence model tends to assign higher score to products with\nmore accumulated user interactions, making limited-stock products being ignored\nand contribute less to the final output. To this end, we propose the Meta-Split\nNetwork (MSNet) to split user history sequence regarding to the volume of stock\nfor each product, and adopt differentiated modeling approaches for different\nsequences. As for the limited-stock products, a meta-learning approach is\napplied to address the problem of inconvergence, which is achieved by designing\nmeta scaling and shifting networks with ID and side information. In addition,\ntraditional approach can hardly update item embedding once the product is\nconsumed. Thereby, we propose an auxiliary loss that makes the parameters\nupdatable even when the product is no longer in distribution. To the best of\nour knowledge, this is the first solution addressing the recommendation of\nlimited-stock product. Experimental results on the production dataset and\nonline A/B testing demonstrate the effectiveness of our proposed method.\n","authors":["Wenhao Wu","Jialiang Zhou","Ailong He","Shuguang Han","Jufeng Chen","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.06747v2.pdf","comment":"WWW'24 Industry Track"},{"id":"http://arxiv.org/abs/2305.13931v3","updated":"2024-03-12T01:28:11Z","published":"2023-05-10T02:16:55Z","title":"Position Bias Estimation with Item Embedding for Sparse Dataset","summary":"  Estimating position bias is a well-known challenge in Learning to Rank (L2R).\nClick data in e-commerce applications, such as targeted advertisements and\nsearch engines, provides implicit but abundant feedback to improve personalized\nrankings. However, click data inherently includes various biases like position\nbias. Based on the position-based click model, Result Randomization and\nRegression Expectation-Maximization algorithm (REM) have been proposed to\nestimate position bias, but they require various paired observations of (item,\nposition). In real-world scenarios of advertising, marketers frequently display\nadvertisements in a fixed pre-determined order, which creates difficulties in\nestimation due to the limited availability of various pairs in the training\ndata, resulting in a sparse dataset. We propose a variant of the REM that\nutilizes item embeddings to alleviate the sparsity of (item, position). Using a\npublic dataset and internal carousel advertisement click dataset, we\nempirically show that item embedding with Latent Semantic Indexing (LSI) and\nVariational Auto-Encoder (VAE) improves the accuracy of position bias\nestimation and the estimated position bias enhances Learning to Rank\nperformance. We also show that LSI is more effective as an embedding creation\nmethod for position bias estimation.\n","authors":["Shion Ishikawa","Yun Ching Liu","Young-Joo Chung","Yu Hirate"],"pdf_url":"https://arxiv.org/pdf/2305.13931v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01614v2","updated":"2024-03-12T23:14:33Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents -- it\ncan successfully complete 51.1 of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out to be not effective for web agents, and the best grounding strategy\nwe develop in this paper leverages both the HTML structure and visuals. Yet,\nthere is still a substantial gap with oracle grounding, leaving ample room for\nfurther improvement. All code, data, and evaluation tools are available at\nhttps://github.com/OSU-NLP-Group/SeeAct.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11767v3","updated":"2024-03-12T20:10:05Z","published":"2023-08-15T23:22:37Z","title":"Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm","summary":"  ChatGPT and generative AI tools are becoming the new reality. This work is\nmotivated by the premise that ``ChatGPT content may exhibit a distinctive\nbehavior that can be separated from scientific articles''. In this study, we\ndemonstrate how we tested this premise in two phases and prove its validity.\nSubsequently, we introduce xFakeSci, a novel learning algorithm, that is\ncapable of distinguishing ChatGPT-generated articles from publications produced\nby scientists. The algorithm is trained using network models driven from\nmultiple types of data sources, such as ChatGPT-generated documents achieved by\nmeans of prompt-engineering, and PubMed articles. To mitigate over-fitting\nissues, we incorporate a calibration step that is built upon data-driven\nheuristics, including ratios. We evaluate the algorithm across multiple\ndatasets covering publication periods and diseases (cancer, depression, and\nAlzheimer's). Further, we show how the algorithm is benchmarked against the\nstate-of-the-art (SOTA) algorithms. While the xFakeSci algorithm achieve F1\nscore ranging from 80% - 94%, SOTA algorithms score F1 values between 38% -\n52%. We attribute the noticeable difference to the introduction of calibration\nand a proximity distance heuristic, which we underscore this promising\nperformance. Indeed, the prediction of fake science generated by ChatGPT\npresents a considerable challenge. Nonetheless, the introduction of xFakeSci\nalgorithm is a significant step on the way to combating fake science.\n","authors":["Ahmed Abdeen Hamed","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2308.11767v3.pdf","comment":"14 pages, 6 figures, 6 tables, 5 algorithms"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.13254v2","updated":"2024-03-12T17:59:56Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v2.pdf","comment":"13 pages, 6 figures, 8 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2403.07869v1","updated":"2024-03-12T17:58:01Z","published":"2024-03-12T17:58:01Z","title":"TeleMoMa: A Modular and Versatile Teleoperation System for Mobile\n  Manipulation","summary":"  A critical bottleneck limiting imitation learning in robotics is the lack of\ndata. This problem is more severe in mobile manipulation, where collecting\ndemonstrations is harder than in stationary manipulation due to the lack of\navailable and easy-to-use teleoperation interfaces. In this work, we\ndemonstrate TeleMoMa, a general and modular interface for whole-body\nteleoperation of mobile manipulators. TeleMoMa unifies multiple human\ninterfaces including RGB and depth cameras, virtual reality controllers,\nkeyboard, joysticks, etc., and any combination thereof. In its more accessible\nversion, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering\nthe entry bar for humans to provide mobile manipulation demonstrations. We\ndemonstrate the versatility of TeleMoMa by teleoperating several existing\nmobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and\nthe real world. We demonstrate the quality of the demonstrations collected with\nTeleMoMa by training imitation learning policies for mobile manipulation tasks\ninvolving synchronized whole-body motion. Finally, we also show that TeleMoMa's\nteleoperation channel enables teleoperation on site, looking at the robot, or\nremote, sending commands and observations through a computer network, and\nperform user studies to evaluate how easy it is for novice users to learn to\ncollect demonstrations with different combinations of human interfaces enabled\nby our system. We hope TeleMoMa becomes a helpful tool for the community\nenabling researchers to collect whole-body mobile manipulation demonstrations.\nFor more information and video results,\nhttps://robin-lab.cs.utexas.edu/telemoma-web.\n","authors":["Shivin Dass","Wensi Ai","Yuqian Jiang","Samik Singh","Jiaheng Hu","Ruohan Zhang","Peter Stone","Ben Abbatematteo","Roberto Martin-Martin"],"pdf_url":"https://arxiv.org/pdf/2403.07869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07865v1","updated":"2024-03-12T17:55:38Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80\\% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07857v1","updated":"2024-03-12T17:48:08Z","published":"2024-03-12T17:48:08Z","title":"Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias","summary":"  Model-induced distribution shifts (MIDS) occur as previous model outputs\npollute new model training sets over generations of models. This is known as\nmodel collapse in the case of generative models, and performative prediction or\nunfairness feedback loops for supervised models. When a model induces a\ndistribution shift, it also encodes its mistakes, biases, and unfairnesses into\nthe ground truth of its data ecosystem. We introduce a framework that allows us\nto track multiple MIDS over many generations, finding that they can lead to\nloss in performance, fairness, and minoritized group representation, even in\ninitially unbiased datasets. Despite these negative consequences, we identify\nhow models might be used for positive, intentional, interventions in their data\necosystems, providing redress for historical discrimination through a framework\ncalled algorithmic reparation (AR). We simulate AR interventions by curating\nrepresentative training batches for stochastic gradient descent to demonstrate\nhow AR can improve upon the unfairnesses of models and data ecosystems subject\nto other MIDS. Our work takes an important step towards identifying,\nmitigating, and taking accountability for the unfair feedback loops enabled by\nthe idea that ML systems are inherently neutral and objective.\n","authors":["Sierra Wyllie","Ilia Shumailov","Nicolas Papernot"],"pdf_url":"https://arxiv.org/pdf/2403.07857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04079v3","updated":"2024-03-12T17:47:59Z","published":"2024-01-08T18:31:38Z","title":"RudolfV: A Foundation Model by Pathologists for Pathologists","summary":"  Histopathology plays a central role in clinical medicine and biomedical\nresearch. While artificial intelligence shows promising results on many\npathological tasks, generalization and dealing with rare diseases, where\ntraining data is scarce, remains a challenge. Distilling knowledge from\nunlabelled data into a foundation model before learning from, potentially\nlimited, labelled data provides a viable path to address these challenges. In\nthis work, we extend the state of the art of foundation models for digital\npathology whole slide images by semi-automated data curation and incorporating\npathologist domain knowledge. Specifically, we combine computational and\npathologist domain knowledge (1) to curate a diverse dataset of 133k slides\ncorresponding to 1.2 billion image patches covering data from different\nfixation, staining, and scanning protocols as well as data from different\nindications and labs across the EU and US, (2) for grouping semantically\nsimilar slides and tissue patches, and (3) to augment the input images during\ntraining. We evaluate the resulting model on a set of public and internal\nbenchmarks and show that although our foundation model is trained with an order\nof magnitude less slides, it performs on par or better than competing models.\nWe expect that scaling our approach to more data and larger models will further\nincrease its performance and capacity to deal with increasingly complex real\nworld tasks in diagnostics and biomedical research.\n","authors":["Jonas Dippel","Barbara Feulner","Tobias Winterhoff","Simon Schallenberg","Gabriel Dernbach","Andreas Kunft","Stephan Tietz","Timo Milbich","Simon Heinke","Marie-Lisa Eich","Julika Ribbat-Idel","Rosemarie Krupar","Philipp Jurmeister","David Horst","Lukas Ruff","Klaus-Robert Müller","Frederick Klauschen","Maximilian Alber"],"pdf_url":"https://arxiv.org/pdf/2401.04079v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07856v1","updated":"2024-03-12T17:46:38Z","published":"2024-03-12T17:46:38Z","title":"Quantum Support Vector Machine for Prostate Cancer Detection: A\n  Performance Analysis","summary":"  This study addresses the urgent need for improved prostate cancer detection\nmethods by harnessing the power of advanced technological solutions. We\nintroduce the application of Quantum Support Vector Machine (QSVM) to this\ncritical healthcare challenge, showcasing an enhancement in diagnostic\nperformance over the classical Support Vector Machine (SVM) approach. Our study\nnot only outlines the remarkable improvements in diagnostic performance made by\nQSVM over the classic SVM technique, but it delves into the advancements\nbrought about by the quantum feature map architecture, which has been carefully\nidentified and evaluated, ensuring it aligns seamlessly with the unique\ncharacteristics of our prostate cancer dataset. This architecture succeded in\ncreating a distinct feature space, enabling the detection of complex,\nnon-linear patterns in the data. The findings reveal not only a comparable\naccuracy with classical SVM ($92\\%$) but also a $7.14\\%$ increase in\nsensitivity and a notably high F1-Score ($93.33\\%$). This study's important\ncombination of quantum computing in medical diagnostics marks a pivotal step\nforward in cancer detection, offering promising implications for the future of\nhealthcare technology.\n","authors":["Walid El Maouaki","Taoufik Said","Mohamed Bennai"],"pdf_url":"https://arxiv.org/pdf/2403.07856v1.pdf","comment":"14 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2308.08079v3","updated":"2024-03-12T17:46:10Z","published":"2023-08-16T00:19:52Z","title":"Rigid Transformations for Stabilized Lower Dimensional Space to Support\n  Subsurface Uncertainty Quantification and Interpretation","summary":"  Subsurface datasets inherently possess big data characteristics such as vast\nvolume, diverse features, and high sampling speeds, further compounded by the\ncurse of dimensionality from various physical, engineering, and geological\ninputs. Among the existing dimensionality reduction (DR) methods, nonlinear\ndimensionality reduction (NDR) methods, especially Metric-multidimensional\nscaling (MDS), are preferred for subsurface datasets due to their inherent\ncomplexity. While MDS retains intrinsic data structure and quantifies\nuncertainty, its limitations include unstabilized unique solutions invariant to\nEuclidean transformations and an absence of out-of-sample points (OOSP)\nextension. To enhance subsurface inferential and machine learning workflows,\ndatasets must be transformed into stable, reduced-dimension representations\nthat accommodate OOSP.\n  Our solution employs rigid transformations for a stabilized Euclidean\ninvariant representation for LDS. By computing an MDS input dissimilarity\nmatrix, and applying rigid transformations on multiple realizations, we ensure\ntransformation invariance and integrate OOSP. This process leverages a convex\nhull algorithm and incorporates loss function and normalized stress for\ndistortion quantification. We validate our approach with synthetic data,\nvarying distance metrics, and real-world wells from the Duvernay Formation.\nResults confirm our method's efficacy in achieving consistent LDS\nrepresentations. Furthermore, our proposed \"stress ratio\" (SR) metric provides\ninsight into uncertainty, beneficial for model adjustments and inferential\nanalysis. Consequently, our workflow promises enhanced repeatability and\ncomparability in NDR for subsurface energy resource engineering and associated\nbig data workflows.\n","authors":["Ademide O. Mabadeje","Michael J. Pyrcz"],"pdf_url":"https://arxiv.org/pdf/2308.08079v3.pdf","comment":"30 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.07854v1","updated":"2024-03-12T17:44:45Z","published":"2024-03-12T17:44:45Z","title":"Distilling the Knowledge in Data Pruning","summary":"  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n","authors":["Emanuel Ben-Baruch","Adam Botach","Igor Kviatkovsky","Manoj Aggarwal","Gérard Medioni"],"pdf_url":"https://arxiv.org/pdf/2403.07854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07851v1","updated":"2024-03-12T17:43:20Z","published":"2024-03-12T17:43:20Z","title":"12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems\nto expand their inference capabilities to new classes using only a few labeled\nexamples, without forgetting the previously learned classes. Classical\nbackpropagation-based learning and its variants are often unsuitable for\nbattery-powered, memory-constrained systems at the extreme edge. In this work,\nwe introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a\nlightweight model consisting of a pretrained and metalearned feature extractor\nand an expandable explicit memory storing the class prototypes. The\narchitecture is pretrained with a novel feature orthogonality regularization\nand metalearned with a multi-margin loss. For learning a new class, our\napproach extends the explicit memory with novel class prototypes, while the\nremaining architecture is kept frozen. This allows learning previously unseen\nclasses based on only a few examples with one single pass (hence online).\nO-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,\nachieving state-of-the-art results. Tailored for ultra-low-power platforms, we\nimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online\nlearning capabilities within just 12 mJ per new class.\n","authors":["Yoga Esa Wibowo","Cristian Cioflan","Thorir Mar Ingolfsson","Michael Hersche","Leo Zhao","Abbas Rahimi","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.07851v1.pdf","comment":"6 pages, 4 tables, 3 figures. Accepted at IEEE DATE 2024"},{"id":"http://arxiv.org/abs/2403.07849v1","updated":"2024-03-12T17:41:27Z","published":"2024-03-12T17:41:27Z","title":"Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining\n  of Explanations","summary":"  We formulate an XAI-based model improvement approach for Graph Neural\nNetworks (GNNs) for node classification, called Explanation Enhanced Graph\nLearning (EEGL). The goal is to improve predictive performance of GNN using\nexplanations. EEGL is an iterative self-improving algorithm, which starts with\na learned \"vanilla\" GNN, and repeatedly uses frequent subgraph mining to find\nrelevant patterns in explanation subgraphs. These patterns are then filtered\nfurther to obtain application-dependent features corresponding to the presence\nof certain subgraphs in the node neighborhoods. Giving an application-dependent\nalgorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL)\nalgorithm has previously been posed as an open problem. We present experimental\nevidence, with synthetic and real-world data, which show that EEGL outperforms\nrelated approaches in predictive performance and that it has a\nnode-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's\ntraining dynamics.\n","authors":["Harish G. Naik","Jan Polster","Raj Shekhar","Tamás Horváth","György Turán"],"pdf_url":"https://arxiv.org/pdf/2403.07849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08156v3","updated":"2024-03-12T17:37:26Z","published":"2024-02-13T01:38:01Z","title":"Group Decision-Making among Privacy-Aware Agents","summary":"  How can individuals exchange information to learn from each other despite\ntheir privacy needs and security concerns? For example, consider individuals\ndeliberating a contentious topic and being concerned about divulging their\nprivate experiences. Preserving individual privacy and enabling efficient\nsocial learning are both important desiderata but seem fundamentally at odds\nwith each other and very hard to reconcile. We do so by controlling information\nleakage using rigorous statistical guarantees that are based on differential\nprivacy (DP). Our agents use log-linear rules to update their beliefs after\ncommunicating with their neighbors. Adding DP randomization noise to beliefs\nprovides communicating agents with plausible deniability with regard to their\nprivate information and their network neighborhoods. We consider two learning\nenvironments one for distributed maximum-likelihood estimation given a finite\nnumber of private signals and another for online learning from an infinite,\nintermittent signal stream. Noisy information aggregation in the finite case\nleads to interesting tradeoffs between rejecting low-quality states and making\nsure all high-quality states are accepted in the algorithm output. Our results\nflesh out the nature of the trade-offs in both cases between the quality of the\ngroup decision outcomes, learning accuracy, communication cost, and the level\nof privacy protections that the agents are afforded.\n","authors":["Marios Papachristou","M. Amin Rahimian"],"pdf_url":"https://arxiv.org/pdf/2402.08156v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14892v2","updated":"2024-03-12T17:34:16Z","published":"2024-02-22T14:13:44Z","title":"Novelty Detection on Radio Astronomy Data using Signatures","summary":"  We introduce SigNova, a new semi-supervised framework for detecting anomalies\nin streamed data. While our initial examples focus on detecting radio-frequency\ninterference (RFI) in digitized signals within the field of radio astronomy, it\nis important to note that SigNova's applicability extends to any type of\nstreamed data. The framework comprises three primary components. Firstly, we\nuse the signature transform to extract a canonical collection of summary\nstatistics from observational sequences. This allows us to represent\nvariable-length visibility samples as finite-dimensional feature vectors.\nSecondly, each feature vector is assigned a novelty score, calculated as the\nMahalanobis distance to its nearest neighbor in an RFI-free training set. By\nthresholding these scores we identify observation ranges that deviate from the\nexpected behavior of RFI-free visibility samples without relying on stringent\ndistributional assumptions. Thirdly, we integrate this anomaly detector with\nPysegments, a segmentation algorithm, to localize consecutive observations\ncontaminated with RFI, if any. This approach provides a compelling alternative\nto classical windowing techniques commonly used for RFI detection. Importantly,\nthe complexity of our algorithm depends on the RFI pattern rather than on the\nsize of the observation window. We demonstrate how SigNova improves the\ndetection of various types of RFI (e.g., broadband and narrowband) in\ntime-frequency visibility data. We validate our framework on the Murchison\nWidefield Array (MWA) telescope and simulated data and the Hydrogen Epoch of\nReionization Array (HERA).\n","authors":["Paola Arrubarrena","Maud Lemercier","Bojan Nikolic","Terry Lyons","Thomas Cass"],"pdf_url":"https://arxiv.org/pdf/2402.14892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07843v1","updated":"2024-03-12T17:32:52Z","published":"2024-03-12T17:32:52Z","title":"A Machine learning and Empirical Bayesian Approach for Predictive Buying\n  in B2B E-commerce","summary":"  In the context of developing nations like India, traditional business to\nbusiness (B2B) commerce heavily relies on the establishment of robust\nrelationships, trust, and credit arrangements between buyers and sellers.\nConsequently, ecommerce enterprises frequently. Established in 2016 with a\nvision to revolutionize trade in India through technology, Udaan is the\ncountrys largest business to business ecommerce platform. Udaan operates across\ndiverse product categories, including lifestyle, electronics, home and employ\ntelecallers to cultivate buyer relationships, streamline order placement\nprocedures, and promote special promotions. The accurate anticipation of buyer\norder placement behavior emerges as a pivotal factor for attaining sustainable\ngrowth, heightening competitiveness, and optimizing the efficiency of these\ntelecallers. To address this challenge, we have employed an ensemble approach\ncomprising XGBoost and a modified version of Poisson Gamma model to predict\ncustomer order patterns with precision. This paper provides an in-depth\nexploration of the strategic fusion of machine learning and an empirical\nBayesian approach, bolstered by the judicious selection of pertinent features.\nThis innovative approach has yielded a remarkable 3 times increase in customer\norder rates, show casing its potential for transformative impact in the\necommerce industry.\n","authors":["Tuhin Subhra De","Pranjal Singh","Alok Patel"],"pdf_url":"https://arxiv.org/pdf/2403.07843v1.pdf","comment":"Published at the 8th International Conference on Machine Learning and\n  Soft Computing (ICMLSC 2024), Singapore"},{"id":"http://arxiv.org/abs/2403.07842v1","updated":"2024-03-12T17:27:49Z","published":"2024-03-12T17:27:49Z","title":"Quantifying and Mitigating Privacy Risks for Tabular Generative Models","summary":"  Synthetic data from generative models emerges as the privacy-preserving\ndata-sharing solution. Such a synthetic data set shall resemble the original\ndata without revealing identifiable private information. The backbone\ntechnology of tabular synthesizers is rooted in image generative models,\nranging from Generative Adversarial Networks (GANs) to recent diffusion models.\nRecent prior work sheds light on the utility-privacy tradeoff on tabular data,\nrevealing and quantifying privacy risks on synthetic data. We first conduct an\nexhaustive empirical analysis, highlighting the utility-privacy tradeoff of\nfive state-of-the-art tabular synthesizers, against eight privacy attacks, with\na special focus on membership inference attacks. Motivated by the observation\nof high data quality but also high privacy risk in tabular diffusion, we\npropose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which\nis composed of an autoencoder network to encode the tabular data and a latent\ndiffusion model to synthesize the latent tables. Following the emerging f-DP\nframework, we apply DP-SGD to train the auto-encoder in combination with batch\nclipping and use the separation value as the privacy metric to better capture\nthe privacy gain from DP algorithms. Our empirical evaluation demonstrates that\nDP-TLDM is capable of achieving a meaningful theoretical privacy guarantee\nwhile also significantly enhancing the utility of synthetic data. Specifically,\ncompared to other DP-protected tabular generative models, DP-TLDM improves the\nsynthetic quality by an average of 35% in data resemblance, 15% in the utility\nfor downstream tasks, and 50% in data discriminability, all while preserving a\ncomparable level of privacy risk.\n","authors":["Chaoyi Zhu","Jiayi Tang","Hans Brouwer","Juan F. Pérez","Marten van Dijk","Lydia Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2403.07842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18679v3","updated":"2024-03-12T17:26:53Z","published":"2024-02-28T19:49:55Z","title":"Data Interpreter: An LLM Agent For Data Science","summary":"  Large Language Model (LLM)-based agents have demonstrated remarkable\neffectiveness. However, their performance can be compromised in data science\nscenarios that require real-time data adjustment, expertise in optimization due\nto complex dependencies among various tasks, and the ability to identify\nlogical errors for precise reasoning. In this study, we introduce the Data\nInterpreter, a solution designed to solve with code that emphasizes three\npivotal techniques to augment problem-solving in data science: 1) dynamic\nplanning with hierarchical graph structures for real-time data adaptability;2)\ntool integration dynamically to enhance code proficiency during execution,\nenriching the requisite expertise;3) logical inconsistency identification in\nfeedback, and efficiency enhancement through experience recording. We evaluate\nthe Data Interpreter on various data science and real-world tasks. Compared to\nopen-source baselines, it demonstrated superior performance, exhibiting\nsignificant improvements in machine learning tasks, increasing from 0.86 to\n0.95. Additionally, it showed a 26% increase in the MATH dataset and a\nremarkable 112% improvement in open-ended tasks. The solution will be released\nat https://github.com/geekan/MetaGPT.\n","authors":["Sirui Hong","Yizhang Lin","Bang Liu","Bangbang Liu","Binhao Wu","Danyang Li","Jiaqi Chen","Jiayi Zhang","Jinlin Wang","Li Zhang","Lingyao Zhang","Min Yang","Mingchen Zhuge","Taicheng Guo","Tuo Zhou","Wei Tao","Wenyi Wang","Xiangru Tang","Xiangtao Lu","Xiawu Zheng","Xinbing Liang","Yaying Fei","Yuheng Cheng","Zongze Xu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2402.18679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13800v2","updated":"2024-03-12T17:25:35Z","published":"2021-11-27T02:54:30Z","title":"A Two-Stage Feature Selection Approach for Robust Evaluation of\n  Treatment Effects in High-Dimensional Observational Data","summary":"  A Randomized Control Trial (RCT) is considered as the gold standard for\nevaluating the effect of any intervention or treatment. However, its\nfeasibility is often hindered by ethical, economical, and legal considerations,\nmaking observational data a valuable alternative for drawing causal\nconclusions. Nevertheless, healthcare observational data presents a difficult\nchallenge due to its high dimensionality, requiring careful consideration to\nensure unbiased, reliable, and robust causal inferences. To overcome this\nchallenge, in this study, we propose a novel two-stage feature selection\ntechnique called, Outcome Adaptive Elastic Net (OAENet), explicitly designed\nfor making robust causal inference decisions using matching techniques. OAENet\noffers several key advantages over existing methods: superior performance on\ncorrelated and high-dimensional data compared to the existing methods and the\nability to select specific sets of variables (including confounders and\nvariables associated only with the outcome). This ensures robustness and\nfacilitates an unbiased estimate of the causal effect. Numerical experiments on\nsimulated data demonstrate that OAENet significantly outperforms\nstate-of-the-art methods by either producing a higher-quality estimate or a\ncomparable estimate in significantly less time. To illustrate the applicability\nof OAENet, we employ large-scale US healthcare data to estimate the effect of\nOpioid Use Disorder (OUD) on suicidal behavior. When compared to competing\nmethods, OAENet closely aligns with existing literature on the relationship\nbetween OUD and suicidal behavior. Performance on both simulated and real-world\ndata highlights that OAENet notably enhances the accuracy of estimating\ntreatment effects or evaluating policy decision-making with causal inference.\n","authors":["Md Saiful Islam","Sahil Shikalgar","Md. Noor-E-Alam"],"pdf_url":"https://arxiv.org/pdf/2111.13800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02058v3","updated":"2024-03-12T17:23:55Z","published":"2023-11-03T17:38:35Z","title":"LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery","summary":"  We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.\n","authors":["Weikang Wan","Yifeng Zhu","Rutav Shah","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.02058v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.07822v1","updated":"2024-03-12T17:03:07Z","published":"2024-03-12T17:03:07Z","title":"Fusing Climate Data Products using a Spatially Varying Autoencoder","summary":"  Autoencoders are powerful machine learning models used to compress\ninformation from multiple data sources. However, autoencoders, like all\nartificial neural networks, are often unidentifiable and uninterpretable. This\nresearch focuses on creating an identifiable and interpretable autoencoder that\ncan be used to meld and combine climate data products. The proposed autoencoder\nutilizes a Bayesian statistical framework, allowing for probabilistic\ninterpretations while also varying spatially to capture useful spatial patterns\nacross the various data products. Constraints are placed on the autoencoder as\nit learns patterns in the data, creating an interpretable consensus that\nincludes the important features from each input. We demonstrate the utility of\nthe autoencoder by combining information from multiple precipitation products\nin High Mountain Asia.\n","authors":["Jacob A. Johnson","Matthew J. Heaton","William F. Christensen","Lynsie R. Warr","Summer B. Rupper"],"pdf_url":"https://arxiv.org/pdf/2403.07822v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.02129v3","updated":"2024-03-12T16:58:53Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07818v1","updated":"2024-03-12T16:57:56Z","published":"2024-03-12T16:57:56Z","title":"Label Dropout: Improved Deep Learning Echocardiography Segmentation\n  Using Multiple Datasets With Domain Shift and Partial Labelling","summary":"  Echocardiography (echo) is the first imaging modality used when assessing\ncardiac function. The measurement of functional biomarkers from echo relies\nupon the segmentation of cardiac structures and deep learning models have been\nproposed to automate the segmentation process. However, in order to translate\nthese tools to widespread clinical use it is important that the segmentation\nmodels are robust to a wide variety of images (e.g. acquired from different\nscanners, by operators with different levels of expertise etc.). To achieve\nthis level of robustness it is necessary that the models are trained with\nmultiple diverse datasets. A significant challenge faced when training with\nmultiple diverse datasets is the variation in label presence, i.e. the combined\ndata are often partially-labelled. Adaptations of the cross entropy loss\nfunction have been proposed to deal with partially labelled data. In this paper\nwe show that training naively with such a loss function and multiple diverse\ndatasets can lead to a form of shortcut learning, where the model associates\nlabel presence with domain characteristics, leading to a drop in performance.\nTo address this problem, we propose a novel label dropout scheme to break the\nlink between domain characteristics and the presence or absence of labels. We\ndemonstrate that label dropout improves echo segmentation Dice score by 62% and\n25% on two cardiac structures when training using multiple diverse partially\nlabelled datasets.\n","authors":["Iman Islam","Esther Puyol-Antón","Bram Ruijsink","Andrew J. Reader","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2403.07818v1.pdf","comment":"10 pages, 5 figures, submitted to MICCAI conference"},{"id":"http://arxiv.org/abs/2403.06279v2","updated":"2024-03-12T16:54:34Z","published":"2024-03-10T18:13:22Z","title":"Fine-tuning of diffusion models via stochastic control: entropy\n  regularization and beyond","summary":"  This paper aims to develop and provide a rigorous treatment to the problem of\nentropy regularized fine-tuning in the context of continuous-time diffusion\nmodels, which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024).\nThe idea is to use stochastic control for sample generation, where the entropy\nregularizer is introduced to mitigate reward collapse. We also show how the\nanalysis can be extended to fine-tuning involving a general $f$-divergence\nregularizer.\n","authors":["Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06279v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2403.07815v1","updated":"2024-03-12T16:53:54Z","published":"2024-03-12T16:53:54Z","title":"Chronos: Learning the Language of Time Series","summary":"  We introduce Chronos, a simple yet effective framework for pretrained\nprobabilistic time series models. Chronos tokenizes time series values using\nscaling and quantization into a fixed vocabulary and trains existing\ntransformer-based language model architectures on these tokenized time series\nvia the cross-entropy loss. We pretrained Chronos models based on the T5 family\n(ranging from 20M to 710M parameters) on a large collection of publicly\navailable datasets, complemented by a synthetic dataset that we generated via\nGaussian processes to improve generalization. In a comprehensive benchmark\nconsisting of 42 datasets, and comprising both classical local models and deep\nlearning methods, we show that Chronos models: (a) significantly outperform\nother methods on datasets that were part of the training corpus; and (b) have\ncomparable and occasionally superior zero-shot performance on new datasets,\nrelative to methods that were trained specifically on them. Our results\ndemonstrate that Chronos models can leverage time series data from diverse\ndomains to improve zero-shot accuracy on unseen forecasting tasks, positioning\npretrained models as a viable tool to greatly simplify forecasting pipelines.\n","authors":["Abdul Fatir Ansari","Lorenzo Stella","Caner Turkmen","Xiyuan Zhang","Pedro Mercado","Huibin Shen","Oleksandr Shchur","Syama Sundar Rangapuram","Sebastian Pineda Arango","Shubham Kapoor","Jasper Zschiegner","Danielle C. Maddix","Michael W. Mahoney","Kari Torkkola","Andrew Gordon Wilson","Michael Bohlke-Schneider","Yuyang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07815v1.pdf","comment":"Inference code and model checkpoints available at\n  https://github.com/amazon-science/chronos-forecasting"},{"id":"http://arxiv.org/abs/2403.07809v1","updated":"2024-03-12T16:46:54Z","published":"2024-03-12T16:46:54Z","title":"pyvene: A Library for Understanding and Improving PyTorch Models via\n  Interventions","summary":"  Interventions on model-internal states are fundamental operations in many\nareas of AI, including model editing, steering, robustness, and\ninterpretability. To facilitate such research, we introduce $\\textbf{pyvene}$,\nan open-source Python library that supports customizable interventions on a\nrange of different PyTorch modules. $\\textbf{pyvene}$ supports complex\nintervention schemes with an intuitive configuration format, and its\ninterventions can be static or include trainable parameters. We show how\n$\\textbf{pyvene}$ provides a unified and extensible framework for performing\ninterventions on neural models and sharing the intervened upon models with\nothers. We illustrate the power of the library via interpretability analyses\nusing causal abstraction and knowledge localization. We publish our library\nthrough Python Package Index (PyPI) and provide code, documentation, and\ntutorials at https://github.com/stanfordnlp/pyvene.\n","authors":["Zhengxuan Wu","Atticus Geiger","Aryaman Arora","Jing Huang","Zheng Wang","Noah D. Goodman","Christopher D. Manning","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2403.07809v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.07802v1","updated":"2024-03-12T16:41:31Z","published":"2024-03-12T16:41:31Z","title":"Boosting keyword spotting through on-device learnable user speech\n  characteristics","summary":"  Keyword spotting systems for always-on TinyML-constrained applications\nrequire on-site tuning to boost the accuracy of offline trained classifiers\nwhen deployed in unseen inference conditions. Adapting to the speech\npeculiarities of target users requires many in-domain samples, often\nunavailable in real-world scenarios. Furthermore, current on-device learning\ntechniques rely on computationally intensive and memory-hungry backbone update\nschemes, unfit for always-on, battery-powered devices. In this work, we propose\na novel on-device learning architecture, composed of a pretrained backbone and\na user-aware embedding learning the user's speech characteristics. The\nso-generated features are fused and used to classify the input utterance. For\ndomain shifts generated by unseen speakers, we measure error rate reductions of\nup to 19% from 30.1% to 24.3% based on the 35-class problem of the Google\nSpeech Commands dataset, through the inexpensive update of the user\nprojections. We moreover demonstrate the few-shot learning capabilities of our\nproposed architecture in sample- and class-scarce learning conditions. With\n23.7 kparameters and 1 MFLOP per epoch required for on-device training, our\nsystem is feasible for TinyML applications aimed at battery-powered\nmicrocontrollers.\n","authors":["Cristian Cioflan","Lukas Cavigelli","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.07802v1.pdf","comment":"5 pages, 3 tables, 2 figures. Accepted as a full paper by the tinyML\n  Research Symposium 2024"},{"id":"http://arxiv.org/abs/2402.18603v3","updated":"2024-03-12T16:35:25Z","published":"2024-02-28T08:29:42Z","title":"MMSR: Symbolic Regression is a Multimodal Task","summary":"  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n","authors":["Yanjie Li","Jingyi Liu","Weijun Li","Lina Yu","Min Wu","Wenqiang Li","Meilan Hao","Su Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2402.18603v3.pdf","comment":"12 page"},{"id":"http://arxiv.org/abs/2403.07797v1","updated":"2024-03-12T16:34:07Z","published":"2024-03-12T16:34:07Z","title":"Joint Selection: Adaptively Incorporating Public Information for Private\n  Synthetic Data","summary":"  Mechanisms for generating differentially private synthetic data based on\nmarginals and graphical models have been successful in a wide range of\nsettings. However, one limitation of these methods is their inability to\nincorporate public data. Initializing a data generating model by pre-training\non public data has shown to improve the quality of synthetic data, but this\ntechnique is not applicable when model structure is not determined a priori. We\ndevelop the mechanism jam-pgm, which expands the adaptive measurements\nframework to jointly select between measuring public data and private data.\nThis technique allows for public data to be included in a graphical-model-based\nmechanism. We show that jam-pgm is able to outperform both publicly assisted\nand non publicly assisted synthetic data generation mechanisms even when the\npublic data distribution is biased.\n","authors":["Miguel Fuentes","Brett Mullins","Ryan McKenna","Gerome Miklau","Daniel Sheldon"],"pdf_url":"https://arxiv.org/pdf/2403.07797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07788v1","updated":"2024-03-12T16:23:49Z","published":"2024-03-12T16:23:49Z","title":"DexCap: Scalable and Portable Mocap Data Collection System for Dexterous\n  Manipulation","summary":"  Imitation learning from human hand motion data presents a promising avenue\nfor imbuing robots with human-like dexterity in real-world manipulation tasks.\nDespite this potential, substantial challenges persist, particularly with the\nportability of existing hand motion capture (mocap) systems and the difficulty\nof translating mocap data into effective control policies. To tackle these\nissues, we introduce DexCap, a portable hand motion capture system, alongside\nDexIL, a novel imitation algorithm for training dexterous robot skills directly\nfrom human hand mocap data. DexCap offers precise, occlusion-resistant tracking\nof wrist and finger motions based on SLAM and electromagnetic field together\nwith 3D observations of the environment. Utilizing this rich dataset, DexIL\nemploys inverse kinematics and point cloud-based imitation learning to\nreplicate human actions with robot hands. Beyond learning from human motion,\nDexCap also offers an optional human-in-the-loop correction mechanism to refine\nand further improve robot performance. Through extensive evaluation across six\ndexterous manipulation tasks, our approach not only demonstrates superior\nperformance but also showcases the system's capability to effectively learn\nfrom in-the-wild mocap data, paving the way for future data collection methods\nfor dexterous manipulation. More details can be found at\nhttps://dex-cap.github.io\n","authors":["Chen Wang","Haochen Shi","Weizhuo Wang","Ruohan Zhang","Li Fei-Fei","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.15411v2","updated":"2024-03-12T16:16:00Z","published":"2021-12-31T12:39:04Z","title":"Disjoint Contrastive Regression Learning for Multi-Sourced Annotations","summary":"  Large-scale datasets are important for the development of deep learning\nmodels. Such datasets usually require a heavy workload of annotations, which\nare extremely time-consuming and expensive. To accelerate the annotation\nprocedure, multiple annotators may be employed to label different subsets of\nthe data. However, the inconsistency and bias among different annotators are\nharmful to the model training, especially for qualitative and subjective\ntasks.To address this challenge, in this paper, we propose a novel contrastive\nregression framework to address the disjoint annotations problem, where each\nsample is labeled by only one annotator and multiple annotators work on\ndisjoint subsets of the data. To take account of both the intra-annotator\nconsistency and inter-annotator inconsistency, two strategies are\nemployed.Firstly, a contrastive-based loss is applied to learn the relative\nranking among different samples of the same annotator, with the assumption that\nthe ranking of samples from the same annotator is unanimous. Secondly, we apply\nthe gradient reversal layer to learn robust representations that are invariant\nto different annotators. Experiments on the facial expression prediction task,\nas well as the image quality assessment task, verify the effectiveness of our\nproposed framework.\n","authors":["Xiaoqian Ruan","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2112.15411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07780v1","updated":"2024-03-12T16:08:47Z","published":"2024-03-12T16:08:47Z","title":"FairRR: Pre-Processing for Group Fairness through Randomized Response","summary":"  The increasing usage of machine learning models in consequential\ndecision-making processes has spurred research into the fairness of these\nsystems. While significant work has been done to study group fairness in the\nin-processing and post-processing setting, there has been little that\ntheoretically connects these results to the pre-processing domain. This paper\nproposes that achieving group fairness in downstream models can be formulated\nas finding the optimal design matrix in which to modify a response variable in\na Randomized Response framework. We show that measures of group fairness can be\ndirectly controlled for with optimal model utility, proposing a pre-processing\nalgorithm called FairRR that yields excellent downstream model utility and\nfairness.\n","authors":["Xianli Zeng","Joshua Ward","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14611v2","updated":"2024-03-12T16:07:12Z","published":"2022-11-26T16:43:40Z","title":"The Principles of Data-Centric AI (DCAI)","summary":"  Data is a crucial infrastructure to how artificial intelligence (AI) systems\nlearn. However, these systems to date have been largely model-centric, putting\na premium on the model at the expense of the data quality. Data quality issues\nbeset the performance of AI systems, particularly in downstream deployments and\nin real-world applications. Data-centric AI (DCAI) as an emerging concept\nbrings data, its quality and its dynamism to the forefront in considerations of\nAI systems through an iterative and systematic approach. As one of the first\noverviews, this article brings together data-centric perspectives and concepts\nto outline the foundations of DCAI. It specifically formulates six guiding\nprinciples for researchers and practitioners and gives direction for future\nadvancement of DCAI.\n","authors":["Mohammad Hossein Jarrahi","Ali Memariani","Shion Guha"],"pdf_url":"https://arxiv.org/pdf/2211.14611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12177v4","updated":"2024-03-12T16:04:23Z","published":"2024-02-19T14:33:24Z","title":"Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning","summary":"  Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.\n","authors":["Mingtian Zhang","Shawn Lan","Peter Hayes","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.12177v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03229v2","updated":"2024-03-12T16:03:03Z","published":"2024-03-04T12:36:31Z","title":"Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel\n  to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of\n  Right Ventricular Volume","summary":"  The right ventricular (RV) function deterioration strongly predicts clinical\noutcomes in numerous circumstances. To boost the clinical deployment of\nensemble regression methods that quantify RV volumes using tabular data from\nthe widely available two-dimensional echocardiography (2DE), we propose to\ncomplement the volume predictions with uncertainty scores. To this end, we\nemploy an instance-based method which uses the learned tree structure to\nidentify the nearest training samples to a target instance and then uses a\nnumber of distribution types to more flexibly model the output. The\nprobabilistic and point-prediction performances of the proposed framework are\nevaluated on a relatively small-scale dataset, comprising 100 end-diastolic and\nend-systolic RV volumes. The reference values for point performance were\nobtained from MRI. The results demonstrate that our flexible approach yields\nimproved probabilistic and point performances over other state-of-the-art\nmethods. The appropriateness of the proposed framework is showcased by\nproviding exemplar cases. The estimated uncertainty embodies both aleatoric and\nepistemic types. This work aligns with trustworthy artificial intelligence\nsince it can be used to enhance the decision-making process and reduce risks.\nThe feature importance scores of our framework can be exploited to reduce the\nnumber of required 2DE views which could enhance the proposed pipeline's\nclinical application.\n","authors":["Tuan A. Bohoran","Polydoros N. Kampaktsis","Laura McLaughlin","Jay Leb","Gerry P. McCann","Archontis Giannakidis"],"pdf_url":"https://arxiv.org/pdf/2403.03229v2.pdf","comment":"In the Proceedings of the 16th International Conference of Machine\n  Vision (ICMV 2023), November 15-18, Yerevan, Armenia"},{"id":"http://arxiv.org/abs/2403.05385v3","updated":"2024-03-12T16:01:02Z","published":"2024-03-08T15:30:58Z","title":"Switching the Loss Reduces the Cost in Batch Reinforcement Learning","summary":"  We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch\nreinforcement learning (RL). We show that the number of samples needed to learn\na near-optimal policy with FQI-LOG scales with the accumulated cost of the\noptimal policy, which is zero in problems where acting optimally achieves the\ngoal and incurs no cost. In doing so, we provide a general framework for\nproving $\\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal\nachievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses\nfewer samples than FQI trained with squared loss on problems where the optimal\npolicy reliably achieves the goal.\n","authors":["Alex Ayoub","Kaiwen Wang","Vincent Liu","Samuel Robertson","James McInerney","Dawen Liang","Nathan Kallus","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2403.05385v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11401v3","updated":"2024-03-12T15:56:41Z","published":"2023-10-17T17:10:56Z","title":"Enhancing Group Fairness in Online Settings Using Oblique Decision\n  Forests","summary":"  Fairness, especially group fairness, is an important consideration in the\ncontext of machine learning systems. The most commonly adopted group\nfairness-enhancing techniques are in-processing methods that rely on a mixture\nof a fairness objective (e.g., demographic parity) and a task-specific\nobjective (e.g., cross-entropy) during the training process. However, when data\narrives in an online fashion -- one instance at a time -- optimizing such\nfairness objectives poses several challenges. In particular, group fairness\nobjectives are defined using expectations of predictions across different\ndemographic groups. In the online setting, where the algorithm has access to a\nsingle instance at a time, estimating the group fairness objective requires\nadditional storage and significantly more computation (e.g., forward/backward\npasses) than the task-specific objective at every time step. In this paper, we\npropose Aranyani, an ensemble of oblique decision trees, to make fair decisions\nin online settings. The hierarchical tree structure of Aranyani enables\nparameter isolation and allows us to efficiently compute the fairness gradients\nusing aggregate statistics of previous decisions, eliminating the need for\nadditional storage and forward/backward passes. We also present an efficient\nframework to train Aranyani and theoretically analyze several of its\nproperties. We conduct empirical evaluations on 5 publicly available benchmarks\n(including vision and language datasets) to show that Aranyani achieves a\nbetter accuracy-fairness trade-off compared to baseline approaches.\n","authors":["Somnath Basu Roy Chowdhury","Nicholas Monath","Ahmad Beirami","Rahul Kidambi","Avinava Dubey","Amr Ahmed","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2310.11401v3.pdf","comment":"ICLR 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2401.08961v3","updated":"2024-03-12T15:55:52Z","published":"2024-01-17T04:20:26Z","title":"Cascading Reinforcement Learning","summary":"  Cascading bandits have gained popularity in recent years due to their\napplicability to recommendation systems and online advertising. In the\ncascading bandit model, at each timestep, an agent recommends an ordered subset\nof items (called an item list) from a pool of items, each associated with an\nunknown attraction probability. Then, the user examines the list, and clicks\nthe first attractive item (if any), and after that, the agent receives a\nreward. The goal of the agent is to maximize the expected cumulative reward.\nHowever, the prior literature on cascading bandits ignores the influences of\nuser states (e.g., historical behaviors) on recommendations and the change of\nstates as the session proceeds. Motivated by this fact, we propose a\ngeneralized cascading RL framework, which considers the impact of user states\nand state transition into decisions. In cascading RL, we need to select items\nnot only with large attraction probabilities but also leading to good successor\nstates. This imposes a huge computational challenge due to the combinatorial\naction space. To tackle this challenge, we delve into the properties of value\nfunctions, and design an oracle BestPerm to efficiently find the optimal item\nlist. Equipped with BestPerm, we develop two algorithms CascadingVI and\nCascadingBPI, which are both computationally-efficient and sample-efficient,\nand provide near-optimal regret and sample complexity guarantees. Furthermore,\nwe present experiments to show the improved computational and sample\nefficiencies of our algorithms compared to straightforward adaptations of\nexisting RL algorithms in practice.\n","authors":["Yihan Du","R. Srikant","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2401.08961v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07767v1","updated":"2024-03-12T15:54:32Z","published":"2024-03-12T15:54:32Z","title":"Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech\n  Recognition Datasets","summary":"  Paralinguistic traits like cognitive load and emotion are increasingly\nrecognized as pivotal areas in speech recognition research, often examined\nthrough specialized datasets like CLSE and IEMOCAP. However, the integrity of\nthese datasets is seldom scrutinized for text-dependency. This paper critically\nevaluates the prevalent assumption that machine learning models trained on such\ndatasets genuinely learn to identify paralinguistic traits, rather than merely\ncapturing lexical features. By examining the lexical overlap in these datasets\nand testing the performance of machine learning models, we expose significant\ntext-dependency in trait-labeling. Our results suggest that some machine\nlearning models, especially large pre-trained models like HuBERT, might\ninadvertently focus on lexical characteristics rather than the intended\nparalinguistic features. The study serves as a call to action for the research\ncommunity to reevaluate the reliability of existing datasets and methodologies,\nensuring that machine learning models genuinely learn what they are designed to\nrecognize.\n","authors":["Jan Pešán","Santosh Kesiraju","Lukáš Burget","Jan ''Honza'' Černocký"],"pdf_url":"https://arxiv.org/pdf/2403.07767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10691v3","updated":"2024-03-12T15:53:06Z","published":"2023-09-19T15:25:42Z","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\n  Feedback","summary":"  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n","authors":["Xingyao Wang","Zihan Wang","Jiateng Liu","Yangyi Chen","Lifan Yuan","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.10691v3.pdf","comment":"ICLR 2024. Code is available on our project website:\n  https://xingyaoww.github.io/mint-bench"},{"id":"http://arxiv.org/abs/2310.07990v2","updated":"2024-03-12T15:34:13Z","published":"2023-10-12T02:34:56Z","title":"Multi-View Variational Autoencoder for Missing Value Imputation in\n  Untargeted Metabolomics","summary":"  Background: Missing data is a common challenge in mass spectrometry-based\nmetabolomics, which can lead to biased and incomplete analyses. The integration\nof whole-genome sequencing (WGS) data with metabolomics data has emerged as a\npromising approach to enhance the accuracy of data imputation in metabolomics\nstudies. Method: In this study, we propose a novel method that leverages the\ninformation from WGS data and reference metabolites to impute unknown\nmetabolites. Our approach utilizes a multi-view variational autoencoder to\njointly model the burden score, polygenetic risk score (PGS), and linkage\ndisequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature\nextraction and missing metabolomics data imputation. By learning the latent\nrepresentations of both omics data, our method can effectively impute missing\nmetabolomics values based on genomic information. Results: We evaluate the\nperformance of our method on empirical metabolomics datasets with missing\nvalues and demonstrate its superiority compared to conventional imputation\ntechniques. Using 35 template metabolites derived burden scores, PGS and\nLD-pruned SNPs, the proposed methods achieved R^2-scores > 0.01 for 71.55% of\nmetabolites. Conclusion: The integration of WGS data in metabolomics imputation\nnot only improves data completeness but also enhances downstream analyses,\npaving the way for more comprehensive and accurate investigations of metabolic\npathways and disease associations. Our findings offer valuable insights into\nthe potential benefits of utilizing WGS data for metabolomics data imputation\nand underscore the importance of leveraging multi-modal data integration in\nprecision medicine research.\n","authors":["Chen Zhao","Kuan-Jui Su","Chong Wu","Xuewei Cao","Qiuying Sha","Wu Li","Zhe Luo","Tian Qin","Chuan Qiu","Lan Juan Zhao","Anqi Liu","Lindong Jiang","Xiao Zhang","Hui Shen","Weihua Zhou","Hong-Wen Deng"],"pdf_url":"https://arxiv.org/pdf/2310.07990v2.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.11851v2","updated":"2024-03-12T15:31:01Z","published":"2022-12-22T16:35:42Z","title":"StoRM: A Diffusion-based Stochastic Regeneration Model for Speech\n  Enhancement and Dereverberation","summary":"  Diffusion models have shown a great ability at bridging the performance gap\nbetween predictive and generative approaches for speech enhancement. We have\nshown that they may even outperform their predictive counterparts for\nnon-additive corruption types or when they are evaluated on mismatched\nconditions. However, diffusion models suffer from a high computational burden,\nmainly as they require to run a neural network for each reverse diffusion step,\nwhereas predictive approaches only require one pass. As diffusion models are\ngenerative approaches they may also produce vocalizing and breathing artifacts\nin adverse conditions. In comparison, in such difficult scenarios, predictive\nmodels typically do not produce such artifacts but tend to distort the target\nspeech instead, thereby degrading the speech quality. In this work, we present\na stochastic regeneration approach where an estimate given by a predictive\nmodel is provided as a guide for further diffusion. We show that the proposed\napproach uses the predictive model to remove the vocalizing and breathing\nartifacts while producing very high quality samples thanks to the diffusion\nmodel, even in adverse conditions. We further show that this approach enables\nto use lighter sampling schemes with fewer diffusion steps without sacrificing\nquality, thus lifting the computational burden by an order of magnitude. Source\ncode and audio examples are available online (https://uhh.de/inf-sp-storm).\n","authors":["Jean-Marie Lemercier","Julius Richter","Simon Welker","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2212.11851v2.pdf","comment":"Published in IEEE/ACM Transactions on Audio, Speech and Language\n  Processing, 2023"},{"id":"http://arxiv.org/abs/2403.07745v1","updated":"2024-03-12T15:28:21Z","published":"2024-03-12T15:28:21Z","title":"Probabilistic Easy Variational Causal Effect","summary":"  Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one\nhand, for the case that $X$ and $Z$ are continuous, by using the ideas from the\ntotal variation and the flux of $g$, we develop a point of view in causal\ninference capable of dealing with a broad domain of causal problems. Indeed, we\nfocus on a function, called Probabilistic Easy Variational Causal Effect\n(PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect\nto continuously and interventionally changing the values of $X$ while keeping\nthe value of $Z$ constant. PEACE is a function of $d\\ge 0$, which is a degree\nmanaging the strengths of probability density values $f(x|z)$. On the other\nhand, we generalize the above idea for the discrete case and show its\ncompatibility with the continuous case. Further, we investigate some properties\nof PEACE using measure theoretical concepts. Furthermore, we provide some\nidentifiability criteria and several examples showing the generic capability of\nPEACE. We note that PEACE can deal with the causal problems for which\nmicro-level or just macro-level changes in the value of the input variables are\nimportant. Finally, PEACE is stable under small changes in $\\partial\ng_{in}/\\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is\nobtained from $g$ by removing all functional relationships defining $X$ and\n$Z$.\n","authors":["Usef Faghihi","Amir Saki"],"pdf_url":"https://arxiv.org/pdf/2403.07745v1.pdf","comment":"45 pages, 9 Figures"},{"id":"http://arxiv.org/abs/2310.04349v2","updated":"2024-03-12T15:22:05Z","published":"2023-10-06T16:16:00Z","title":"Toward a Plug-and-Play Vision-Based Grasping Module for Robotics","summary":"  Despite recent advancements in AI for robotics, grasping remains a partially\nsolved challenge, hindered by the lack of benchmarks and reproducibility\nconstraints. This paper introduces a vision-based grasping framework that can\neasily be transferred across multiple manipulators. Leveraging\nQuality-Diversity (QD) algorithms, the framework generates diverse repertoires\nof open-loop grasping trajectories, enhancing adaptability while maintaining a\ndiversity of grasps. This framework addresses two main issues: the lack of an\noff-the-shelf vision module for detecting object pose and the generalization of\nQD trajectories to the whole robot operational space. The proposed solution\ncombines multiple vision modules for 6DoF object detection and tracking while\nrigidly transforming QD-generated trajectories into the object frame.\nExperiments on a Franka Research 3 arm and a UR5 arm with a SIH Schunk hand\ndemonstrate comparable performance when the real scene aligns with the\nsimulation used for grasp generation. This work represents a significant stride\ntoward building a reliable vision-based grasping module transferable to new\nplatforms, while being adaptable to diverse scenarios without further training\niterations.\n","authors":["François Hélénon","Johann Huber","Faïz Ben Amar","Stéphane Doncieux"],"pdf_url":"https://arxiv.org/pdf/2310.04349v2.pdf","comment":"6 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.07743v1","updated":"2024-03-12T15:22:05Z","published":"2024-03-12T15:22:05Z","title":"Equipping Computational Pathology Systems with Artifact Processing\n  Pipelines: A Showcase for Computation and Performance Trade-offs","summary":"  Histopathology is a gold standard for cancer diagnosis under a microscopic\nexamination. However, histological tissue processing procedures result in\nartifacts, which are ultimately transferred to the digitized version of glass\nslides, known as whole slide images (WSIs). Artifacts are diagnostically\nirrelevant areas and may result in wrong deep learning (DL) algorithms\npredictions. Therefore, detecting and excluding artifacts in the computational\npathology (CPATH) system is essential for reliable automated diagnosis. In this\npaper, we propose a mixture of experts (MoE) scheme for detecting five notable\nartifacts, including damaged tissue, blur, folded tissue, air bubbles, and\nhistologically irrelevant blood from WSIs. First, we train independent binary\nDL models as experts to capture particular artifact morphology. Then, we\nensemble their predictions using a fusion mechanism. We apply probabilistic\nthresholding over the final probability distribution to improve the sensitivity\nof the MoE. We developed DL pipelines using two MoEs and two multiclass models\nof state-of-the-art deep convolutional neural networks (DCNNs) and vision\ntransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed\nsimpler multiclass models and were tested on datasets from different hospitals\nand cancer types, where MoE using DCNNs yielded the best results. The proposed\nMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining\nless computational cost for inference than MoE using ViTs. This best\nperformance of MoEs comes with relatively higher computational trade-offs than\nmulticlass models. The proposed artifact detection pipeline will not only\nensure reliable CPATH predictions but may also provide quality control.\n","authors":["Neel Kanwal","Farbod Khoraminia","Umay Kiraz","Andres Mosquera-Zamudio","Carlos Monteagudo","Emiel A. M. Janssen","Tahlita C. M. Zuiverloon","Chunmig Rong","Kjersti Engan"],"pdf_url":"https://arxiv.org/pdf/2403.07743v1.pdf","comment":"Submitted to BMC Medical Informatics and Decision Making Journal"},{"id":"http://arxiv.org/abs/2403.07735v1","updated":"2024-03-12T15:13:21Z","published":"2024-03-12T15:13:21Z","title":"The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels","summary":"  Kernel techniques are among the most influential approaches in data science\nand statistics. Under mild conditions, the reproducing kernel Hilbert space\nassociated to a kernel is capable of encoding the independence of $M\\ge 2$\nrandom variables. Probably the most widespread independence measure relying on\nkernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also\nreferred to as distance covariance in the statistics literature). Despite\nvarious existing HSIC estimators designed since its introduction close to two\ndecades ago, the fundamental question of the rate at which HSIC can be\nestimated is still open. In this work, we prove that the minimax optimal rate\nof HSIC estimation on $\\mathbb R^d$ for Borel measures containing the Gaussians\nwith continuous bounded translation-invariant characteristic kernels is\n$\\mathcal O\\!\\left(n^{-1/2}\\right)$. Specifically, our result implies the\noptimality in the minimax sense of many of the most-frequently used estimators\n(including the U-statistic, the V-statistic, and the Nystr\\\"om-based one) on\n$\\mathbb R^d$.\n","authors":["Florian Kalinke","Zoltan Szabo"],"pdf_url":"https://arxiv.org/pdf/2403.07735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07728v1","updated":"2024-03-12T15:07:20Z","published":"2024-03-12T15:07:20Z","title":"CAS: A General Algorithm for Online Selective Conformal Prediction with\n  FCR Control","summary":"  We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) to\nmeasure the averaged miscoverage error. We develop a general framework named\nCAS (Calibration after Adaptive Selection) that can wrap around any prediction\nmodel and online selection rule to output post-selection prediction intervals.\nIf the current individual is selected, we first perform an adaptive selection\non historical data to construct a calibration set, then output a conformal\nprediction interval for the unobserved label. We provide tractable\nconstructions for the calibration set for popular online selection rules. We\nproved that CAS can achieve an exact selection-conditional coverage guarantee\nin the finite-sample and distribution-free regimes. For the decision-driven\nselection rule, including most online multiple-testing procedures, CAS can\nexactly control the real-time FCR below the target level without any\ndistributional assumptions. For the online selection with symmetric thresholds,\nwe establish the error bound for the control gap of FCR under mild\ndistributional assumptions. To account for the distribution shift in online\ndata, we also embed CAS into some recent dynamic conformal prediction methods\nand examine the long-run FCR control. Numerical results on both synthetic and\nreal data corroborate that CAS can effectively control FCR around the target\nlevel and yield more narrowed prediction intervals over existing baselines\nacross various settings.\n","authors":["Yajie Bao","Yuyang Huo","Haojie Ren","Changliang Zou"],"pdf_url":"https://arxiv.org/pdf/2403.07728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05525v3","updated":"2024-03-12T15:05:23Z","published":"2023-08-10T12:06:03Z","title":"Robustifying Point Cloud Networks by Refocusing","summary":"  The ability to cope with out-of-distribution (OOD) corruptions and\nadversarial attacks is crucial in real-world safety-demanding applications. In\nthis study, we develop a general mechanism to increase neural network\nrobustness based on focus analysis.\n  Recent studies have revealed the phenomenon of \\textit{Overfocusing}, which\nleads to a performance drop. When the network is primarily influenced by small\ninput regions, it becomes less robust and prone to misclassify under noise and\ncorruptions.\n  However, quantifying overfocusing is still vague and lacks clear definitions.\nHere, we provide a mathematical definition of \\textbf{focus},\n\\textbf{overfocusing} and \\textbf{underfocusing}. The notions are general, but\nin this study, we specifically investigate the case of 3D point clouds.\n  We observe that corrupted sets result in a biased focus distribution compared\nto the clean training set.\n  We show that as focus distribution deviates from the one learned in the\ntraining phase - classification performance deteriorates.\n  We thus propose a parameter-free \\textbf{refocusing} algorithm that aims to\nunify all corruptions under the same distribution.\n  We validate our findings on a 3D zero-shot classification task, achieving\nSOTA in robust 3D classification on ModelNet-C dataset, and in adversarial\ndefense against Shape-Invariant attack. Code is available in:\nhttps://github.com/yossilevii100/refocusing.\n","authors":["Meir Yossef Levi","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2308.05525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07724v1","updated":"2024-03-12T15:01:27Z","published":"2024-03-12T15:01:27Z","title":"Balancing Fairness and Accuracy in Data-Restricted Binary Classification","summary":"  Applications that deal with sensitive information may have restrictions\nplaced on the data available to a machine learning (ML) classifier. For\nexample, in some applications, a classifier may not have direct access to\nsensitive attributes, affecting its ability to produce accurate and fair\ndecisions. This paper proposes a framework that models the trade-off between\naccuracy and fairness under four practical scenarios that dictate the type of\ndata available for analysis. Prior works examine this trade-off by analyzing\nthe outputs of a scoring function that has been trained to implicitly learn the\nunderlying distribution of the feature vector, class label, and sensitive\nattribute of a dataset. In contrast, our framework directly analyzes the\nbehavior of the optimal Bayesian classifier on this underlying distribution by\nconstructing a discrete approximation it from the dataset itself. This approach\nenables us to formulate multiple convex optimization problems, which allow us\nto answer the question: How is the accuracy of a Bayesian classifier affected\nin different data restricting scenarios when constrained to be fair? Analysis\nis performed on a set of fairness definitions that include group and individual\nfairness. Experiments on three datasets demonstrate the utility of the proposed\nframework as a tool for quantifying the trade-offs among different fairness\nnotions and their distributional dependencies.\n","authors":["Zachary McBride Lazri","Danial Dervovic","Antigoni Polychroniadou","Ivan Brugere","Dana Dachman-Soled","Min Wu"],"pdf_url":"https://arxiv.org/pdf/2403.07724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07723v1","updated":"2024-03-12T15:01:17Z","published":"2024-03-12T15:01:17Z","title":"On the Last-Iterate Convergence of Shuffling Gradient Methods","summary":"  Shuffling gradient methods, which are also known as stochastic gradient\ndescent (SGD) without replacement, are widely implemented in practice,\nparticularly including three popular algorithms: Random Reshuffle (RR), Shuffle\nOnce (SO), and Incremental Gradient (IG). Compared to the empirical success,\nthe theoretical guarantee of shuffling gradient methods was not\nwell-understanding for a long time. Until recently, the convergence rates had\njust been established for the average iterate for convex functions and the last\niterate for strongly convex problems (using squared distance as the metric).\nHowever, when using the function value gap as the convergence criterion,\nexisting theories cannot interpret the good performance of the last iterate in\ndifferent settings (e.g., constrained optimization). To bridge this gap between\npractice and theory, we prove last-iterate convergence rates for shuffling\ngradient methods with respect to the objective value even without strong\nconvexity. Our new results either (nearly) match the existing last-iterate\nlower bounds or are as fast as the previous best upper bounds for the average\niterate.\n","authors":["Zijian Liu","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.07723v1.pdf","comment":"In submission (comments welcome)"},{"id":"http://arxiv.org/abs/2403.07718v1","updated":"2024-03-12T14:58:45Z","published":"2024-03-12T14:58:45Z","title":"WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work\n  Tasks?","summary":"  We study the use of large language model-based agents for interacting with\nsoftware via web browsers. Unlike prior work, we focus on measuring the agents'\nability to perform tasks that span the typical daily work of knowledge workers\nutilizing enterprise software systems. To this end, we propose WorkArena, a\nremote-hosted benchmark of 29 tasks based on the widely-used ServiceNow\nplatform. We also introduce BrowserGym, an environment for the design and\nevaluation of such agents, offering a rich set of actions as well as multimodal\nobservations. Our empirical evaluation reveals that while current agents show\npromise on WorkArena, there remains a considerable gap towards achieving full\ntask automation. Notably, our analysis uncovers a significant performance\ndisparity between open and closed-source LLMs, highlighting a critical area for\nfuture exploration and development in the field.\n","authors":["Alexandre Drouin","Maxime Gasse","Massimo Caccia","Issam H. Laradji","Manuel Del Verme","Tom Marty","Léo Boisvert","Megh Thakkar","Quentin Cappart","David Vazquez","Nicolas Chapados","Alexandre Lacoste"],"pdf_url":"https://arxiv.org/pdf/2403.07718v1.pdf","comment":"27 pages, 10 figures, preprint"},{"id":"http://arxiv.org/abs/2403.07706v1","updated":"2024-03-12T14:51:23Z","published":"2024-03-12T14:51:23Z","title":"Fast and Simple Explainability for Point Cloud Networks","summary":"  We propose a fast and simple explainable AI (XAI) method for point cloud\ndata. It computes pointwise importance with respect to a trained network\ndownstream task. This allows better understanding of the network properties,\nwhich is imperative for safety-critical applications. In addition to debugging\nand visualization, our low computational complexity facilitates online feedback\nto the network at inference. This can be used to reduce uncertainty and to\nincrease robustness. In this work, we introduce \\emph{Feature Based\nInterpretability} (FBI), where we compute the features' norm, per point, before\nthe bottleneck. We analyze the use of gradients and post- and pre-bottleneck\nstrategies, showing pre-bottleneck is preferred, in terms of smoothness and\nranking. We obtain at least three orders of magnitude speedup, compared to\ncurrent XAI methods, thus, scalable for big point clouds or large-scale\narchitectures. Our approach achieves SOTA results, in terms of classification\nexplainability. We demonstrate how the proposed measure is helpful in analyzing\nand characterizing various aspects of 3D learning, such as rotation invariance,\nrobustness to out-of-distribution (OOD) outliers or domain shift and dataset\nbias.\n","authors":["Meir Yossef Levi","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2403.07706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07704v1","updated":"2024-03-12T14:49:19Z","published":"2024-03-12T14:49:19Z","title":"Symmetric Q-learning: Reducing Skewness of Bellman Error in Online\n  Reinforcement Learning","summary":"  In deep reinforcement learning, estimating the value function to evaluate the\nquality of states and actions is essential. The value function is often trained\nusing the least squares method, which implicitly assumes a Gaussian error\ndistribution. However, a recent study suggested that the error distribution for\ntraining the value function is often skewed because of the properties of the\nBellman operator, and violates the implicit assumption of normal error\ndistribution in the least squares method. To address this, we proposed a method\ncalled Symmetric Q-learning, in which the synthetic noise generated from a\nzero-mean distribution is added to the target values to generate a Gaussian\nerror distribution. We evaluated the proposed method on continuous control\nbenchmark tasks in MuJoCo. It improved the sample efficiency of a\nstate-of-the-art reinforcement learning method by reducing the skewness of the\nerror distribution.\n","authors":["Motoki Omura","Takayuki Osa","Yusuke Mukuta","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2403.07704v1.pdf","comment":"Accepted at AAAI 2024: The 38th Annual AAAI Conference on Artificial\n  Intelligence (Main Tech Track)"},{"id":"http://arxiv.org/abs/2112.12989v3","updated":"2024-03-12T14:47:47Z","published":"2021-12-24T08:17:18Z","title":"Domain-Aware Continual Zero-Shot Learning","summary":"  Modern visual systems have a wide range of potential applications in vision\ntasks for natural science research, such as aiding in species discovery,\nmonitoring animals in the wild, and so on. However, real-world vision tasks may\nexperience changes in environmental conditions, leading to shifts in how\ncaptured images are presented. To address this issue, we introduce Domain-Aware\nContinual Zero-Shot Learning (DACZSL), a task to recognize images of unseen\ncategories in continuously changing domains. Accordingly, we propose a\nDomain-Invariant Network (DIN) to learn factorized features for shifting\ndomains and improved textual representation for unseen classes. DIN continually\nlearns a global shared network for domain-invariant and task-invariant\nfeatures, and per-task private networks for task-specific features.\nFurthermore, we enhance the dual network with class-wise learnable prompts to\nimprove class-level text representation, thereby improving zero-shot prediction\nof future unseen classes. To evaluate DACZSL, we introduce two benchmarks,\nDomainNet-CZSL and iWildCam-CZSL. Our results show that DIN significantly\noutperforms existing baselines by over 5% in harmonic accuracy and over 1% in\nbackward transfer and achieves a new SoTA.\n","authors":["Kai Yi","Paul Janson","Wenxuan Zhang","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2112.12989v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05482v2","updated":"2024-03-12T14:44:58Z","published":"2024-02-08T08:23:33Z","title":"A Non-Intrusive Neural Quality Assessment Model for Surface\n  Electromyography Signals","summary":"  In practical scenarios involving the measurement of surface electromyography\n(sEMG) in muscles, particularly those areas near the heart, one of the primary\nsources of contamination is the presence of electrocardiogram (ECG) signals. To\nassess the quality of real-world sEMG data more effectively, this study\nproposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG\nsignals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an\nend-to-end training strategy. Our experimental framework utilizes real-world\nsEMG and ECG data from two open-access databases, the Non-Invasive Adaptive\nProsthetics Database and the MIT-BIH Normal Sinus Rhythm Database,\nrespectively. The experimental results demonstrate the superiority of QASE-net\nover the previous assessment model, exhibiting significantly reduced prediction\nerrors and notably higher linear correlations with the ground truth. These\nfindings show the potential of QASE-net to substantially enhance the\nreliability and precision of sEMG quality assessment in practical applications.\n","authors":["Cho-Yuan Lee","Kuan-Chen Wang","Kai-Chun Liu","Xugang Lu","Ping-Cheng Yeh","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2402.05482v2.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.07688v1","updated":"2024-03-12T14:28:06Z","published":"2024-03-12T14:28:06Z","title":"Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of\n  Neurons","summary":"  When training deep neural networks, the phenomenon of $\\textit{dying\nneurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero\nduring training$\\unicode{x2013}$ has traditionally been viewed as undesirable,\nlinked with optimization challenges, and contributing to plasticity loss in\ncontinual learning scenarios. In this paper, we reassess this phenomenon,\nfocusing on sparsity and pruning. By systematically exploring the impact of\nvarious hyperparameter configurations on dying neurons, we unveil their\npotential to facilitate simple yet effective structured pruning algorithms. We\nintroduce $\\textit{Demon Pruning}$ (DemP), a method that controls the\nproliferation of dead neurons, dynamically leading to network sparsity.\nAchieved through a combination of noise injection on active units and a\none-cycled schedule regularization strategy, DemP stands out for its simplicity\nand broad applicability. Experiments on CIFAR10 and ImageNet datasets\ndemonstrate that DemP surpasses existing structured pruning techniques,\nshowcasing superior accuracy-sparsity tradeoffs and training speedups. These\nfindings suggest a novel perspective on dying neurons as a valuable resource\nfor efficient model compression and optimization.\n","authors":["Simon Dufort-Labbé","Pierluca D'Oro","Evgenii Nikishin","Razvan Pascanu","Pierre-Luc Bacon","Aristide Baratin"],"pdf_url":"https://arxiv.org/pdf/2403.07688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05881v2","updated":"2024-03-12T14:09:56Z","published":"2023-07-12T03:03:40Z","title":"tdCoxSNN: Time-Dependent Cox Survival Neural Network for Continuous-time\n  Dynamic Prediction","summary":"  The aim of dynamic prediction is to provide individualized risk predictions\nover time, which are updated as new data become available. In pursuit of\nconstructing a dynamic prediction model for a progressive eye disorder,\nage-related macular degeneration (AMD), we propose a time-dependent Cox\nsurvival neural network (tdCoxSNN) to predict its progression using\nlongitudinal fundus images. tdCoxSNN builds upon the time-dependent Cox model\nby utilizing a neural network to capture the non-linear effect of\ntime-dependent covariates on the survival outcome. Moreover, by concurrently\nintegrating a convolutional neural network (CNN) with the survival network,\ntdCoxSNN can directly take longitudinal images as input. We evaluate and\ncompare our proposed method with joint modeling and landmarking approaches\nthrough extensive simulations. We applied the proposed approach to two real\ndatasets. One is a large AMD study, the Age-Related Eye Disease Study (AREDS),\nin which more than 50,000 fundus images were captured over a period of 12 years\nfor more than 4,000 participants. Another is a public dataset of the primary\nbiliary cirrhosis (PBC) disease, where multiple lab tests were longitudinally\ncollected to predict the time-to-liver transplant. Our approach demonstrates\ncommendable predictive performance in both simulation studies and the analysis\nof the two real datasets.\n","authors":["Lang Zeng","Jipeng Zhang","Wei Chen","Ying Ding"],"pdf_url":"https://arxiv.org/pdf/2307.05881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14302v4","updated":"2024-03-12T14:02:49Z","published":"2022-11-25T18:58:28Z","title":"Neural DAEs: Constrained neural networks","summary":"  This article investigates the effect of explicitly adding auxiliary algebraic\ntrajectory information to neural networks for dynamical systems. We draw\ninspiration from the field of differential-algebraic equations and differential\nequations on manifolds and implement related methods in residual neural\nnetworks, despite some fundamental scenario differences. Constraint or\nauxiliary information effects are incorporated through stabilization as well as\nprojection methods, and we show when to use which method based on experiments\ninvolving simulations of multi-body pendulums and molecular dynamics scenarios.\nSeveral of our methods are easy to implement in existing code and have limited\nimpact on training performance while giving significant boosts in terms of\ninference.\n","authors":["Tue Boesen","Eldad Haber","Uri Michael Ascher"],"pdf_url":"https://arxiv.org/pdf/2211.14302v4.pdf","comment":"Extended the paper to PDEs, added a third experiment denoising a\n  vector field and updated the introduction to make the distinction between\n  this work and physics informed neural networks more clear"},{"id":"http://arxiv.org/abs/2403.07669v1","updated":"2024-03-12T14:00:50Z","published":"2024-03-12T14:00:50Z","title":"Machine Learning for Soccer Match Result Prediction","summary":"  Machine learning has become a common approach to predicting the outcomes of\nsoccer matches, and the body of literature in this domain has grown\nsubstantially in the past decade and a half. This chapter discusses available\ndatasets, the types of models and features, and ways of evaluating model\nperformance in this application domain. The aim of this chapter is to give a\nbroad overview of the current state and potential future developments in\nmachine learning for soccer match results prediction, as a resource for those\ninterested in conducting future studies in the area. Our main findings are that\nwhile gradient-boosted tree models such as CatBoost, applied to soccer-specific\nratings such as pi-ratings, are currently the best-performing models on\ndatasets containing only goals as the match features, there needs to be a more\nthorough comparison of the performance of deep learning models and Random\nForest on a range of datasets with different types of features. Furthermore,\nnew rating systems using both player- and team-level information and\nincorporating additional information from, e.g., spatiotemporal tracking and\nevent data, could be investigated further. Finally, the interpretability of\nmatch result prediction models needs to be enhanced for them to be more useful\nfor team management.\n","authors":["Rory Bunker","Calvin Yeung","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2403.07669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04778v2","updated":"2024-03-12T13:47:52Z","published":"2024-03-02T01:05:25Z","title":"An Efficient Difference-of-Convex Solver for Privacy Funnel","summary":"  We propose an efficient solver for the privacy funnel (PF) method, leveraging\nits difference-of-convex (DC) structure. The proposed DC separation results in\na closed-form update equation, which allows straightforward application to both\nknown and unknown distribution settings. For known distribution case, we prove\nthe convergence (local stationary points) of the proposed non-greedy solver,\nand empirically show that it outperforms the state-of-the-art approaches in\ncharacterizing the privacy-utility trade-off. The insights of our DC approach\napply to unknown distribution settings where labeled empirical samples are\navailable instead. Leveraging the insights, our alternating minimization solver\nsatisfies the fundamental Markov relation of PF in contrast to previous\nvariational inference-based solvers. Empirically, we evaluate the proposed\nsolver with MNIST and Fashion-MNIST datasets. Our results show that under a\ncomparable reconstruction quality, an adversary suffers from higher prediction\nerror from clustering our compressed codes than that with the compared methods.\nMost importantly, our solver is independent to private information in inference\nphase contrary to the baselines.\n","authors":["Teng-Hui Huang","Hesham El Gamal"],"pdf_url":"https://arxiv.org/pdf/2403.04778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07657v1","updated":"2024-03-12T13:47:50Z","published":"2024-03-12T13:47:50Z","title":"Scalable Spatiotemporal Prediction with Bayesian Neural Fields","summary":"  Spatiotemporal datasets, which consist of spatially-referenced time series,\nare ubiquitous in many scientific and business-intelligence applications, such\nas air pollution monitoring, disease tracking, and cloud-demand forecasting. As\nmodern datasets continue to increase in size and complexity, there is a growing\nneed for new statistical methods that are flexible enough to capture complex\nspatiotemporal dynamics and scalable enough to handle large prediction\nproblems. This work presents the Bayesian Neural Field (BayesNF), a\ndomain-general statistical model for inferring rich probability distributions\nover a spatiotemporal domain, which can be used for data-analysis tasks\nincluding forecasting, interpolation, and variography. BayesNF integrates a\nnovel deep neural network architecture for high-capacity function estimation\nwith hierarchical Bayesian inference for robust uncertainty quantification. By\ndefining the prior through a sequence of smooth differentiable transforms,\nposterior inference is conducted on large-scale data using variationally\nlearned surrogates trained via stochastic gradient descent. We evaluate BayesNF\nagainst prominent statistical and machine-learning baselines, showing\nconsiderable improvements on diverse prediction problems from climate and\npublic health datasets that contain tens to hundreds of thousands of\nmeasurements. The paper is accompanied with an open-source software package\n(https://github.com/google/bayesnf) that is easy-to-use and compatible with\nmodern GPU and TPU accelerators on the JAX machine learning platform.\n","authors":["Feras Saad","Jacob Burnim","Colin Carroll","Brian Patton","Urs Köster","Rif A. Saurous","Matthew Hoffman"],"pdf_url":"https://arxiv.org/pdf/2403.07657v1.pdf","comment":"22 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2307.00543v3","updated":"2024-03-12T13:44:55Z","published":"2023-07-02T11:23:33Z","title":"Defending Against Poisoning Attacks in Federated Learning with\n  Blockchain","summary":"  In the era of deep learning, federated learning (FL) presents a promising\napproach that allows multi-institutional data owners, or clients, to\ncollaboratively train machine learning models without compromising data\nprivacy. However, most existing FL approaches rely on a centralized server for\nglobal model aggregation, leading to a single point of failure. This makes the\nsystem vulnerable to malicious attacks when dealing with dishonest clients. In\nthis work, we address this problem by proposing a secure and reliable FL system\nbased on blockchain and distributed ledger technology. Our system incorporates\na peer-to-peer voting mechanism and a reward-and-slash mechanism, which are\npowered by on-chain smart contracts, to detect and deter malicious behaviors.\nBoth theoretical and empirical analyses are presented to demonstrate the\neffectiveness of the proposed approach, showing that our framework is robust\nagainst malicious client-side behaviors.\n","authors":["Nanqing Dong","Zhipeng Wang","Jiahao Sun","Michael Kampffmeyer","William Knottenbelt","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2307.00543v3.pdf","comment":"Accepted by IEEE Transactions on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2403.07652v1","updated":"2024-03-12T13:41:15Z","published":"2024-03-12T13:41:15Z","title":"Harder Tasks Need More Experts: Dynamic Routing in MoE Models","summary":"  In this paper, we introduce a novel dynamic expert selection framework for\nMixture of Experts (MoE) models, aiming to enhance computational efficiency and\nmodel performance by adjusting the number of activated experts based on input\ndifficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,\nwhich activates a predetermined number of experts regardless of the input's\ncomplexity, our method dynamically selects experts based on the confidence\nlevel in expert selection for each input. This allows for a more efficient\nutilization of computational resources, activating more experts for complex\ntasks requiring advanced reasoning and fewer for simpler tasks. Through\nextensive evaluations, our dynamic routing method demonstrates substantial\nimprovements over conventional Top-2 routing across various benchmarks,\nachieving an average improvement of 0.7% with less than 90% activated\nparameters. Further analysis shows our model dispatches more experts to tasks\nrequiring complex reasoning skills, like BBH, confirming its ability to\ndynamically allocate computational resources in alignment with the input's\ncomplexity. Our findings also highlight a variation in the number of experts\nneeded across different layers of the transformer model, offering insights into\nthe potential for designing heterogeneous MoE frameworks. The code and models\nare available at https://github.com/ZhenweiAn/Dynamic_MoE.\n","authors":["Quzhe Huang","Zhenwei An","Nan Zhuang","Mingxu Tao","Chen Zhang","Yang Jin","Kun Xu","Kun Xu","Liwei Chen","Songfang Huang","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03646v2","updated":"2024-03-12T13:38:31Z","published":"2023-10-05T16:21:36Z","title":"TRAM: Bridging Trust Regions and Sharpness Aware Minimization","summary":"  Sharpness-aware minimization (SAM) reports improving domain generalization by\nreducing the loss surface curvature in the parameter space. However,\ngeneralization during fine-tuning is often more dependent on the\ntransferability of representations in the function space. Trust-region methods\n(TR) target this goal by regularizing representation curvature to reduce\ncatastrophic forgetting of pre-trained task-agnostic information while adopting\ntask-specific skills. We consider unifying these strategies for low curvature\nin both parameter space and function space to improve out-of-domain (OOD)\ngeneralization. We propose Trust Region Aware Minimization (TRAM), a SAM\nalgorithm fine-tuning for low parameter sharpness and smooth, informative\nrepresentations preserving pre-trained structure. TRAM uses a trust region\nbound to inform the SAM adversarial neighborhood, introducing an awareness of\nfunction curvature within optimization for flatter minima. We empirically\nvalidate TRAM in vision (cross-dataset adaptation) and text (OOD language\nmodeling, zero-shot cross-lingual transfer) tasks where robust domain transfer\nand representation generality are critical. TRAM outperforms SAM- and TR-based\noptimization across all tasks, notably surpassing competing methods for hard\ntransfer between anticorrelated domains. TRAM establishes a novel standard in\nfine-tuning for domain-generalizable models with minimal additional computation\nover previous sharpness-aware methods.\n","authors":["Tom Sherborne","Naomi Saphra","Pradeep Dasigi","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2310.03646v2.pdf","comment":"Camera Ready for ICLR 2024 (Accepted as Spotlight). 21 pages, 14\n  tables, 2 figures"},{"id":"http://arxiv.org/abs/2402.09786v3","updated":"2024-03-12T13:36:23Z","published":"2024-02-15T08:34:21Z","title":"Examining Pathological Bias in a Generative Adversarial Network\n  Discriminator: A Case Study on a StyleGAN3 Model","summary":"  Generative adversarial networks (GANs) generate photorealistic faces that are\noften indistinguishable by humans from real faces. While biases in machine\nlearning models are often assumed to be due to biases in training data, we find\npathological internal color and luminance biases in the discriminator of a\npre-trained StyleGAN3-r model that are not explicable by the training data. We\nalso find that the discriminator systematically stratifies scores by both\nimage- and face-level qualities and that this disproportionately affects images\nacross gender, race, and other categories. We examine axes common in research\non stereotyping in social psychology.\n","authors":["Alvin Grissom II","Ryan F. Lei","Matt Gusdorff","Jeova Farias Sales Rocha Neto","Bailey Lin","Ryan Trotter"],"pdf_url":"https://arxiv.org/pdf/2402.09786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07648v1","updated":"2024-03-12T13:31:14Z","published":"2024-03-12T13:31:14Z","title":"Characterization of Large Language Model Development in the Datacenter","summary":"  Large Language Models (LLMs) have presented impressive performance across\nseveral transformative tasks. However, it is non-trivial to efficiently utilize\nlarge-scale cluster resources to develop LLMs, often riddled with numerous\nchallenges such as frequent hardware failures, intricate parallelization\nstrategies, and imbalanced resource utilization. In this paper, we present an\nin-depth characterization study of a six-month LLM development workload trace\ncollected from our GPU datacenter Acme. Specifically, we investigate\ndiscrepancies between LLMs and prior task-specific Deep Learning (DL)\nworkloads, explore resource utilization patterns, and identify the impact of\nvarious job failures. Our analysis summarizes hurdles we encountered and\nuncovers potential opportunities to optimize systems tailored for LLMs.\nFurthermore, we introduce our system efforts: (1) fault-tolerant pretraining,\nwhich enhances fault tolerance through LLM-involved failure diagnosis and\nautomatic recovery. (2) decoupled scheduling for evaluation, which achieves\ntimely performance feedback via trial decomposition and scheduling\noptimization.\n","authors":["Qinghao Hu","Zhisheng Ye","Zerui Wang","Guoteng Wang","Meng Zhang","Qiaoling Chen","Peng Sun","Dahua Lin","Xiaolin Wang","Yingwei Luo","Yonggang Wen","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07249v3","updated":"2024-03-12T13:30:16Z","published":"2022-12-14T14:34:15Z","title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning","summary":"  Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.\n","authors":["Jiashuo Sun","Hang Zhang","Chen Lin","Xiangdong Su","Yeyun Gong","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2212.07249v3.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2403.07632v1","updated":"2024-03-12T13:12:24Z","published":"2024-03-12T13:12:24Z","title":"CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs\n  for Reduced hERG Liability","summary":"  Drug-induced cardiotoxicity is a major health concern which can lead to\nserious adverse effects including life-threatening cardiac arrhythmias via the\nblockade of the voltage-gated hERG potassium ion channel. It is therefore of\ntremendous interest to develop advanced methods to identify hERG-active\ncompounds in early stages of drug development, as well as to optimize\ncommercially available drugs for reduced hERG activity. In this work, we\npresent CardioGenAI, a machine learning-based framework for re-engineering both\ndevelopmental and marketed drugs for reduced hERG activity while preserving\ntheir pharmacological activity. The framework incorporates novel\nstate-of-the-art discriminative models for predicting hERG channel activity, as\nwell as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to\ntheir potential implications in modulating the arrhythmogenic potential induced\nby hERG channel blockade. These models can also serve independently as\neffective components of a virtual screening pipeline. We applied the complete\nframework to pimozide, an FDA-approved antipsychotic agent that demonstrates\nhigh affinity to the hERG channel, and generated 100 refined candidates.\nRemarkably, among the candidates is fluspirilene, a compound which is of the\nsame class of drugs (diphenylmethanes) as pimozide and therefore has similar\npharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We\nhave made all of our software open-source to facilitate integration of the\nCardioGenAI framework for molecular hypothesis generation into drug discovery\nworkflows.\n","authors":["Gregory W. Kyro","Matthew T. Martin","Eric D. Watt","Victor S. Batista"],"pdf_url":"https://arxiv.org/pdf/2403.07632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07627v1","updated":"2024-03-12T13:09:15Z","published":"2024-03-12T13:09:15Z","title":"generAItor: Tree-in-the-Loop Text Generation for Language Model\n  Explainability and Adaptation","summary":"  Large language models (LLMs) are widely deployed in various downstream tasks,\ne.g., auto-completion, aided writing, or chat-based text generation. However,\nthe considered output candidates of the underlying search algorithm are\nunder-explored and under-explained. We tackle this shortcoming by proposing a\ntree-in-the-loop approach, where a visual representation of the beam search\ntree is the central component for analyzing, explaining, and adapting the\ngenerated outputs. To support these tasks, we present generAItor, a visual\nanalytics technique, augmenting the central beam search tree with various\ntask-specific widgets, providing targeted visualizations and interaction\npossibilities. Our approach allows interactions on multiple levels and offers\nan iterative pipeline that encompasses generating, exploring, and comparing\noutput candidates, as well as fine-tuning the model based on adapted data. Our\ncase study shows that our tool generates new insights in gender bias analysis\nbeyond state-of-the-art template-based methods. Additionally, we demonstrate\nthe applicability of our approach in a qualitative user study. Finally, we\nquantitatively evaluate the adaptability of the model to few samples, as\noccurring in text-generation use cases.\n","authors":["Thilo Spinner","Rebecca Kehlbeck","Rita Sevastjanova","Tobias Stähle","Daniel A. Keim","Oliver Deussen","Mennatallah El-Assady"],"pdf_url":"https://arxiv.org/pdf/2403.07627v1.pdf","comment":"24 pages paper, 4 pages references, 3 pages appendix, 8 figures"},{"id":"http://arxiv.org/abs/2006.14347v2","updated":"2024-03-12T13:00:14Z","published":"2020-06-25T12:45:17Z","title":"Epoch-evolving Gaussian Process Guided Learning","summary":"  In this paper, we propose a novel learning scheme called epoch-evolving\nGaussian Process Guided Learning (GPGL), which aims at characterizing the\ncorrelation information between the batch-level distribution and the global\ndata distribution. Such correlation information is encoded as context labels\nand needs renewal every epoch. With the guidance of the context label and\nground truth label, GPGL scheme provides a more efficient optimization through\nupdating the model parameters with a triangle consistency loss. Furthermore,\nour GPGL scheme can be further generalized and naturally applied to the current\ndeep models, outperforming the existing batch-based state-of-the-art models on\nmainstream datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) remarkably.\n","authors":["Jiabao Cui","Xuewei Li","Bin Li","Hanbin Zhao","Bourahla Omar","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2006.14347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00012v2","updated":"2024-03-12T12:59:45Z","published":"2024-02-27T02:23:07Z","title":"PreRoutGNN for Timing Prediction with Order Preserving Partition: Global\n  Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling","summary":"  Pre-routing timing prediction has been recently studied for evaluating the\nquality of a candidate cell placement in chip design. It involves directly\nestimating the timing metrics for both pin-level (slack, slew) and edge-level\n(net delay, cell delay), without time-consuming routing. However, it often\nsuffers from signal decay and error accumulation due to the long timing paths\nin large-scale industrial circuits. To address these challenges, we propose a\ntwo-stage approach. First, we propose global circuit training to pre-train a\ngraph auto-encoder that learns the global graph embedding from circuit netlist.\nSecond, we use a novel node updating scheme for message passing on GCN,\nfollowing the topological sorting sequence of the learned graph embedding and\ncircuit graph. This scheme residually models the local time delay between two\nadjacent pins in the updating sequence, and extracts the lookup table\ninformation inside each cell via a new attention mechanism. To handle\nlarge-scale circuits efficiently, we introduce an order preserving partition\nscheme that reduces memory consumption while maintaining the topological\ndependencies. Experiments on 21 real world circuits achieve a new SOTA R2 of\n0.93 for slack prediction, which is significantly surpasses 0.59 by previous\nSOTA method. Code will be available at:\nhttps://github.com/Thinklab-SJTU/EDA-AI.\n","authors":["Ruizhe Zhong","Junjie Ye","Zhentao Tang","Shixiong Kai","Mingxuan Yuan","Jianye Hao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.00012v2.pdf","comment":"13 pages, 5 figures, The 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2201.13001v7","updated":"2024-03-12T12:57:20Z","published":"2022-01-31T05:07:16Z","title":"Deep Discriminative to Kernel Density Graph for In- and\n  Out-of-distribution Calibrated Inference","summary":"  Deep discriminative approaches like random forests and deep neural networks\nhave recently found applications in many important real-world scenarios.\nHowever, deploying these learning algorithms in safety-critical applications\nraises concerns, particularly when it comes to ensuring confidence calibration\nfor both in-distribution and out-of-distribution data points. Many popular\nmethods for in-distribution (ID) calibration, such as isotonic regression and\nPlatt's sigmoidal regression, exhibit excellent ID calibration performance.\nHowever, these methods are not calibrated for the entire feature space, leading\nto overconfidence in the case of out-of-distribution (OOD) samples. On the\nother end of the spectrum, existing out-of-distribution (OOD) calibration\nmethods generally exhibit poor in-distribution (ID) calibration. In this paper,\nwe address ID and OOD calibration problems jointly. We leveraged the fact that\ndeep models, including both random forests and deep-nets, learn internal\nrepresentations which are unions of polytopes with affine activation functions\nto conceptualize them both as partitioning rules of the feature space. We\nreplace the affine function in each polytope populated by the training data\nwith a Gaussian kernel. We propose sufficient conditions for our proposed\nmethods to be consistent estimators of the corresponding class conditional\ndensities. Moreover, our experiments on both tabular and vision benchmarks show\nthat the proposed approaches obtain well-calibrated posteriors while mostly\npreserving or improving the classification accuracy of the original algorithm\nfor in-distribution region, and extrapolates beyond the training data to handle\nout-of-distribution inputs appropriately.\n","authors":["Jayanta Dey","Will LeVine","Haoyin Xu","Ashwin De Silva","Tyler M. Tomita","Ali Geisa","Tiffany Chu","Jacob Desman","Joshua T. Vogelstein"],"pdf_url":"https://arxiv.org/pdf/2201.13001v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.15524v4","updated":"2024-03-12T12:53:14Z","published":"2020-06-28T06:12:49Z","title":"MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot\n  Class-Incremental Learning","summary":"  As a challenging problem, few-shot class-incremental learning (FSCIL)\ncontinually learns a sequence of tasks, confronting the dilemma between slow\nforgetting of old knowledge and fast adaptation to new knowledge. In this\npaper, we concentrate on this \"slow vs. fast\" (SvF) dilemma to determine which\nknowledge components to be updated in a slow fashion or a fast fashion, and\nthereby balance old-knowledge preservation and new-knowledge adaptation. We\npropose a multi-grained SvF learning strategy to cope with the SvF dilemma from\ntwo different grains: intra-space (within the same feature space) and\ninter-space (between two different feature spaces). The proposed strategy\ndesigns a novel frequency-aware regularization to boost the intra-space SvF\ncapability, and meanwhile develops a new feature space composition operation to\nenhance the inter-space SvF learning performance. With the multi-grained SvF\nlearning strategy, our method outperforms the state-of-the-art approaches by a\nlarge margin.\n","authors":["Hanbin Zhao","Yongjian Fu","Mintong Kang","Qi Tian","Fei Wu","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2006.15524v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07611v1","updated":"2024-03-12T12:49:47Z","published":"2024-03-12T12:49:47Z","title":"Efficient Knowledge Deletion from Trained Models through Layer-wise\n  Partial Machine Unlearning","summary":"  Machine unlearning has garnered significant attention due to its ability to\nselectively erase knowledge obtained from specific training data samples in an\nalready trained machine learning model. This capability enables data holders to\nadhere strictly to data protection regulations. However, existing unlearning\ntechniques face practical constraints, often causing performance degradation,\ndemanding brief fine-tuning post unlearning, and requiring significant storage.\nIn response, this paper introduces a novel class of machine unlearning\nalgorithms. First method is partial amnesiac unlearning, integration of\nlayer-wise pruning with amnesiac unlearning. In this method, updates made to\nthe model during training are pruned and stored, subsequently used to forget\nspecific data from trained model. The second method assimilates layer-wise\npartial-updates into label-flipping and optimization-based unlearning to\nmitigate the adverse effects of data deletion on model efficacy. Through a\ndetailed experimental evaluation, we showcase the effectiveness of proposed\nunlearning methods. Experimental results highlight that the partial amnesiac\nunlearning not only preserves model efficacy but also eliminates the necessity\nfor brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover,\nemploying layer-wise partial updates in label-flipping and optimization-based\nunlearning techniques demonstrates superiority in preserving model efficacy\ncompared to their naive counterparts.\n","authors":["Vinay Chakravarthi Gogineni","Esmaeil S. Nadimi"],"pdf_url":"https://arxiv.org/pdf/2403.07611v1.pdf","comment":"16pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.07608v1","updated":"2024-03-12T12:47:32Z","published":"2024-03-12T12:47:32Z","title":"Couler: Unified Machine Learning Workflow Optimization in Cloud","summary":"  Machine Learning (ML) has become ubiquitous, fueling data-driven applications\nacross various organizations. Contrary to the traditional perception of ML in\nresearch, ML workflows can be complex, resource-intensive, and time-consuming.\nExpanding an ML workflow to encompass a wider range of data infrastructure and\ndata types may lead to larger workloads and increased deployment costs.\nCurrently, numerous workflow engines are available (with over ten being widely\nrecognized). This variety poses a challenge for end-users in terms of mastering\ndifferent engine APIs. While efforts have primarily focused on optimizing ML\nOperations (MLOps) for a specific workflow engine, current methods largely\noverlook workflow optimization across different engines.\n  In this work, we design and implement Couler, a system designed for unified\nML workflow optimization in the cloud. Our main insight lies in the ability to\ngenerate an ML workflow using natural language (NL) descriptions. We integrate\nLarge Language Models (LLMs) into workflow generation, and provide a unified\nprogramming interface for various workflow engines. This approach alleviates\nthe need to understand various workflow engines' APIs. Moreover, Couler\nenhances workflow computation efficiency by introducing automated caching at\nmultiple stages, enabling large workflow auto-parallelization and automatic\nhyperparameters tuning. These enhancements minimize redundant computational\ncosts and improve fault tolerance during deep learning workflow training.\nCouler is extensively deployed in real-world production scenarios at Ant Group,\nhandling approximately 22k workflows daily, and has successfully improved the\nCPU/Memory utilization by more than 15% and the workflow completion rate by\naround 17%.\n","authors":["Xiaoda Wang","Yuan Tang","Tengda Guo","Bo Sang","Jingji Wu","Jian Sha","Ke Zhang","Jiang Qian","Mingjie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07605v1","updated":"2024-03-12T12:44:34Z","published":"2024-03-12T12:44:34Z","title":"Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in\n  Text-To-Image Generation","summary":"  In text-to-image generation, using negative prompts, which describe\nundesirable image characteristics, can significantly boost image quality.\nHowever, producing good negative prompts is manual and tedious. To address\nthis, we propose NegOpt, a novel method for optimizing negative prompt\ngeneration toward enhanced image generation, using supervised fine-tuning and\nreinforcement learning. Our combined approach results in a substantial increase\nof 25% in Inception Score compared to other approaches and surpasses\nground-truth negative prompts from the test set. Furthermore, with NegOpt we\ncan preferentially optimize the metrics most important to us. Finally, we\nconstruct Negative Prompts DB, a dataset of negative prompts.\n","authors":["Michael Ogezi","Ning Shi"],"pdf_url":"https://arxiv.org/pdf/2403.07605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03403v2","updated":"2024-03-12T12:41:04Z","published":"2023-06-06T04:49:51Z","title":"SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic\n  Segmentation","summary":"  As an important and challenging problem in computer vision, PAnoramic\nSemantic Segmentation (PASS) gives complete scene perception based on an\nultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic\nimage input focus on solving image distortions but lack consideration of the 3D\nproperties of original $360^{\\circ}$ data. Therefore, their performance will\ndrop a lot when inputting panoramic images with the 3D disturbance. To be more\nrobust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer\nfor PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical\ngeometry knowledge. Specifically, a spherical geometry-aware framework is\nproposed for PASS. It includes three modules, i.e., spherical geometry-aware\nimage projection, spherical deformable patch embedding, and a panorama-aware\nloss, which takes input images with 3D disturbance into account, adds a\nspherical geometry-aware constraint on the existing deformable patch embedding,\nand indicates the pixel density of original $360^{\\circ}$ data, respectively.\nExperimental results on Stanford2D3D Panoramic datasets show that SGAT4PASS\nsignificantly improves performance and robustness, with approximately a 2%\nincrease in mIoU, and when small 3D disturbances occur in the data, the\nstability of our performance is improved by an order of magnitude. Our code and\nsupplementary material are available at\nhttps://github.com/TencentARC/SGAT4PASS.\n","authors":["Xuewei Li","Tao Wu","Zhongang Qi","Gaoang Wang","Ying Shan","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2306.03403v2.pdf","comment":"Accepted by IJCAI 2023"},{"id":"http://arxiv.org/abs/2403.07603v1","updated":"2024-03-12T12:40:23Z","published":"2024-03-12T12:40:23Z","title":"ProPML: Probability Partial Multi-label Learning","summary":"  Partial Multi-label Learning (PML) is a type of weakly supervised learning\nwhere each training instance corresponds to a set of candidate labels, among\nwhich only some are true. In this paper, we introduce \\our{}, a novel\nprobabilistic approach to this problem that extends the binary cross entropy to\nthe PML setup. In contrast to existing methods, it does not require suboptimal\ndisambiguation and, as such, can be applied to any deep architecture.\nFurthermore, experiments conducted on artificial and real-world datasets\nindicate that \\our{} outperforms existing approaches, especially for high noise\nin a candidate set.\n","authors":["Łukasz Struski","Adam Pardyl","Jacek Tabor","Bartosz Zieliński"],"pdf_url":"https://arxiv.org/pdf/2403.07603v1.pdf","comment":"Accepted to the International Conference on Data Science and Advanced\n  Analytics (DSAA 2023)"},{"id":"http://arxiv.org/abs/2306.14114v2","updated":"2024-03-12T12:39:03Z","published":"2023-06-25T03:31:47Z","title":"TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning\n  Granger Causal Structure from Event Sequences","summary":"  Learning Granger causality from event sequences is a challenging but\nessential task across various applications. Most existing methods rely on the\nassumption that event sequences are independent and identically distributed\n(i.i.d.). However, this i.i.d. assumption is often violated due to the inherent\ndependencies among the event sequences. Fortunately, in practice, we find these\ndependencies can be modeled by a topological network, suggesting a potential\nsolution to the non-i.i.d. problem by introducing the prior topological network\ninto Granger causal discovery. This observation prompts us to tackle two\nensuing challenges: 1) how to model the event sequences while incorporating\nboth the prior topological network and the latent Granger causal structure, and\n2) how to learn the Granger causal structure. To this end, we devise a unified\ntopological neural Poisson auto-regressive model with two processes. In the\ngeneration process, we employ a variant of the neural Poisson process to model\nthe event sequences, considering influences from both the topological network\nand the Granger causal structure. In the inference process, we formulate an\namortized inference algorithm to infer the latent Granger causal structure. We\nencapsulate these two processes within a unified likelihood function, providing\nan end-to-end framework for this task. Experiments on simulated and real-world\ndata demonstrate the effectiveness of our approach.\n","authors":["Yuequn Liu","Ruichu Cai","Wei Chen","Jie Qiao","Yuguang Yan","Zijian Li","Keli Zhang","Zhifeng Hao"],"pdf_url":"https://arxiv.org/pdf/2306.14114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02409v2","updated":"2024-03-12T12:25:24Z","published":"2023-07-23T03:16:14Z","title":"Mental Workload Estimation with Electroencephalogram Signals by\n  Combining Multi-Space Deep Models","summary":"  The human brain remains continuously active, whether an individual is working\nor at rest. Mental activity is a daily process, and if the brain becomes\nexcessively active, known as overload, it can adversely affect human health.\nRecently, advancements in early prediction of mental health conditions have\nemerged, aiming to prevent serious consequences and enhance the overall quality\nof life. Consequently, the estimation of mental status has garnered significant\nattention from diverse researchers due to its potential benefits. While various\nsignals are employed to assess mental state, the electroencephalogram,\ncontaining extensive information about the brain, is widely utilized by\nresearchers. In this paper, we categorize mental workload into three states\n(low, middle, and high) and estimate a continuum of mental workload levels. Our\nmethod leverages information from multiple spatial dimensions to achieve\noptimal results in mental estimation. For the time domain approach, we employ\nTemporal Convolutional Networks. In the frequency domain, we introduce a novel\narchitecture based on combining residual blocks, termed the Multi-Dimensional\nResidual Block. The integration of these two domains yields significant results\ncompared to individual estimates in each domain. Our approach achieved a 74.98%\naccuracy in the three-class classification, surpassing the provided data\nresults at 69.00%. Specially, our method demonstrates efficacy in estimating\ncontinuous levels, evidenced by a corresponding Concordance Correlation\nCoefficient (CCC) result of 0.629. The combination of time and frequency domain\nanalysis in our approach highlights the exciting potential to improve\nhealthcare applications in the future.\n","authors":["Hong-Hai Nguyen","Ngumimi Karen Iyortsuun","Seungwon Kim","Hyung-Jeong Yang","Soo-Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2308.02409v2.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.07591v1","updated":"2024-03-12T12:24:11Z","published":"2024-03-12T12:24:11Z","title":"Robustifying and Boosting Training-Free Neural Architecture Search","summary":"  Neural architecture search (NAS) has become a key component of AutoML and a\nstandard tool to automate the design of deep neural networks. Recently,\ntraining-free NAS as an emerging paradigm has successfully reduced the search\ncosts of standard training-based NAS by estimating the true architecture\nperformance with only training-free metrics. Nevertheless, the estimation\nability of these metrics typically varies across different tasks, making it\nchallenging to achieve robust and consistently good search performance on\ndiverse tasks with only a single training-free metric. Meanwhile, the\nestimation gap between training-free metrics and the true architecture\nperformances limits training-free NAS to achieve superior performance. To\naddress these challenges, we propose the robustifying and boosting\ntraining-free NAS (RoBoT) algorithm which (a) employs the optimized combination\nof existing training-free metrics explored from Bayesian optimization to\ndevelop a robust and consistently better-performing metric on diverse tasks,\nand (b) applies greedy search, i.e., the exploitation, on the newly developed\nmetric to bridge the aforementioned gap and consequently to boost the search\nperformance of standard training-free NAS further. Remarkably, the expected\nperformance of our RoBoT can be theoretically guaranteed, which improves over\nthe existing training-free NAS under mild conditions with additional\ninteresting insights. Our extensive experiments on various NAS benchmark tasks\nyield substantial empirical evidence to support our theoretical results.\n","authors":["Zhenfeng He","Yao Shu","Zhongxiang Dai","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2403.07591v1.pdf","comment":"Accepted by ICLR 2024. Code available at\n  https://github.com/hzf1174/RoBoT"},{"id":"http://arxiv.org/abs/2403.00673v2","updated":"2024-03-12T12:20:59Z","published":"2024-03-01T17:05:22Z","title":"Snapshot Reinforcement Learning: Leveraging Prior Trajectories for\n  Efficiency","summary":"  Deep reinforcement learning (DRL) algorithms require substantial samples and\ncomputational resources to achieve higher performance, which restricts their\npractical application and poses challenges for further development. Given the\nconstraint of limited resources, it is essential to leverage existing\ncomputational work (e.g., learned policies, samples) to enhance sample\nefficiency and reduce the computational resource consumption of DRL algorithms.\nPrevious works to leverage existing computational work require intrusive\nmodifications to existing algorithms and models, designed specifically for\nspecific algorithms, lacking flexibility and universality. In this paper, we\npresent the Snapshot Reinforcement Learning (SnapshotRL) framework, which\nenhances sample efficiency by simply altering environments, without making any\nmodifications to algorithms and models. By allowing student agents to choose\nstates in teacher trajectories as the initial state to sample, SnapshotRL can\neffectively utilize teacher trajectories to assist student agents in training,\nallowing student agents to explore a larger state space at the early training\nphase. We propose a simple and effective SnapshotRL baseline algorithm, S3RL,\nwhich integrates well with existing DRL algorithms. Our experiments demonstrate\nthat integrating S3RL with TD3, SAC, and PPO algorithms on the MuJoCo benchmark\nsignificantly improves sample efficiency and average return, without extra\nsamples and additional computational resources.\n","authors":["Yanxiao Zhao","Yangge Qian","Tianyi Wang","Jingyang Shan","Xiaolin Qin"],"pdf_url":"https://arxiv.org/pdf/2403.00673v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2307.09883v2","updated":"2024-03-12T12:20:37Z","published":"2023-07-19T10:27:34Z","title":"Symmetric Equilibrium Learning of VAEs","summary":"  We view variational autoencoders (VAE) as decoder-encoder pairs, which map\ndistributions in the data space to distributions in the latent space and vice\nversa. The standard learning approach for VAEs is the maximisation of the\nevidence lower bound (ELBO). It is asymmetric in that it aims at learning a\nlatent variable model while using the encoder as an auxiliary means only.\nMoreover, it requires a closed form a-priori latent distribution. This limits\nits applicability in more complex scenarios, such as general semi-supervised\nlearning and employing complex generative models as priors. We propose a Nash\nequilibrium learning approach, which is symmetric with respect to the encoder\nand decoder and allows learning VAEs in situations where both the data and the\nlatent distributions are accessible only by sampling. The flexibility and\nsimplicity of this approach allows its application to a wide range of learning\nscenarios and downstream tasks.\n","authors":["Boris Flach","Dmitrij Schlesinger","Alexander Shekhovtsov"],"pdf_url":"https://arxiv.org/pdf/2307.09883v2.pdf","comment":"13 pages, 6 figures, accepted for AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.07588v1","updated":"2024-03-12T12:18:55Z","published":"2024-03-12T12:18:55Z","title":"Visual Privacy Auditing with Diffusion Models","summary":"  Image reconstruction attacks on machine learning models pose a significant\nrisk to privacy by potentially leaking sensitive information. Although\ndefending against such attacks using differential privacy (DP) has proven\neffective, determining appropriate DP parameters remains challenging. Current\nformal guarantees on data reconstruction success suffer from overly theoretical\nassumptions regarding adversary knowledge about the target data, particularly\nin the image domain. In this work, we empirically investigate this discrepancy\nand find that the practicality of these assumptions strongly depends on the\ndomain shift between the data prior and the reconstruction target. We propose a\nreconstruction attack based on diffusion models (DMs) that assumes adversary\naccess to real-world image priors and assess its implications on privacy\nleakage under DP-SGD. We show that (1) real-world data priors significantly\ninfluence reconstruction success, (2) current reconstruction bounds do not\nmodel the risk posed by data priors well, and (3) DMs can serve as effective\nauditing tools for visualizing privacy leakage.\n","authors":["Kristian Schwethelm","Johannes Kaiser","Moritz Knolle","Daniel Rueckert","Georgios Kaissis","Alexander Ziller"],"pdf_url":"https://arxiv.org/pdf/2403.07588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07586v1","updated":"2024-03-12T12:16:40Z","published":"2024-03-12T12:16:40Z","title":"Federated Learning of Socially Appropriate Agent Behaviours in Simulated\n  Home Environments","summary":"  As social robots become increasingly integrated into daily life, ensuring\ntheir behaviours align with social norms is crucial. For their widespread\nopen-world application, it is important to explore Federated Learning (FL)\nsettings where individual robots can learn about their unique environments\nwhile also learning from each others' experiences. In this paper, we present a\nnovel FL benchmark that evaluates different strategies, using multi-label\nregression objectives, where each client individually learns to predict the\nsocial appropriateness of different robot actions while also sharing their\nlearning with others. Furthermore, splitting the training data by different\ncontexts such that each client incrementally learns across contexts, we present\na novel Federated Continual Learning (FCL) benchmark that adapts FL-based\nmethods to use state-of-the-art Continual Learning (CL) methods to continually\nlearn socially appropriate agent behaviours under different contextual\nsettings. Federated Averaging (FedAvg) of weights emerges as a robust FL\nstrategy while rehearsal-based FCL enables incrementally learning the social\nappropriateness of robot actions, across contextual splits.\n","authors":["Saksham Checker","Nikhil Churamani","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2403.07586v1.pdf","comment":"Accepted at the Workshop on Lifelong Learning and Personalization in\n  Long-Term Human-Robot Interaction (LEAP-HRI) at the 19th ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI), 2024"},{"id":"http://arxiv.org/abs/2403.07585v1","updated":"2024-03-12T12:15:57Z","published":"2024-03-12T12:15:57Z","title":"Communication Optimization for Distributed Training: Architecture,\n  Advances, and Opportunities","summary":"  The past few years have witnessed the flourishing of large-scale deep neural\nnetwork models with ever-growing parameter numbers. Training such large-scale\nmodels typically requires massive memory and computing resources that exceed\nthose of a single GPU, necessitating distributed training. As GPU performance\nhas rapidly evolved in recent years, computation time has shrunk, thereby\nincreasing the proportion of communication in the overall training time.\nTherefore, optimizing communication for distributed training has become an\nurgent issue. In this article, we briefly introduce the general architecture of\ndistributed deep neural network training and analyze relationships among\nParallelization Strategy, Collective Communication Library, and Network from\nthe perspective of communication optimization, which forms a three-layer\nparadigm. We then review current representative research advances with this\nthree-layer paradigm. We find that layers in the current three-layer paradigm\nare relatively independent, but there is a rich design space for cross-layer\ncollaborative optimization in distributed training scenarios. Therefore, we\nfurther advocate a communication-efficient five-layer paradigm underlining\nopportunities for collaboration designs and look forward to the perspectives of\n\"Vertical\", \"Horizontal\", \"Intra-Inter\" and \"Host-Net\" collaboration designs.\nWe hope this article can shed some light on future research on communication\noptimization for distributed training.\n","authors":["Yunze Wei","Tianshuo Hu","Cong Liang","Yong Cui"],"pdf_url":"https://arxiv.org/pdf/2403.07585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.03423v2","updated":"2024-03-12T12:12:29Z","published":"2021-10-05T07:42:41Z","title":"Efficient GPU implementation of randomized SVD and its applications","summary":"  Matrix decompositions are ubiquitous in machine learning, including\napplications in dimensionality reduction, data compression and deep learning\nalgorithms. Typical solutions for matrix decompositions have polynomial\ncomplexity which significantly increases their computational cost and time. In\nthis work, we leverage efficient processing operations that can be run in\nparallel on modern Graphical Processing Units (GPUs), predominant computing\narchitecture used e.g. in deep learning, to reduce the computational burden of\ncomputing matrix decompositions. More specifically, we reformulate the\nrandomized decomposition problem to incorporate fast matrix multiplication\noperations (BLAS-3) as building blocks. We show that this formulation, combined\nwith fast random number generators, allows to fully exploit the potential of\nparallel processing implemented in GPUs. Our extensive evaluation confirms the\nsuperiority of this approach over the competing methods and we release the\nresults of this research as a part of the official CUDA implementation\n(https://docs.nvidia.com/cuda/cusolver/index.html).\n","authors":["Łukasz Struski","Paweł Morkisz","Przemysław Spurek","Samuel Rodriguez Bernabeu","Tomasz Trzciński"],"pdf_url":"https://arxiv.org/pdf/2110.03423v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07573v1","updated":"2024-03-12T12:03:16Z","published":"2024-03-12T12:03:16Z","title":"Towards a Dynamic Future with Adaptable Computing and Network\n  Convergence (ACNC)","summary":"  In the context of advancing 6G, a substantial paradigm shift is anticipated,\nhighlighting comprehensive everything-to-everything interactions characterized\nby numerous connections and stringent adherence to Quality of\nService/Experience (QoS/E) prerequisites. The imminent challenge stems from\nresource scarcity, prompting a deliberate transition to Computing-Network\nConvergence (CNC) as an auspicious approach for joint resource orchestration.\nWhile CNC-based mechanisms have garnered attention, their effectiveness in\nrealizing future services, particularly in use cases like the Metaverse, may\nencounter limitations due to the continually changing nature of users,\nservices, and resources. Hence, this paper presents the concept of Adaptable\nCNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for\nthe joint orchestration of computing and network resources, catering to dynamic\nand voluminous user requests with stringent requirements. ACNC encompasses two\nprimary functionalities: state recognition and context detection. Given the\nintricate nature of the user-service-computing-network space, the paper employs\ndimension reduction to generate live, holistic, abstract system states in a\nhierarchical structure. To address the challenges posed by dynamic changes,\nContinual Learning (CL) is employed, classifying the system state into contexts\ncontrolled by dedicated ML agents, enabling them to operate efficiently. These\ntwo functionalities are intricately linked within a closed loop overseen by the\nEnd-to-End (E2E) orchestrator to allocate resources. The paper introduces the\ncomponents of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in\nresource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow,\ndetails a numerical analysis for efficiency assessment, and concludes with\ndiscussions on relevant challenges and potential avenues for future research.\n","authors":["Masoud Shokrnezhad","Hao Yu","Tarik Taleb","Richard Li","Kyunghan Lee","Jaeseung Song","Cedric Westphal"],"pdf_url":"https://arxiv.org/pdf/2403.07573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10535v2","updated":"2024-03-12T11:57:00Z","published":"2023-06-18T11:56:52Z","title":"ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging","summary":"  Multiple Instance Learning (MIL) is a weakly-supervised problem in which one\nlabel is assigned to the whole bag of instances. An important class of MIL\nmodels is instance-based, where we first classify instances and then aggregate\nthose predictions to obtain a bag label. The most common MIL model is when we\nconsider a bag as positive if at least one of its instances has a positive\nlabel. However, this reasoning does not hold in many real-life scenarios, where\nthe positive bag label is often a consequence of a certain percentage of\npositive instances. To address this issue, we introduce a dedicated\ninstance-based method called ProMIL, based on deep neural networks and\nBernstein polynomial estimation. An important advantage of ProMIL is that it\ncan automatically detect the optimal percentage level for decision-making. We\nshow that ProMIL outperforms standard instance-based MIL in real-world medical\napplications. We make the code available.\n","authors":["Łukasz Struski","Dawid Rymarczyk","Arkadiusz Lewicki","Robert Sabiniewicz","Jacek Tabor","Bartosz Zieliński"],"pdf_url":"https://arxiv.org/pdf/2306.10535v2.pdf","comment":"Accepted Paper to European Conference on Artificial Intelligence\n  (ECAI 2023)"},{"id":"http://arxiv.org/abs/2403.07569v1","updated":"2024-03-12T11:56:50Z","published":"2024-03-12T11:56:50Z","title":"Exploring Challenges in Deep Learning of Single-Station Ground Motion\n  Records","summary":"  Contemporary deep learning models have demonstrated promising results across\nvarious applications within seismology and earthquake engineering. These models\nrely primarily on utilizing ground motion records for tasks such as earthquake\nevent classification, localization, earthquake early warning systems, and\nstructural health monitoring. However, the extent to which these models\neffectively learn from these complex time-series signals has not been\nthoroughly analyzed. In this study, our objective is to evaluate the degree to\nwhich auxiliary information, such as seismic phase arrival times or seismic\nstation distribution within a network, dominates the process of deep learning\nfrom ground motion records, potentially hindering its effectiveness. We perform\na hyperparameter search on two deep learning models to assess their\neffectiveness in deep learning from ground motion records while also examining\nthe impact of auxiliary information on model performance. Experimental results\nreveal a strong reliance on the highly correlated P and S phase arrival\ninformation. Our observations highlight a potential gap in the field,\nindicating an absence of robust methodologies for deep learning of\nsingle-station ground motion recordings independent of any auxiliary\ninformation.\n","authors":["Ümit Mert Çağlar","Baris Yilmaz","Melek Türkmen","Erdem Akagündüz","Salih Tileylioglu"],"pdf_url":"https://arxiv.org/pdf/2403.07569v1.pdf","comment":"9 Pages, 12 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2403.07563v1","updated":"2024-03-12T11:51:55Z","published":"2024-03-12T11:51:55Z","title":"Learning Generalizable Feature Fields for Mobile Manipulation","summary":"  An open problem in mobile manipulation is how to represent objects and scenes\nin a unified manner, so that robots can use it both for navigating in the\nenvironment and manipulating objects. The latter requires capturing intricate\ngeometry while understanding fine-grained semantics, whereas the former\ninvolves capturing the complexity inherit to an expansive physical scale. In\nthis work, we present GeFF (Generalizable Feature Fields), a scene-level\ngeneralizable neural feature field that acts as a unified representation for\nboth navigation and manipulation that performs in real-time. To do so, we treat\ngenerative novel view synthesis as a pre-training task, and then align the\nresulting rich scene priors with natural language via CLIP feature\ndistillation. We demonstrate the effectiveness of this approach by deploying\nGeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's\nability to generalize to open-set objects as well as running time, when\nperforming open-vocabulary mobile manipulation in dynamic scenes.\n","authors":["Ri-Zhao Qiu","Yafei Hu","Ge Yang","Yuchen Song","Yang Fu","Jianglong Ye","Jiteng Mu","Ruihan Yang","Nikolay Atanasov","Sebastian Scherer","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.07563v1.pdf","comment":"Preprint. Project website is at: https://geff-b1.github.io/"},{"id":"http://arxiv.org/abs/2403.07562v1","updated":"2024-03-12T11:50:47Z","published":"2024-03-12T11:50:47Z","title":"A Flexible Cell Classification for ML Projects in Jupyter Notebooks","summary":"  Jupyter Notebook is an interactive development environment commonly used for\nrapid experimentation of machine learning (ML) solutions. Describing the ML\nactivities performed along code cells improves the readability and\nunderstanding of Notebooks. Manual annotation of code cells is time-consuming\nand error-prone. Therefore, tools have been developed that classify the cells\nof a notebook concerning the ML activity performed in them. However, the\ncurrent tools are not flexible, as they work based on look-up tables that have\nbeen created, which map function calls of commonly used ML libraries to ML\nactivities. These tables must be manually adjusted to account for new or\nchanged libraries.\n  This paper presents a more flexible approach to cell classification based on\na hybrid classification approach that combines a rule-based and a decision tree\nclassifier. We discuss the design rationales and describe the developed\nclassifiers in detail. We implemented the new flexible cell classification\napproach in a tool called JupyLabel. Its evaluation and the obtained metric\nscores regarding precision, recall, and F1-score are discussed. Additionally,\nwe compared JupyLabel with HeaderGen, an existing cell classification tool. We\nwere able to show that the presented flexible cell classification approach\noutperforms this tool significantly.\n","authors":["Miguel Perez","Selin Aydin","Horst Lichter"],"pdf_url":"https://arxiv.org/pdf/2403.07562v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.07559v1","updated":"2024-03-12T11:47:12Z","published":"2024-03-12T11:47:12Z","title":"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding","summary":"  Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding\n(MAPF) has recently gained attention due to its efficiency and scalability.\nSeveral MARL-MAPF methods choose to use communication to enrich the information\none agent can perceive. However, existing works still struggle in structured\nenvironments with high obstacle density and a high number of agents. To further\nimprove the performance of the communication-based MARL-MAPF solvers, we\npropose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first\npropose a selective communication block to gather richer information for better\nagent coordination within multi-agent environments and train the model with a\nQ-learning-based algorithm. We further introduce three advanced inference\nstrategies aimed at bolstering performance during the execution phase. First,\nwe hybridize the neural policy with single-agent expert guidance for navigating\nconflict-free zones. Secondly, we propose Q value-based methods for prioritized\nresolution of conflicts as well as deadlock situations. Finally, we introduce a\nrobust ensemble method that can efficiently collect the best out of multiple\npossible solutions. We empirically evaluate EPH in complex multi-agent\nenvironments and demonstrate competitive performance against state-of-the-art\nneural methods for MAPF.\n","authors":["Huijie Tang","Federico Berto","Jinkyoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.07559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04558v2","updated":"2024-03-12T11:42:06Z","published":"2024-03-07T14:56:06Z","title":"Reducing self-supervised learning complexity improves weakly-supervised\n  classification performance in computational pathology","summary":"  Deep Learning models have been successfully utilized to extract clinically\nactionable insights from routinely available histology data. Generally, these\nmodels require annotations performed by clinicians, which are scarce and costly\nto generate. The emergence of self-supervised learning (SSL) methods remove\nthis barrier, allowing for large-scale analyses on non-annotated data. However,\nrecent SSL approaches apply increasingly expansive model architectures and\nlarger datasets, causing the rapid escalation of data volumes, hardware\nprerequisites, and overall expenses, limiting access to these resources to few\ninstitutions. Therefore, we investigated the complexity of contrastive SSL in\ncomputational pathology in relation to classification performance with the\nutilization of consumer-grade hardware. Specifically, we analyzed the effects\nof adaptations in data volume, architecture, and algorithms on downstream\nclassification tasks, emphasizing their impact on computational resources. We\ntrained breast cancer foundation models on a large public patient cohort and\nvalidated them on various downstream classification tasks in a weakly\nsupervised manner on two external public patient cohorts. Our experiments\ndemonstrate that we can improve downstream classification performance whilst\nreducing SSL training duration by 90%. In summary, we propose a set of\nadaptations which enable the utilization of SSL in computational pathology in\nnon-resource abundant environments.\n","authors":["Tim Lenz","Omar S. M. El Nahhas","Marta Ligero","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2403.04558v2.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.07557v1","updated":"2024-03-12T11:41:51Z","published":"2024-03-12T11:41:51Z","title":"SIFiD: Reassess Summary Factual Inconsistency Detection with LLM","summary":"  Ensuring factual consistency between the summary and the original document is\nparamount in summarization tasks. Consequently, considerable effort has been\ndedicated to detecting inconsistencies. With the advent of Large Language\nModels (LLMs), recent studies have begun to leverage their advanced language\nunderstanding capabilities for inconsistency detection. However, early attempts\nhave shown that LLMs underperform traditional models due to their limited\nability to follow instructions and the absence of an effective detection\nmethodology. In this study, we reassess summary inconsistency detection with\nLLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in\nLLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency\nDetection with Filtered Document) that identify key sentences within documents\nby either employing natural language inference or measuring semantic similarity\nbetween summaries and documents.\n","authors":["Jiuding Yang","Hui Liu","Weidong Guo","Zhuwei Rao","Yu Xu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2403.07557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06569v2","updated":"2024-03-12T11:40:33Z","published":"2024-03-11T10:10:45Z","title":"Enhancing Joint Motion Prediction for Individuals with Limb Loss Through\n  Model Reprogramming","summary":"  Mobility impairment caused by limb loss is a significant challenge faced by\nmillions of individuals worldwide. The development of advanced assistive\ntechnologies, such as prosthetic devices, has the potential to greatly improve\nthe quality of life for amputee patients. A critical component in the design of\nsuch technologies is the accurate prediction of reference joint motion for the\nmissing limb. However, this task is hindered by the scarcity of joint motion\ndata available for amputee patients, in contrast to the substantial quantity of\ndata from able-bodied subjects. To overcome this, we leverage deep learning's\nreprogramming property to repurpose well-trained models for a new goal without\naltering the model parameters. With only data-level manipulation, we adapt\nmodels originally designed for able-bodied people to forecast joint motion in\namputees. The findings in this study have significant implications for\nadvancing assistive tech and amputee mobility.\n","authors":["Sharmita Dey","Sarath R. Nair"],"pdf_url":"https://arxiv.org/pdf/2403.06569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04985v3","updated":"2024-03-12T11:35:08Z","published":"2023-12-08T11:47:35Z","title":"SparQ Attention: Bandwidth-Efficient LLM Inference","summary":"  The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data-transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data-transfers without substantial drops in accuracy, by evaluating\nLlama 2, Mistral and Pythia models on a wide range of downstream tasks.\n","authors":["Luka Ribar","Ivan Chelombiev","Luke Hudlass-Galley","Charlie Blake","Carlo Luschi","Douglas Orr"],"pdf_url":"https://arxiv.org/pdf/2312.04985v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07548v1","updated":"2024-03-12T11:33:48Z","published":"2024-03-12T11:33:48Z","title":"Online Continual Learning For Interactive Instruction Following Agents","summary":"  In learning an embodied agent executing daily tasks via language directives,\nthe literature largely assumes that the agent learns all training data at the\nbeginning. We argue that such a learning scenario is less realistic since a\nrobotic agent is supposed to learn the world continuously as it explores and\nperceives it. To take a step towards a more realistic embodied agent learning\nscenario, we propose two continual learning setups for embodied agents;\nlearning new behaviors (Behavior Incremental Learning, Behavior-IL) and new\nenvironments (Environment Incremental Learning, Environment-IL) For the tasks,\nprevious 'data prior' based continual learning methods maintain logits for the\npast tasks. However, the stored information is often insufficiently learned\ninformation and requires task boundary information, which might not always be\navailable. Here, we propose to update them based on confidence scores without\ntask boundary information during training (i.e., task-free) in a moving average\nfashion, named Confidence-Aware Moving Average (CAMA). In the proposed\nBehavior-IL and Environment-IL setups, our simple CAMA outperforms prior state\nof the art in our empirical validations by noticeable margins. The project page\nincluding codes is https://github.com/snumprlab/cl-alfred.\n","authors":["Byeonghwi Kim","Minhyuk Seo","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2403.07548v1.pdf","comment":"ICLR 2024 (Project page:\n  $\\href{https://bhkim94.github.io/projects/CL-ALFRED>}{\\text{https}}$)"},{"id":"http://arxiv.org/abs/2207.06316v4","updated":"2024-03-12T11:29:58Z","published":"2022-07-13T16:09:29Z","title":"Majorization-minimization for Sparse Nonnegative Matrix Factorization\n  with the $β$-divergence","summary":"  This article introduces new multiplicative updates for nonnegative matrix\nfactorization with the $\\beta$-divergence and sparse regularization of one of\nthe two factors (say, the activation matrix). It is well known that the norm of\nthe other factor (the dictionary matrix) needs to be controlled in order to\navoid an ill-posed formulation. Standard practice consists in constraining the\ncolumns of the dictionary to have unit norm, which leads to a nontrivial\noptimization problem. Our approach leverages a reparametrization of the\noriginal problem into the optimization of an equivalent scale-invariant\nobjective function. From there, we derive block-descent\nmajorization-minimization algorithms that result in simple multiplicative\nupdates for either $\\ell_{1}$-regularization or the more \"aggressive\"\nlog-regularization. In contrast with other state-of-the-art methods, our\nalgorithms are universal in the sense that they can be applied to any\n$\\beta$-divergence (i.e., any value of $\\beta$) and that they come with\nconvergence guarantees. We report numerical comparisons with existing heuristic\nand Lagrangian methods using various datasets: face images, an audio\nspectrogram, hyperspectral data, and song play counts. We show that our methods\nobtain solutions of similar quality at convergence (similar objective values)\nbut with significantly reduced CPU times.\n","authors":["Arthur Marmin","José Henrique de Morais Goulart","Cédric Févotte"],"pdf_url":"https://arxiv.org/pdf/2207.06316v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07542v1","updated":"2024-03-12T11:29:40Z","published":"2024-03-12T11:29:40Z","title":"A Survey of Vision Transformers in Autonomous Driving: Current Trends\n  and Future Directions","summary":"  This survey explores the adaptation of visual transformer models in\nAutonomous Driving, a transition inspired by their success in Natural Language\nProcessing. Surpassing traditional Recurrent Neural Networks in tasks like\nsequential image processing and outperforming Convolutional Neural Networks in\nglobal context capture, as evidenced in complex scene recognition, Transformers\nare gaining traction in computer vision. These capabilities are crucial in\nAutonomous Driving for real-time, dynamic visual scene processing. Our survey\nprovides a comprehensive overview of Vision Transformer applications in\nAutonomous Driving, focusing on foundational concepts such as self-attention,\nmulti-head attention, and encoder-decoder architecture. We cover applications\nin object detection, segmentation, pedestrian detection, lane detection, and\nmore, comparing their architectural merits and limitations. The survey\nconcludes with future research directions, highlighting the growing role of\nVision Transformers in Autonomous Driving.\n","authors":["Quoc-Vinh Lai-Dang"],"pdf_url":"https://arxiv.org/pdf/2403.07542v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.03227v3","updated":"2024-03-12T11:28:20Z","published":"2024-02-05T17:38:49Z","title":"IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of\n  brain MR images","summary":"  In MRI studies, the aggregation of imaging data from multiple acquisition\nsites enhances sample size but may introduce site-related variabilities that\nhinder consistency in subsequent analyses. Deep learning methods for image\ntranslation have emerged as a solution for harmonizing MR images across sites.\nIn this study, we introduce IGUANe (Image Generation with Unified Adversarial\nNetworks), an original 3D model that leverages the strengths of domain\ntranslation and straightforward application of style transfer methods for\nmulticenter brain MR image harmonization. IGUANe extends CycleGAN architecture\nby integrating an arbitrary number of domains for training through a\nmany-to-one strategy. During inference, the model can be applied to any image,\neven from an unknown acquisition site, making it a universal generator for\nharmonization. Trained on a dataset comprising T1-weighted images from 11\ndifferent scanners, IGUANe was evaluated on data from unseen sites. The\nassessments included the transformation of MR images with traveling subjects,\nthe preservation of pairwise distances between MR images within domains, the\nevolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease\n(AD), and the performance in age regression and patient classification tasks.\nComparisons with other harmonization and normalization methods suggest that\nIGUANe better preserves individual information in MR images and is more\nsuitable for maintaining and reinforcing variabilities related to age and AD.\nFuture studies may further assess IGUANe in other multicenter contexts, either\nusing the same model or retraining it for applications to different image\nmodalities.\n","authors":["Vincent Roca","Grégory Kuchcinski","Jean-Pierre Pruvo","Dorian Manouvriez","Renaud Lopes"],"pdf_url":"https://arxiv.org/pdf/2402.03227v3.pdf","comment":"23 pages, 8 figures; typos corrected"},{"id":"http://arxiv.org/abs/2403.07536v1","updated":"2024-03-12T11:19:46Z","published":"2024-03-12T11:19:46Z","title":"LaB-GATr: geometric algebra transformers for large biomedical surface\n  and volume meshes","summary":"  Many anatomical structures can be described by surface or volume meshes.\nMachine learning is a promising tool to extract information from these 3D\nmodels. However, high-fidelity meshes often contain hundreds of thousands of\nvertices, which creates unique challenges in building deep neural network\narchitectures. Furthermore, patient-specific meshes may not be canonically\naligned which limits the generalisation of machine learning algorithms. We\npropose LaB-GATr, a transfomer neural network with geometric tokenisation that\ncan effectively learn with large-scale (bio-)medical surface and volume meshes\nthrough sequence compression and interpolation. Our method extends the recently\nproposed geometric algebra transformer (GATr) and thus respects all Euclidean\nsymmetries, i.e. rotation, translation and reflection, effectively mitigating\nthe problem of canonical alignment between patients. LaB-GATr achieves\nstate-of-the-art results on three tasks in cardiovascular hemodynamics\nmodelling and neurodevelopmental phenotype prediction, featuring meshes of up\nto 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful\narchitecture for learning with high-fidelity meshes which has the potential to\nenable interesting downstream applications. Our implementation is publicly\navailable.\n","authors":["Julian Suk","Baris Imre","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2403.07536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07526v1","updated":"2024-03-12T11:05:05Z","published":"2024-03-12T11:05:05Z","title":"Physics-Transfer Learning for Material Strength Screening","summary":"  The strength of materials, like many problems in the natural sciences, spans\nmultiple length and time scales, and the solution has to balance accuracy and\nperformance. Peierls stress is one of the central concepts in crystal\nplasticity that measures the strength through the resistance of a dislocation\nto plastic flow. The determination of Peierls stress involves a multiscale\nnature depending on both elastic lattice responses and the energy landscape of\ncrystal slips. Material screening by strength via the Peierls stress from\nfirst-principles calculations is computationally intractable for the nonlocal\ncharacteristics of dislocations, and not included in the state-of-the-art\ncomputational material databases. In this work, we propose a physics-transfer\nframework to learn the physics of crystal plasticity from empirical atomistic\nsimulations and then predict the Peierls stress from chemically accurate\ndensity functional theory-based calculations of material parameters. Notably,\nthe strengths of single-crystalline metals can be predicted from a few\nsingle-point calculations for the deformed lattice and on the {\\gamma} surface,\nallowing efficient, high-throughput screening for material discovery.\nUncertainty quantification is carried out to assess the accuracy of models and\nsources of errors, showing reduced physical and system uncertainties in the\npredictions by elevating the fidelity of training models. This physics-transfer\nframework can be generalized to other problems facing the accuracy-performance\ndilemma, by harnessing the hierarchy of physics in the multiscale models of\nmaterials science.\n","authors":["Yingjie Zhao","Zian Zhang","Zhiping Xu"],"pdf_url":"https://arxiv.org/pdf/2403.07526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10547v3","updated":"2024-03-12T10:58:02Z","published":"2023-08-21T08:02:16Z","title":"Decentralized Riemannian Conjugate Gradient Method on the Stiefel\n  Manifold","summary":"  The conjugate gradient method is a crucial first-order optimization method\nthat generally converges faster than the steepest descent method, and its\ncomputational cost is much lower than that of second-order methods. However,\nwhile various types of conjugate gradient methods have been studied in\nEuclidean spaces and on Riemannian manifolds, there is little study for those\nin distributed scenarios. This paper proposes a decentralized Riemannian\nconjugate gradient descent (DRCGD) method that aims at minimizing a global\nfunction over the Stiefel manifold. The optimization problem is distributed\namong a network of agents, where each agent is associated with a local\nfunction, and the communication between agents occurs over an undirected\nconnected graph. Since the Stiefel manifold is a non-convex set, a global\nfunction is represented as a finite sum of possibly non-convex (but smooth)\nlocal functions. The proposed method is free from expensive Riemannian\ngeometric operations such as retractions, exponential maps, and vector\ntransports, thereby reducing the computational complexity required by each\nagent. To the best of our knowledge, DRCGD is the first decentralized\nRiemannian conjugate gradient algorithm to achieve global convergence over the\nStiefel manifold.\n","authors":["Jun Chen","Haishan Ye","Mengmeng Wang","Tianxin Huang","Guang Dai","Ivor W. Tsang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2308.10547v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10207v4","updated":"2024-03-12T10:57:49Z","published":"2023-10-16T09:19:18Z","title":"Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in\n  the Real World","summary":"  We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities.\n","authors":["Rujie Wu","Xiaojian Ma","Zhenliang Zhang","Wei Wang","Qing Li","Song-Chun Zhu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2310.10207v4.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07507v1","updated":"2024-03-12T10:43:52Z","published":"2024-03-12T10:43:52Z","title":"Reconstructions of Jupiter's magnetic field using physics informed\n  neural networks","summary":"  Magnetic sounding using data collected from the Juno mission can be used to\nprovide constraints on Jupiter's interior. However, inwards continuation of\nreconstructions assuming zero electrical conductivity and a representation in\nspherical harmonics are limited by the enhancement of noise at small scales. In\nthis paper we describe new reconstructions of Jupiter's internal magnetic field\nbased on physics-informed neural networks and either the first 33 (PINN33) or\nthe first 50 (PINN50) of Juno's orbits. The method can resolve local\nstructures, and allows for weak ambient electrical currents. Compared with\nother methods, our reconstructions of Jupiter's magnetic field both on and\nabove the surface are similar, and we achieve a similar fit to the Juno data.\nHowever, our models are not hampered by noise at depth, and so offer a much\nclearer picture of the interior structure. We estimate that the dynamo boundary\nis at a fractional radius of 0.8. At this depth, the magnetic field is arranged\ninto longitudinal bands, and the great blue spot appears to be rooted in\nneighbouring structures of oppositely signed flux.\n","authors":["Philip W. Livermore","Leyuan Wu","Longwei Chen","Sjoerd A. L. de Ridder"],"pdf_url":"https://arxiv.org/pdf/2403.07507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07503v1","updated":"2024-03-12T10:42:32Z","published":"2024-03-12T10:42:32Z","title":"Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement\n  Learning Approach","summary":"  Hybrid electric vehicles (HEVs) are becoming increasingly popular because\nthey can better combine the working characteristics of internal combustion\nengines and electric motors. However, the minimum fuel consumption of an HEV\nfor a battery electrical balance case under a specific assembly condition and a\nspecific speed curve still needs to be clarified in academia and industry.\nRegarding this problem, this work provides the mathematical expression of\nconstrained optimal fuel consumption (COFC) from the perspective of constrained\nreinforcement learning (CRL) for the first time globally. Also, two mainstream\napproaches of CRL, constrained variational policy optimization (CVPO) and\nLagrangian-based approaches, are utilized for the first time to obtain the\nvehicle's minimum fuel consumption under the battery electrical balance\ncondition. We conduct case studies on the well-known Prius TOYOTA hybrid system\n(THS) under the NEDC condition; we give vital steps to implement CRL approaches\nand compare the performance between the CVPO and Lagrangian-based approaches.\nOur case study found that CVPO and Lagrangian-based approaches can obtain the\nlowest fuel consumption while maintaining the SOC balance constraint. The CVPO\napproach converges stable, but the Lagrangian-based approach can obtain the\nlowest fuel consumption at 3.95 L/100km, though with more significant\noscillations. This result verifies the effectiveness of our proposed CRL\napproaches to the COFC problem.\n","authors":["Shuchang Yan"],"pdf_url":"https://arxiv.org/pdf/2403.07503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07501v1","updated":"2024-03-12T10:38:54Z","published":"2024-03-12T10:38:54Z","title":"Detecting Security-Relevant Methods using Multi-label Machine Learning","summary":"  To detect security vulnerabilities, static analysis tools need to be\nconfigured with security-relevant methods. Current approaches can automatically\nidentify such methods using binary relevance machine learning approaches.\nHowever, they ignore dependencies among security-relevant methods,\nover-generalize and perform poorly in practice. Additionally, users have to\nnevertheless manually configure static analysis tools using the detected\nmethods. Based on feedback from users and our observations, the excessive\nmanual steps can often be tedious, error-prone and counter-intuitive.\n  In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects\nsecurity-relevant methods using a multi-label machine learning approach that\nconsiders dependencies among labels. The plugin can automatically generate\nconfigurations for static analysis tools, run the static analysis, and show the\nresults in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine\nlearning approach has a higher F1-Measure than related approaches. Moreover,\nthe plugin reduces and simplifies the manual effort required when configuring\nand using static analysis tools.\n","authors":["Oshando Johnson","Goran Piskachev","Ranjith Krishnamurthy","Eric Bodden"],"pdf_url":"https://arxiv.org/pdf/2403.07501v1.pdf","comment":"6 pages, 3 figures, The IDE Workshop"},{"id":"http://arxiv.org/abs/2403.07493v1","updated":"2024-03-12T10:32:35Z","published":"2024-03-12T10:32:35Z","title":"Signed graphs in data sciences via communicability geometry","summary":"  Signed graphs are an emergent way of representing data in a variety of\ncontexts were conflicting interactions exist. These include data from\nbiological, ecological, and social systems. Here we propose the concept of\ncommunicability geometry for signed graphs, proving that metrics in this space,\nsuch as the communicability distance and angles, are Euclidean and spherical.\nWe then apply these metrics to solve several problems in data analysis of\nsigned graphs in a unified way. They include the partitioning of signed graphs,\ndimensionality reduction, finding hierarchies of alliances in signed networks\nas well as the quantification of the degree of polarization between the\nexisting factions in systems represented by this type of graphs.\n","authors":["Fernando Diaz-Diaz","Ernesto Estrada"],"pdf_url":"https://arxiv.org/pdf/2403.07493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07486v1","updated":"2024-03-12T10:21:31Z","published":"2024-03-12T10:21:31Z","title":"XpertAI: uncovering model strategies for sub-manifolds","summary":"  In recent years, Explainable AI (XAI) methods have facilitated profound\nvalidation and knowledge extraction from ML models. While extensively studied\nfor classification, few XAI solutions have addressed the challenges specific to\nregression models. In regression, explanations need to be precisely formulated\nto address specific user queries (e.g.\\ distinguishing between `Why is the\noutput above 0?' and `Why is the output above 50?'). They should furthermore\nreflect the model's behavior on the relevant data sub-manifold. In this paper,\nwe introduce XpertAI, a framework that disentangles the prediction strategy\ninto multiple range-specific sub-strategies and allows the formulation of\nprecise queries about the model (the `explanandum') as a linear combination of\nthose sub-strategies. XpertAI is formulated generally to work alongside popular\nXAI attribution techniques, based on occlusion, gradient integration, or\nreverse propagation. Qualitative and quantitative results, demonstrate the\nbenefits of our approach.\n","authors":["Simon Letzgus","Klaus-Robert Müller","Grégoire Montavon"],"pdf_url":"https://arxiv.org/pdf/2403.07486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07485v1","updated":"2024-03-12T10:21:21Z","published":"2024-03-12T10:21:21Z","title":"PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial\n  Surrogates","summary":"  We introduce a surrogate-based black-box optimization method, termed\nPolynomial-model-based optimization (PMBO). The algorithm alternates polynomial\napproximation with Bayesian optimization steps, using Gaussian processes to\nmodel the error between the objective and its polynomial fit. We describe the\nalgorithmic design of PMBO and compare the results of the performance of PMBO\nwith several optimization methods for a set of analytic test functions.\n  The results show that PMBO outperforms the classic Bayesian optimization and\nis robust with respect to the choice of its correlation function family and its\nhyper-parameter setting, which, on the contrary, need to be carefully tuned in\nclassic Bayesian optimization. Remarkably, PMBO performs comparably with\nstate-of-the-art evolutionary algorithms such as the Covariance Matrix\nAdaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO\nemerges as the pivotal choice among surrogate-based optimization methods when\naddressing low-dimensional optimization problems. Hereby, the simple nature of\npolynomials opens the opportunity for interpretation and analysis of the\ninferred surrogate model, providing a macroscopic perspective on the landscape\nof the objective function.\n","authors":["Janina Schreiber","Pau Batlle","Damar Wicaksono","Michael Hecht"],"pdf_url":"https://arxiv.org/pdf/2403.07485v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.07483v1","updated":"2024-03-12T10:18:59Z","published":"2024-03-12T10:18:59Z","title":"A Deep Learning Approach to Diabetes Diagnosis","summary":"  Diabetes, resulting from inadequate insulin production or utilization, causes\nextensive harm to the body. Existing diagnostic methods are often invasive and\ncome with drawbacks, such as cost constraints. Although there are machine\nlearning models like Classwise k Nearest Neighbor (CkNN) and General Regression\nNeural Network (GRNN), they struggle with imbalanced data and result in\nunder-performance. Leveraging advancements in sensor technology and machine\nlearning, we propose a non-invasive diabetes diagnosis using a Back Propagation\nNeural Network (BPNN) with batch normalization, incorporating data re-sampling\nand normalization for class balancing. Our method addresses existing challenges\nsuch as limited performance associated with traditional machine learning.\nExperimental results on three datasets show significant improvements in overall\naccuracy, sensitivity, and specificity compared to traditional methods.\nNotably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in\nCDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores\nthe potential of deep learning models for robust diabetes diagnosis. See\nproject website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/\n","authors":["Zeyu Zhang","Khandaker Asif Ahmed","Md Rakibul Hasan","Tom Gedeon","Md Zakir Hossain"],"pdf_url":"https://arxiv.org/pdf/2403.07483v1.pdf","comment":"Accepted to ACIIDS 2024"},{"id":"http://arxiv.org/abs/2309.05436v3","updated":"2024-03-12T10:18:09Z","published":"2023-09-11T13:18:19Z","title":"Quantized Fourier and Polynomial Features for more Expressive Tensor\n  Network Models","summary":"  In the context of kernel machines, polynomial and Fourier features are\ncommonly used to provide a nonlinear extension to linear models by mapping the\ndata to a higher-dimensional space. Unless one considers the dual formulation\nof the learning problem, which renders exact large-scale learning unfeasible,\nthe exponential increase of model parameters in the dimensionality of the data\ncaused by their tensor-product structure prohibits to tackle high-dimensional\nproblems. One of the possible approaches to circumvent this exponential scaling\nis to exploit the tensor structure present in the features by constraining the\nmodel weights to be an underparametrized tensor network. In this paper we\nquantize, i.e. further tensorize, polynomial and Fourier features. Based on\nthis feature quantization we propose to quantize the associated model weights,\nyielding quantized models. We show that, for the same number of model\nparameters, the resulting quantized models have a higher bound on the\nVC-dimension as opposed to their non-quantized counterparts, at no additional\ncomputational cost while learning from identical features. We verify\nexperimentally how this additional tensorization regularizes the learning\nproblem by prioritizing the most salient features in the data and how it\nprovides models with increased generalization capabilities. We finally\nbenchmark our approach on large regression task, achieving state-of-the-art\nresults on a laptop computer.\n","authors":["Frederiek Wesel","Kim Batselier"],"pdf_url":"https://arxiv.org/pdf/2309.05436v3.pdf","comment":"9 pages, 4 figures. Reviewed version after peer-review. To be\n  published in the proceedings of the 27th International Conference on\n  Artificial Intelligence and Statistics (AISTATS)"},{"id":"http://arxiv.org/abs/2403.07478v1","updated":"2024-03-12T10:12:59Z","published":"2024-03-12T10:12:59Z","title":"Towards Graph Foundation Models for Personalization","summary":"  In the realm of personalization, integrating diverse information sources such\nas consumption signals and content-based representations is becoming\nincreasingly critical to build state-of-the-art solutions. In this regard, two\nof the biggest trends in research around this subject are Graph Neural Networks\n(GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in\nindustry for powering personalization at scale, FMs have only recently caught\nattention for their promising performance in personalization tasks like ranking\nand retrieval. In this paper, we present a graph-based foundation modeling\napproach tailored to personalization. Central to this approach is a\nHeterogeneous GNN (HGNN) designed to capture multi-hop content and consumption\nrelationships across a range of recommendable item types. To ensure the\ngenerality required from a Foundation Model, we employ a Large Language Model\n(LLM) text-based featurization of nodes that accommodates all item types, and\nconstruct the graph using co-interaction signals, which inherently transcend\ncontent specificity. To facilitate practical generalization, we further couple\nthe HGNN with an adaptation mechanism based on a two-tower (2T) architecture,\nwhich also operates agnostically to content type. This multi-stage approach\nensures high scalability; while the HGNN produces general purpose embeddings,\nthe 2T component models in a continuous space the sheer size of user-item\ninteraction data. Our comprehensive approach has been rigorously tested and\nproven effective in delivering recommendations across a diverse array of\nproducts within a real-world, industrial audio streaming platform.\n","authors":["Andreas Damianou","Francesco Fabbri","Paul Gigioli","Marco De Nadai","Alice Wang","Enrico Palumbo","Mounia Lalmas"],"pdf_url":"https://arxiv.org/pdf/2403.07478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07472v1","updated":"2024-03-12T10:08:36Z","published":"2024-03-12T10:08:36Z","title":"Imbalance-aware Presence-only Loss Function for Species Distribution\n  Modeling","summary":"  In the face of significant biodiversity decline, species distribution models\n(SDMs) are essential for understanding the impact of climate change on species\nhabitats by connecting environmental conditions to species occurrences.\nTraditionally limited by a scarcity of species observations, these models have\nsignificantly improved in performance through the integration of larger\ndatasets provided by citizen science initiatives. However, they still suffer\nfrom the strong class imbalance between species within these datasets, often\nresulting in the penalization of rare species--those most critical for\nconservation efforts. To tackle this issue, this study assesses the\neffectiveness of training deep learning models using a balanced presence-only\nloss function on large citizen science-based datasets. We demonstrate that this\nimbalance-aware loss function outperforms traditional loss functions across\nvarious datasets and tasks, particularly in accurately modeling rare species\nwith limited observations.\n","authors":["Robin Zbinden","Nina van Tiel","Marc Rußwurm","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2403.07472v1.pdf","comment":"Tackling Climate Change with Machine Learning at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07471v1","updated":"2024-03-12T10:06:48Z","published":"2024-03-12T10:06:48Z","title":"On the nonconvexity of some push-forward constraints and its\n  consequences in machine learning","summary":"  The push-forward operation enables one to redistribute a probability measure\nthrough a deterministic map. It plays a key role in statistics and\noptimization: many learning problems (notably from optimal transport,\ngenerative modeling, and algorithmic fairness) include constraints or penalties\nframed as push-forward conditions on the model. However, the literature lacks\ngeneral theoretical insights on the (non)convexity of such constraints and its\nconsequences on the associated learning problems. This paper aims at filling\nthis gap. In a first part, we provide a range of sufficient and necessary\nconditions for the (non)convexity of two sets of functions: the maps\ntransporting one probability measure to another; the maps inducing equal output\ndistributions across distinct probability measures. This highlights that for\nmost probability measures, these push-forward constraints are not convex. In a\nsecond time, we show how this result implies critical limitations on the design\nof convex optimization problems for learning generative models or group-fair\npredictors. This work will hopefully help researchers and practitioners have a\nbetter understanding of the critical impact of push-forward conditions onto\nconvexity.\n","authors":["Lucas de Lara","Mathis Deronzier","Alberto González-Sanz","Virgile Foy"],"pdf_url":"https://arxiv.org/pdf/2403.07471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07465v1","updated":"2024-03-12T10:00:06Z","published":"2024-03-12T10:00:06Z","title":"One for All and All for One: GNN-based Control-Flow Attestation for\n  Embedded Devices","summary":"  Control-Flow Attestation (CFA) is a security service that allows an entity\n(verifier) to verify the integrity of code execution on a remote computer\nsystem (prover). Existing CFA schemes suffer from impractical assumptions, such\nas requiring access to the prover's internal state (e.g., memory or code), the\ncomplete Control-Flow Graph (CFG) of the prover's software, large sets of\nmeasurements, or tailor-made hardware. Moreover, current CFA schemes are\ninadequate for attesting embedded systems due to their high computational\noverhead and resource usage.\n  In this paper, we overcome the limitations of existing CFA schemes for\nembedded devices by introducing RAGE, a novel, lightweight CFA approach with\nminimal requirements. RAGE can detect Code Reuse Attacks (CRA), including\ncontrol- and non-control-data attacks. It efficiently extracts features from\none execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to\nidentify deviations from benign executions. The core intuition behind RAGE is\nto exploit the correspondence between execution trace, execution graph, and\nexecution embeddings to eliminate the unrealistic requirement of having access\nto a complete CFG.\n  We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects\n40 real-world attacks on embedded software; (ii) Further, we stress our scheme\nwith synthetic return-oriented programming (ROP) and data-oriented programming\n(DOP) attacks on the real-world embedded software benchmark Embench, achieving\n98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive\nRate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by\nmillions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP\nattack detection, with an FPR of 5.47%.\n","authors":["Marco Chilese","Richard Mitev","Meni Orenbach","Robert Thorburn","Ahmad Atamli","Ahmad-Reza Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2403.07465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07460v1","updated":"2024-03-12T09:57:45Z","published":"2024-03-12T09:57:45Z","title":"Experimental Comparison of Ensemble Methods and Time-to-Event Analysis\n  Models Through Integrated Brier Score and Concordance Index","summary":"  Time-to-event analysis is a branch of statistics that has increased in\npopularity during the last decades due to its many application fields, such as\npredictive maintenance, customer churn prediction and population lifetime\nestimation. In this paper, we review and compare the performance of several\nprediction models for time-to-event analysis. These consist of semi-parametric\nand parametric statistical models, in addition to machine learning approaches.\nOur study is carried out on three datasets and evaluated in two different\nscores (the integrated Brier score and concordance index). Moreover, we show\nhow ensemble methods, which surprisingly have not yet been much studied in\ntime-to-event analysis, can improve the prediction accuracy and enhance the\nrobustness of the prediction performance. We conclude the analysis with a\nsimulation experiment in which we evaluate the factors influencing the\nperformance ranking of the methods using both scores.\n","authors":["Camila Fernandez","Chung Shue Chen","Chen Pierre Gaillard","Alonso Silva"],"pdf_url":"https://arxiv.org/pdf/2403.07460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07456v1","updated":"2024-03-12T09:51:05Z","published":"2024-03-12T09:51:05Z","title":"A tutorial on multi-view autoencoders using the multi-view-AE library","summary":"  There has been a growing interest in recent years in modelling multiple\nmodalities (or views) of data to for example, understand the relationship\nbetween modalities or to generate missing data. Multi-view autoencoders have\ngained significant traction for their adaptability and versatility in modelling\nmulti-modal data, demonstrating an ability to tailor their approach to suit the\ncharacteristics of the data at hand. However, most multi-view autoencoders have\ninconsistent notation and are often implemented using different coding\nframeworks. To address this, we present a unified mathematical framework for\nmulti-view autoencoders, consolidating their formulations. Moreover, we offer\ninsights into the motivation and theoretical advantages of each model. To\nfacilitate accessibility and practical use, we extend the documentation and\nfunctionality of the previously introduced \\texttt{multi-view-AE} library. This\nlibrary offers Python implementations of numerous multi-view autoencoder\nmodels, presented within a user-friendly framework. Through benchmarking\nexperiments, we evaluate our implementations against previous ones,\ndemonstrating comparable or superior performance. This work aims to establish a\ncohesive foundation for multi-modal modelling, serving as a valuable\neducational resource in the field.\n","authors":["Ana Lawry Aguila","Andre Altmann"],"pdf_url":"https://arxiv.org/pdf/2403.07456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01548v3","updated":"2024-03-12T09:49:28Z","published":"2024-03-03T15:53:41Z","title":"In-Context Sharpness as Alerts: An Inner Representation Perspective for\n  Hallucination Mitigation","summary":"  Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.\n","authors":["Shiqi Chen","Miao Xiong","Junteng Liu","Zhengxuan Wu","Teng Xiao","Siyang Gao","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2403.01548v3.pdf","comment":"code repo is available at:\n  https://github.com/hkust-nlp/Activation_decoding.git"},{"id":"http://arxiv.org/abs/2403.07454v1","updated":"2024-03-12T09:48:17Z","published":"2024-03-12T09:48:17Z","title":"Fast, accurate and lightweight sequential simulation-based inference\n  using Gaussian locally linear mappings","summary":"  Bayesian inference for complex models with an intractable likelihood can be\ntackled using algorithms performing many calls to computer simulators. These\napproaches are collectively known as \"simulation-based inference\" (SBI). Recent\nSBI methods have made use of neural networks (NN) to provide approximate, yet\nexpressive constructs for the unavailable likelihood function and the posterior\ndistribution. However, they do not generally achieve an optimal trade-off\nbetween accuracy and computational demand. In this work, we propose an\nalternative that provides both approximations to the likelihood and the\nposterior distribution, using structured mixtures of probability distributions.\nOur approach produces accurate posterior inference when compared to\nstate-of-the-art NN-based SBI methods, while exhibiting a much smaller\ncomputational footprint. We illustrate our results on several benchmark models\nfrom the SBI literature.\n","authors":["Henrik Häggström","Pedro L. C. Rodrigues","Geoffroy Oudoumanessah","Florence Forbes","Umberto Picchini"],"pdf_url":"https://arxiv.org/pdf/2403.07454v1.pdf","comment":"58 pages, 55 figures"},{"id":"http://arxiv.org/abs/2403.07447v1","updated":"2024-03-12T09:37:22Z","published":"2024-03-12T09:37:22Z","title":"Ab-initio variational wave functions for the time-dependent\n  many-electron Schrödinger equation","summary":"  Describing the dynamics of many-electron quantum systems is crucial for\napplications such as predicting electronic structures in quantum chemistry, the\nproperties of condensed matter systems, and the behaviors of complex materials.\nHowever, the real-time evolution of non-equilibrium quantum electronic systems\nposes a significant challenge for theoretical and computational approaches, due\nto the system's exploration of a vast configuration space. This work introduces\na variational approach for fermionic time-dependent wave functions, surpassing\nmean-field approximations by capturing many-body correlations. The proposed\nmethodology involves parameterizing the time-evolving quantum state, enabling\nthe approximation of the state's evolution. To account for electron\ncorrelations, we employ time-dependent Jastrow factors and backflow\ntransformations. We also show that we can incorporate neural networks to\nparameterize these functions. The time-dependent variational Monte Carlo\ntechnique is employed to efficiently compute the optimal time-dependent\nparameters. The approach is demonstrated in three distinct systems: the\nsolvable harmonic interaction model, the dynamics of a diatomic molecule in\nintense laser fields, and a quenched quantum dot. In all cases, we show clear\nsignatures of many-body correlations in the dynamics not captured by mean-field\nmethods. The results showcase the ability of our variational approach to\naccurately capture the time evolution of quantum states, providing insight into\nthe quantum dynamics of interacting electronic systems, beyond the capabilities\nof mean-field.\n","authors":["Jannes Nys","Gabriel Pescia","Giuseppe Carleo"],"pdf_url":"https://arxiv.org/pdf/2403.07447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00892v2","updated":"2024-03-12T09:36:27Z","published":"2024-03-01T13:47:39Z","title":"PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase\n  Distribution Systems","summary":"  Efficiently solving unbalanced three-phase power flow in distribution grids\nis pivotal for grid analysis and simulation. There is a pressing need for\nscalable algorithms capable of handling large-scale unbalanced power grids that\ncan provide accurate and fast solutions. To address this, deep learning\ntechniques, especially Graph Neural Networks (GNNs), have emerged. However,\nexisting literature primarily focuses on balanced networks, leaving a critical\ngap in supporting unbalanced three-phase power grids. This letter introduces\nPowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for\nunbalanced three-phase power grids. The proposed approach models each phase\nseparately in a multigraph representation, effectively capturing the inherent\nasymmetry in unbalanced grids. A graph embedding mechanism utilizing message\npassing is introduced to capture spatial dependencies within the power system\nnetwork. PowerFlowMultiNet outperforms traditional methods and other deep\nlearning approaches in terms of accuracy and computational speed. Rigorous\ntesting reveals significantly lower error rates and a notable hundredfold\nincrease in computational speed for large power networks compared to\nmodel-based methods.\n","authors":["Salah Ghamizi","Jun Cao","Aoxiang Ma","Pedro Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.00892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07442v1","updated":"2024-03-12T09:32:41Z","published":"2024-03-12T09:32:41Z","title":"Proxy Methods for Domain Adaptation","summary":"  We study the problem of domain adaptation under distribution shift, where the\nshift is due to a change in the distribution of an unobserved, latent variable\nthat confounds both the covariates and the labels. In this setting, neither the\ncovariate shift nor the label shift assumptions apply. Our approach to\nadaptation employs proximal causal learning, a technique for estimating causal\neffects in settings where proxies of unobserved confounders are available. We\ndemonstrate that proxy variables allow for adaptation to distribution shift\nwithout explicitly recovering or modeling latent variables. We consider two\nsettings, (i) Concept Bottleneck: an additional ''concept'' variable is\nobserved that mediates the relationship between the covariates and labels; (ii)\nMulti-domain: training data from multiple source domains is available, where\neach source domain exhibits a different distribution over the latent\nconfounder. We develop a two-stage kernel estimation approach to adapt to\ncomplex distribution shifts in both settings. In our experiments, we show that\nour approach outperforms other methods, notably those which explicitly recover\nthe latent confounder.\n","authors":["Katherine Tsai","Stephen R. Pfohl","Olawale Salaudeen","Nicole Chiou","Matt J. Kusner","Alexander D'Amour","Sanmi Koyejo","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2403.07442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03580v5","updated":"2024-03-12T09:27:38Z","published":"2022-02-04T09:11:13Z","title":"Convolutional Neural Networks on Graphs with Chebyshev Approximation,\n  Revisited","summary":"  Designing spectral convolutional networks is a challenging problem in graph\nlearning. ChebNet, one of the early attempts, approximates the spectral graph\nconvolutions using Chebyshev polynomials. GCN simplifies ChebNet by utilizing\nonly the first two Chebyshev polynomials while still outperforming it on\nreal-world datasets. GPR-GNN and BernNet demonstrate that the Monomial and\nBernstein bases also outperform the Chebyshev basis in terms of learning the\nspectral graph convolutions. Such conclusions are counter-intuitive in the\nfield of approximation theory, where it is established that the Chebyshev\npolynomial achieves the optimum convergent rate for approximating a function.\n  In this paper, we revisit the problem of approximating the spectral graph\nconvolutions with Chebyshev polynomials. We show that ChebNet's inferior\nperformance is primarily due to illegal coefficients learnt by ChebNet\napproximating analytic filter functions, which leads to over-fitting. We then\npropose ChebNetII, a new GNN model based on Chebyshev interpolation, which\nenhances the original Chebyshev polynomial approximation while reducing the\nRunge phenomenon. We conducted an extensive experimental study to demonstrate\nthat ChebNetII can learn arbitrary graph convolutions and achieve superior\nperformance in both full- and semi-supervised node classification tasks. Most\nnotably, we scale ChebNetII to a billion graph ogbn-papers100M, showing that\nspectral-based GNNs have superior performance. Our code is available at\nhttps://github.com/ivam-he/ChebNetII.\n","authors":["Mingguo He","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2202.03580v5.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2305.03515v6","updated":"2024-03-12T09:22:51Z","published":"2023-05-05T13:24:35Z","title":"GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent","summary":"  Decision Trees (DTs) are commonly used for many machine learning tasks due to\ntheir high degree of interpretability. However, learning a DT from data is a\ndifficult optimization problem, as it is non-convex and non-differentiable.\nTherefore, common approaches learn DTs using a greedy growth algorithm that\nminimizes the impurity locally at each internal node. Unfortunately, this\ngreedy procedure can lead to inaccurate trees. In this paper, we present a\nnovel approach for learning hard, axis-aligned DTs with gradient descent. The\nproposed method uses backpropagation with a straight-through operator on a\ndense DT representation, to jointly optimize all tree parameters. Our approach\noutperforms existing methods on binary classification benchmarks and achieves\ncompetitive results for multi-class tasks. The method is available under:\nhttps://github.com/s-marton/GradTree\n","authors":["Sascha Marton","Stefan Lüdtke","Christian Bartelt","Heiner Stuckenschmidt"],"pdf_url":"https://arxiv.org/pdf/2305.03515v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07431v1","updated":"2024-03-12T09:15:12Z","published":"2024-03-12T09:15:12Z","title":"Knowledge Transfer across Multiple Principal Component Analysis Studies","summary":"  Transfer learning has aroused great interest in the statistical community. In\nthis article, we focus on knowledge transfer for unsupervised learning tasks in\ncontrast to the supervised learning tasks in the literature. Given the\ntransferable source populations, we propose a two-step transfer learning\nalgorithm to extract useful information from multiple source principal\ncomponent analysis (PCA) studies, thereby enhancing estimation accuracy for the\ntarget PCA task. In the first step, we integrate the shared subspace\ninformation across multiple studies by a proposed method named as Grassmannian\nbarycenter, instead of directly performing PCA on the pooled dataset. The\nproposed Grassmannian barycenter method enjoys robustness and computational\nadvantages in more general cases. Then the resulting estimator for the shared\nsubspace from the first step is further utilized to estimate the target private\nsubspace in the second step. Our theoretical analysis credits the gain of\nknowledge transfer between PCA studies to the enlarged eigenvalue gap, which is\ndifferent from the existing supervised transfer learning tasks where sparsity\nplays the central role. In addition, we prove that the bilinear forms of the\nempirical spectral projectors have asymptotic normality under weaker eigenvalue\ngap conditions after knowledge transfer. When the set of informativesources is\nunknown, we endow our algorithm with the capability of useful dataset selection\nby solving a rectified optimization problem on the Grassmann manifold, which in\nturn leads to a computationally friendly rectified Grassmannian K-means\nprocedure. In the end, extensive numerical simulation results and a real data\ncase concerning activity recognition are reported to support our theoretical\nclaims and to illustrate the empirical usefulness of the proposed transfer\nlearning methods.\n","authors":["Zeyu Li","Kangxiang Qin","Yong He","Wang Zhou","Xinsheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05818v2","updated":"2024-03-12T08:55:00Z","published":"2024-03-09T06:58:21Z","title":"PR-NET: Leveraging Pathway Refined Network Structures for Prostate\n  Cancer Patient Condition Prediction","summary":"  The diagnosis and monitoring of Castrate Resistant Prostate Cancer (CRPC) are\ncrucial for cancer patients, but the current models (such as P-NET) have\nlimitations in terms of parameter count, generalization, and cost. To address\nthe issue, we develop a more accurate and efficient Prostate Cancer patient\ncondition prediction model, named PR-NET. By compressing and optimizing the\nnetwork structure of P-NET, the model complexity is reduced while maintaining\nhigh accuracy and interpretability. The PR-NET demonstrated superior\nperformance in predicting prostate cancer patient outcomes, outshining P-NET\nand six other traditional models with a significant margin. In our rigorous\nevaluation, PR-NET not only achieved impressive average AUC and Recall scores\nof 0.94 and 0.83, respectively, on known data but also maintained robust\ngeneralizability on five unknown datasets with a higher average AUC of 0.73 and\nRecall of 0.72, compared to P-NET's 0.68 and 0.5. PR-NET's efficiency was\nevidenced by its shorter average training and inference times, and its\ngene-level analysis revealed 46 key genes, demonstrating its enhanced\npredictive power and efficiency in identifying critical biomarkers for prostate\ncancer. Future research can further expand its application domains and optimize\nthe model's performance and reliability.\n","authors":["R. Li","J. Liu","X. L. Deng","X. Liu","J. C. Guo","W. Y. Wu","L. Yang"],"pdf_url":"https://arxiv.org/pdf/2403.05818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07413v1","updated":"2024-03-12T08:40:21Z","published":"2024-03-12T08:40:21Z","title":"Learning-Augmented Algorithms with Explicit Predictors","summary":"  Recent advances in algorithmic design show how to utilize predictions\nobtained by machine learning models from past and present data. These\napproaches have demonstrated an enhancement in performance when the predictions\nare accurate, while also ensuring robustness by providing worst-case guarantees\nwhen predictions fail. In this paper we focus on online problems; prior\nresearch in this context was focused on a paradigm where the predictor is\npre-trained on past data and then used as a black box (to get the predictions\nit was trained for). In contrast, in this work, we unpack the predictor and\nintegrate the learning problem it gives rise for within the algorithmic\nchallenge. In particular we allow the predictor to learn as it receives larger\nparts of the input, with the ultimate goal of designing online learning\nalgorithms specifically tailored for the algorithmic task at hand. Adopting\nthis perspective, we focus on a number of fundamental problems, including\ncaching and scheduling, which have been well-studied in the black-box setting.\nFor each of the problems we consider, we introduce new algorithms that take\nadvantage of explicit learning algorithms which we carefully design towards\noptimizing the overall performance. We demonstrate the potential of our\napproach by deriving performance bounds which improve over those established in\nprevious work.\n","authors":["Marek Elias","Haim Kaplan","Yishay Mansour","Shay Moran"],"pdf_url":"https://arxiv.org/pdf/2403.07413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07404v1","updated":"2024-03-12T08:33:26Z","published":"2024-03-12T08:33:26Z","title":"Accelerated Inference and Reduced Forgetting: The Dual Benefits of\n  Early-Exit Networks in Continual Learning","summary":"  Driven by the demand for energy-efficient employment of deep neural networks,\nearly-exit methods have experienced a notable increase in research attention.\nThese strategies allow for swift predictions by making decisions early in the\nnetwork, thereby conserving computation time and resources. However, so far the\nearly-exit networks have only been developed for stationary data distributions,\nwhich restricts their application in real-world scenarios with continuous\nnon-stationary data. This study aims to explore the continual learning of the\nearly-exit networks. We adapt existing continual learning methods to fit with\nearly-exit architectures and investigate their behavior in the continual\nsetting. We notice that early network layers exhibit reduced forgetting and can\noutperform standard networks even when using significantly fewer resources.\nFurthermore, we analyze the impact of task-recency bias on early-exit inference\nand propose Task-wise Logits Correction (TLC), a simple method that equalizes\nthis bias and improves the network performance for every given compute budget\nin the class-incremental setting. We assess the accuracy and computational cost\nof various continual learning techniques enhanced with early-exits and TLC\nacross standard class-incremental learning benchmarks such as 10 split CIFAR100\nand ImageNetSubset and show that TLC can achieve the accuracy of the standard\nmethods using less than 70\\% of their computations. Moreover, at full\ncomputational budget, our method outperforms the accuracy of the standard\ncounterparts by up to 15 percentage points. Our research underscores the\ninherent synergy between early-exit networks and continual learning,\nemphasizing their practical utility in resource-constrained environments.\n","authors":["Filip Szatkowski","Fei Yang","Bartłomiej Twardowski","Tomasz Trzciński","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2403.07404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17130v3","updated":"2024-03-12T08:30:39Z","published":"2023-09-29T10:49:14Z","title":"GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data","summary":"  Despite the success of deep learning for text and image data, tree-based\nensemble models are still state-of-the-art for machine learning with\nheterogeneous tabular data. However, there is a significant need for\ntabular-specific gradient-based methods due to their high flexibility. In this\npaper, we propose $\\text{GRANDE}$, $\\text{GRA}$die$\\text{N}$t-Based\n$\\text{D}$ecision Tree $\\text{E}$nsembles, a novel approach for learning hard,\naxis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE\nis based on a dense representation of tree ensembles, which affords to use\nbackpropagation with a straight-through operator to jointly optimize all model\nparameters. Our method combines axis-aligned splits, which is a useful\ninductive bias for tabular data, with the flexibility of gradient-based\noptimization. Furthermore, we introduce an advanced instance-wise weighting\nthat facilitates learning representations for both, simple and complex\nrelations, within a single model. We conducted an extensive evaluation on a\npredefined benchmark with 19 classification datasets and demonstrate that our\nmethod outperforms existing gradient-boosting and deep learning frameworks on\nmost datasets. The method is available under:\nhttps://github.com/s-marton/GRANDE\n","authors":["Sascha Marton","Stefan Lüdtke","Christian Bartelt","Heiner Stuckenschmidt"],"pdf_url":"https://arxiv.org/pdf/2309.17130v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14992v2","updated":"2024-03-12T08:19:09Z","published":"2023-10-23T14:45:51Z","title":"Bayesian Regression Markets","summary":"  Machine learning tasks are vulnerable to the quality of data used as input.\nYet, it is often challenging for firms to obtain adequate datasets, with them\nbeing naturally distributed amongst owners, that in practice, may be\ncompetitors in a downstream market and reluctant to share information. Focusing\non supervised learning for regression tasks, we develop a regression market to\nprovide a monetary incentive for data sharing. Our proposed mechanism adopts a\nBayesian framework, allowing us to consider a more general class of regression\ntasks. We present a thorough exploration of the market properties, and show\nthat similar proposals in current literature expose the market agents to\nsizeable financial risks, which can be mitigated in our setup.\n","authors":["Thomas Falconer","Jalal Kazempour","Pierre Pinson"],"pdf_url":"https://arxiv.org/pdf/2310.14992v2.pdf","comment":"41 pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2106.10866v3","updated":"2024-03-12T08:14:31Z","published":"2021-06-21T05:38:26Z","title":"Customizing Graph Neural Networks using Path Reweighting","summary":"  Graph Neural Networks (GNNs) have been extensively used for mining\ngraph-structured data with impressive performance. However, because these\ntraditional GNNs do not distinguish among various downstream tasks, embeddings\nembedded by them are not always effective. Intuitively, paths in a graph imply\ndifferent semantics for different downstream tasks. Inspired by this, we design\na novel GNN solution, namely Customized Graph Neural Network with Path\nReweighting (CustomGNN for short). Specifically, the proposed CustomGNN can\nautomatically learn the high-level semantics for specific downstream tasks to\nhighlight semantically relevant paths as well to filter out task-irrelevant\nnoises in a graph. Furthermore, we empirically analyze the semantics learned by\nCustomGNN and demonstrate its ability to avoid the three inherent problems in\ntraditional GNNs, i.e., over-smoothing, poor robustness, and overfitting. In\nexperiments with the node classification task, CustomGNN achieves\nstate-of-the-art accuracies on three standard graph datasets and four large\ngraph datasets. The source code of the proposed CustomGNN is available at\n\\url{https://github.com/cjpcool/CustomGNN}.\n","authors":["Jianpeng Chen","Yujing Wang","Ming Zeng","Zongyi Xiang","Bitan Hou","Yunhai Tong","Ole J. Mengshoel","Yazhou Ren"],"pdf_url":"https://arxiv.org/pdf/2106.10866v3.pdf","comment":"25 pages with 14 figures"},{"id":"http://arxiv.org/abs/2401.15879v3","updated":"2024-03-12T07:53:36Z","published":"2024-01-29T04:21:47Z","title":"lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold\n  Gap","summary":"  Good arm identification (GAI) is a pure-exploration bandit problem in which a\nsingle learner outputs an arm as soon as it is identified as a good arm. A good\narm is defined as an arm with an expected reward greater than or equal to a\ngiven threshold. This paper focuses on the GAI problem under a small threshold\ngap, which refers to the distance between the expected rewards of arms and the\ngiven threshold. We propose a new algorithm called lil'HDoC to significantly\nimprove the total sample complexity of the HDoC algorithm. We demonstrate that\nthe sample complexity of the first $\\lambda$ output arm in lil'HDoC is bounded\nby the original HDoC algorithm, except for one negligible term, when the\ndistance between the expected reward and threshold is small. Extensive\nexperiments confirm that our algorithm outperforms the state-of-the-art\nalgorithms in both synthetic and real-world datasets.\n","authors":["Tzu-Hsien Tsai","Yun-Da Tsai","Shou-De Lin"],"pdf_url":"https://arxiv.org/pdf/2401.15879v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13212v2","updated":"2024-03-12T07:49:44Z","published":"2023-08-25T07:15:58Z","title":"SEGNO: Generalizing Equivariant Graph Neural Networks with Physical\n  Inductive Biases","summary":"  Graph Neural Networks (GNNs) with equivariant properties have emerged as\npowerful tools for modeling complex dynamics of multi-object physical systems.\nHowever, their generalization ability is limited by the inadequate\nconsideration of physical inductive biases: (1) Existing studies overlook the\ncontinuity of transitions among system states, opting to employ several\ndiscrete transformation layers to learn the direct mapping between two adjacent\nstates; (2) Most models only account for first-order velocity information,\ndespite the fact that many physical systems are governed by second-order motion\nlaws. To incorporate these inductive biases, we propose the Second-order\nEquivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically,\nwe show how the second-order continuity can be incorporated into GNNs while\nmaintaining the equivariant property. Furthermore, we offer theoretical\ninsights into SEGNO, highlighting that it can learn a unique trajectory between\nadjacent states, which is crucial for model generalization. Additionally, we\nprove that the discrepancy between this learned trajectory of SEGNO and the\ntrue trajectory is bounded. Extensive experiments on complex dynamical systems\nincluding molecular dynamics and motion capture demonstrate that our model\nyields a significant improvement over the state-of-the-art baselines.\n","authors":["Yang Liu","Jiashun Cheng","Haihong Zhao","Tingyang Xu","Peilin Zhao","Fugee Tsung","Jia Li","Yu Rong"],"pdf_url":"https://arxiv.org/pdf/2308.13212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07384v1","updated":"2024-03-12T07:45:33Z","published":"2024-03-12T07:45:33Z","title":"SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models","summary":"  Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.\n","authors":["Yu Yang","Siddhartha Mishra","Jeffrey N Chiang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2403.07384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15441v2","updated":"2024-03-12T07:44:00Z","published":"2024-02-13T09:19:05Z","title":"Active Few-Shot Fine-Tuning","summary":"  We study the active few-shot fine-tuning of large neural networks to\ndownstream tasks. We show that few-shot fine-tuning is an instance of a\ngeneralization of classical active learning, transductive active learning, and\nwe propose ITL, short for information-based transductive learning, an approach\nwhich samples adaptively to maximize the information gained about specified\ndownstream tasks. Under general regularity assumptions, we prove that ITL\nconverges uniformly to the smallest possible uncertainty obtainable from the\naccessible data. To the best of our knowledge, we are the first to derive\ngeneralization bounds of this kind, and they may be of independent interest for\nactive learning. We apply ITL to the few-shot fine-tuning of large neural\nnetworks and show that ITL substantially improves upon the state-of-the-art.\n","authors":["Jonas Hübotter","Bhavya Sukhija","Lenart Treven","Yarden As","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2402.15441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15898v2","updated":"2024-03-12T07:37:03Z","published":"2024-02-13T09:22:45Z","title":"Information-based Transductive Active Learning","summary":"  We generalize active learning to address real-world settings where sampling\nis restricted to an accessible region of the domain, while prediction targets\nmay lie outside this region. To this end, we propose ITL, short for\ninformation-based transductive learning, an approach which samples adaptively\nto maximize the information gained about specified prediction targets. We show,\nunder general regularity assumptions, that ITL converges uniformly to the\nsmallest possible uncertainty obtainable from the accessible data. We\ndemonstrate ITL in two key applications: Few-shot fine-tuning of large neural\nnetworks and safe Bayesian optimization, and in both cases, ITL significantly\noutperforms the state-of-the-art.\n","authors":["Jonas Hübotter","Bhavya Sukhija","Lenart Treven","Yarden As","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2402.15898v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.15441"},{"id":"http://arxiv.org/abs/2311.06295v2","updated":"2024-03-12T07:36:05Z","published":"2023-11-05T11:48:08Z","title":"Gradual Optimization Learning for Conformational Energy Minimization","summary":"  Molecular conformation optimization is crucial to computer-aided drug\ndiscovery and materials design. Traditional energy minimization techniques rely\non iterative optimization methods that use molecular forces calculated by a\nphysical simulator (oracle) as anti-gradients. However, this is a\ncomputationally expensive approach that requires many interactions with a\nphysical simulator. One way to accelerate this procedure is to replace the\nphysical simulator with a neural network. Despite recent progress in neural\nnetworks for molecular conformation energy prediction, such models are prone to\ndistribution shift, leading to inaccurate energy minimization. We find that the\nquality of energy minimization with neural networks can be improved by\nproviding optimization trajectories as additional training data. Still, it\ntakes around $5 \\times 10^5$ additional conformations to match the physical\nsimulator's optimization quality. In this work, we present the Gradual\nOptimization Learning Framework (GOLF) for energy minimization with neural\nnetworks that significantly reduces the required additional data. The framework\nconsists of an efficient data-collecting scheme and an external optimizer. The\nexternal optimizer utilizes gradients from the energy prediction model to\ngenerate optimization trajectories, and the data-collecting scheme selects\nadditional training data to be processed by the physical simulator. Our results\ndemonstrate that the neural network trained with GOLF performs on par with the\noracle on a benchmark of diverse drug-like molecules using $50$x less\nadditional data.\n","authors":["Artem Tsypin","Leonid Ugadiarov","Kuzma Khrabrov","Alexander Telepov","Egor Rumiantsev","Alexey Skrynnik","Aleksandr I. Panov","Dmitry Vetrov","Elena Tutubalina","Artur Kadurin"],"pdf_url":"https://arxiv.org/pdf/2311.06295v2.pdf","comment":"Published as a conference paper at ICLR2024 (Poster)"},{"id":"http://arxiv.org/abs/2403.01841v2","updated":"2024-03-12T07:34:28Z","published":"2024-03-04T08:38:56Z","title":"Making Pre-trained Language Models Great on Tabular Prediction","summary":"  The transferability of deep neural networks (DNNs) has made significant\nprogress in image and language processing. However, due to the heterogeneity\namong tables, such DNN bonus is still far from being well exploited on tabular\ndata prediction (e.g., regression or classification tasks). Condensing\nknowledge from diverse domains, language models (LMs) possess the capability to\ncomprehend feature names from various tables, potentially serving as versatile\nlearners in transferring knowledge across distinct tables and diverse\nprediction tasks, but their discrete text representation space is inherently\nincompatible with numerical feature values in tables. In this paper, we present\nTP-BERTa, a specifically pre-trained LM for tabular data prediction.\nConcretely, a novel relative magnitude tokenization converts scalar numerical\nfeature values to finely discrete, high-dimensional tokens, and an\nintra-feature attention approach integrates feature values with the\ncorresponding feature names. Comprehensive experiments demonstrate that our\npre-trained TP-BERTa leads the performance among tabular DNNs and is\ncompetitive with Gradient Boosted Decision Tree models in typical tabular data\nregime.\n","authors":["Jiahuan Yan","Bo Zheng","Hongxia Xu","Yiheng Zhu","Danny Z. Chen","Jimeng Sun","Jian Wu","Jintai Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01841v2.pdf","comment":"Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).\n  OpenReview link is https://openreview.net/forum?id=anzIzGZuLi, codes will be\n  available at https://github.com/jyansir/tp-berta"},{"id":"http://arxiv.org/abs/2403.07379v1","updated":"2024-03-12T07:32:47Z","published":"2024-03-12T07:32:47Z","title":"Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The\n  Lengths, Bends, and Dead Ends","summary":"  We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich structure of parameters contained within their optimization\ntrajectories. Towards this end, we introduce some natural notions of the\ncomplexity of optimization trajectories, both qualitative and quantitative,\nwhich reveal the inherent nuance and interplay involved between various\noptimization choices, such as momentum, weight decay, and batch size. We use\nthem to provide key hallmarks about the nature of optimization in deep neural\nnetworks: when it goes right, and when it finds itself in a dead end. Further,\nthanks to our trajectory perspective, we uncover an intertwined behaviour of\nmomentum and weight decay that promotes directional exploration, as well as a\ndirectional regularization behaviour of some others. We perform experiments\nover large-scale vision and language settings, including large language models\n(LLMs) with up to 12 billion parameters, to demonstrate the value of our\napproach.\n","authors":["Sidak Pal Singh","Bobby He","Thomas Hofmann","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.07379v1.pdf","comment":"Preprint, 51 pages"},{"id":"http://arxiv.org/abs/2403.07378v1","updated":"2024-03-12T07:31:18Z","published":"2024-03-12T07:31:18Z","title":"SVD-LLM: Truncation-aware Singular Value Decomposition for Large\n  Language Model Compression","summary":"  The advancements in Large Language Models (LLMs) have been hindered by their\nsubstantial sizes, which necessitate LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression methods\nhave two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the remaining model parameters\nafter SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM\ncompression method that addresses the limitations of existing methods. SVD-LLM\nincorporates a truncation-aware data whitening strategy to ensure a direct\nmapping between singular values and compression loss. Moreover, SVD-LLM adopts\na layer-wise closed-form model parameter update strategy to compensate for\naccuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total\nof 11 datasets and seven models from three different LLM families at four\ndifferent scales. Our results demonstrate the superiority of SVD-LLM over\nstate-of-the-arts, especially at high model compression ratios. The source code\nis available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.\n","authors":["Xin Wang","Yu Zheng","Zhongwei Wan","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07378v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2305.10015v2","updated":"2024-03-12T07:26:45Z","published":"2023-05-17T07:49:16Z","title":"Utility Theory of Synthetic Data Generation","summary":"  Synthetic data algorithms are widely employed in industries to generate\nartificial data for downstream learning tasks. While existing research\nprimarily focuses on empirically evaluating utility of synthetic data, its\ntheoretical understanding is largely lacking. This paper bridges the\npractice-theory gap by establishing relevant utility theory in a statistical\nlearning framework. It considers two utility metrics: generalization and\nranking of models trained on synthetic data. The former is defined as the\ngeneralization difference between models trained on synthetic and on real data.\nBy deriving analytical bounds for this utility metric, we demonstrate that the\nsynthetic feature distribution does not need to be similar as that of real data\nfor ensuring comparable generalization of synthetic models, provided proper\nmodel specifications in downstream learning tasks. The latter utility metric\nstudies the relative performance of models trained on synthetic data. In\nparticular, we discover that the distribution of synthetic data is not\nnecessarily similar as the real one to ensure consistent model comparison.\nInterestingly, consistent model comparison is still achievable even when\nsynthetic responses are not well generated, as long as downstream models are\nseparable by a generalization gap. Finally, extensive experiments on\nnon-parametric models and deep neural networks have been conducted to validate\nthese theoretical findings.\n","authors":["Shirong Xu","Will Wei Sun","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2305.10015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13339v2","updated":"2024-03-12T07:21:04Z","published":"2023-04-26T07:13:50Z","title":"OpenBox: A Python Toolkit for Generalized Black-box Optimization","summary":"  Black-box optimization (BBO) has a broad range of applications, including\nautomatic machine learning, experimental design, and database knob tuning.\nHowever, users still face challenges when applying BBO methods to their\nproblems at hand with existing software packages in terms of applicability,\nperformance, and efficiency. This paper presents OpenBox, an open-source BBO\ntoolkit with improved usability. It implements user-friendly inferfaces and\nvisualization for users to define and manage their tasks. The modular design\nbehind OpenBox facilitates its flexible deployment in existing systems.\nExperimental results demonstrate the effectiveness and efficiency of OpenBox\nover existing systems. The source code of OpenBox is available at\nhttps://github.com/PKU-DAIR/open-box.\n","authors":["Huaijun Jiang","Yu Shen","Yang Li","Beicheng Xu","Sixian Du","Wentao Zhang","Ce Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2304.13339v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07366v1","updated":"2024-03-12T07:01:57Z","published":"2024-03-12T07:01:57Z","title":"Entropy is not Enough for Test-Time Adaptation: From the Perspective of\n  Disentangled Factors","summary":"  Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for\nunseen test data. The primary challenge of TTA is limited access to the entire\ntest dataset during online updates, causing error accumulation. To mitigate it,\nTTA methods have utilized the model output's entropy as a confidence metric\nthat aims to determine which samples have a lower likelihood of causing error.\nThrough experimental studies, however, we observed the unreliability of entropy\nas a confidence metric for TTA under biased scenarios and theoretically\nrevealed that it stems from the neglect of the influence of latent disentangled\nfactors of data on predictions. Building upon these findings, we introduce a\nnovel TTA method named Destroy Your Object (DeYO), which leverages a newly\nproposed confidence metric named Pseudo-Label Probability Difference (PLPD).\nPLPD quantifies the influence of the shape of an object on prediction by\nmeasuring the difference between predictions before and after applying an\nobject-destructive transformation. DeYO consists of sample selection and sample\nweighting, which employ entropy and PLPD concurrently. For robust adaptation,\nDeYO prioritizes samples that dominantly incorporate shape information when\nmaking predictions. Our extensive experiments demonstrate the consistent\nsuperiority of DeYO over baseline methods across various scenarios, including\nbiased and wild. Project page is publicly available at\nhttps://whitesnowdrop.github.io/DeYO/.\n","authors":["Jonghyun Lee","Dahuin Jung","Saehyung Lee","Junsung Park","Juhyeon Shin","Uiwon Hwang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2403.07366v1.pdf","comment":"ICLR 2024 Spotlight; 26 pages, 9 figures, 20 tables;"},{"id":"http://arxiv.org/abs/2307.09476v3","updated":"2024-03-12T07:00:02Z","published":"2023-07-18T17:56:50Z","title":"Overthinking the Truth: Understanding how Language Models Process False\n  Demonstrations","summary":"  Modern language models can imitate complex patterns through few-shot\nlearning, enabling them to complete challenging tasks without fine-tuning.\nHowever, imitation can also lead models to reproduce inaccuracies or harmful\ncontent if present in the context. We study harmful imitation through the lens\nof a model's internal representations, and identify two related phenomena:\n\"overthinking\" and \"false induction heads\". The first phenomenon, overthinking,\nappears when we decode predictions from intermediate layers, given correct vs.\nincorrect few-shot demonstrations. At early layers, both demonstrations induce\nsimilar model behavior, but the behavior diverges sharply at some \"critical\nlayer\", after which the accuracy given incorrect demonstrations progressively\ndecreases. The second phenomenon, false induction heads, are a possible\nmechanistic cause of overthinking: these are heads in late layers that attend\nto and copy false information from previous demonstrations, and whose ablation\nreduces overthinking. Beyond scientific understanding, our results suggest that\nstudying intermediate model computations could be a promising avenue for\nunderstanding and guarding against harmful model behaviors.\n","authors":["Danny Halawi","Jean-Stanislas Denain","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2307.09476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07362v1","updated":"2024-03-12T06:50:32Z","published":"2024-03-12T06:50:32Z","title":"Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine\n  Unlearning","summary":"  The trustworthy machine learning (ML) community is increasingly recognizing\nthe crucial need for models capable of selectively 'unlearning' data points\nafter training. This leads to the problem of machine unlearning (MU), aiming to\neliminate the influence of chosen data points on model performance, while still\nmaintaining the model's utility post-unlearning. Despite various MU methods for\ndata influence erasure, evaluations have largely focused on random data\nforgetting, ignoring the vital inquiry into which subset should be chosen to\ntruly gauge the authenticity of unlearning performance. To tackle this issue,\nwe introduce a new evaluative angle for MU from an adversarial viewpoint. We\npropose identifying the data subset that presents the most significant\nchallenge for influence erasure, i.e., pinpointing the worst-case forget set.\nUtilizing a bi-level optimization principle, we amplify unlearning challenges\nat the upper optimization level to emulate worst-case scenarios, while\nsimultaneously engaging in standard training and unlearning at the lower level,\nachieving a balance between data influence erasure and model utility. Our\nproposal offers a worst-case evaluation of MU's resilience and effectiveness.\nThrough extensive experiments across different datasets (including CIFAR-10,\n100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image\nclassifiers and generative models), we expose critical pros and cons in\nexisting (approximate) unlearning strategies. Our results illuminate the\ncomplex challenges of MU in practice, guiding the future development of more\naccurate and robust unlearning algorithms. The code is available at\nhttps://github.com/OPTML-Group/Unlearn-WorstCase.\n","authors":["Chongyu Fan","Jiancheng Liu","Alfred Hero","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04990v2","updated":"2024-03-12T06:35:49Z","published":"2024-03-08T02:02:23Z","title":"Jet Discrimination with Quantum Complete Graph Neural Network","summary":"  Machine learning, particularly deep neural networks, has been widely utilized\nin high energy physics and has shown remarkable results in various\napplications. Moreover, the concept of machine learning has been extended to\nquantum computers, giving rise to a new research area known as quantum machine\nlearning. In this paper, we propose a novel variational quantum circuit model,\nQuantum Complete Graph Neural Network (QCGNN), designed for learning complete\ngraphs. We argue that QCGNN has a polynomial speedup against its classical\ncounterpart, due to the property of quantum parallelism. In this paper, we\nstudy the application of QCGNN through the challenging jet discrimination,\nwhere the jets are represented with complete graphs. Subsequently, we conduct a\ncomparative analysis with classical graph neural networks to establish a\nbenchmark.\n","authors":["Yi-An Chen","Kai-Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.04990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07356v1","updated":"2024-03-12T06:29:54Z","published":"2024-03-12T06:29:54Z","title":"Premonition: Using Generative Models to Preempt Future Data Changes in\n  Continual Learning","summary":"  Continual learning requires a model to adapt to ongoing changes in the data\ndistribution, and often to the set of tasks to be performed. It is rare,\nhowever, that the data and task changes are completely unpredictable. Given a\ndescription of an overarching goal or data theme, which we call a realm, humans\ncan often guess what concepts are associated with it. We show here that the\ncombination of a large language model and an image generation model can\nsimilarly provide useful premonitions as to how a continual learning challenge\nmight develop over time. We use the large language model to generate text\ndescriptions of semantically related classes that might potentially appear in\nthe data stream in future. These descriptions are then rendered using Stable\nDiffusion to generate new labelled image samples. The resulting synthetic\ndataset is employed for supervised pre-training, but is discarded prior to\ncommencing continual learning, along with the pre-training classification head.\nWe find that the backbone of our pre-trained networks can learn representations\nuseful for the downstream continual learning problem, thus becoming a valuable\ninput to any existing continual learning method. Although there are\ncomplexities arising from the domain gap between real and synthetic images, we\nshow that pre-training models in this manner improves multiple Class Incremenal\nLearning (CIL) methods on fine-grained image classification benchmarks.\nSupporting code can be found at https://github.com/cl-premonition/premonition.\n","authors":["Mark D. McDonnell","Dong Gong","Ehsan Abbasnejad","Anton van den Hengel"],"pdf_url":"https://arxiv.org/pdf/2403.07356v1.pdf","comment":"31 pages total (14 main paper, 5 references, 12 appendices)"},{"id":"http://arxiv.org/abs/2403.02624v2","updated":"2024-03-12T06:28:39Z","published":"2024-03-05T03:32:02Z","title":"Pareto-Optimal Estimation and Policy Learning on Short-term and\n  Long-term Treatment Effects","summary":"  This paper focuses on developing Pareto-optimal estimation and policy\nlearning to identify the most effective treatment that maximizes the total\nreward from both short-term and long-term effects, which might conflict with\neach other. For example, a higher dosage of medication might increase the speed\nof a patient's recovery (short-term) but could also result in severe long-term\nside effects. Although recent works have investigated the problems about\nshort-term or long-term effects or the both, how to trade-off between them to\nachieve optimal treatment remains an open challenge. Moreover, when multiple\nobjectives are directly estimated using conventional causal representation\nlearning, the optimization directions among various tasks can conflict as well.\nIn this paper, we systematically investigate these issues and introduce a\nPareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and\nPareto-Optimal Policy Learning (POPL), to tackle them. POE incorporates a\ncontinuous Pareto module with representation balancing, enhancing estimation\nefficiency across multiple tasks. As for POPL, it involves deriving short-term\nand long-term outcomes linked with various treatment levels, facilitating an\nexploration of the Pareto frontier emanating from these outcomes. Results on\nboth the synthetic and real-world datasets demonstrate the superiority of our\nmethod.\n","authors":["Yingrong Wang","Anpeng Wu","Haoxuan Li","Weiming Liu","Qiaowei Miao","Ruoxuan Xiong","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2403.02624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07353v1","updated":"2024-03-12T06:22:10Z","published":"2024-03-12T06:22:10Z","title":"Graph Unlearning with Efficient Partial Retraining","summary":"  Graph Neural Networks (GNNs) have achieved remarkable success in various\nreal-world applications. However, GNNs may be trained on undesirable graph\ndata, which can degrade their performance and reliability. To enable trained\nGNNs to efficiently unlearn unwanted data, a desirable solution is\nretraining-based graph unlearning, which partitions the training graph into\nsubgraphs and trains sub-models on them, allowing fast unlearning through\npartial retraining. However, the graph partition process causes information\nloss in the training graph, resulting in the low model utility of sub-GNN\nmodels. In this paper, we propose GraphRevoker, a novel graph unlearning\nframework that better maintains the model utility of unlearnable GNNs.\nSpecifically, we preserve the graph property with graph property-aware sharding\nand effectively aggregate the sub-GNN models for prediction with graph\ncontrastive sub-model aggregation. We conduct extensive experiments to\ndemonstrate the superiority of our proposed approach.\n","authors":["Jiahao Zhang","Lin Wang","Shijie Wang","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2403.07353v1.pdf","comment":"8 pages, 3 figures, accepted by The Web Conference 2024"},{"id":"http://arxiv.org/abs/2305.17283v3","updated":"2024-03-12T05:54:22Z","published":"2023-05-26T22:06:24Z","title":"Sharpened Lazy Incremental Quasi-Newton Method","summary":"  The problem of minimizing the sum of $n$ functions in $d$ dimensions is\nubiquitous in machine learning and statistics. In many applications where the\nnumber of observations $n$ is large, it is necessary to use incremental or\nstochastic methods, as their per-iteration cost is independent of $n$. Of\nthese, Quasi-Newton (QN) methods strike a balance between the per-iteration\ncost and the convergence rate. Specifically, they exhibit a superlinear rate\nwith $O(d^2)$ cost in contrast to the linear rate of first-order methods with\n$O(d)$ cost and the quadratic rate of second-order methods with $O(d^3)$ cost.\nHowever, existing incremental methods have notable shortcomings: Incremental\nQuasi-Newton (IQN) only exhibits asymptotic superlinear convergence. In\ncontrast, Incremental Greedy BFGS (IGS) offers explicit superlinear convergence\nbut suffers from poor empirical performance and has a per-iteration cost of\n$O(d^3)$. To address these issues, we introduce the Sharpened Lazy Incremental\nQuasi-Newton Method (SLIQN) that achieves the best of both worlds: an explicit\nsuperlinear convergence rate, and superior empirical performance at a\nper-iteration $O(d^2)$ cost. SLIQN features two key changes: first, it\nincorporates a hybrid strategy of using both classic and greedy BFGS updates,\nallowing it to empirically outperform both IQN and IGS. Second, it employs a\nclever constant multiplicative factor along with a lazy propagation strategy,\nwhich enables it to have a cost of $O(d^2)$. Additionally, our experiments\ndemonstrate the superiority of SLIQN over other incremental and stochastic\nQuasi-Newton variants and establish its competitiveness with second-order\nincremental methods.\n","authors":["Aakash Lahoti","Spandan Senapati","Ketan Rajawat","Alec Koppel"],"pdf_url":"https://arxiv.org/pdf/2305.17283v3.pdf","comment":"36 pages, 2 figures; Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.07339v1","updated":"2024-03-12T05:44:27Z","published":"2024-03-12T05:44:27Z","title":"IM-Unpack: Training and Inference with Arbitrarily Low Precision\n  Integers","summary":"  GEneral Matrix Multiply (GEMM) is a central operation in deep learning and\ncorresponds to the largest chunk of the compute footprint. Therefore, improving\nits efficiency is an active topic of ongoing research. A popular strategy is\nthe use of low bit-width integers to approximate the original entries in a\nmatrix. This allows efficiency gains, but often requires sophisticated\ntechniques to control the rounding error incurred. In this work, we first\nverify/check that when the low bit-width restriction is removed, for a variety\nof Transformer-based models, whether integers are sufficient for all GEMMs need\n-- for {\\em both} training and inference stages, and can achieve parity with\nfloating point counterparts. No sophisticated techniques are needed. We find\nthat while a large majority of entries in matrices (encountered in such models)\ncan be easily represented by {\\em low} bit-width integers, the existence of a\nfew heavy hitter entries make it difficult to achieve efficiency gains via the\nexclusive use of low bit-width GEMMs alone. To address this issue, we develop a\nsimple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\em unpack} a\nmatrix with large integer entries into a larger matrix whose entries all lie\nwithin the representable range of arbitrarily low bit-width integers. This\nallows {\\em equivalence} with the original GEMM, i.e., the exact result can be\nobtained using purely low bit-width integer GEMMs. This comes at the cost of\nadditional operations -- we show that for many popular models, this overhead is\nquite small.\n","authors":["Zhanpeng Zeng","Karthikeyan Sankaralingam","Vikas Singh"],"pdf_url":"https://arxiv.org/pdf/2403.07339v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.07839v1","updated":"2024-03-12T17:24:26Z","published":"2024-03-12T17:24:26Z","title":"MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with\n  Module-wise Pruning Error Metric","summary":"  Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.\n","authors":["Haokun Lin","Haoli Bai","Zhili Liu","Lu Hou","Muyi Sun","Linqi Song","Ying Wei","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2403.07839v1.pdf","comment":"18 pages, 8 figures, Published in CVPR2024"},{"id":"http://arxiv.org/abs/2403.07613v1","updated":"2024-03-12T12:50:19Z","published":"2024-03-12T12:50:19Z","title":"Imagine a dragon made of seaweed: How images enhance learning in\n  Wikipedia","summary":"  Though images are ubiquitous across Wikipedia, it is not obvious that the\nimage choices optimally support learning. When well selected, images can\nenhance learning by dual coding, complementing, or supporting articles. When\nchosen poorly, images can mislead, distract, and confuse. We developed a large\ndataset containing 470 questions & answers to 94 Wikipedia articles with images\non a wide range of topics. Through an online experiment (n=704), we determined\nwhether the images displayed alongside the text of the article are effective in\nhelping readers understand and learn. For certain tasks, such as learning to\nidentify targets visually (e.g., \"which of these pictures is a gujia?\"),\narticle images significantly improve accuracy. Images did not significantly\nimprove general knowledge questions (e.g., \"where are gujia from?\"). Most\ninterestingly, only some images helped with visual knowledge questions (e.g.,\n\"what shape is a gujia?\"). Using our findings, we reflect on the implications\nfor editors and tools to support image selection.\n","authors":["Anita Silva","Maria Tracy","Katharina Reinecke","Eytan Adar","Miriam Redi"],"pdf_url":"https://arxiv.org/pdf/2403.07613v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2306.03403v2","updated":"2024-03-12T12:41:04Z","published":"2023-06-06T04:49:51Z","title":"SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic\n  Segmentation","summary":"  As an important and challenging problem in computer vision, PAnoramic\nSemantic Segmentation (PASS) gives complete scene perception based on an\nultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic\nimage input focus on solving image distortions but lack consideration of the 3D\nproperties of original $360^{\\circ}$ data. Therefore, their performance will\ndrop a lot when inputting panoramic images with the 3D disturbance. To be more\nrobust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer\nfor PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical\ngeometry knowledge. Specifically, a spherical geometry-aware framework is\nproposed for PASS. It includes three modules, i.e., spherical geometry-aware\nimage projection, spherical deformable patch embedding, and a panorama-aware\nloss, which takes input images with 3D disturbance into account, adds a\nspherical geometry-aware constraint on the existing deformable patch embedding,\nand indicates the pixel density of original $360^{\\circ}$ data, respectively.\nExperimental results on Stanford2D3D Panoramic datasets show that SGAT4PASS\nsignificantly improves performance and robustness, with approximately a 2%\nincrease in mIoU, and when small 3D disturbances occur in the data, the\nstability of our performance is improved by an order of magnitude. Our code and\nsupplementary material are available at\nhttps://github.com/TencentARC/SGAT4PASS.\n","authors":["Xuewei Li","Tao Wu","Zhongang Qi","Gaoang Wang","Ying Shan","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2306.03403v2.pdf","comment":"Accepted by IJCAI 2023"},{"id":"http://arxiv.org/abs/2402.09430v2","updated":"2024-03-12T11:48:02Z","published":"2024-01-24T16:10:14Z","title":"WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing","summary":"  WiFi-based human sensing has exhibited remarkable potential to analyze user\nbehaviors in a non-intrusive and device-free manner, benefiting applications as\ndiverse as smart homes and healthcare. However, most previous works focus on\nsingle-user sensing, which has limited practicability in scenarios involving\nmultiple users. Although recent studies have begun to investigate WiFi-based\nmulti-user sensing, there remains a lack of benchmark datasets to facilitate\nreproducible and comparable research. To bridge this gap, we present WiMANS, to\nour knowledge, the first dataset for multi-user sensing based on WiFi. WiMANS\ncontains over 9.4 hours of dual-band WiFi Channel State Information (CSI), as\nwell as synchronized videos, monitoring simultaneous activities of multiple\nusers. We exploit WiMANS to benchmark the performance of state-of-the-art\nWiFi-based human sensing models and video-based models, posing new challenges\nand opportunities for future work. We believe WiMANS can push the boundaries of\ncurrent studies and catalyze the research on WiFi-based multi-user sensing.\n","authors":["Shuokang Huang","Kaihan Li","Di You","Yichong Chen","Arvin Lin","Siying Liu","Xiaohui Li","Julie A. McCann"],"pdf_url":"https://arxiv.org/pdf/2402.09430v2.pdf","comment":"We present WiMANS, to our knowledge, the first dataset for multi-user\n  activity sensing based on WiFi"},{"id":"http://arxiv.org/abs/2403.07338v1","updated":"2024-03-12T05:43:16Z","published":"2024-03-12T05:43:16Z","title":"D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic\n  Communications","summary":"  Semantic communications (SemCom) have emerged as a new paradigm for\nsupporting sixth-generation applications, where semantic features of data are\ntransmitted using artificial intelligence algorithms to attain high\ncommunication efficiencies. Most existing SemCom techniques utilize deep neural\nnetworks (DNNs) to implement analog source-channel mappings, which are\nincompatible with existing digital communication architectures. To address this\nissue, this paper proposes a novel framework of digital deep joint\nsource-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The\nframework features digital source and channel codings that are jointly\noptimized to reduce the end-to-end (E2E) distortion. First, deep source coding\nwith an adaptive density model is designed to encode semantic features\naccording to their distributions. Second, digital channel coding is employed to\nprotect encoded features against channel distortion. To facilitate their joint\ndesign, the E2E distortion is characterized as a function of the source and\nchannel rates via the analysis of the Bayesian model and Lipschitz assumption\non the DNNs. Then to minimize the E2E distortion, a two-step algorithm is\nproposed to control the source-channel rates for a given channel\nsignal-to-noise ratio. Simulation results reveal that the proposed framework\noutperforms classic deep JSCC and mitigates the cliff and leveling-off effects,\nwhich commonly exist for separation-based approaches.\n","authors":["Jianhao Huang","Kai Yuan","Chuan Huang","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.07338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00414v2","updated":"2024-03-12T02:39:23Z","published":"2023-12-01T08:38:27Z","title":"Vision-Language Models Learn Super Images for Efficient Partially\n  Relevant Video Retrieval","summary":"  In this paper, we propose an efficient and high-performance method for\npartially relevant video retrieval, which aims to retrieve long videos that\ncontain at least one moment relevant to the input text query. The challenge\nlies in encoding dense frames using visual backbones. This requires models to\nhandle the increased frames, resulting in significant computation costs for\nlong videos. To mitigate the costs, previous studies use lightweight visual\nbackbones, yielding sub-optimal retrieval performance due to their limited\ncapabilities. However, it is undesirable to simply replace the backbones with\nhigh-performance large vision-and-language models (VLMs) due to their low\nefficiency. To address this dilemma, instead of dense frames, we focus on super\nimages, which are created by rearranging the video frames in an $N \\times N$\ngrid layout. This reduces the number of visual encodings to $\\frac{1}{N^2}$ and\nmitigates the low efficiency of large VLMs. Based on this idea, we make two\ncontributions. First, we explore whether VLMs generalize to super images in a\nzero-shot setting. To this end, we propose a method called query-attentive\nsuper image retrieval (QASIR), which attends to partial moments relevant to the\ninput query. The zero-shot QASIR yields two discoveries: (1) it enables VLMs to\ngeneralize to super images and (2) the grid size $N$, image resolution, and VLM\nsize are key trade-off parameters between performance and computation costs.\nSecond, we introduce fine-tuning and hybrid QASIR that combines high- and\nlow-efficiency models to strike a balance between performance and computation\ncosts. This reveals two findings: (1) the fine-tuning QASIR enhances VLMs to\nlearn super images effectively, and (2) the hybrid QASIR minimizes the\nperformance drop of large VLMs while reducing the computation costs.\n","authors":["Taichi Nishimura","Shota Nakada","Masayoshi Kondo"],"pdf_url":"https://arxiv.org/pdf/2312.00414v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2308.04522v3","updated":"2024-03-12T00:16:38Z","published":"2023-08-08T18:37:24Z","title":"Deep Learning for Steganalysis of Diverse Data Types: A review of\n  methods, taxonomy, challenges and future directions","summary":"  Steganography and steganalysis are two interrelated aspects of the field of\ninformation security. Steganography seeks to conceal communications, whereas\nsteganalysis is aimed to either find them or even, if possible, recover the\ndata they contain. Steganography and steganalysis have attracted a great deal\nof interest, particularly from law enforcement. Steganography is often used by\ncybercriminals and even terrorists to avoid being captured while in possession\nof incriminating evidence, even encrypted, since cryptography is prohibited or\nrestricted in many countries. Therefore, knowledge of cutting-edge techniques\nto uncover concealed information is crucial in exposing illegal acts. Over the\nlast few years, a number of strong and reliable steganography and steganalysis\ntechniques have been introduced in the literature. This review paper provides a\ncomprehensive overview of deep learning-based steganalysis techniques used to\ndetect hidden information within digital media. The paper covers all types of\ncover in steganalysis, including image, audio, and video, and discusses the\nmost commonly used deep learning techniques. In addition, the paper explores\nthe use of more advanced deep learning techniques, such as deep transfer\nlearning (DTL) and deep reinforcement learning (DRL), to enhance the\nperformance of steganalysis systems. The paper provides a systematic review of\nrecent research in the field, including data sets and evaluation metrics used\nin recent studies. It also presents a detailed analysis of DTL-based\nsteganalysis approaches and their performance on different data sets. The\nreview concludes with a discussion on the current state of deep learning-based\nsteganalysis, challenges, and future research directions.\n","authors":["Hamza Kheddar","Mustapha Hemis","Yassine Himeur","David Megías","Abbes Amira"],"pdf_url":"https://arxiv.org/pdf/2308.04522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12333v4","updated":"2024-03-12T21:40:53Z","published":"2023-05-21T03:50:44Z","title":"GRACE: Loss-Resilient Real-Time Video through Neural Codecs","summary":"  In real-time video communication, retransmitting lost packets over\nhigh-latency networks is not viable due to strict latency requirements. To\ncounter packet losses without retransmission, two primary strategies are\nemployed -- encoder-based forward error correction (FEC) and decoder-based\nerror concealment. The former encodes data with redundancy before transmission,\nyet determining the optimal redundancy level in advance proves challenging. The\nlatter reconstructs video from partially received frames, but dividing a frame\ninto independently coded partitions inherently compromises compression\nefficiency, and the lost information cannot be effectively recovered by the\ndecoder without adapting the encoder. We present a loss-resilient real-time\nvideo system called GRACE, which preserves the user's quality of experience\n(QoE) across a wide range of packet losses through a new neural video codec.\nCentral to GRACE's enhanced loss resilience is its joint training of the neural\nencoder and decoder under a spectrum of simulated packet losses. In lossless\nscenarios, GRACE achieves video quality on par with conventional codecs (e.g.,\nH.265). As the loss rate escalates, GRACE exhibits a more graceful, less\npronounced decline in quality, consistently outperforming other loss-resilient\nschemes. Through extensive evaluation on various videos and real network\ntraces, we demonstrate that GRACE reduces undecodable frames by 95% and stall\nduration by 90% compared with FEC, while markedly boosting video quality over\nerror concealment methods. In a user study with 240 crowdsourced participants\nand 960 subjective ratings, GRACE registers a 38% higher mean opinion score\n(MOS) than other baselines.\n","authors":["Yihua Cheng","Ziyi Zhang","Hanchen Li","Anton Arapin","Yue Zhang","Qizheng Zhang","Yuhan Liu","Xu Zhang","Francis Y. Yan","Amrita Mazumdar","Nick Feamster","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2305.12333v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07952v1","updated":"2024-03-12T02:30:50Z","published":"2024-03-12T02:30:50Z","title":"AesopAgent: Agent-driven Evolutionary System on Story-to-Video\n  Production","summary":"  The Agent and AIGC (Artificial Intelligence Generated Content) technologies\nhave recently made significant progress. We propose AesopAgent, an Agent-driven\nEvolutionary System on Story-to-Video Production. AesopAgent is a practical\napplication of agent technology for multimodal content generation. The system\nintegrates multiple generative capabilities within a unified framework, so that\nindividual users can leverage these modules easily. This innovative system\nwould convert user story proposals into scripts, images, and audio, and then\nintegrate these multimodal contents into videos. Additionally, the animating\nunits (e.g., Gen-2 and Sora) could make the videos more infectious. The\nAesopAgent system could orchestrate task workflow for video generation,\nensuring that the generated video is both rich in content and coherent. This\nsystem mainly contains two layers, i.e., the Horizontal Layer and the Utility\nLayer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary\nsystem that optimizes the whole video generation workflow and the steps within\nthe workflow. It continuously evolves and iteratively optimizes workflow by\naccumulating expert experience and professional knowledge, including optimizing\nthe LLM prompts and utilities usage. The Utility Layer provides multiple\nutilities, leading to consistent image generation that is visually coherent in\nterms of composition, characters, and style. Meanwhile, it provides audio and\nspecial effects, integrating them into expressive and logically arranged\nvideos. Overall, our AesopAgent achieves state-of-the-art performance compared\nwith many previous works in visual storytelling. Our AesopAgent is designed for\nconvenient service for individual users, which is available on the following\npage: https://aesopai.github.io/.\n","authors":["Jiuniu Wang","Zehua Du","Yuyuan Zhao","Bo Yuan","Kexiang Wang","Jian Liang","Yaxi Zhao","Yihen Lu","Gengliang Li","Junlong Gao","Xin Tu","Zhenyu Guo"],"pdf_url":"https://arxiv.org/pdf/2403.07952v1.pdf","comment":"22 pages, 13 figures"}]},"2024-03-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.08763v1","updated":"2024-03-13T17:58:57Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08755v1","updated":"2024-03-13T17:53:47Z","published":"2024-03-13T17:53:47Z","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","summary":"  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n","authors":["Feng Cheng","Ziyang Wang","Yi-Lin Sung","Yan-Bo Lin","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2403.08755v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2403.08743v1","updated":"2024-03-13T17:46:28Z","published":"2024-03-13T17:46:28Z","title":"Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing\n  Framework","summary":"  Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.08739v1","updated":"2024-03-13T17:42:32Z","published":"2024-03-13T17:42:32Z","title":"The Garden of Forking Paths: Observing Dynamic Parameters Distribution\n  in Large Language Models","summary":"  A substantial gap persists in understanding the reasons behind the\nexceptional performance of the Transformer architecture in NLP. A particularly\nunexplored area involves the mechanistic description of how the distribution of\nparameters evolves over time during training. In this work we suggest that\nlooking at the time evolution of the statistic distribution of model\nparameters, and specifically at bifurcation effects, can help understanding the\nmodel quality, potentially reducing training costs and evaluation efforts and\nempirically showing the reasons behind the effectiveness of weights\nsparsification.\n","authors":["Carlo Nicolini","Jacopo Staiano","Bruno Lepri","Raffaele Marino"],"pdf_url":"https://arxiv.org/pdf/2403.08739v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2403.08738v1","updated":"2024-03-13T17:42:03Z","published":"2024-03-13T17:42:03Z","title":"Improving Acoustic Word Embeddings through Correspondence Training of\n  Self-supervised Speech Representations","summary":"  Acoustic word embeddings (AWEs) are vector representations of spoken words.\nAn effective method for obtaining AWEs is the Correspondence Auto-Encoder\n(CAE). In the past, the CAE method has been associated with traditional MFCC\nfeatures. Representations obtained from self-supervised learning (SSL)-based\nspeech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many\ndownstream tasks. However, they have not been well studied in the context of\nlearning AWEs. This work explores the effectiveness of CAE with SSL-based\nspeech representations to obtain improved AWEs. Additionally, the capabilities\nof SSL-based speech models are explored in cross-lingual scenarios for\nobtaining AWEs. Experiments are conducted on five languages: Polish,\nPortuguese, Spanish, French, and English. HuBERT-based CAE model achieves the\nbest results for word discrimination in all languages, despite Hu-BERT being\npre-trained on English only. Also, the HuBERT-based CAE model works well in\ncross-lingual settings. It outperforms MFCC-based CAE models trained on the\ntarget languages when trained on one source language and tested on target\nlanguages.\n","authors":["Amit Meghanani","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2403.08738v1.pdf","comment":"Accepted to EACL 2024 Main Conference, Long paper"},{"id":"http://arxiv.org/abs/2212.12799v2","updated":"2024-03-13T17:41:14Z","published":"2022-12-24T18:35:35Z","title":"A Comprehensive Study of Gender Bias in Chemical Named Entity\n  Recognition Models","summary":"  Chemical named entity recognition (NER) models are used in many downstream\ntasks, from adverse drug reaction identification to pharmacoepidemiology.\nHowever, it is unknown whether these models work the same for everyone.\nPerformance disparities can potentially cause harm rather than the intended\ngood. This paper assesses gender-related performance disparities in chemical\nNER systems. We develop a framework for measuring gender bias in chemical NER\nmodels using synthetic data and a newly annotated corpus of over 92,405 words\nwith self-identified gender information from Reddit. Our evaluation of multiple\nbiomedical NER models reveals evident biases. For instance, synthetic data\nsuggests female-related names are frequently misclassified as chemicals,\nespecially for brand name mentions. Additionally, we observe performance\ndisparities between female- and male-associated data in both datasets. Many\nsystems fail to detect contraceptives such as birth control. Our findings\nemphasize the biases in chemical NER models, urging practitioners to account\nfor these biases in downstream applications.\n","authors":["Xingmeng Zhao","Ali Niazi","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2212.12799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04475v2","updated":"2024-03-13T17:40:04Z","published":"2023-10-06T05:27:28Z","title":"Demystifying Embedding Spaces using Large Language Models","summary":"  Embeddings have become a pivotal means to represent complex, multi-faceted\ninformation about entities, concepts, and relationships in a condensed and\nuseful format. Nevertheless, they often preclude direct interpretation. While\ndownstream tasks make use of these compressed representations, meaningful\ninterpretation usually requires visualization using dimensionality reduction or\nspecialized machine learning interpretability methods. This paper addresses the\nchallenge of making such embeddings more interpretable and broadly useful, by\nemploying Large Language Models (LLMs) to directly interact with embeddings --\ntransforming abstract vectors into understandable narratives. By injecting\nembeddings into LLMs, we enable querying and exploration of complex embedding\ndata. We demonstrate our approach on a variety of diverse tasks, including:\nenhancing concept activation vectors (CAVs), communicating novel embedded\nentities, and decoding user preferences in recommender systems. Our work\ncouples the immense information potential of embeddings with the interpretative\npower of LLMs.\n","authors":["Guy Tennenholtz","Yinlam Chow","Chih-Wei Hsu","Jihwan Jeong","Lior Shani","Azamat Tulepbergenov","Deepak Ramachandran","Martin Mladenov","Craig Boutilier"],"pdf_url":"https://arxiv.org/pdf/2310.04475v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08737v1","updated":"2024-03-13T17:38:05Z","published":"2024-03-13T17:38:05Z","title":"ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation","summary":"  Existing Machine Learning approaches for local citation recommendation\ndirectly map or translate a query, which is typically a claim or an entity\nmention, to citation-worthy research papers. Within such a formulation, it is\nchallenging to pinpoint why one should cite a specific research paper for a\nparticular query, leading to limited recommendation interpretability. To\nalleviate this, we introduce the evidence-grounded local citation\nrecommendation task, where the target latent space comprises evidence spans for\nrecommending specific papers. Using a distantly-supervised evidence retrieval\nand multi-step re-ranking framework, our proposed system, ILCiteR, recommends\npapers to cite for a query grounded on similar evidence spans extracted from\nthe existing research literature. Unlike past formulations that simply output\nrecommendations, ILCiteR retrieves ranked lists of evidence span and\nrecommended paper pairs. Secondly, previously proposed neural models for\ncitation recommendation require expensive training on massive labeled data,\nideally after every significant update to the pool of candidate papers. In\ncontrast, ILCiteR relies solely on distant supervision from a dynamic evidence\ndatabase and pre-trained Transformer-based Language Models without any model\ntraining. We contribute a novel dataset for the evidence-grounded local\ncitation recommendation task and demonstrate the efficacy of our proposed\nconditional neural rank-ensembling approach for re-ranking evidence spans.\n","authors":["Sayar Ghosh Roy","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2403.08737v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.08981v2","updated":"2024-03-13T17:32:50Z","published":"2023-11-15T14:15:30Z","title":"Speculative Contrastive Decoding","summary":"  Large language models~(LLMs) exhibit exceptional performance in language\ntasks, yet their auto-regressive inference is limited due to high computational\nrequirements and is sub-optimal due to the exposure bias. Inspired by\nspeculative decoding and contrastive decoding, we introduce Speculative\nContrastive Decoding~(SCD), a straightforward yet powerful decoding approach\nthat leverages predictions from smaller language models~(LMs) to achieve both\ndecoding acceleration and quality improvement. Extensive evaluations and\nanalyses on four diverse language tasks demonstrate the effectiveness of SCD,\nshowing that decoding efficiency and quality can compatibly benefit from one\nsmaller LM.\n","authors":["Hongyi Yuan","Keming Lu","Fei Huang","Zheng Yuan","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08981v2.pdf","comment":"Revised version"},{"id":"http://arxiv.org/abs/2403.08730v1","updated":"2024-03-13T17:29:45Z","published":"2024-03-13T17:29:45Z","title":"Strengthening Multimodal Large Language Model with Bootstrapped\n  Preference Optimization","summary":"  Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.\n","authors":["Renjie Pi","Tianyang Han","Wei Xiong","Jipeng Zhang","Runtao Liu","Rui Pan","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08715v1","updated":"2024-03-13T17:17:48Z","published":"2024-03-13T17:17:48Z","title":"SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language\n  Agents","summary":"  Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.\n","authors":["Ruiyi Wang","Haofei Yu","Wenxin Zhang","Zhengyang Qi","Maarten Sap","Graham Neubig","Yonatan Bisk","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.08715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07793v4","updated":"2024-03-13T17:10:48Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Yangzhe Li","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v4.pdf","comment":"14 pages, Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.09818v2","updated":"2024-03-13T17:07:02Z","published":"2023-11-16T11:48:17Z","title":"SUQL: Conversational Search over Structured and Unstructured Data with\n  Large Language Models","summary":"  While most conversational agents are grounded on either free-text or\nstructured knowledge, many knowledge corpora consist of hybrid sources. This\npaper presents the first conversational agent that supports the full generality\nof hybrid data access for large knowledge corpora, through a language we\ndeveloped called SUQL (Structured and Unstructured Query Language).\nSpecifically, SUQL extends SQL with free-text primitives (summary and answer),\nso information retrieval can be composed with structured data accesses\narbitrarily in a formal, succinct, precise, and interpretable notation. With\nSUQL, we propose the first semantic parser, an LLM with in-context learning,\nthat can handle hybrid data sources.\n  Our in-context learning-based approach, when applied to the HybridQA dataset,\ncomes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62K\ndata samples. More significantly, unlike previous approaches, our technique is\napplicable to large databases and free-text corpora. We introduce a dataset\nconsisting of crowdsourced questions and conversations on Yelp, a large, real\nrestaurant knowledge base with structured and unstructured data. We show that\nour few-shot conversational agent based on SUQL finds an entity satisfying all\nuser requirements 90.3% of the time, compared to 63.4% for a baseline based on\nlinearization.\n","authors":["Shicheng Liu","Jialiang Xu","Wesley Tjangnaka","Sina J. Semnani","Chen Jie Yu","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2311.09818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08694v1","updated":"2024-03-13T16:57:57Z","published":"2024-03-13T16:57:57Z","title":"TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via\n  Reinforcement Learning","summary":"  The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n$5.73\\%$ of WizardLM's total), along with enhanced capabilities of LLMs in\ncrafting and comprehending complex instructions compared to strong baselines,\nand substantially improved model privacy protection.\n","authors":["Shangding Gu","Alois Knoll","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2403.08694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08693v1","updated":"2024-03-13T16:56:33Z","published":"2024-03-13T16:56:33Z","title":"Do Language Models Care About Text Quality? Evaluating Web-Crawled\n  Corpora Across 11 Languages","summary":"  Large, curated, web-crawled corpora play a vital role in training language\nmodels (LMs). They form the lion's share of the training data in virtually all\nrecent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However,\ndespite this importance, relatively little attention has been given to the\nquality of these corpora. In this paper, we compare four of the currently most\nrelevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across\neleven lower-resourced European languages. Our approach is two-fold: first, we\nperform an intrinsic evaluation by performing a human evaluation of the quality\nof samples taken from different corpora; then, we assess the practical impact\nof the qualitative differences by training specific LMs on each of the corpora\nand evaluating their performance on downstream tasks. We find that there are\nclear differences in quality of the corpora, with MaCoCu and OSCAR obtaining\nthe best results. However, during the extrinsic evaluation, we actually find\nthat the CC100 corpus achieves the highest scores. We conclude that, in our\nexperiments, the quality of the web-crawled corpora does not seem to play a\nsignificant role when training LMs.\n","authors":["Rik van Noord","Taja Kuzman","Peter Rupnik","Nikola Ljubešić","Miquel Esplà-Gomis","Gema Ramírez-Sánchez","Antonio Toral"],"pdf_url":"https://arxiv.org/pdf/2403.08693v1.pdf","comment":"Accepted to LREC-COLING 2024 (long)"},{"id":"http://arxiv.org/abs/2402.18060v3","updated":"2024-03-13T16:44:45Z","published":"2024-02-28T05:44:41Z","title":"Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions","summary":"  LLMs have demonstrated impressive performance in answering medical questions,\nsuch as passing scores on medical licensing examinations. However, medical\nboard exam questions or general clinical questions do not capture the\ncomplexity of realistic clinical cases. Moreover, the lack of reference\nexplanations means we cannot easily evaluate the reasoning of model decisions,\na crucial component of supporting doctors in making complex medical decisions.\nTo address these challenges, we construct two new datasets: JAMA Clinical\nChallenge and Medbullets. JAMA Clinical Challenge consists of questions based\non challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style\nclinical questions. Both datasets are structured as multiple-choice\nquestion-answering tasks, where each question is accompanied by an\nexpert-written explanation. We evaluate four LLMs on the two datasets using\nvarious prompts. Experiments demonstrate that our datasets are harder than\nprevious benchmarks. The inconsistency between automatic and human evaluations\nof model-generated explanations highlights the need to develop new metrics to\nsupport future research on explainable medical QA.\n","authors":["Hanjie Chen","Zhouxiang Fang","Yash Singla","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2402.18060v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08688v1","updated":"2024-03-13T16:44:39Z","published":"2024-03-13T16:44:39Z","title":"Token Alignment via Character Matching for Subword Completion","summary":"  Generative models, widely utilized in various applications, can often\nstruggle with prompts corresponding to partial tokens. This struggle stems from\ntokenization, where partial tokens fall out of distribution during inference,\nleading to incorrect or nonsensical outputs. This paper examines a technique to\nalleviate the tokenization artifact on text completion in generative models,\nmaintaining performance even in regular non-subword cases. The method, termed\ntoken alignment, involves backtracking to the last complete tokens and ensuring\nthe model's generation aligns with the prompt. This approach showcases marked\nimprovement across many partial token scenarios, including nuanced cases like\nspace-prefix and partial indentation, with only a minor time increase. The\ntechnique and analysis detailed in this paper contribute to the continuous\nadvancement of generative models in handling partial inputs, bearing relevance\nfor applications like code completion and text autocompletion.\n","authors":["Ben Athiwaratkun","Shiqi Wang","Mingyue Shang","Yuchen Tian","Zijian Wang","Sujan Kumar Gonugondla","Sanjay Krishna Gouda","Rob Kwiatowski","Ramesh Nallapati","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.08688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14158v2","updated":"2024-03-13T16:38:42Z","published":"2024-02-21T22:41:38Z","title":"TOOLVERIFIER: Generalization to New Tools via Self-Verification","summary":"  Teaching language models to use tools is an important milestone towards\nbuilding general assistants, but remains an open problem. While there has been\nsignificant progress on learning to use specific tools via fine-tuning,\nlanguage models still struggle with learning how to robustly use new tools from\nonly a few demonstrations. In this work we introduce a self-verification method\nwhich distinguishes between close candidates by self-asking contrastive\nquestions during (1) tool selection; and (2) parameter generation. We construct\nsynthetic, high-quality, self-generated data for this goal using Llama-2 70B,\nwhich we intend to release publicly. Extensive experiments on 4 tasks from the\nToolBench benchmark, consisting of 17 unseen tools, demonstrate an average\nimprovement of 22% over few-shot baselines, even in scenarios where the\ndistinctions between candidate tools are finely nuanced.\n","authors":["Dheeraj Mekala","Jason Weston","Jack Lanchantin","Roberta Raileanu","Maria Lomeli","Jingbo Shang","Jane Dwivedi-Yu"],"pdf_url":"https://arxiv.org/pdf/2402.14158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v1","updated":"2024-03-13T16:17:09Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.04780v2","updated":"2024-03-13T15:52:33Z","published":"2024-03-02T09:27:32Z","title":"MuseGraph: Graph-oriented Instruction Tuning of Large Language Models\n  for Generic Graph Mining","summary":"  Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.\n","authors":["Yanchao Tan","Hang Lv","Xinyi Huang","Jiawei Zhang","Shiping Wang","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2403.04780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08607v1","updated":"2024-03-13T15:20:30Z","published":"2024-03-13T15:20:30Z","title":"MedInsight: A Multi-Source Context Augmentation Framework for Generating\n  Patient-Centric Medical Responses using Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive capabilities in generating\nhuman-like responses. However, their lack of domain-specific knowledge limits\ntheir applicability in healthcare settings, where contextual and comprehensive\nresponses are vital. To address this challenge and enable the generation of\npatient-centric responses that are contextually relevant and comprehensive, we\npropose MedInsight:a novel retrieval augmented framework that augments LLM\ninputs (prompts) with relevant background information from multiple sources.\nMedInsight extracts pertinent details from the patient's medical record or\nconsultation transcript. It then integrates information from authoritative\nmedical textbooks and curated web resources based on the patient's health\nhistory and condition. By constructing an augmented context combining the\npatient's record with relevant medical knowledge, MedInsight generates\nenriched, patient-specific responses tailored for healthcare applications such\nas diagnosis, treatment recommendations, or patient education. Experiments on\nthe MTSamples dataset validate MedInsight's effectiveness in generating\ncontextually appropriate medical responses. Quantitative evaluation using the\nRagas metric and TruLens for answer similarity and answer correctness\ndemonstrates the model's efficacy. Furthermore, human evaluation studies\ninvolving Subject Matter Expert (SMEs) confirm MedInsight's utility, with\nmoderate inter-rater agreement on the relevance and correctness of the\ngenerated responses.\n","authors":["Subash Neupane","Shaswata Mitra","Sudip Mittal","Noorbakhsh Amiri Golilarz","Shahram Rahimi","Amin Amirlatifi"],"pdf_url":"https://arxiv.org/pdf/2403.08607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08604v1","updated":"2024-03-13T15:13:44Z","published":"2024-03-13T15:13:44Z","title":"DevBench: A Comprehensive Benchmark for Software Development","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their coding capabilities. However, existing benchmarks predominantly\nfocused on simplified or isolated aspects of programming, such as single-file\ncode generation or repository issue debugging, falling short of measuring the\nfull spectrum of challenges raised by real-world programming activities. To\nthis end, we propose DevBench, a comprehensive benchmark that evaluates LLMs\nacross various stages of the software development lifecycle, including software\ndesign, environment setup, implementation, acceptance testing, and unit\ntesting. DevBench features a wide range of programming languages and domains,\nhigh-quality data collection, and carefully designed and verified metrics for\neach task. Empirical studies show that current LLMs, including GPT-4-Turbo,\nfail to solve the challenges presented within DevBench. Analyses reveal that\nmodels struggle with understanding the complex structures in the repository,\nmanaging the compilation process, and grasping advanced programming concepts.\nOur findings offer actionable insights for the future development of LLMs\ntoward real-world programming applications. Our benchmark is available at\nhttps://github.com/open-compass/DevBench\n","authors":["Bowen Li","Wenhan Wu","Ziwei Tang","Lin Shi","John Yang","Jinyang Li","Shunyu Yao","Chen Qian","Binyuan Hui","Qicheng Zhang","Zhiyin Yu","He Du","Ping Yang","Dahua Lin","Chao Peng","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08604v1.pdf","comment":"Our data and code are available at\n  https://github.com/open-compass/DevBench"},{"id":"http://arxiv.org/abs/2307.12375v4","updated":"2024-03-13T15:00:20Z","published":"2023-07-23T16:54:41Z","title":"In-Context Learning Learns Label Relationships but Is Not Conventional\n  Learning","summary":"  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n","authors":["Jannik Kossen","Yarin Gal","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2307.12375v4.pdf","comment":"Accepted for publication at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08593v1","updated":"2024-03-13T14:59:07Z","published":"2024-03-13T14:59:07Z","title":"Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over\n  Structured Environments","summary":"  Large Language Models (LLMs) have shown potential in reasoning over\nstructured environments, e.g., knowledge graph and table. Such tasks typically\nrequire multi-hop reasoning, i.e., match natural language utterance with\ninstances in the environment. Previous methods leverage LLMs to incrementally\nbuild a reasoning path, where the LLMs either invoke tools or pick up schemas\nby step-by-step interacting with the environment. We propose\nReasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently\nand faithfully reason over structured environments. In Readi, LLMs initially\ngenerate a reasoning path given a query, and edit the path only when necessary.\nWe instantiate the path on structured environments and provide feedback to edit\nthe path if anything goes wrong. Experimental results on three KGQA datasets\nand two TableQA datasets show the effectiveness of Readi, significantly\nsurpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%\non WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and\n74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).\nOur code will be available upon publication.\n","authors":["Sitao Cheng","Ziyuan Zhuang","Yong Xu","Fangkai Yang","Chaoyun Zhang","Xiaoting Qin","Xiang Huang","Ling Chen","Qingwei Lin","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08593v1.pdf","comment":"17 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2203.11155v5","updated":"2024-03-13T14:46:05Z","published":"2022-03-16T12:23:25Z","title":"A New Quantum CNN Model for Image Classification","summary":"  Quantum density matrix represents all the information of the entire quantum\nsystem, and novel models of meaning employing density matrices naturally model\nlinguistic phenomena such as hyponymy and linguistic ambiguity, among others in\nquantum question answering tasks. Naturally, we argue that the quantum density\nmatrix can enhance the image feature information and the relationship between\nthe features for the classical image classification. Specifically, we (i)\ncombine density matrices and CNN to design a new mechanism; (ii) apply the new\nmechanism to some representative classical image classification tasks. A series\nof experiments show that the application of quantum density matrix in image\nclassification has the generalization and high efficiency on different\ndatasets. The application of quantum density matrix both in classical question\nanswering tasks and classical image classification tasks show more effective\nperformance.\n","authors":["X. Q. Zhao","T. L. Chen"],"pdf_url":"https://arxiv.org/pdf/2203.11155v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08564v1","updated":"2024-03-13T14:19:08Z","published":"2024-03-13T14:19:08Z","title":"Non-discrimination Criteria for Generative Language Models","summary":"  Within recent years, generative AI, such as large language models, has\nundergone rapid development. As these models become increasingly available to\nthe public, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.\n","authors":["Sara Sterlie","Nina Weng","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.08564v1.pdf","comment":"14 pages, 5 figures. Submitted to ACM Conference on Fairness,\n  Accountability, and Transparency (ACM FAccT 2024)"},{"id":"http://arxiv.org/abs/2403.07714v2","updated":"2024-03-13T14:08:19Z","published":"2024-03-12T14:57:40Z","title":"StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models","summary":"  Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.\n","authors":["Zhicheng Guo","Sijie Cheng","Hao Wang","Shihao Liang","Yujia Qin","Peng Li","Zhiyuan Liu","Maosong Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08540v1","updated":"2024-03-13T13:54:00Z","published":"2024-03-13T13:54:00Z","title":"Language models scale reliably with over-training and on downstream\n  tasks","summary":"  Scaling laws are useful guides for developing language models, but there are\nstill gaps between current scaling studies and how language models are\nultimately trained and evaluated. For instance, scaling is usually studied in\nthe compute-optimal training regime (i.e., \"Chinchilla optimal\" regime);\nhowever, in practice, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but\nultimately models are compared based on downstream task performance. In this\npaper, we address both shortcomings. To do so, we create a testbed of 104\nmodels with 0.011B to 6.9B parameters trained with various numbers of tokens on\nthree data distributions. First, we investigate scaling in the over-trained\nregime. We fit scaling laws that extrapolate in both the number of model\nparameters and the ratio of training tokens to parameters. This enables us to\npredict the validation loss of a 1.4B parameter, 900B token run (i.e.,\n32$\\times$ over-trained) and a 6.9B parameter, 138B token\nrun$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.\nSecond, we relate the perplexity of a language model to its downstream task\nperformance via a power law. We use this law to predict top-1 error averaged\nover downstream tasks for the two aforementioned models using experiments that\ntake 20$\\times$ less compute. Our experiments are available at\nhttps://github.com/mlfoundations/scaling.\n","authors":["Samir Yitzhak Gadre","Georgios Smyrnis","Vaishaal Shankar","Suchin Gururangan","Mitchell Wortsman","Rulin Shao","Jean Mercat","Alex Fang","Jeffrey Li","Sedrick Keh","Rui Xin","Marianna Nezhurina","Igor Vasiljevic","Jenia Jitsev","Alexandros G. Dimakis","Gabriel Ilharco","Shuran Song","Thomas Kollar","Yair Carmon","Achal Dave","Reinhard Heckel","Niklas Muennighoff","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2403.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08495v1","updated":"2024-03-13T13:04:58Z","published":"2024-03-13T13:04:58Z","title":"Automatic Interactive Evaluation for Large Language Models with State\n  Aware Patient Simulator","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.\n","authors":["Yusheng Liao","Yutong Meng","Yuhao Wang","Hongcheng Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08495v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.01678v4","updated":"2024-03-13T13:02:57Z","published":"2023-12-04T07:01:54Z","title":"Jellyfish: A Large Language Model for Data Preprocessing","summary":"  This paper explores the utilization of LLMs for data preprocessing (DP), a\ncrucial step in the data mining pipeline that transforms raw data into a clean\nformat conducive to easy processing. Whereas the use of LLMs has sparked\ninterest in devising universal solutions to DP, recent initiatives in this\ndomain typically rely on GPT APIs, raising inevitable data breach concerns.\nUnlike these approaches, we consider instruction-tuning local LLMs (7 - 13B\nmodels) as universal DP ask solver. We select a collection of datasets across\nfour representative DP tasks and construct instruction-tuning data using\nserialization and knowledge injection techniques tailored to DP. As such, the\ninstruction-tuned LLMs empower users to manually craft instructions for DP.\nMeanwhile, they can operate on a local, single, and low-priced GPU, ensuring\ndata security and enabling further tuning. Our experiments show that our\ndataset constructed for DP instruction tuning, namely Jellyfish, effectively\nenhances LLMs' DP performances and barely compromises their abilities in NLP\ntasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the\nmodels deliver competitiveness compared to state-of-the-art DP methods and\nstrong generalizability to unseen tasks. The models' performance rivals that of\nGPT series models, and the interpretation offers enhanced reasoning\ncapabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available\nat Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B\nhttps://huggingface.co/NECOUDBFM/Jellyfish-13B\n","authors":["Haochen Zhang","Yuyang Dong","Chuan Xiao","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2312.01678v4.pdf","comment":"a.k.a. \"Jellyfish: Instruction-Tuning Local Large Language Models for\n  Data Preprocessing''"},{"id":"http://arxiv.org/abs/2403.08492v1","updated":"2024-03-13T12:55:43Z","published":"2024-03-13T12:55:43Z","title":"Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking","summary":"  Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework.\n","authors":["Ming Dong","Yujing Chen","Miao Zhang","Hao Sun","Tingting He"],"pdf_url":"https://arxiv.org/pdf/2403.08492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08484v1","updated":"2024-03-13T12:50:23Z","published":"2024-03-13T12:50:23Z","title":"Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH\n  Mask based Efficient Fine-tuning","summary":"  In view of the huge number of parameters of Large language models (LLMs) ,\ntuning all parameters is very costly, and accordingly fine-tuning specific\nparameters is more sensible. Most of parameter efficient fine-tuning (PEFT)\nconcentrate on parameter selection strategies, such as additive method,\nselective method and reparametrization-based method. However, there are few\nmethods that consider the impact of data samples on parameter selecting, such\nas Fish Mask based method. Fish Mask randomly choose a part of data samples and\ntreat them equally during parameter selection, which is unable to dynamically\nselect optimal parameters for inconstant data distributions. In this work, we\nadopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline\nI}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline\nD}$ecreasing) algorithm to search the best setting of sample-parameter pair for\nFISH Mask. In each iteration, by searching the set of samples and parameters\nwith larger Fish information, IRD can find better sample-parameter pair in most\nscale. We demonstrate the effectiveness and rationality of proposed strategy by\nconducting experiments on GLUE benchmark. Experimental results show our\nstrategy optimizes the parameter selection and achieves preferable performance.\n","authors":["Ming Dong","Kang Xue","Bolong Zheng","Tingting He"],"pdf_url":"https://arxiv.org/pdf/2403.08484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07805v2","updated":"2024-03-13T12:46:38Z","published":"2024-03-12T16:42:44Z","title":"Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models","summary":"  Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.\n","authors":["Tongyao Zhu","Qian Liu","Liang Pang","Zhengbao Jiang","Min-Yen Kan","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2403.07805v2.pdf","comment":"8 pages, 4 figures; fixed typos"},{"id":"http://arxiv.org/abs/2305.09144v2","updated":"2024-03-13T12:34:17Z","published":"2023-05-16T03:50:38Z","title":"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism\n  of Language Models","summary":"  Memory is one of the most essential cognitive functions serving as a\nrepository of world knowledge and episodes of activities. In recent years,\nlarge-scale pre-trained language models have shown remarkable memorizing\nability. On the contrary, vanilla neural networks without pre-training have\nbeen long observed suffering from the catastrophic forgetting problem. To\ninvestigate such a retentive-forgetful contradiction and understand the memory\nmechanism of language models, we conduct thorough experiments by controlling\nthe target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads\nto retentive language models; 3) Knowledge relevance and diversification\nsignificantly influence the memory formation. These conclusions are useful for\nunderstanding the abilities of pre-trained language models and shed light on\ndesigning and evaluating new learning and inference algorithms of language\nmodels.\n","authors":["Boxi Cao","Qiaoyu Tang","Hongyu Lin","Shanshan Jiang","Bin Dong","Xianpei Han","Jiawei Chen","Tianshu Wang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2305.09144v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08462v1","updated":"2024-03-13T12:25:47Z","published":"2024-03-13T12:25:47Z","title":"Authorship Verification based on the Likelihood Ratio of Grammar Models","summary":"  Authorship Verification (AV) is the process of analyzing a set of documents\nto determine whether they were written by a specific author. This problem often\narises in forensic scenarios, e.g., in cases where the documents in question\nconstitute evidence for a crime. Existing state-of-the-art AV methods use\ncomputational solutions that are not supported by a plausible scientific\nexplanation for their functioning and that are often difficult for analysts to\ninterpret. To address this, we propose a method relying on calculating a\nquantity we call $\\lambda_G$ (LambdaG): the ratio between the likelihood of a\ndocument given a model of the Grammar for the candidate author and the\nlikelihood of the same document given a model of the Grammar for a reference\npopulation. These Grammar Models are estimated using $n$-gram language models\nthat are trained solely on grammatical features. Despite not needing large\namounts of data for training, LambdaG still outperforms other established AV\nmethods with higher computational complexity, including a fine-tuned Siamese\nTransformer network. Our empirical evaluation based on four baseline methods\napplied to twelve datasets shows that LambdaG leads to better results in terms\nof both accuracy and AUC in eleven cases and in all twelve cases if considering\nonly topic-agnostic methods. The algorithm is also highly robust to important\nvariations in the genre of the reference population in many cross-genre\ncomparisons. In addition to these properties, we demonstrate how LambdaG is\neasier to interpret than the current state-of-the-art. We argue that the\nadvantage of LambdaG over other methods is due to fact that it is compatible\nwith Cognitive Linguistic theories of language processing.\n","authors":["Andrea Nini","Oren Halvani","Lukas Graner","Valerio Gherardi","Shunichi Ishihara"],"pdf_url":"https://arxiv.org/pdf/2403.08462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02684v2","updated":"2024-03-13T12:24:06Z","published":"2023-11-05T15:48:29Z","title":"Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE","summary":"  Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/paper_list/Octavius.\n","authors":["Zeren Chen","Ziqin Wang","Zhen Wang","Huayang Liu","Zhenfei Yin","Si Liu","Lu Sheng","Wanli Ouyang","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2311.02684v2.pdf","comment":"22 pages, 12 figures. Accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2312.05720v3","updated":"2024-03-13T11:19:24Z","published":"2023-12-10T01:19:59Z","title":"Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer\n  Inputs of Language Models in Federated Learning","summary":"  Language models trained via federated learning (FL) demonstrate impressive\ncapabilities in handling complex tasks while protecting user privacy. Recent\nstudies indicate that leveraging gradient information and prior knowledge can\npotentially reveal training samples within FL setting. However, these\ninvestigations have overlooked the potential privacy risks tied to the\nintrinsic architecture of the models. This paper presents a two-stage privacy\nattack strategy that targets the vulnerabilities in the architecture of\ncontemporary language models, significantly enhancing attack performance by\ninitially recovering certain feature directions as additional supervisory\nsignals. Our comparative experiments demonstrate superior attack performance\nacross various datasets and scenarios, highlighting the privacy leakage risk\nassociated with the increasingly complex architectures of language models. We\ncall for the community to recognize and address these potential privacy risks\nin designing large language models.\n","authors":["Jianwei Li","Sheng Liu","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2312.05720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08424v1","updated":"2024-03-13T11:16:43Z","published":"2024-03-13T11:16:43Z","title":"Tastle: Distract Large Language Models for Automatic Jailbreak Attack","summary":"  Large language models (LLMs) have achieved significant advances in recent\ndays. Extensive efforts have been made before the public release of LLMs to\nalign their behaviors with human values. The primary goal of alignment is to\nensure their helpfulness, honesty and harmlessness. However, even meticulously\naligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,\nleading to unintended behaviors. The jailbreak is to intentionally develop a\nmalicious prompt that escapes from the LLM security restrictions to produce\nuncensored detrimental contents. Previous works explore different jailbreak\nmethods for red teaming LLMs, yet they encounter challenges regarding to\neffectiveness and scalability. In this work, we propose Tastle, a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.\n","authors":["Zeguan Xiao","Yan Yang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05657v2","updated":"2024-03-13T10:54:21Z","published":"2023-11-09T00:30:13Z","title":"Agent Lumos: Unified and Modular Training for Open-Source Language\n  Agents","summary":"  Closed-source agents suffer from several issues such as a lack of\naffordability, transparency, and reproducibility, particularly on complex\ninteractive tasks. This motivates the development of open-source alternatives.\nWe introduce LUMOS, one of the first frameworks for training open-source\nLLM-based agents. LUMOS features a learnable, unified, and modular architecture\nwith a planning module that learns high-level subgoal generation, and a\ngrounding module trained to translate these into actions using various tools in\nthe execution module. The design allows for modular upgrades and wider\napplicability to diverse interactive tasks. To foster generalizable agent\nlearning, we collect large-scale, unified, and high-quality training\nannotations derived from diverse ground-truth reasoning rationales across\nvarious complex interactive tasks. On 9 datasets, LUMOS exhibits several key\nadvantages: (1) LUMOS excels multiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type. LUMOS even surpasses GPT\nagents on QA and web tasks; (2) LUMOS outperforms open-source agents produced\nby chain-of-thoughts and unmodularized integrated training; and (3) LUMOS\neffectively generalizes to unseen tasks, outperforming 33B-scale agents and\ndomain-specific agents.\n","authors":["Da Yin","Faeze Brahman","Abhilasha Ravichander","Khyathi Chandu","Kai-Wei Chang","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2311.05657v2.pdf","comment":"Project website: https://allenai.github.io/lumos/"},{"id":"http://arxiv.org/abs/2403.08391v1","updated":"2024-03-13T10:10:07Z","published":"2024-03-13T10:10:07Z","title":"Misinformation is not about Bad Facts: An Analysis of the Production and\n  Consumption of Fringe Content","summary":"  What if misinformation is not an information problem at all? Our findings\nsuggest that online fringe ideologies spread through the use of content that is\nconsensus-based and \"factually correct\". We found that Australian news\npublishers with both moderate and far-right political leanings contain\ncomparable levels of information completeness and quality; and furthermore,\nthat far-right Twitter users often share from moderate sources. However, a\nstark difference emerges when we consider two additional factors: 1) the narrow\ntopic selection of articles by far-right users, suggesting that they cherrypick\nonly news articles that engage with specific topics of their concern, and 2)\nthe difference between moderate and far-right publishers when we examine the\nwriting style of their articles. Furthermore, we can even identify users prone\nto sharing misinformation based on their communication style. These findings\nhave important implications for countering online misinformation, as they\nhighlight the powerful role that users' personal bias towards specific topics,\nand publishers' writing styles, have in amplifying fringe ideologies online.\n","authors":["JooYoung Lee","Emily Booth","Hany Farid","Marian-Andrei Rizoiu"],"pdf_url":"https://arxiv.org/pdf/2403.08391v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.03522v2","updated":"2024-03-13T09:50:40Z","published":"2024-03-06T08:03:05Z","title":"Non-verbal information in spontaneous speech -- towards a new framework\n  of analysis","summary":"  Non-verbal signals in speech are encoded by prosody and carry information\nthat ranges from conversation action to attitude and emotion. Despite its\nimportance, the principles that govern prosodic structure are not yet\nadequately understood. This paper offers an analytical schema and a\ntechnological proof-of-concept for the categorization of prosodic signals and\ntheir association with meaning. The schema interprets surface-representations\nof multi-layered prosodic events. As a first step towards implementation, we\npresent a classification process that disentangles prosodic phenomena of three\norders. It relies on fine-tuning a pre-trained speech recognition model,\nenabling the simultaneous multi-class/multi-label detection. It generalizes\nover a large variety of spontaneous data, performing on a par with, or superior\nto, human annotation. In addition to a standardized formalization of prosody,\ndisentangling prosodic patterns can direct a theory of communication and speech\norganization. A welcome by-product is an interpretation of prosody that will\nenhance speech- and language-related technologies.\n","authors":["Tirza Biron","Moshe Barboy","Eran Ben-Artzy","Alona Golubchik","Yanir Marmor","Smadar Szekely","Yaron Winter","David Harel"],"pdf_url":"https://arxiv.org/pdf/2403.03522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16589v2","updated":"2024-03-13T09:45:02Z","published":"2024-01-29T21:44:27Z","title":"ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence\n  Labeling Tasks","summary":"  Prompt-based methods have been successfully applied to multilingual\npretrained language models for zero-shot cross-lingual understanding. However,\nmost previous studies primarily focused on sentence-level classification tasks,\nand only a few considered token-level labeling tasks such as Named Entity\nRecognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose\nToken-Level Prompt Decomposition (ToPro), which facilitates the prompt-based\nmethod for token-level sequence labeling tasks. The ToPro method decomposes an\ninput sentence into single tokens and applies one prompt template to each\ntoken. Our experiments on multilingual NER and POS tagging datasets demonstrate\nthat ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning\nin zero-shot cross-lingual transfer, especially for languages that are\ntypologically different from the source language English. Our method also\nattains state-of-the-art performance when employed with the mT5 model. Besides,\nour exploratory study in multilingual large language models shows that ToPro\nperforms much better than the current in-context learning method. Overall, the\nperformance improvements show that ToPro could potentially serve as a novel and\nsimple benchmarking method for sequence labeling tasks.\n","authors":["Bolei Ma","Ercong Nie","Shuzhou Yuan","Helmut Schmid","Michael Färber","Frauke Kreuter","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2401.16589v2.pdf","comment":"EACL 2024"},{"id":"http://arxiv.org/abs/2403.08377v1","updated":"2024-03-13T09:42:46Z","published":"2024-03-13T09:42:46Z","title":"Learning to Describe for Predicting Zero-shot Drug-Drug Interactions","summary":"  Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of\nconcurrent drug administration, posing a significant challenge in healthcare.\nAs the development of new drugs continues, the potential for unknown adverse\neffects resulting from DDIs becomes a growing concern. Traditional\ncomputational methods for DDI prediction may fail to capture interactions for\nnew drugs due to the lack of knowledge. In this paper, we introduce a new\nproblem setup as zero-shot DDI prediction that deals with the case of new\ndrugs. Leveraging textual information from online databases like DrugBank and\nPubChem, we propose an innovative approach TextDDI with a language model-based\nDDI predictor and a reinforcement learning~(RL)-based information selector,\nenabling the selection of concise and pertinent text for accurate DDI\nprediction on new drugs. Empirical results show the benefits of the proposed\napproach on several settings including zero-shot and few-shot DDI prediction,\nand the selected texts are semantically relevant. Our code and data are\navailable at \\url{https://github.com/zhufq00/DDIs-Prediction}.\n","authors":["Fangqi Zhu","Yongqi Zhang","Lei Chen","Bing Qin","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08375v1","updated":"2024-03-13T09:38:39Z","published":"2024-03-13T09:38:39Z","title":"Translating between SQL Dialects for Cloud Migration","summary":"  Migrations of systems from on-site premises to the cloud has been a\nfundamental endeavor by many industrial institutions. A crucial component of\nsuch cloud migrations is the transition of databases to be hosted online. In\nthis work, we consider the difficulties of this migration for SQL databases.\nWhile SQL is one of the prominent methods for storing database procedures,\nthere are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)\nwhich can complicate migrations when the on-premise SQL dialect differs to the\ndialect hosted on the cloud. Tools exist by common cloud provides such as AWS\nand Azure to aid in translating between dialects in order to mitigate the\nmajority of the difficulties. However, these tools do not successfully\ntranslate $100\\%$ of the code. Consequently, software engineers must manually\nconvert the remainder of the untranslated database. For large organizations,\nthis task quickly becomes intractable and so more innovative solutions are\nrequired. We consider this challenge a novel yet vital industrial research\nproblem for any large corporation that is considering cloud migrations.\nFurthermore, we introduce potential avenues of research to tackle this\nchallenge that have yielded promising preliminary results.\n","authors":["Ran Zmigrod","Salwa Alamir","Xiaomo Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08370v1","updated":"2024-03-13T09:31:50Z","published":"2024-03-13T09:31:50Z","title":"SMART: Submodular Data Mixture Strategy for Instruction Tuning","summary":"  Instruction Tuning involves finetuning a language model on a collection of\ninstruction-formatted datasets in order to enhance the generalizability of the\nmodel to unseen tasks. Studies have shown the importance of balancing different\ntask proportions during finetuning, but finding the right balance remains\nchallenging. Unfortunately, there's currently no systematic method beyond\nmanual tuning or relying on practitioners' intuition. In this paper, we\nintroduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a\nnovel data mixture strategy which makes use of a submodular function to assign\nimportance scores to tasks which are then used to determine the mixture\nweights. Given a fine-tuning budget, SMART redistributes the budget among tasks\nand selects non-redundant samples from each task. Experimental results\ndemonstrate that SMART significantly outperforms traditional methods such as\nexamples proportional mixing and equal mixing. Furthermore, SMART facilitates\nthe creation of data mixtures based on a few representative subsets of tasks\nalone and through task pruning analysis, we reveal that in a limited budget\nsetting, allocating budget among a subset of representative tasks yields\nsuperior performance compared to distributing the budget among all tasks. The\ncode for reproducing our results is open-sourced at\nhttps://github.com/kowndinya-renduchintala/SMART.\n","authors":["H S V N S Kowndinya Renduchintala","Sumit Bhatia","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06935v2","updated":"2024-03-13T09:26:26Z","published":"2024-03-11T17:20:12Z","title":"Naming, Describing, and Quantifying Visual Objects in Humans and LLMs","summary":"  While human speakers use a variety of different expressions when describing\nthe same object in an image, giving rise to a distribution of plausible labels\ndriven by pragmatic constraints, the extent to which current Vision \\& Language\nLarge Language Models (VLLMs) can mimic this crucial feature of language use is\nan open question. This applies to common, everyday objects, but it is\nparticularly interesting for uncommon or novel objects for which a category\nlabel may be lacking or fuzzy. Furthermore, humans show clear production\npreferences for highly context-sensitive expressions, such as the quantifiers\n`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on\nthree categories (nouns, attributes, and quantifiers) where humans show great\nsubjective variability concerning the distribution over plausible labels, using\ndatasets and resources mostly under-explored in previous work. Our results\nreveal mixed evidence on the ability of VLLMs to capture human naming\npreferences, with all models failing in tasks that require high-level reasoning\nsuch as assigning quantifiers.\n","authors":["Alberto Testoni","Juell Sprott","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2403.06935v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08358v1","updated":"2024-03-13T09:18:46Z","published":"2024-03-13T09:18:46Z","title":"Log Summarisation for Defect Evolution Analysis","summary":"  Log analysis and monitoring are essential aspects in software maintenance and\nidentifying defects. In particular, the temporal nature and vast size of log\ndata leads to an interesting and important research question: How can logs be\nsummarised and monitored over time? While this has been a fundamental topic of\nresearch in the software engineering community, work has typically focused on\nheuristic-, syntax-, or static-based methods. In this work, we suggest an\nonline semantic-based clustering approach to error logs that dynamically\nupdates the log clusters to enable monitoring code error life-cycles. We also\nintroduce a novel metric to evaluate the performance of temporal log clusters.\nWe test our system and evaluation metric with an industrial dataset and find\nthat our solution outperforms similar systems. We hope that our work encourages\nfurther temporal exploration in defect datasets.\n","authors":["Rares Dolga","Ran Zmigrod","Rui Silva","Salwa Alamir","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2403.08358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09702v2","updated":"2024-03-13T09:11:15Z","published":"2023-11-16T09:27:36Z","title":"Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go\n  without Hallucination?","summary":"  Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.\n","authors":["Bangzheng Li","Ben Zhou","Fei Wang","Xingyu Fu","Dan Roth","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.09702v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.08345v1","updated":"2024-03-13T08:50:15Z","published":"2024-03-13T08:50:15Z","title":"From human experts to machines: An LLM supported approach to ontology\n  and knowledge graph construction","summary":"  The conventional process of building Ontologies and Knowledge Graphs (KGs)\nheavily relies on human domain experts to define entities and relationship\ntypes, establish hierarchies, maintain relevance to the domain, fill the ABox\n(or populate with instances), and ensure data quality (including amongst others\naccuracy and completeness). On the other hand, Large Language Models (LLMs)\nhave recently gained popularity for their ability to understand and generate\nhuman-like natural language, offering promising ways to automate aspects of\nthis process. This work explores the (semi-)automatic construction of KGs\nfacilitated by open-source LLMs. Our pipeline involves formulating competency\nquestions (CQs), developing an ontology (TBox) based on these CQs, constructing\nKGs using the developed ontology, and evaluating the resultant KG with minimal\nto no involvement of human experts. We showcase the feasibility of our\nsemi-automated pipeline by creating a KG on deep learning methodologies by\nexploiting scholarly publications. To evaluate the answers generated via\nRetrieval-Augmented-Generation (RAG) as well as the KG concepts automatically\nextracted using LLMs, we design a judge LLM, which rates the generated content\nbased on ground truth. Our findings suggest that employing LLMs could\npotentially reduce the human effort involved in the construction of KGs,\nalthough a human-in-the-loop approach is recommended to evaluate automatically\ngenerated KGs.\n","authors":["Vamsi Krishna Kommineni","Birgitta König-Ries","Sheeba Samuel"],"pdf_url":"https://arxiv.org/pdf/2403.08345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08852v2","updated":"2024-03-13T08:49:54Z","published":"2023-06-15T04:41:28Z","title":"BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection","summary":"  This paper introduces a novel method leveraging bi-encoder-based detectors\nalong with a comprehensive study comparing different out-of-distribution (OOD)\ndetection methods in NLP using different feature extractors. The feature\nextraction stage employs popular methods such as Universal Sentence Encoder\n(USE), BERT, MPNET, and GLOVE to extract informative representations from\ntextual data. The evaluation is conducted on several datasets, including\nCLINC150, ROSTD-Coarse, SNIPS, and YELLOW. Performance is assessed using\nmetrics such as F1-Score, MCC, FPR@90, FPR@95, AUPR, an AUROC. The experimental\nresults demonstrate that the proposed bi-encoder-based detectors outperform\nother methods, both those that require OOD labels in training and those that do\nnot, across all datasets, showing great potential for OOD detection in NLP. The\nsimplicity of the training process and the superior detection performance make\nthem applicable to real-world scenarios. The presented methods and benchmarking\nmetrics serve as a valuable resource for future research in OOD detection,\nenabling further advancements in this field. The code and implementation\ndetails can be found on our GitHub repository:\nhttps://github.com/yellowmessenger/ood-detection.\n","authors":["Louis Owen","Biddwan Ahmed","Abhay Kumar"],"pdf_url":"https://arxiv.org/pdf/2306.08852v2.pdf","comment":"Published in IEEE: https://ieeexplore.ieee.org/document/10389907"},{"id":"http://arxiv.org/abs/2311.18765v3","updated":"2024-03-13T08:47:32Z","published":"2023-11-30T18:05:52Z","title":"MLLMs-Augmented Visual-Language Representation Learning","summary":"  Visual-language pre-training has achieved remarkable success in many\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that Multi-modal Large\nLanguage Models (MLLMs) can enhance visual-language representation learning by\nestablishing richer image-text associations for image-text datasets. Our\napproach is simple, utilizing MLLMs to extend multiple diverse captions for\neach image. To prevent the bias introduced by MLLMs' hallucinations and\nmonotonous language styles, we propose \"text shearing\" to maintain the quality\nand availability of extended captions. In image-text retrieval, without\nintroducing additional training cost, our method consistently obtains 5.6 ~\n35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and\nzero-shot settings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.\n","authors":["Yanqing Liu","Kai Wang","Wenqi Shao","Ping Luo","Yu Qiao","Mike Zheng Shou","Kaipeng Zhang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2311.18765v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01188v2","updated":"2024-03-13T08:45:53Z","published":"2023-10-02T13:26:43Z","title":"Quantifying the Plausibility of Context Reliance in Neural Machine\n  Translation","summary":"  Establishing whether language models can use contextual information in a\nhuman-plausible way is important to ensure their trustworthiness in real-world\nsettings. However, the questions of when and which parts of the context affect\nmodel generations are typically tackled separately, with current plausibility\nevaluations being practically limited to a handful of artificial benchmarks. To\naddress this, we introduce Plausibility Evaluation of Context Reliance\n(PECoRe), an end-to-end interpretability framework designed to quantify context\nusage in language models' generations. Our approach leverages model internals\nto (i) contrastively identify context-sensitive target tokens in generated\ntexts and (ii) link them to contextual cues justifying their prediction. We use\n\\pecore to quantify the plausibility of context-aware machine translation\nmodels, comparing model rationales with human annotations across several\ndiscourse-level phenomena. Finally, we apply our method to unannotated model\ntranslations to identify context-mediated predictions and highlight instances\nof (im)plausible context usage throughout generation.\n","authors":["Gabriele Sarti","Grzegorz Chrupała","Malvina Nissim","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2310.01188v2.pdf","comment":"ICLR 2024 Camera Ready. Code: https://github.com/gsarti/pecore.\n  Artifacts:\n  https://huggingface.co/collections/gsarti/pecore-iclr-2024-65edab42e28439e21b612c2e"},{"id":"http://arxiv.org/abs/2403.08332v1","updated":"2024-03-13T08:34:53Z","published":"2024-03-13T08:34:53Z","title":"Autoregressive Score Generation for Multi-trait Essay Scoring","summary":"  Recently, encoder-only pre-trained models such as BERT have been successfully\napplied in automated essay scoring (AES) to predict a single overall score.\nHowever, studies have yet to explore these models in multi-trait AES, possibly\ndue to the inefficiency of replicating BERT-based models for each trait.\nBreaking away from the existing sole use of encoder, we propose an\nautoregressive prediction of multi-trait scores (ArTS), incorporating a\ndecoding process by leveraging the pre-trained T5. Unlike prior regression or\nclassification methods, we redefine AES as a score-generation task, allowing a\nsingle model to predict multiple scores. During decoding, the subsequent trait\nprediction can benefit by conditioning on the preceding trait scores.\nExperimental results proved the efficacy of ArTS, showing over 5% average\nimprovements in both prompts and traits.\n","authors":["Heejin Do","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.08332v1.pdf","comment":"Accepted at EACL2024 Findings"},{"id":"http://arxiv.org/abs/2307.09249v2","updated":"2024-03-13T08:20:34Z","published":"2023-07-18T13:28:31Z","title":"UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model\n  in Data Science","summary":"  Recent advancements in NLP have witnessed the groundbreaking impact of\npretrained models, yielding impressive outcomes across various tasks. This\nstudy seeks to extend the power of pretraining methodologies to facilitating\nthe prediction over tables in data science, a domain traditionally overlooked,\nyet inherently challenging due to the plethora of table schemas intrinsic to\ndifferent tasks. The primary research questions underpinning this work revolve\naround the establishment of a universal pretraining protocol for tables with\nvaried structures, the generalizability and transferability of learned\nknowledge across tasks, the adaptation to diverse downstream applications, and\nthe incorporation of incremental columns over time. In response to these\nchallenges, we introduce UniTabE, a straightforward yet effective method\ndesigned to process tables in a uniform manner, devoid of constraints imposed\nby specific table structures. UniTabE's core concept relies on representing\neach basic table element with a module, termed TabUnit. This is subsequently\nfollowed by a Transformer encoder to refine the representation. Moreover, our\nmodel is designed to facilitate pretraining and finetuning through the\nutilization of free-form prompts. In order to implement the pretraining phase,\nwe curated an expansive tabular dataset comprising approximately 13B samples,\nmeticulously gathered from the Kaggle platform. This research primarily centers\non classification and regression tasks involving tabular data, and conducts\nrigorous experimental testing and analyses to validate the effectiveness of our\nmethodology. The experimental results demonstrate UniTabE's superior\nperformance against several baselines across massive benchmarks. This,\ntherefore, underscores UniTabE's potential to significantly enhance the\nsemantic representation of tabular data, thereby marking a significant stride\nfor tabular data analysis.\n","authors":["Yazheng Yang","Yuqi Wang","Guang Liu","Ledell Wu","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2307.09249v2.pdf","comment":"ICLR 2024, 9 pages"},{"id":"http://arxiv.org/abs/2310.05209v2","updated":"2024-03-13T08:14:47Z","published":"2023-10-08T15:50:36Z","title":"Scaling Laws of RoPE-based Extrapolation","summary":"  The extrapolation capability of Large Language Models (LLMs) based on Rotary\nPosition Embedding is currently a topic of considerable interest. The\nmainstream approach to addressing extrapolation with LLMs involves modifying\nRoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the\noriginal RoPE, with a larger value and providing longer fine-tuning text. In\nthis work, we first observe that fine-tuning a RoPE-based LLM with either a\nsmaller or larger base in pre-training context length could significantly\nenhance its extrapolation performance. After that, we propose\n\\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework\nfrom the periodic perspective, to describe the relationship between the\nextrapolation performance and base value as well as tuning context length. In\nthis process, we also explain the origin of the RoPE-based extrapolation issue\nby \\textbf{\\textit{critical dimension for extrapolation}}. Besides these\nobservations and analyses, we achieve extrapolation up to 1 million context\nlength within only 16K training length on LLaMA2 7B and 13B.\n","authors":["Xiaoran Liu","Hang Yan","Shuo Zhang","Chenxin An","Xipeng Qiu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2310.05209v2.pdf","comment":"26 pages, 12 figures, Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08319v1","updated":"2024-03-13T08:02:23Z","published":"2024-03-13T08:02:23Z","title":"Knowledge Conflicts for LLMs: A Survey","summary":"  This survey provides an in-depth analysis of knowledge conflicts for large\nlanguage models (LLMs), highlighting the complex challenges they encounter when\nblending contextual and parametric knowledge. Our focus is on three categories\nof knowledge conflicts: context-memory, inter-context, and intra-memory\nconflict. These conflicts can significantly impact the trustworthiness and\nperformance of LLMs, especially in real-world applications where noise and\nmisinformation are common. By categorizing these conflicts, exploring the\ncauses, examining the behaviors of LLMs under such conflicts, and reviewing\navailable solutions, this survey aims to shed light on strategies for improving\nthe robustness of LLMs, thereby serving as a valuable resource for advancing\nresearch in this evolving area.\n","authors":["Rongwu Xu","Zehan Qi","Cunxiang Wang","Hongru Wang","Yue Zhang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08314v1","updated":"2024-03-13T07:49:50Z","published":"2024-03-13T07:49:50Z","title":"Is Context Helpful for Chat Translation Evaluation?","summary":"  Despite the recent success of automatic metrics for assessing translation\nquality, their application in evaluating the quality of machine-translated\nchats has been limited. Unlike more structured texts like news, chat\nconversations are often unstructured, short, and heavily reliant on contextual\ninformation. This poses questions about the reliability of existing\nsentence-level metrics in this domain as well as the role of context in\nassessing the translation quality. Motivated by this, we conduct a\nmeta-evaluation of existing sentence-level automatic metrics, primarily\ndesigned for structured domains such as news, to assess the quality of\nmachine-translated chats. We find that reference-free metrics lag behind\nreference-based ones, especially when evaluating translation quality in\nout-of-English settings. We then investigate how incorporating conversational\ncontextual information in these metrics affects their performance. Our findings\nshow that augmenting neural learned metrics with contextual information helps\nimprove correlation with human judgments in the reference-free scenario and\nwhen evaluating translations in out-of-English settings. Finally, we propose a\nnew evaluation metric, Context-MQM, that utilizes bilingual context with a\nlarge language model (LLM) and further validate that adding context helps even\nfor LLM-based evaluation metrics.\n","authors":["Sweta Agrawal","Amin Farajian","Patrick Fernandes","Ricardo Rei","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2403.08314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08312v1","updated":"2024-03-13T07:44:14Z","published":"2024-03-13T07:44:14Z","title":"StreamingDialogue: Prolonged Dialogue Learning via Long Context\n  Compression with Minimal Losses","summary":"  Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200k or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200k of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation.\n","authors":["Jia-Nan Li","Quan Tu","Cunli Mao","Zhengtao Yu","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.08312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06185v4","updated":"2024-03-13T07:35:18Z","published":"2023-12-11T07:56:25Z","title":"KnowGPT: Knowledge Injection for Large Language Models","summary":"  Generative Large Language Models (LLMs), such as ChatGPT, offer interactive\nAPIs that can answer common questions at a human-expert level. However, these\nmodels often give inaccurate or incorrect responses when faced with questions\nrequiring domain-specific or professional-specific knowledge not covered in\ntheir training corpus. Furthermore, many state-of-the-art LLMs are not\nopen-source, making it challenging to inject knowledge with model APIs only. In\nthis work, we introduce KnowGPT, a black-box knowledge injection framework for\nLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)\nto extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed\nBandit (MAB) to construct the most suitable prompt for each question. Our\nextensive experiments on three benchmark datasets showcase that KnowGPT\nsignificantly enhances the existing methods. Notably, KnowGPT achieves an\naverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%\nover GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA\nofficial leaderboard, which is comparable to human-level performance.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Daochen Zha","Zailiang Yu","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.06185v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08305v1","updated":"2024-03-13T07:31:20Z","published":"2024-03-13T07:31:20Z","title":"Towards Personalized Evaluation of Large Language Models with An\n  Anonymous Crowd-Sourcing Platform","summary":"  Large language model evaluation plays a pivotal role in the enhancement of\nits capacity. Previously, numerous methods for evaluating large language models\nhave been proposed in this area. Despite their effectiveness, these existing\nworks mainly focus on assessing objective questions, overlooking the capability\nto evaluate subjective questions which is extremely common for large language\nmodels. Additionally, these methods predominantly utilize centralized datasets\nfor evaluation, with question banks concentrated within the evaluation\nplatforms themselves. Moreover, the evaluation processes employed by these\nplatforms often overlook personalized factors, neglecting to consider the\nindividual characteristics of both the evaluators and the models being\nevaluated. To address these limitations, we propose a novel anonymous\ncrowd-sourcing evaluation platform, BingJian, for large language models that\nemploys a competitive scoring mechanism where users participate in ranking\nmodels based on their performance. This platform stands out not only for its\nsupport of centralized evaluations to assess the general capabilities of models\nbut also for offering an open evaluation gateway. Through this gateway, users\nhave the opportunity to submit their questions, testing the models on a\npersonalized and potentially broader range of capabilities. Furthermore, our\nplatform introduces personalized evaluation scenarios, leveraging various forms\nof human-computer interaction to assess large language models in a manner that\naccounts for individual user preferences and contexts. The demonstration of\nBingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.\n","authors":["Mingyue Cheng","Hao Zhang","Jiqian Yang","Qi Liu","Li Li","Xin Huang","Liwei Song","Zhi Li","Zhenya Huang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07759v3","updated":"2024-03-13T07:27:10Z","published":"2023-09-14T14:45:47Z","title":"PROGrasp: Pragmatic Human-Robot Communication for Object Grasping","summary":"  Interactive Object Grasping (IOG) is the task of identifying and grasping the\ndesired object via human-robot natural language interaction. Current IOG\nsystems assume that a human user initially specifies the target object's\ncategory (e.g., bottle). Inspired by pragmatics, where humans often convey\ntheir intentions by relying on context to achieve goals, we introduce a new IOG\ntask, Pragmatic-IOG, and the corresponding dataset, Intention-oriented\nMulti-modal Dialogue (IM-Dial). In our proposed task scenario, an\nintention-oriented utterance (e.g., \"I am thirsty\") is initially given to the\nrobot. The robot should then identify the target object by interacting with a\nhuman user. Based on the task setup, we propose a new robotic system that can\ninterpret the user's intention and pick up the target object, Pragmatic Object\nGrasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules\nfor visual grounding, question asking, object grasping, and most importantly,\nanswer interpretation for pragmatic inference. Experimental results show that\nPROGrasp is effective in offline (i.e., target object discovery) and online\n(i.e., IOG with a physical robot arm) settings. Code and data are available at\nhttps://github.com/gicheonkang/prograsp.\n","authors":["Gi-Cheon Kang","Junghyun Kim","Jaein Kim","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07759v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2402.11550v2","updated":"2024-03-13T07:16:42Z","published":"2024-02-18T11:46:52Z","title":"LongAgent: Scaling Language Models to 128k Context through Multi-Agent\n  Collaboration","summary":"  Large language models (LLMs) have demonstrated impressive performance in\nunderstanding language and executing complex reasoning tasks. However, LLMs\nwith long context windows have been notorious for their expensive training\ncosts and high inference latency. Even the most advanced models such as GPT-4\nand Claude2 often make mistakes when processing inputs of over $100k$ tokens, a\nphenomenon also known as \\textit{lost in the middle}. In this paper, we propose\n\\textsc{LongAgent}, a method based on multi-agent collaboration, which scales\nLLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority\nin long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is\nresponsible for understanding user intent and directing team members to acquire\ninformation from documents. Due to members' hallucinations, it is non-trivial\nfor a leader to obtain accurate information from the responses of dozens to\nhundreds of members. To address this, we develop an \\textit{inter-member\ncommunication} mechanism to resolve response conflicts caused by hallucinations\nthrough information sharing. Our experimental results indicate that\n\\textsc{LongAgent} offers a promising alternative for long-text processing. The\nagent team instantiated with LLaMA-7B achieves significant improvements in\ntasks such as 128k-long text retrieval, multi-hop question answering, compared\nto GPT-4.\n","authors":["Jun Zhao","Can Zu","Hao Xu","Yi Lu","Wei He","Yiwen Ding","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.11550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08295v1","updated":"2024-03-13T06:59:16Z","published":"2024-03-13T06:59:16Z","title":"Gemma: Open Models Based on Gemini Research and Technology","summary":"  This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.\n","authors":[" Gemma Team","Thomas Mesnard","Cassidy Hardin","Robert Dadashi","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane Rivière","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","Léonard Hussenot","Aakanksha Chowdhery","Adam Roberts","Aditya Barua","Alex Botev","Alex Castro-Ros","Ambrose Slone","Amélie Héliou","Andrea Tacchetti","Anna Bulanova","Antonia Paterson","Beth Tsai","Bobak Shahriari","Charline Le Lan","Christopher A. Choquette-Choo","Clément Crepy","Daniel Cer","Daphne Ippolito","David Reid","Elena Buchatskaya","Eric Ni","Eric Noland","Geng Yan","George Tucker","George-Christian Muraru","Grigory Rozhdestvenskiy","Henryk Michalewski","Ian Tenney","Ivan Grishchenko","Jacob Austin","James Keeling","Jane Labanowski","Jean-Baptiste Lespiau","Jeff Stanway","Jenny Brennan","Jeremy Chen","Johan Ferret","Justin Chiu","Justin Mao-Jones","Katherine Lee","Kathy Yu","Katie Millican","Lars Lowe Sjoesund","Lisa Lee","Lucas Dixon","Machel Reid","Maciej Mikuła","Mateo Wirth","Michael Sharman","Nikolai Chinaev","Nithum Thain","Olivier Bachem","Oscar Chang","Oscar Wahltinez","Paige Bailey","Paul Michel","Petko Yotov","Pier Giuseppe Sessa","Rahma Chaabouni","Ramona Comanescu","Reena Jana","Rohan Anil","Ross McIlroy","Ruibo Liu","Ryan Mullins","Samuel L Smith","Sebastian Borgeaud","Sertan Girgin","Sholto Douglas","Shree Pandya","Siamak Shakeri","Soham De","Ted Klimenko","Tom Hennigan","Vlad Feinberg","Wojciech Stokowiec","Yu-hui Chen","Zafarali Ahmed","Zhitao Gong","Tris Warkentin","Ludovic Peran","Minh Giang","Clément Farabet","Oriol Vinyals","Jeff Dean","Koray Kavukcuoglu","Demis Hassabis","Zoubin Ghahramani","Douglas Eck","Joelle Barral","Fernando Pereira","Eli Collins","Armand Joulin","Noah Fiedel","Evan Senter","Alek Andreev","Kathleen Kenealy"],"pdf_url":"https://arxiv.org/pdf/2403.08295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08293v1","updated":"2024-03-13T06:54:47Z","published":"2024-03-13T06:54:47Z","title":"Generative Pretrained Structured Transformers: Unsupervised Syntactic\n  Language Models at Scale","summary":"  A syntactic language model (SLM) incrementally generates a sentence with its\nsyntactic tree in a left-to-right manner. We present Generative Pretrained\nStructured Transformers (GPST), an unsupervised SLM at scale capable of being\npre-trained from scratch on raw texts with high parallelism. GPST circumvents\nthe limitations of previous SLMs such as relying on gold trees and sequential\ntraining. It consists of two components, a usual SLM supervised by a\nuni-directional language modeling loss, and an additional composition model,\nwhich induces syntactic parse trees and computes constituent representations,\nsupervised by a bi-directional language modeling loss. We propose a\nrepresentation surrogate to enable joint parallel training of the two models in\na hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion\ntokens, and demonstrate the superiority of GPST over GPT-2 with a comparable\nsize in numerous tasks covering both language understanding and language\ngeneration. Meanwhile, GPST also significantly outperforms existing\nunsupervised SLMs on left-to-right grammar induction, while holding a\nsubstantial acceleration on training.\n","authors":["Xiang Hu","Pengyu Ji","Qingyang Zhu","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2403.08293v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.04945v2","updated":"2024-03-13T06:20:47Z","published":"2024-03-07T23:20:56Z","title":"Electrocardiogram Instruction Tuning for Report Generation","summary":"  Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool\nfor cardiac conditions monitoring, are crucial in assisting clinicians. Recent\nstudies have concentrated on classifying cardiac conditions using ECG data but\nhave overlooked ECG report generation, which is not only time-consuming but\nalso requires clinical expertise. To automate ECG report generation and ensure\nits versatility, we propose the Multimodal ECG Instruction Tuning (MEIT)\nframework, the \\textit{first} attempt to tackle ECG report generation with LLMs\nand multimodal instructions. To facilitate future research, we establish a\nbenchmark to evaluate MEIT with various LLMs backbones across two large-scale\nECG datasets. Our approach uniquely aligns the representations of the ECG\nsignal and the report, and we conduct extensive experiments to benchmark MEIT\nwith nine open source LLMs, using more than 800,000 ECG reports. MEIT's results\nunderscore the superior performance of instruction-tuned LLMs, showcasing their\nproficiency in quality report generation, zero-shot capabilities, and\nresilience to signal perturbation. These findings emphasize the efficacy of our\nMEIT framework and its potential for real-world clinical application.\n","authors":["Zhongwei Wan","Che Liu","Xin Wang","Chaofan Tao","Hui Shen","Zhenwu Peng","Jie Fu","Rossella Arcucci","Huaxiu Yao","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.04945v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.08281v1","updated":"2024-03-13T06:18:48Z","published":"2024-03-13T06:18:48Z","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models","summary":"  Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.\n","authors":["Ning Ding","Yulin Chen","Ganqu Cui","Xingtai Lv","Ruobing Xie","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08272v1","updated":"2024-03-13T05:51:57Z","published":"2024-03-13T05:51:57Z","title":"RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education","summary":"  The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale and real-world interactions between students and AI\nsystems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE\nfor University), a dataset sourced from a semester-long experiment with 212\ncollege students in English as Foreign Language (EFL) writing courses. During\nthe study, students engaged in dialogues with ChatGPT to revise their essays.\nRECIPE4U includes comprehensive records of these interactions, including\nconversation logs, students' intent, students' self-rated satisfaction, and\nstudents' essay edit histories. In particular, we annotate the students'\nutterances in RECIPE4U with 13 intention labels based on our coding schemes. We\nestablish baseline results for two subtasks in task-oriented dialogue systems\nwithin educational contexts: intent detection and satisfaction estimation. As a\nfoundational step, we explore student-ChatGPT interaction patterns through\nRECIPE4U and analyze them by focusing on students' dialogue, essay data\nstatistics, and students' essay edits. We further illustrate potential\napplications of RECIPE4U dataset for enhancing the incorporation of LLMs in\neducational frameworks. RECIPE4U is publicly available at\nhttps://zeunie.github.io/RECIPE4U/.\n","authors":["Jieun Han","Haneul Yoo","Junho Myung","Minsun Kim","Tak Yeon Lee","So-Yeon Ahn","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2403.08272v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2309.13243"},{"id":"http://arxiv.org/abs/2309.17428v2","updated":"2024-03-13T05:39:25Z","published":"2023-09-29T17:40:26Z","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized\n  Toolsets","summary":"  Large language models (LLMs) are often augmented with tools to solve complex\ntasks. By generating code snippets and executing them through task-specific\nApplication Programming Interfaces (APIs), they can offload certain functions\nto dedicated external modules, such as image encoding and performing\ncalculations. However, most existing approaches to augment LLMs with tools are\nconstrained by general-purpose APIs and lack the flexibility for tailoring them\nto specific tasks. In this work, we present CRAFT, a general tool creation and\nretrieval framework for LLMs. It creates toolsets specifically curated for the\ntasks and equips LLMs with a component that retrieves tools from these sets to\nenhance their capability to solve complex tasks. For each task, we collect\nspecific code solutions by prompting GPT-4 to solve the training examples.\nFollowing a validation step ensuring the correctness, these solutions are\nabstracted into code snippets to enhance reusability, and deduplicated for\nhigher quality. At inference time, the language model retrieves snippets from\nthe toolsets and then executes them or generates the output conditioning on the\nretrieved snippets. Our method is designed to be flexible and offers a\nplug-and-play approach to adapt off-the-shelf LLMs to unseen domains and\nmodalities, without any finetuning. Experiments on vision-language, tabular\nprocessing, and mathematical reasoning tasks show that our approach achieves\nsubstantial improvements compared to strong baselines. In addition, our\nin-depth analysis reveals that: (1) consistent performance improvement can be\nachieved by scaling up the number of tools and the capability of the backbone\nmodels; (2) each component of our approach contributes to the performance\ngains; (3) the created tools are well-structured and reliable with low\ncomplexity and atomicity. The code is available at\nhttps://github.com/lifan-yuan/CRAFT.\n","authors":["Lifan Yuan","Yangyi Chen","Xingyao Wang","Yi R. Fung","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.17428v2.pdf","comment":"Accepted to ICLR 2024. Code is available at\n  https://github.com/lifan-yuan/CRAFT"},{"id":"http://arxiv.org/abs/2403.08258v1","updated":"2024-03-13T05:20:45Z","published":"2024-03-13T05:20:45Z","title":"Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition","summary":"  Conformer-based attention models have become the de facto backbone model for\nAutomatic Speech Recognition tasks. A blank symbol is usually introduced to\nalign the input and output sequences for CTC or RNN-T models. Unfortunately,\nthe long input length overloads computational budget and memory consumption\nquadratically by attention mechanism. In this work, we propose a\n\"Skip-and-Recover\" Conformer architecture, named Skipformer, to squeeze\nsequence input length dynamically and inhomogeneously. Skipformer uses an\nintermediate CTC output as criteria to split frames into three groups: crucial,\nskipping and ignoring. The crucial group feeds into next conformer blocks and\nits output joint with skipping group by original temporal order as the final\nencoder output. Experiments show that our model reduces the input sequence\nlength by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile,\nthe model can achieve better recognition accuracy and faster inference speed\nthan recent baseline models. Our code is open-sourced and available online.\n","authors":["Wenjing Zhu","Sining Sun","Changhao Shan","Peng Fan","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2403.08258v1.pdf","comment":"Accepted by ICME2024"},{"id":"http://arxiv.org/abs/2310.19208v2","updated":"2024-03-13T05:11:57Z","published":"2023-10-30T00:30:34Z","title":"LitCab: Lightweight Language Model Calibration over Short- and Long-form\n  Responses","summary":"  A model is considered well-calibrated when its probability estimate aligns\nwith the actual likelihood of the output being correct. Calibrating language\nmodels (LMs) is crucial, as it plays a vital role in detecting and mitigating\nhallucinations of LMs as well as building more trustworthy models. However,\nstandard calibration techniques may not be suited for LM calibration. For\ninstance, post-processing methods such as temperature scaling do not reorder\nthe candidate generations. On the other hand, training-based methods require\nfine-tuning the entire model, which is impractical for LMs of large scale. We\npresent LitCab, a lightweight calibration mechanism consisting of a single\nlinear layer that takes the input text representation and predicts a bias term,\nwhich is then added to the LM output logits. LitCab improves model calibration\nby only adding < 2% of the original model parameters. For evaluation, we\nconstruct CaT, a benchmark consisting of eight text generation tasks, covering\nresponses ranging from short phrases to paragraphs. We test LitCab with\nLlama2-7B, where it improves calibration across all tasks, reducing the average\nECE score by as large as 30%. We further conduct a comprehensive evaluation\nwith multiple popular open-sourced LMs from GPT and LLaMA families, yielding\nthe following key findings: (i) Larger models within the same family exhibit\nbetter calibration on tasks with short generation tasks, but not necessarily\nfor longer ones. (ii) GPT-family models show superior calibration compared to\nLLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii)\nFine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose\n(e.g., conversations) may lead to worse calibration, highlighting the\nimportance of fine-tuning setups for calibrating LMs.\n","authors":["Xin Liu","Muhammad Khalifa","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19208v2.pdf","comment":"accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08229v1","updated":"2024-03-13T04:14:33Z","published":"2024-03-13T04:14:33Z","title":"Boosting Disfluency Detection with Large Language Model as Disfluency\n  Generator","summary":"  Current disfluency detection methods heavily rely on costly and scarce\nhuman-annotated data. To tackle this issue, some approaches employ heuristic or\nstatistical features to generate disfluent sentences, partially improving\ndetection performance. However, these sentences often deviate from real-life\nscenarios, constraining overall model enhancement. In this study, we propose a\nlightweight data augmentation approach for disfluency detection, utilizing the\nsuperior generative and semantic understanding capabilities of large language\nmodel (LLM) to generate disfluent sentences as augmentation data. We leverage\nLLM to generate diverse and more realistic sentences guided by specific\nprompts, without the need for fine-tuning the LLM. Subsequently, we apply an\nuncertainty-aware data filtering approach to improve the quality of the\ngenerated sentences, utilized in training a small detection model for improved\nperformance. Experiments using enhanced data yielded state-of-the-art results.\nThe results showed that using a small amount of LLM-generated enhanced data can\nsignificantly improve performance, thereby further enhancing\ncost-effectiveness.\n","authors":["Zhenrong Cheng","Jiayan Guo","Hao Sun","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08217v1","updated":"2024-03-13T03:31:26Z","published":"2024-03-13T03:31:26Z","title":"Research on the Application of Deep Learning-based BERT Model in\n  Sentiment Analysis","summary":"  This paper explores the application of deep learning techniques, particularly\nfocusing on BERT models, in sentiment analysis. It begins by introducing the\nfundamental concept of sentiment analysis and how deep learning methods are\nutilized in this domain. Subsequently, it delves into the architecture and\ncharacteristics of BERT models. Through detailed explanation, it elucidates the\napplication effects and optimization strategies of BERT models in sentiment\nanalysis, supported by experimental validation. The experimental findings\nindicate that BERT models exhibit robust performance in sentiment analysis\ntasks, with notable enhancements post fine-tuning. Lastly, the paper concludes\nby summarizing the potential applications of BERT models in sentiment analysis\nand suggests directions for future research and practical implementations.\n","authors":["Yichao Wu","Zhengyu Jin","Chenxi Shi","Penghao Liang","Tong Zhan"],"pdf_url":"https://arxiv.org/pdf/2403.08217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08213v1","updated":"2024-03-13T03:22:02Z","published":"2024-03-13T03:22:02Z","title":"Can Large Language Models Identify Authorship?","summary":"  The ability to accurately identify authorship is crucial for verifying\ncontent authenticity and mitigating misinformation. Large Language Models\n(LLMs) have demonstrated exceptional capacity for reasoning and\nproblem-solving. However, their potential in authorship analysis, encompassing\nauthorship verification and attribution, remains underexplored. This paper\nconducts a comprehensive evaluation of LLMs in these critical tasks.\nTraditional studies have depended on hand-crafted stylistic features, whereas\nstate-of-the-art approaches leverage text embeddings from pre-trained language\nmodels. These methods, which typically require fine-tuning on labeled data,\noften suffer from performance degradation in cross-domain applications and\nprovide limited explainability. This work seeks to address three research\nquestions: (1) Can LLMs perform zero-shot, end-to-end authorship verification\neffectively? (2) Are LLMs capable of accurately attributing authorship among\nmultiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide\nexplainability in authorship analysis, particularly through the role of\nlinguistic features? Moreover, we investigate the integration of explicit\nlinguistic features to guide LLMs in their reasoning processes. Our extensive\nassessment demonstrates LLMs' proficiency in both tasks without the need for\ndomain-specific fine-tuning, providing insights into their decision-making via\na detailed analysis of linguistic features. This establishes a new benchmark\nfor future research on LLM-based authorship analysis. The code and data are\navailable at https://github.com/baixianghuang/authorship-llm.\n","authors":["Baixiang Huang","Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2403.08213v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.08211v1","updated":"2024-03-13T03:15:05Z","published":"2024-03-13T03:15:05Z","title":"Large Language Models are Contrastive Reasoners","summary":"  Prompting methods play a crucial role in enhancing the capabilities of\npre-trained large language models (LLMs). We explore how contrastive prompting\n(CP) significantly improves the ability of large language models to perform\ncomplex reasoning. We demonstrate that LLMs are decent contrastive reasoners by\nsimply adding \"Let's give a correct and a wrong answer.\" before LLMs provide\nanswers. Experiments on two large language models show that zero-shot\ncontrastive prompting improves performance on a range of arithmetic,\ncommonsense, and symbolic reasoning tasks without any hand-crafted few-shot\nexamples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and\nAQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method\nnot only surpasses zero-shot CoT and few-shot CoT in most arithmetic and\ncommonsense reasoning tasks but also can seamlessly integrate with existing\nprompting methods, resulting in improved or comparable results when compared to\nstate-of-the-art methods. Our code is available at\nhttps://github.com/yao8839836/cp\n","authors":["Liang Yao"],"pdf_url":"https://arxiv.org/pdf/2403.08211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08198v1","updated":"2024-03-13T02:46:17Z","published":"2024-03-13T02:46:17Z","title":"Validating and Exploring Large Geographic Corpora","summary":"  This paper investigates the impact of corpus creation decisions on large\nmulti-lingual geographic web corpora. Beginning with a 427 billion word corpus\nderived from the Common Crawl, three methods are used to improve the quality of\nsub-corpora representing specific language-country pairs like New Zealand\nEnglish: (i) the agreement of independent language identification systems, (ii)\nhash-based deduplication, and (iii) location-specific outlier detection. The\nimpact of each of these steps is then evaluated at the language level and the\ncountry level by using corpus similarity measures to compare each resulting\ncorpus with baseline data sets. The goal is to understand the impact of\nupstream data cleaning decisions on downstream corpora with a specific focus on\nunder-represented languages and populations. The evaluation shows that the\nvalidity of sub-corpora is improved with each stage of cleaning but that this\nimprovement is unevenly distributed across languages and populations. This\nresult shows how standard corpus creation techniques can accidentally exclude\nunder-represented populations.\n","authors":["Jonathan Dunn"],"pdf_url":"https://arxiv.org/pdf/2403.08198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08196v1","updated":"2024-03-13T02:41:53Z","published":"2024-03-13T02:41:53Z","title":"SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech\n  Recognition Evaluation","summary":"  In the wake of the surging tide of deep learning over the past decade,\nAutomatic Speech Recognition (ASR) has garnered substantial attention, leading\nto the emergence of numerous publicly accessible ASR systems that are actively\nbeing integrated into our daily lives. Nonetheless, the impartial and\nreplicable evaluation of these ASR systems encounters challenges due to various\ncrucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a\ngeneral-purpose, open-source platform designed for ASR evaluation. With this\nplatform: (i) We report a comprehensive benchmark, unveiling the current\nstate-of-the-art panorama for ASR systems, covering both open-source models and\nindustrial commercial services. (ii) We quantize how distinct nuances in the\nscoring pipeline influence the final benchmark outcomes. These include nuances\nrelated to capitalization, punctuation, interjection, contraction, synonym\nusage, compound words, etc. These issues have gained prominence in the context\nof the transition towards an End-to-End future. (iii) We propose a practical\nmodification to the conventional Token-Error-Rate (TER) evaluation metric, with\ninspirations from Kolmogorov complexity and Normalized Information Distance\n(NID). This adaptation, called modified-TER (mTER), achieves proper\nnormalization and symmetrical treatment of reference and hypothesis. By\nleveraging this platform as a large-scale testing ground, this study\ndemonstrates the robustness and backward compatibility of mTER when compared to\nTER. The SpeechColab Leaderboard is accessible at\nhttps://github.com/SpeechColab/Leaderboard\n","authors":["Jiayu Du","Jinpeng Li","Guoguo Chen","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11522v2","updated":"2024-03-13T02:40:21Z","published":"2024-02-18T09:42:41Z","title":"Unveiling the Secrets of Engaging Conversations: Factors that Keep Users\n  Hooked on Role-Playing Dialog Agents","summary":"  With the growing humanlike nature of dialog agents, people are now engaging\nin extended conversations that can stretch from brief moments to substantial\nperiods of time. Understanding the factors that contribute to sustaining these\ninteractions is crucial, yet existing studies primarily focusing on short-term\nsimulations that rarely explore such prolonged and real conversations.\n  In this paper, we investigate the factors influencing retention rates in real\ninteractions with roleplaying models. By analyzing a large dataset of\ninteractions between real users and thousands of characters, we systematically\nexamine multiple factors and assess their impact on user retention rate.\nSurprisingly, we find that the degree to which the bot embodies the roles it\nplays has limited influence on retention rates, while the length of each turn\nit speaks significantly affects retention rates. This study sheds light on the\ncritical aspects of user engagement with role-playing models and provides\nvaluable insights for future improvements in the development of large language\nmodels for role-playing purposes.\n","authors":["Shuai Zhang","Yu Lu","Junwen Liu","Jia Yu","Huachuan Qiu","Yuming Yan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2402.11522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16480v3","updated":"2024-03-13T02:31:30Z","published":"2023-11-27T05:05:41Z","title":"WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images","summary":"  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text\npairs for visual-language models by recognizing and cleaning pathology reports\nwhich narrate diagnostic slides in TCGA. On the model end, we propose the\nmultiple instance generative model (MI-Gen) which can produce pathology reports\nfor gigapixel WSIs. We benchmark our model on the largest subset of\nTCGA-PathoText. Experimental results show our model can generate pathology\nreports which contain multiple clinical clues and achieve competitive\nperformance on certain slide-level tasks. We observe that simple semantic\nextraction from the pathology reports can achieve the best performance (0.838\nof F1 score) on BRCA subtyping surpassing previous state-of-the-art approaches.\nOur collected dataset and related code are available.\n","authors":["Pingyi Chen","Honglin Li","Chenglu Zhu","Sunyi Zheng","Zhongyi Shui","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.16480v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08192v1","updated":"2024-03-13T02:26:16Z","published":"2024-03-13T02:26:16Z","title":"MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular\n  Comprehension","summary":"  Large language models are playing an increasingly significant role in\nmolecular research, yet existing models often generate erroneous information,\nposing challenges to accurate molecular comprehension. Traditional evaluation\nmetrics for generated content fail to assess a model's accuracy in molecular\nunderstanding. To rectify the absence of factual evaluation, we present\nMoleculeQA, a novel question answering (QA) dataset which possesses 62K QA\npairs over 23K molecules. Each QA pair, composed of a manual question, a\npositive option and three negative options, has consistent semantics with a\nmolecular description from authoritative molecular corpus. MoleculeQA is not\nonly the first benchmark for molecular factual bias evaluation but also the\nlargest QA dataset for molecular research. A comprehensive evaluation on\nMoleculeQA for existing molecular LLMs exposes their deficiencies in specific\nareas and pinpoints several particularly crucial factors for molecular\nunderstanding.\n","authors":["Xingyu Lu","He Cao","Zijing Liu","Shengyuan Bai","Leqing Chen","Yuan Yao","Hai-Tao Zheng","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2403.08192v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.08189v1","updated":"2024-03-13T02:23:13Z","published":"2024-03-13T02:23:13Z","title":"Embedded Translations for Low-resource Automated Glossing","summary":"  We investigate automatic interlinear glossing in low-resource settings. We\naugment a hard-attentional neural model with embedded translation information\nextracted from interlinear glossed text. After encoding these translations\nusing large language models, specifically BERT and T5, we introduce a\ncharacter-level decoder for generating glossed output. Aided by these\nenhancements, our model demonstrates an average improvement of 3.97\\%-points\nover the previous state of the art on datasets from the SIGMORPHON 2023 Shared\nTask on Interlinear Glossing. In a simulated ultra low-resource setting,\ntrained on as few as 100 sentences, our system achieves an average 9.78\\%-point\nimprovement over the plain hard-attentional baseline. These results highlight\nthe critical role of translation information in boosting the system's\nperformance, especially in processing and interpreting modest data sources. Our\nfindings suggest a promising avenue for the documentation and preservation of\nlanguages, with our experiments on shared task datasets indicating significant\nadvancements over the existing state of the art.\n","authors":["Changbing Yang","Garrett Nicolai","Miikka Silfverberg"],"pdf_url":"https://arxiv.org/pdf/2403.08189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08187v1","updated":"2024-03-13T02:20:05Z","published":"2024-03-13T02:20:05Z","title":"Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of\n  Speech Sound Disorders in Korean children","summary":"  This study presents a model of automatic speech recognition (ASR) designed to\ndiagnose pronunciation issues in children with speech sound disorders (SSDs) to\nreplace manual transcriptions in clinical procedures. Since ASR models trained\nfor general purposes primarily predict input speech into real words, employing\na well-known high-performance ASR model for evaluating pronunciation in\nchildren with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to\nrecognize speech as pronounced rather than as existing words. The model was\nfine-tuned with a speech dataset from 137 children with inadequate speech\nproduction pronouncing 73 Korean words selected for actual clinical diagnosis.\nThe model's predictions of the pronunciations of the words matched the human\nannotations with about 90% accuracy. While the model still requires improvement\nin recognizing unclear pronunciation, this study demonstrates that ASR models\ncan streamline complex pronunciation error diagnostic procedures in clinical\nfields.\n","authors":["Taekyung Ahn","Yeonjung Hong","Younggon Im","Do Hyung Kim","Dayoung Kang","Joo Won Jeong","Jae Won Kim","Min Jung Kim","Ah-ra Cho","Dae-Hyun Jang","Hosung Nam"],"pdf_url":"https://arxiv.org/pdf/2403.08187v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.08174v1","updated":"2024-03-13T01:56:32Z","published":"2024-03-13T01:56:32Z","title":"Rethinking Loss Functions for Fact Verification","summary":"  We explore loss functions for fact verification in the FEVER shared task.\nWhile the cross-entropy loss is a standard objective for training verdict\npredictors, it fails to capture the heterogeneity among the FEVER verdict\nclasses. In this paper, we develop two task-specific objectives tailored to\nFEVER. Experimental results confirm that the proposed objective functions\noutperform the standard cross-entropy. Performance is further improved when\nthese objectives are combined with simple class weighting, which effectively\novercomes the imbalance in the training data. The souce code is available at\nhttps://github.com/yuta-mukobara/RLF-KGAT\n","authors":["Yuta Mukobara","Yutaro Shigeto","Masashi Shimbo"],"pdf_url":"https://arxiv.org/pdf/2403.08174v1.pdf","comment":"Accepted to EACL 2024 (short paper). The souce code is available at\n  https://github.com/yuta-mukobara/RLF-KGAT"},{"id":"http://arxiv.org/abs/2403.06199v2","updated":"2024-03-13T01:56:18Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08167v1","updated":"2024-03-13T01:38:42Z","published":"2024-03-13T01:38:42Z","title":"MolBind: Multimodal Alignment of Language, Molecules, and Proteins","summary":"  Recent advancements in biology and chemistry have leveraged multi-modal\nlearning, integrating molecules and their natural language descriptions to\nenhance drug discovery. However, current pre-training frameworks are limited to\ntwo modalities, and designing a unified network to process different modalities\n(e.g., natural language, 2D molecular graphs, 3D molecular conformations, and\n3D proteins) remains challenging due to inherent gaps among them. In this work,\nwe propose MolBind, a framework that trains encoders for multiple modalities\nthrough contrastive learning, mapping all modalities to a shared feature space\nfor multi-modal semantic alignment. To facilitate effective pre-training of\nMolBind on multiple modalities, we also build and collect a high-quality\ndataset with four modalities, MolBind-M4, including graph-language,\nconformation-language, graph-conformation, and conformation-protein paired\ndata. MolBind shows superior zero-shot learning performance across a wide range\nof tasks, demonstrating its strong capability of capturing the underlying\nsemantics of multiple modalities.\n","authors":["Teng Xiao","Chao Cui","Huaisheng Zhu","Vasant G. Honavar"],"pdf_url":"https://arxiv.org/pdf/2403.08167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06487v2","updated":"2024-03-13T00:41:36Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v2.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2401.15127v2","updated":"2024-03-13T23:51:13Z","published":"2024-01-26T13:15:24Z","title":"Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness","summary":"  Knowledge sharing about emerging threats is crucial in the rapidly advancing\nfield of cybersecurity and forms the foundation of Cyber Threat Intelligence\n(CTI). In this context, Large Language Models are becoming increasingly\nsignificant in the field of cybersecurity, presenting a wide range of\nopportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly,\nStanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary\nclassification and Named Entity Recognition (NER) tasks performed using Open\nSource INTelligence (OSINT). We utilize well-established data collected in\nprevious research from Twitter to assess the competitiveness of these chatbots\nwhen compared to specialized models trained for those tasks. In binary\nclassification experiments, Chatbot GPT-4 as a commercial model achieved an\nacceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1\nscore of 0.90. However, concerning cybersecurity entity recognition, all\nevaluated chatbots have limitations and are less effective. This study\ndemonstrates the capability of chatbots for OSINT binary classification and\nshows that they require further improvement in NER to effectively replace\nspecially trained models. Our results shed light on the limitations of the LLM\nchatbots when compared to specialized models, and can help researchers improve\nchatbots technology with the objective to reduce the required effort to\nintegrate machine learning in OSINT-based CTI tools.\n","authors":["Samaneh Shafee","Alysson Bessani","Pedro M. Ferreira"],"pdf_url":"https://arxiv.org/pdf/2401.15127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07311v2","updated":"2024-03-13T23:44:30Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of predicting multiple links within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, a challenge increasingly\nresolvable due to advancements in natural language processing (NLP) and KG\nembedding techniques. This paper introduces a novel methodology, the Knowledge\nGraph Large Language Model Framework (KG-LLM), which leverages pivotal NLP\nparadigms, including chain-of-thought (CoT) prompting and in-context learning\n(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a\nCoT prompt, our framework is designed to discern and learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)\nwithin this framework, employing both non-ICL and ICL tasks for a comprehensive\nevaluation. Further, we explore the framework's potential to provide LLMs with\nzero-shot capabilities for handling previously unseen prompts. Our experimental\nfindings discover that integrating ICL and CoT not only augments the\nperformance of our approach but also significantly boosts the models'\ngeneralization capacity, thereby ensuring more precise predictions in\nunfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Yiting Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v2.pdf","comment":"23 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.09862v2","updated":"2024-03-13T23:32:32Z","published":"2023-11-16T12:45:41Z","title":"Which Modality should I use -- Text, Motif, or Image? : Understanding\n  Graphs with Large Language Models","summary":"  Our research integrates graph data with Large Language Models (LLMs), which,\ndespite their advancements in various fields using large text corpora, face\nlimitations in encoding entire graphs due to context size constraints. This\npaper introduces a new approach to encoding a graph with diverse modalities,\nsuch as text, image, and motif, coupled with prompts to approximate a graph's\nglobal connectivity, thereby enhancing LLMs' efficiency in processing complex\ngraph structures. The study also presents GraphTMI, a novel benchmark for\nevaluating LLMs in graph structure analysis, focusing on homophily, motif\npresence, and graph difficulty. Key findings indicate that the image modality,\nespecially with vision-language models like GPT-4V, is superior to text in\nbalancing token limits and preserving essential information and outperforms\nprior graph neural net (GNN) encoders. Furthermore, the research assesses how\nvarious factors affect the performance of each encoding modality and outlines\nthe existing challenges and potential future developments for LLMs in graph\nunderstanding and reasoning tasks. All data will be publicly available upon\nacceptance.\n","authors":["Debarati Das","Ishaan Gupta","Jaideep Srivastava","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2311.09862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08994v1","updated":"2024-03-13T23:25:30Z","published":"2024-03-13T23:25:30Z","title":"Ethos: Rectifying Language Models in Orthogonal Parameter Space","summary":"  Language models (LMs) have greatly propelled the research on natural language\nprocessing. However, LMs also raise concerns regarding the generation of biased\nor toxic content and the potential disclosure of private information from the\ntraining dataset. In this work, we present a new efficient approach, Ethos,\nthat rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy\nleakage. Ethos is built on task arithmetic. However, unlike current task\narithmetic algorithms, Ethos distinguishes general beneficial and undesired\nknowledge when reconstructing task vectors. Specifically, Ethos first obtains a\nset of principal components from the pre-trained models using singular value\ndecomposition. Then, by projecting the task vector onto principal components,\nEthos identifies the principal components that encode general or undesired\nknowledge. Ethos performs negating using the task vector with undesired\nknowledge only, thereby minimizing collateral damage on general model utility.\nWe demonstrate the efficacy of our approach on three different tasks:\ndebiasing, detoxification, and memorization unlearning. Evaluations show Ethos\nis more effective in removing undesired knowledge and maintaining the overall\nmodel performance compared to current task arithmetic methods.\n","authors":["Lei Gao","Yue Niu","Tingting Tang","Salman Avestimehr","Murali Annavaram"],"pdf_url":"https://arxiv.org/pdf/2403.08994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04363v2","updated":"2024-03-13T22:48:14Z","published":"2023-10-06T16:36:08Z","title":"Amortizing intractable inference in large language models","summary":"  Autoregressive large language models (LLMs) compress knowledge from their\ntraining data through next-token conditional distributions. This limits\ntractable querying of this knowledge to start-to-end autoregressive sampling.\nHowever, many tasks of interest -- including sequence continuation, infilling,\nand other forms of constrained generation -- involve sampling from intractable\nposterior distributions. We address this limitation by using amortized Bayesian\ninference to sample from these intractable posteriors. Such amortization is\nalgorithmically achieved by fine-tuning LLMs via diversity-seeking\nreinforcement learning algorithms: generative flow networks (GFlowNets). We\nempirically demonstrate that this distribution-matching paradigm of LLM\nfine-tuning can serve as an effective alternative to maximum-likelihood\ntraining and reward-maximizing policy optimization. As an important\napplication, we interpret chain-of-thought reasoning as a latent variable\nmodeling problem and demonstrate that our approach enables data-efficient\nadaptation of LLMs to tasks that require multi-step rationalization and tool\nuse.\n","authors":["Edward J. Hu","Moksh Jain","Eric Elmoznino","Younesse Kaddar","Guillaume Lajoie","Yoshua Bengio","Nikolay Malkin"],"pdf_url":"https://arxiv.org/pdf/2310.04363v2.pdf","comment":"ICLR 2024; 23 pages; code: https://github.com/GFNOrg/gfn-lm-tuning"},{"id":"http://arxiv.org/abs/2310.02226v2","updated":"2024-03-13T22:33:41Z","published":"2023-10-03T17:32:41Z","title":"Think before you speak: Training Language Models With Pause Tokens","summary":"  Language models generate responses by producing a series of tokens in\nimmediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$\nhidden vectors per layer, one vector per preceding token. What if instead we\nwere to let the model manipulate say, $K+10$ hidden vectors, before it outputs\nthe $(K+1)^{th}$ token? We operationalize this idea by performing training and\ninference on language models with a (learnable) $\\textit{pause}$ token, a\nsequence of which is appended to the input prefix. We then delay extracting the\nmodel's outputs until the last pause token is seen, thereby allowing the model\nto process extra computation before committing to an answer. We empirically\nevaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M\nparameters with causal pretraining on C4, and on downstream tasks covering\nreasoning, question-answering, general understanding and fact recall. Our main\nfinding is that inference-time delays show gains when the model is both\npre-trained and finetuned with delays. For the 1B model, we witness gains on 8\nof 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of\nSQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of\nGSM8k. Our work raises a range of conceptual and practical future research\nquestions on making delayed next-token prediction a widely applicable new\nparadigm.\n","authors":["Sachin Goyal","Ziwei Ji","Ankit Singh Rawat","Aditya Krishna Menon","Sanjiv Kumar","Vaishnavh Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2310.02226v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08978v1","updated":"2024-03-13T22:06:03Z","published":"2024-03-13T22:06:03Z","title":"AutoGuide: Automated Generation and Selection of State-Aware Guidelines\n  for Large Language Model Agents","summary":"  The primary limitation of large language models (LLMs) is their restricted\nunderstanding of the world. This poses significant difficulties for LLM-based\nagents, particularly in domains where pre-trained LLMs lack sufficient\nknowledge. In this paper, we introduce a novel framework, called AutoGuide,\nthat bridges the knowledge gap in pre-trained LLMs by leveraging implicit\nknowledge in offline experiences. Specifically, AutoGuide effectively extracts\nknowledge embedded in offline data by extracting a set of state-aware\nguidelines. Importantly, each state-aware guideline is expressed in concise\nnatural language and follows a conditional structure, clearly describing the\nstate where it is applicable. As such, the resulting guidelines enable a\nprincipled way to provide helpful knowledge pertinent to an agent's current\ndecision-making process. We show that our approach outperforms competitive\nLLM-based baselines by a large margin in sequential decision-making benchmarks.\n","authors":["Yao Fu","Dong-Ki Kim","Jaekyeom Kim","Sungryull Sohn","Lajanugen Logeswaran","Kyunghoon Bae","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2403.08978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18235v4","updated":"2024-03-13T21:58:59Z","published":"2023-10-27T16:20:10Z","title":"Davidsonian Scene Graph: Improving Reliability in Fine-grained\n  Evaluation for Text-to-Image Generation","summary":"  Evaluating text-to-image models is notoriously difficult. A strong recent\napproach for assessing text-image faithfulness is based on QG/A (question\ngeneration and answering), which uses pre-trained foundational models to\nautomatically generate a set of questions and answers from the prompt, and\noutput images are scored based on whether these answers extracted with a visual\nquestion answering model are consistent with the prompt-based answers. This\nkind of evaluation is naturally dependent on the quality of the underlying QG\nand VQA models. We identify and address several reliability challenges in\nexisting QG/A work: (a) QG questions should respect the prompt (avoiding\nhallucinations, duplications, and omissions) and (b) VQA answers should be\nconsistent (not asserting that there is no motorcycle in an image while also\nclaiming the motorcycle is blue). We address these issues with Davidsonian\nScene Graph (DSG), an empirically grounded evaluation framework inspired by\nformal semantics, which is adaptable to any QG/A frameworks. DSG produces\natomic and unique questions organized in dependency graphs, which (i) ensure\nappropriate semantic coverage and (ii) sidestep inconsistent answers. With\nextensive experimentation and human evaluation on a range of model\nconfigurations (LLM, VQA, and T2I), we empirically demonstrate that DSG\naddresses the challenges noted above. Finally, we present DSG-1k, an\nopen-sourced evaluation benchmark that includes 1,060 prompts, covering a wide\nrange of fine-grained semantic categories with a balanced distribution. We\nrelease the DSG-1k prompts and the corresponding DSG questions.\n","authors":["Jaemin Cho","Yushi Hu","Roopal Garg","Peter Anderson","Ranjay Krishna","Jason Baldridge","Mohit Bansal","Jordi Pont-Tuset","Su Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18235v4.pdf","comment":"ICLR 2024; Project website: https://google.github.io/dsg"},{"id":"http://arxiv.org/abs/2403.01193v2","updated":"2024-03-13T21:57:19Z","published":"2024-03-02T12:19:04Z","title":"RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots","summary":"  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\nof artificial intelligence. However, their tendency to hallucinate -- generate\nplausible but false information -- poses a significant challenge. This issue is\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\nGeneration (RAG) can counter hallucinations by integrating external knowledge\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\nin some cases, but can still be misled when prompts directly contradict the\nmodel's pre-trained understanding. These findings highlight the complex nature\nof hallucinations and the need for more robust solutions to ensure LLM\nreliability in real-world applications. We offer practical recommendations for\nRAG deployment and discuss implications for the development of more trustworthy\nLLMs.\n","authors":["Philip Feldman. James R. Foulds","Shimei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.01193v2.pdf","comment":"7 Pages, 1 Figure, 1 Table"},{"id":"http://arxiv.org/abs/2403.08946v1","updated":"2024-03-13T20:25:27Z","published":"2024-03-13T20:25:27Z","title":"Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM\n  Era","summary":"  Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended towards Large Language Models (LLMs) which are often criticized for\ntheir lack of transparency. This extension calls for a significant\ntransformation in XAI methodologies because of two reasons. First, many\nexisting XAI methods cannot be directly applied to LLMs due to their complexity\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse\nindustry applications, the role of XAI shifts from merely opening the \"black\nbox\" to actively enhancing the productivity and applicability of LLMs in\nreal-world settings. Meanwhile, unlike traditional machine learning models that\nare passive recipients of XAI insights, the distinct abilities of LLMs can\nreciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in\nthe context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10\nstrategies, introducing the key techniques for each and discussing their\nassociated challenges. We also provide case studies to demonstrate how to\nobtain and leverage explanations. The code used in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.\n","authors":["Xuansheng Wu","Haiyan Zhao","Yaochen Zhu","Yucheng Shi","Fan Yang","Tianming Liu","Xiaoming Zhai","Wenlin Yao","Jundong Li","Mengnan Du","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08946v1.pdf","comment":"38 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.08943v1","updated":"2024-03-13T20:19:30Z","published":"2024-03-13T20:19:30Z","title":"LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots","summary":"  Since the breakthrough of ChatGPT, large language models (LLMs) have garnered\nsignificant attention in the research community. With the development of LLMs,\nthe question of text style transfer for conversational models has emerged as a\nnatural extension, where chatbots may possess their own styles or even\ncharacters. However, standard evaluation metrics have not yet been established\nfor this new settings. This paper aims to address this issue by proposing the\nLMStyle Benchmark, a novel evaluation framework applicable to chat-style text\nstyle transfer (C-TST), that can measure the quality of style transfer for LLMs\nin an automated and scalable manner. In addition to conventional style strength\nmetrics, LMStyle Benchmark further considers a novel aspect of metrics called\nappropriateness, a high-level metrics take account of coherence, fluency and\nother implicit factors without the aid of reference samples. Our experiments\ndemonstrate that the new evaluation methods introduced by LMStyle Benchmark\nhave a higher correlation with human judgments in terms of appropriateness.\nBased on LMStyle Benchmark, we present a comprehensive list of evaluation\nresults for popular LLMs, including LLaMA, Alpaca, and Vicuna, reflecting their\nstylistic properties, such as formality and sentiment strength, along with\ntheir appropriateness.\n","authors":["Jianlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13492v2","updated":"2024-03-13T20:09:46Z","published":"2024-02-21T03:05:50Z","title":"Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval\n  Augmentation to Language Models","summary":"  While large language models (LMs) demonstrate remarkable performance, they\nencounter challenges in providing accurate responses when queried for\ninformation beyond their pre-trained memorization. Although augmenting them\nwith relevant external information can mitigate these issues, failure to\nconsider the necessity of retrieval may adversely affect overall performance.\nPrevious research has primarily focused on examining how entities influence\nretrieval models and knowledge recall in LMs, leaving other aspects relatively\nunexplored. In this work, our goal is to offer a more detailed, fact-centric\nanalysis by exploring the effects of combinations of entities and relations. To\nfacilitate this, we construct a new question answering (QA) dataset called\nWiTQA (Wikipedia Triple Question Answers). This dataset includes questions\nabout entities and relations of various popularity levels, each accompanied by\na supporting passage. Our extensive experiments with diverse LMs and retrievers\nreveal when retrieval does not consistently enhance LMs from the viewpoints of\nfact-centric popularity.Confirming earlier findings, we observe that larger LMs\nexcel in recalling popular facts. However, they notably encounter difficulty\nwith infrequent entity-relation pairs compared to retrievers. Interestingly,\nthey can effectively retain popular relations of less common entities. We\ndemonstrate the efficacy of our finer-grained metric and insights through an\nadaptive retrieval system that selectively employs retrieval and recall based\non the frequencies of entities and relations in the question.\n","authors":["Seiji Maekawa","Hayate Iso","Sairam Gurajada","Nikita Bhutani"],"pdf_url":"https://arxiv.org/pdf/2402.13492v2.pdf","comment":"NAACL2024 (main)"},{"id":"http://arxiv.org/abs/2403.08904v1","updated":"2024-03-13T18:47:00Z","published":"2024-03-13T18:47:00Z","title":"Detecting Hallucination and Coverage Errors in Retrieval Augmented\n  Generation for Controversial Topics","summary":"  We explore a strategy to handle controversial topics in LLM-based chatbots\nbased on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the\nabsence of a single true answer and surface multiple perspectives. We frame\nthis as retrieval augmented generation, where perspectives are retrieved from a\nknowledge base and the LLM is tasked with generating a fluent and faithful\nresponse from the given perspectives. As a starting point, we use a\ndeterministic retrieval system and then focus on common LLM failure modes that\narise during this approach to text generation, namely hallucination and\ncoverage errors. We propose and evaluate three methods to detect such errors\nbased on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our\nresults demonstrate that LLM-based classifiers, even when trained only on\nsynthetic errors, achieve high error detection performance, with ROC AUC scores\nof 95.3% for hallucination and 90.5% for coverage error detection on\nunambiguous error cases. We show that when no training data is available, our\nother methods still yield good results on hallucination (84.0%) and coverage\nerror (85.2%) detection.\n","authors":["Tyler A. Chang","Katrin Tomanek","Jessica Hoffmann","Nithum Thain","Erin van Liemt","Kathleen Meier-Hellstern","Lucas Dixon"],"pdf_url":"https://arxiv.org/pdf/2403.08904v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08890v1","updated":"2024-03-13T18:20:24Z","published":"2024-03-13T18:20:24Z","title":"From \"um\" to \"yeah\": Producing, predicting, and regulating information\n  flow in human conversation","summary":"  Conversation demands attention. Speakers must call words to mind, listeners\nmust make sense of them, and both together must negotiate this flow of\ninformation, all in fractions of a second. We used large language models to\nstudy how this works in a large-scale dataset of English-language conversation,\nthe CANDOR corpus. We provide a new estimate of the information density of\nunstructured conversation, of approximately 13 bits/second, and find\nsignificant effects associated with the cognitive load of both retrieving, and\npresenting, that information. We also reveal a role for backchannels -- the\nbrief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the\nproduction of novelty: the lead-up to a backchannel is associated with\ndeclining information rate, while speech downstream rebounds to previous rates.\nOur results provide new insights into long-standing theories of how we respond\nto fluctuating demands on cognitive resources, and how we negotiate those\ndemands in partnership with others.\n","authors":["Claire Augusta Bergey","Simon DeDeo"],"pdf_url":"https://arxiv.org/pdf/2403.08890v1.pdf","comment":"18 pages, 4 figures, comments welcome"},{"id":"http://arxiv.org/abs/2403.08851v1","updated":"2024-03-13T18:00:00Z","published":"2024-03-13T18:00:00Z","title":"PAPERCLIP: Associating Astronomical Observations and Natural Language\n  with Multi-Modal Models","summary":"  We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation\nfor Contrastive Language-Image Pre-training), a method which associates\nastronomical observations imaged by telescopes with natural language using a\nneural network model. The model is fine-tuned from a pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) model using successful observing proposal\nabstracts and corresponding downstream observations, with the abstracts\noptionally summarized via guided generation using large language models (LLMs).\nUsing observations from the Hubble Space Telescope (HST) as an example, we show\nthat the fine-tuned model embodies a meaningful joint representation between\nobservations and natural language through tests targeting image retrieval\n(i.e., finding the most relevant observations using natural language queries)\nand description retrieval (i.e., querying for astrophysical object classes and\nuse cases most relevant to a given observation). Our study demonstrates the\npotential for using generalist foundation models rather than task-specific\nmodels for interacting with astronomical data by leveraging text as an\ninterface.\n","authors":["Siddharth Mishra-Sharma","Yiding Song","Jesse Thaler"],"pdf_url":"https://arxiv.org/pdf/2403.08851v1.pdf","comment":"17+6 pages, 3+1 figures, 5+2 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.08770v1","updated":"2024-03-13T17:59:56Z","published":"2024-03-13T17:59:56Z","title":"FastMAC: Stochastic Spectral Sampling of Correspondence Graph","summary":"  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.\n","authors":["Yifei Zhang","Hao Zhao","Hongyang Li","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08770v1.pdf","comment":"CVPR 2024, Code: https://github.com/Forrest-110/FastMAC"},{"id":"http://arxiv.org/abs/2403.08768v1","updated":"2024-03-13T17:59:50Z","published":"2024-03-13T17:59:50Z","title":"3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface","summary":"  This paper introduces 3DFIRES, a novel system for scene-level 3D\nreconstruction from posed images. Designed to work with as few as one view,\n3DFIRES reconstructs the complete geometry of unseen scenes, including hidden\nsurfaces. With multiple view inputs, our method produces full reconstruction\nwithin all camera frustums. A key feature of our approach is the fusion of\nmulti-view information at the feature level, enabling the production of\ncoherent and comprehensive 3D reconstruction. We train our system on\nnon-watertight scans from large-scale real scene dataset. We show it matches\nthe efficacy of single-view reconstruction methods with only one input and\nsurpasses existing techniques in both quantitative and qualitative measures for\nsparse-view 3D reconstruction.\n","authors":["Linyi Jin","Nilesh Kulkarni","David Fouhey"],"pdf_url":"https://arxiv.org/pdf/2403.08768v1.pdf","comment":"Accepted to CVPR 2024. Project Page\n  https://jinlinyi.github.io/3DFIRES/"},{"id":"http://arxiv.org/abs/2403.08766v1","updated":"2024-03-13T17:59:04Z","published":"2024-03-13T17:59:04Z","title":"MonoOcc: Digging into Monocular Semantic Occupancy Prediction","summary":"  Monocular Semantic Occupancy Prediction aims to infer the complete 3D\ngeometry and semantic information of scenes from only 2D images. It has\ngarnered significant attention, particularly due to its potential to enhance\nthe 3D perception of autonomous vehicles. However, existing methods rely on a\ncomplex cascaded framework with relatively limited information to restore 3D\nscenes, including a dependency on supervision solely on the whole network's\noutput, single-frame input, and the utilization of a small backbone. These\nchallenges, in turn, hinder the optimization of the framework and yield\ninferior prediction results, particularly concerning smaller and long-tailed\nobjects. To address these issues, we propose MonoOcc. In particular, we (i)\nimprove the monocular occupancy prediction framework by proposing an auxiliary\nsemantic loss as supervision to the shallow layers of the framework and an\nimage-conditioned cross-attention module to refine voxel features with visual\nclues, and (ii) employ a distillation module that transfers temporal\ninformation and richer knowledge from a larger image backbone to the monocular\nsemantic occupancy prediction framework with low cost of hardware. With these\nadvantages, our method yields state-of-the-art performance on the camera-based\nSemanticKITTI Scene Completion benchmark. Codes and models can be accessed at\nhttps://github.com/ucaszyp/MonoOcc\n","authors":["Yupeng Zheng","Xiang Li","Pengfei Li","Yuhang Zheng","Bu Jin","Chengliang Zhong","Xiaoxiao Long","Hao Zhao","Qichao Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08766v1.pdf","comment":"Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.08764v1","updated":"2024-03-13T17:59:02Z","published":"2024-03-13T17:59:02Z","title":"VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis","summary":"  We propose VLOGGER, a method for audio-driven human video generation from a\nsingle input image of a person, which builds on the success of recent\ngenerative diffusion models. Our method consists of 1) a stochastic\nhuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture\nthat augments text-to-image models with both spatial and temporal controls.\nThis supports the generation of high quality video of variable length, easily\ncontrollable through high-level representations of human faces and bodies. In\ncontrast to previous work, our method does not require training for each\nperson, does not rely on face detection and cropping, generates the complete\nimage (not just the face or the lips), and considers a broad spectrum of\nscenarios (e.g. visible torso or diverse subject identities) that are critical\nto correctly synthesize humans who communicate. We also curate MENTOR, a new\nand diverse dataset with 3d pose and expression annotations, one order of\nmagnitude larger than previous ones (800,000 identities) and with dynamic\ngestures, on which we train and ablate our main technical contributions.\n  VLOGGER outperforms state-of-the-art methods in three public benchmarks,\nconsidering image quality, identity preservation and temporal consistency while\nalso generating upper-body gestures. We analyze the performance of VLOGGER with\nrespect to multiple diversity metrics, showing that our architectural choices\nand the use of MENTOR benefit training a fair and unbiased model at scale.\nFinally we show applications in video editing and personalization.\n","authors":["Enric Corona","Andrei Zanfir","Eduard Gabriel Bazavan","Nikos Kolotouros","Thiemo Alldieck","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2403.08764v1.pdf","comment":"Project web: https://enriccorona.github.io/vlogger/"},{"id":"http://arxiv.org/abs/2403.08761v1","updated":"2024-03-13T17:58:34Z","published":"2024-03-13T17:58:34Z","title":"Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative\n  Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches","summary":"  Knee osteoarthritis is a degenerative joint disease that induces chronic pain\nand disability. Bone morphological analysis is a promising tool to understand\nthe mechanical aspect of this disorder. This study proposes a 2D bone\nmorphological analysis using manually segmented bones to explore morphological\nfeatures related to distinct pain conditions. Furthermore, six semantic\nsegmentation algorithms are assessed for extracting femur and tibia bones from\nX-ray images. Our analysis reveals that the morphology of the femur undergoes\nsignificant changes in instances where pain worsens. Conversely, improvements\nin pain may not manifest pronounced alterations in bone shape. The\nfew-shot-learning-based algorithm, UniverSeg, demonstrated superior\nsegmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.\nRegarding pain condition classification, the zero-shot-learning-based\nalgorithm, CP-SAM, achieved the highest accuracy at 66% among all models.\nUniverSeg is recommended for automatic knee bone segmentation, while SAM models\nshow potential with prompt encoder modifications for optimized outcomes. These\nfindings highlight the effectiveness of few-shot learning for semantic\nsegmentation and the potential of zero-shot learning in enhancing\nclassification models for knee osteoarthritis diagnosis.\n","authors":["Yun Xin Teoh","Alice Othmani","Siew Li Goh","Juliana Usman","Khin Wee Lai"],"pdf_url":"https://arxiv.org/pdf/2403.08761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08760v1","updated":"2024-03-13T17:58:00Z","published":"2024-03-13T17:58:00Z","title":"MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving\n  Representation Learning","summary":"  Learning robust and scalable visual representations from massive multi-view\nvideo data remains a challenge in computer vision and autonomous driving.\nExisting pre-training methods either rely on expensive supervised learning with\n3D annotations, limiting the scalability, or focus on single-frame or monocular\ninputs, neglecting the temporal information. We propose MIM4D, a novel\npre-training paradigm based on dual masked image modeling (MIM). MIM4D\nleverages both spatial and temporal relations by training on masked multi-view\nvideo inputs. It constructs pseudo-3D features using continuous scene flow and\nprojects them onto 2D plane for supervision. To address the lack of dense 3D\nsupervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable\nrendering to learn geometric representations. We demonstrate that MIM4D\nachieves state-of-the-art performance on the nuScenes dataset for visual\nrepresentation learning in autonomous driving. It significantly improves\nexisting methods on multiple downstream tasks, including BEV segmentation (8.7%\nIoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our\nwork offers a new choice for learning representation at scale in autonomous\ndriving. Code and models are released at https://github.com/hustvl/MIM4D\n","authors":["Jialv Zou","Bencheng Liao","Qian Zhang","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08758v1","updated":"2024-03-13T17:56:12Z","published":"2024-03-13T17:56:12Z","title":"Spatiotemporal Diffusion Model with Paired Sampling for Accelerated\n  Cardiac Cine MRI","summary":"  Current deep learning reconstruction for accelerated cardiac cine MRI suffers\nfrom spatial and temporal blurring. We aim to improve image sharpness and\nmotion delineation for cine MRI under high undersampling rates. A\nspatiotemporal diffusion enhancement model conditional on an existing deep\nlearning reconstruction along with a novel paired sampling strategy was\ndeveloped. The diffusion model provided sharper tissue boundaries and clearer\nmotion than the original reconstruction in experts evaluation on clinical data.\nThe innovative paired sampling strategy substantially reduced artificial noises\nin the generative results.\n","authors":["Shihan Qiu","Shaoyan Pan","Yikang Liu","Lin Zhao","Jian Xu","Qi Liu","Terrence Chen","Eric Z. Chen","Xiao Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08755v1","updated":"2024-03-13T17:53:47Z","published":"2024-03-13T17:53:47Z","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","summary":"  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n","authors":["Feng Cheng","Ziyang Wang","Yi-Lin Sung","Yan-Bo Lin","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2403.08755v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2403.08749v1","updated":"2024-03-13T17:51:01Z","published":"2024-03-13T17:51:01Z","title":"Clinically Feasible Diffusion Reconstruction for Highly-Accelerated\n  Cardiac Cine MRI","summary":"  The currently limited quality of accelerated cardiac cine reconstruction may\npotentially be improved by the emerging diffusion models, but the clinically\nunacceptable long processing time poses a challenge. We aim to develop a\nclinically feasible diffusion-model-based reconstruction pipeline to improve\nthe image quality of cine MRI. A multi-in multi-out diffusion enhancement model\ntogether with fast inference strategies were developed to be used in\nconjunction with a reconstruction model. The diffusion reconstruction reduced\nspatial and temporal blurring in prospectively undersampled clinical data, as\nvalidated by experts inspection. The 1.5s per video processing time enabled the\napproach to be applied in clinical scenarios.\n","authors":["Shihan Qiu","Shaoyan Pan","Yikang Liu","Lin Zhao","Jian Xu","Qi Liu","Terrence Chen","Eric Z. Chen","Xiao Chen","Shanhui Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08748v1","updated":"2024-03-13T17:50:59Z","published":"2024-03-13T17:50:59Z","title":"Real-time 3D semantic occupancy prediction for autonomous vehicles using\n  memory-efficient sparse convolution","summary":"  In autonomous vehicles, understanding the surrounding 3D environment of the\nego vehicle in real-time is essential. A compact way to represent scenes while\nencoding geometric distances and semantic object information is via 3D semantic\noccupancy maps. State of the art 3D mapping methods leverage transformers with\ncross-attention mechanisms to elevate 2D vision-centric camera features into\nthe 3D domain. However, these methods encounter significant challenges in\nreal-time applications due to their high computational demands during\ninference. This limitation is particularly problematic in autonomous vehicles,\nwhere GPU resources must be shared with other tasks such as localization and\nplanning. In this paper, we introduce an approach that extracts features from\nfront-view 2D camera images and LiDAR scans, then employs a sparse convolution\nnetwork (Minkowski Engine), for 3D semantic occupancy prediction. Given that\noutdoor scenes in autonomous driving scenarios are inherently sparse, the\nutilization of sparse convolution is particularly apt. By jointly solving the\nproblems of 3D scene completion of sparse scenes and 3D semantic segmentation,\nwe provide a more efficient learning framework suitable for real-time\napplications in autonomous vehicles. We also demonstrate competitive accuracy\non the nuScenes dataset.\n","authors":["Samuel Sze","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.08748v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.08746v1","updated":"2024-03-13T17:48:39Z","published":"2024-03-13T17:48:39Z","title":"iCONTRA: Toward Thematic Collection Design Via Interactive Concept\n  Transfer","summary":"  Creating thematic collections in industries demands innovative designs and\ncohesive concepts. Designers may face challenges in maintaining thematic\nconsistency when drawing inspiration from existing objects, landscapes, or\nartifacts. While AI-powered graphic design tools offer help, they often fail to\ngenerate cohesive sets based on specific thematic concepts. In response, we\nintroduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly\ninterface, iCONTRA enables both experienced designers and novices to\neffortlessly explore creative design concepts and efficiently generate thematic\ncollections. We also propose a zero-shot image editing algorithm, eliminating\nthe need for fine-tuning models, which gradually integrates information from\ninitial objects, ensuring consistency in the generation process without\ninfluencing the background. A pilot study suggests iCONTRA's potential to\nreduce designers' efforts. Experimental results demonstrate its effectiveness\nin producing consistent and high-quality object concept transfers. iCONTRA\nstands as a promising tool for innovation and creative exploration in thematic\ncollection design. The source code will be available at:\nhttps://github.com/vdkhoi20/iCONTRA.\n","authors":["Dinh-Khoi Vo","Duy-Nam Ly","Khanh-Duy Le","Tam V. Nguyen","Minh-Triet Tran","Trung-Nghia Le"],"pdf_url":"https://arxiv.org/pdf/2403.08746v1.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.08733v1","updated":"2024-03-13T17:35:28Z","published":"2024-03-13T17:35:28Z","title":"GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting\n  Editing","summary":"  We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed\nby the 3D Gaussian Splatting (3DGS).\n  Our method first renders a collection of images by using the 3DGS and edits\nthem by using a pre-trained 2D diffusion model (ControlNet) based on the input\nprompt, which is then used to optimise the 3D model.\n  Our key contribution is multi-view consistent editing, which enables editing\nall images together instead of iteratively editing one image while updating the\n3D model as in previous works.\n  It leads to faster editing as well as higher visual quality.\n  This is achieved by the two terms:\n  (a) depth-conditioned editing that enforces geometric consistency across\nmulti-view images by leveraging naturally consistent depth maps.\n  (b) attention-based latent code alignment that unifies the appearance of\nedited images by conditioning their editing to several reference views through\nself and cross-view attention between images' latent representations.\n  Experiments demonstrate that our method achieves faster editing and better\nvisual results than previous state-of-the-art methods.\n","authors":["Jing Wu","Jia-Wang Bian","Xinghui Li","Guangrun Wang","Ian Reid","Philip Torr","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2403.08733v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.08730v1","updated":"2024-03-13T17:29:45Z","published":"2024-03-13T17:29:45Z","title":"Strengthening Multimodal Large Language Model with Bootstrapped\n  Preference Optimization","summary":"  Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.\n","authors":["Renjie Pi","Tianyang Han","Wei Xiong","Jipeng Zhang","Runtao Liu","Rui Pan","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08407v2","updated":"2024-03-13T17:28:33Z","published":"2024-01-16T14:45:41Z","title":"Cross-Domain Few-Shot Segmentation via Iterative Support-Query\n  Correspondence Mining","summary":"  Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting\nnovel categories from a distinct domain using only limited exemplars. In this\npaper, we undertake a comprehensive study of CD-FSS and uncover two crucial\ninsights: (i) the necessity of a fine-tuning stage to effectively transfer the\nlearned meta-knowledge across domains, and (ii) the overfitting risk during the\nna\\\"ive fine-tuning due to the scarcity of novel category examples. With these\ninsights, we propose a novel cross-domain fine-tuning strategy that addresses\nthe challenging CD-FSS tasks. We first design Bi-directional Few-shot\nPrediction (BFP), which establishes support-query correspondence in a\nbi-directional manner, crafting augmented supervision to reduce the overfitting\nrisk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which\nis a recursive framework to capture the support-query correspondence\niteratively, targeting maximal exploitation of supervisory signals from the\nsparse novel category samples. Extensive empirical evaluations show that our\nmethod significantly outperforms the state-of-the-arts (+7.8\\%), which verifies\nthat IFA tackles the cross-domain challenges and mitigates the overfitting\nsimultaneously. The code is available at: https://github.com/niejiahao1998/IFA.\n","authors":["Jiahao Nie","Yun Xing","Gongjie Zhang","Pei Yan","Aoran Xiao","Yap-Peng Tan","Alex C. Kot","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2401.08407v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08728v1","updated":"2024-03-13T17:28:20Z","published":"2024-03-13T17:28:20Z","title":"Ambient Diffusion Posterior Sampling: Solving Inverse Problems with\n  Diffusion Models trained on Corrupted Data","summary":"  We provide a framework for solving inverse problems with diffusion models\nlearned from linearly corrupted data. Our method, Ambient Diffusion Posterior\nSampling (A-DPS), leverages a generative model pre-trained on one type of\ncorruption (e.g. image inpainting) to perform posterior sampling conditioned on\nmeasurements from a potentially different forward process (e.g. image\nblurring). We test the efficacy of our approach on standard natural image\ndatasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes\noutperform models trained on clean data for several image restoration tasks in\nboth speed and performance. We further extend the Ambient Diffusion framework\nto train MRI models with access only to Fourier subsampled multi-coil MRI\nmeasurements at various acceleration factors (R=2, 4, 6, 8). We again observe\nthat models trained on highly subsampled data are better priors for solving\ninverse problems in the high acceleration regime than models trained on fully\nsampled data. We open-source our code and the trained Ambient Diffusion MRI\nmodels: https://github.com/utcsilab/ambient-diffusion-mri .\n","authors":["Asad Aali","Giannis Daras","Brett Levac","Sidharth Kumar","Alexandros G. Dimakis","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2403.08728v1.pdf","comment":"Pre-print, work in progress"},{"id":"http://arxiv.org/abs/2306.14306v2","updated":"2024-03-13T17:20:27Z","published":"2023-06-25T18:29:29Z","title":"Adaptive Sharpness-Aware Pruning for Robust Sparse Networks","summary":"  Robustness and compactness are two essential attributes of deep learning\nmodels that are deployed in the real world. The goals of robustness and\ncompactness may seem to be at odds, since robustness requires generalization\nacross domains, while the process of compression exploits specificity in one\ndomain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies\nthese goals through the lens of network sharpness. The AdaSAP method produces\nsparse networks that are robust to input variations which are unseen at\ntraining time. We achieve this by strategically incorporating weight\nperturbations in order to optimize the loss landscape. This allows the model to\nbe both primed for pruning and regularized for improved robustness. AdaSAP\nimproves the robust accuracy of pruned models on image classification by up to\n+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a\ncorrupted Pascal VOC dataset, over a wide range of compression ratios, pruning\ncriteria, and network architectures, outperforming recent pruning art by large\nmargins.\n","authors":["Anna Bair","Hongxu Yin","Maying Shen","Pavlo Molchanov","Jose Alvarez"],"pdf_url":"https://arxiv.org/pdf/2306.14306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08721v1","updated":"2024-03-13T17:20:25Z","published":"2024-03-13T17:20:25Z","title":"Historical Astronomical Diagrams Decomposition in Geometric Primitives","summary":"  Automatically extracting the geometric content from the hundreds of thousands\nof diagrams drawn in historical manuscripts would enable historians to study\nthe diffusion of astronomical knowledge on a global scale. However,\nstate-of-the-art vectorization methods, often designed to tackle modern data,\nare not adapted to the complexity and diversity of historical astronomical\ndiagrams. Our contribution is thus twofold. First, we introduce a unique\ndataset of 303 astronomical diagrams from diverse traditions, ranging from the\nXIIth to the XVIIIth century, annotated with more than 3000 line segments,\ncircles and arcs. Second, we develop a model that builds on DINO-DETR to enable\nthe prediction of multiple geometric primitives. We show that it can be trained\nsolely on synthetic data and accurately predict primitives on our challenging\ndataset. Our approach widely improves over the LETR baseline, which is\nrestricted to lines, by introducing a meaningful parametrization for multiple\nprimitives, jointly training for detection and parameter refinement, using\ndeformable attention and training on rich synthetic data. Our dataset and code\nare available on our webpage.\n","authors":["Syrine Kalleli","Scott Trigg","Ségolène Albouy","Mathieu Husson","Mathieu Aubry"],"pdf_url":"https://arxiv.org/pdf/2403.08721v1.pdf","comment":"Code and dataset are available in\n  http://imagine.enpc.fr/~kallelis/icdar2024/"},{"id":"http://arxiv.org/abs/2403.08700v1","updated":"2024-03-13T17:04:56Z","published":"2024-03-13T17:04:56Z","title":"Diffusion-based Iterative Counterfactual Explanations for Fetal\n  Ultrasound Image Quality Assessment","summary":"  Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, producing high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or the fetus dynamics. In this work, we propose using\ndiffusion-based counterfactual explainable AI to generate realistic\nhigh-quality standard planes from low-quality non-standard ones. Through\nquantitative and qualitative evaluation, we demonstrate the effectiveness of\nour method in producing plausible counterfactuals of increased quality. This\nshows future promise both for enhancing training of clinicians by providing\nvisual feedback, as well as for improving image quality and, consequently,\ndownstream diagnosis and monitoring.\n","authors":["Paraskevas Pegios","Manxi Lin","Nina Weng","Morten Bo Søndergaard Svendsen","Zahra Bashir","Siavash Bigdeli","Anders Nymark Christensen","Martin Tolsgaard","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.08700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03298v2","updated":"2024-03-13T16:59:52Z","published":"2023-12-06T05:39:00Z","title":"DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction","summary":"  Point cloud streaming is increasingly getting popular, evolving into the norm\nfor interactive service delivery and the future Metaverse. However, the\nsubstantial volume of data associated with point clouds presents numerous\nchallenges, particularly in terms of high bandwidth consumption and large\nstorage capacity. Despite various solutions proposed thus far, with a focus on\npoint cloud compression, upsampling, and completion, these\nreconstruction-related methods continue to fall short in delivering high\nfidelity point cloud output. As a solution, in DiffPMAE, we propose an\neffective point cloud reconstruction architecture. Inspired by self-supervised\nlearning concepts, we combine Masked Auto-Encoding and Diffusion Model\nmechanism to remotely reconstruct point cloud data. By the nature of this\nreconstruction process, DiffPMAE can be extended to many related downstream\ntasks including point cloud compression, upsampling and completion. Leveraging\nShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the\nperformance of DiffPMAE exceeding many state-of-the-art methods in-terms of\nauto-encoding and downstream tasks considered.\n","authors":["Yanlong Li","Chamara Madarasingha","Kanchana Thilakarathna"],"pdf_url":"https://arxiv.org/pdf/2312.03298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08695v1","updated":"2024-03-13T16:58:37Z","published":"2024-03-13T16:58:37Z","title":"Deep Learning for In-Orbit Cloud Segmentation and Classification in\n  Hyperspectral Satellite Data","summary":"  This article explores the latest Convolutional Neural Networks (CNNs) for\ncloud detection aboard hyperspectral satellites. The performance of the latest\n1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and\n2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.\nEvaluation criteria include precision and computational efficiency for in-orbit\ndeployment. Experiments utilize NASA's EO-1 Hyperion data, with varying\nspectral channel numbers after Principal Component Analysis. Results indicate\nthat 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs,\nwhile maintaining compactness with larger spectral channel sets, albeit with\nincreased inference times. However, the performance of 1D CNN degrades with\nsignificant channel reduction. In this context, the 2D-Justo-UNet-Simple offers\nthe best balance for in-orbit deployment, considering precision, memory, and\ntime costs. While nnU-net is suitable for on-ground processing, deployment of\nlightweight 1D-Justo-LiuNet is recommended for high-precision applications.\nAlternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced\ncosts between timing and precision in orbit.\n","authors":["Daniel Kovac","Jan Mucha","Jon Alvarez Justo","Jiri Mekyska","Zoltan Galaz","Krystof Novotny","Radoslav Pitonak","Jan Knezik","Jonas Herec","Tor Arne Johansen"],"pdf_url":"https://arxiv.org/pdf/2403.08695v1.pdf","comment":"Hyperspectral Satellite Data, Cloud Segmentation, Classification,\n  Convolutional Neural Networks, Principal Component Analysis"},{"id":"http://arxiv.org/abs/2403.08689v1","updated":"2024-03-13T16:44:49Z","published":"2024-03-13T16:44:49Z","title":"Exploiting Structural Consistency of Chest Anatomy for Unsupervised\n  Anomaly Detection in Radiography Images","summary":"  Radiography imaging protocols focus on particular body regions, therefore\nproducing images of great similarity and yielding recurrent anatomical\nstructures across patients. Exploiting this structured information could\npotentially ease the detection of anomalies from radiography images. To this\nend, we propose a Simple Space-Aware Memory Matrix for In-painting and\nDetecting anomalies from radiography images (abbreviated as SimSID). We\nformulate anomaly detection as an image reconstruction task, consisting of a\nspace-aware memory matrix and an in-painting block in the feature space. During\nthe training, SimSID can taxonomize the ingrained anatomical structures into\nrecurrent visual patterns, and in the inference, it can identify anomalies\n(unseen/modified visual patterns) from the test image. Our SimSID surpasses the\nstate of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9%\nAUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively.\nCode: https://github.com/MrGiovanni/SimSID\n","authors":["Tiange Xiang","Yixiao Zhang","Yongyi Lu","Alan Yuille","Chaoyi Zhang","Weidong Cai","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.08689v1.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). arXiv admin note: substantial text overlap with arXiv:2111.13495"},{"id":"http://arxiv.org/abs/2403.08682v1","updated":"2024-03-13T16:38:26Z","published":"2024-03-13T16:38:26Z","title":"OneVOS: Unifying Video Object Segmentation with All-in-One Transformer\n  Framework","summary":"  Contemporary Video Object Segmentation (VOS) approaches typically consist\nstages of feature extraction, matching, memory management, and multiple objects\naggregation. Recent advanced models either employ a discrete modeling for these\ncomponents in a sequential manner, or optimize a combined pipeline through\nsubstructure aggregation. However, these existing explicit staged approaches\nprevent the VOS framework from being optimized as a unified whole, leading to\nthe limited capacity and suboptimal performance in tackling complex videos. In\nthis paper, we propose OneVOS, a novel framework that unifies the core\ncomponents of VOS with All-in-One Transformer. Specifically, to unify all\naforementioned modules into a vision transformer, we model all the features of\nframes, masks and memory for multiple objects as transformer tokens, and\nintegrally accomplish feature extraction, matching and memory management of\nmultiple objects through the flexible attention mechanism. Furthermore, a\nUnidirectional Hybrid Attention is proposed through a double decoupling of the\noriginal attention operation, to rectify semantic errors and ambiguities of\nstored tokens in OneVOS framework. Finally, to alleviate the storage burden and\nexpedite inference, we propose the Dynamic Token Selector, which unveils the\nworking mechanism of OneVOS and naturally leads to a more efficient version of\nOneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving\nstate-of-the-art performance across 7 datasets, particularly excelling in\ncomplex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing\nprevious state-of-the-art methods by 4.2% and 7.0%, respectively. And our code\nwill be available for reproducibility and further research.\n","authors":["Wanyun Li","Pinxue Guo","Xinyu Zhou","Lingyi Hong","Yangji He","Xiangyu Zheng","Wei Zhang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08682v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.11598v2","updated":"2024-03-13T16:29:50Z","published":"2023-12-18T18:16:52Z","title":"SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution","summary":"  Diffusion models have demonstrated strong potential for robotic trajectory\nplanning. However, generating coherent trajectories from high-level\ninstructions remains challenging, especially for long-range composition tasks\nrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-end\nhierarchical planning framework integrating interpretable skill learning with\nconditional diffusion planning to address this problem. At the higher level,\nthe skill abstraction module learns discrete, human-understandable skill\nrepresentations from visual observations and language instructions. These\nlearned skill embeddings are then used to condition the diffusion model to\ngenerate customized latent trajectories aligned with the skills. This allows\ngenerating diverse state trajectories that adhere to the learnable skills. By\nintegrating skill learning with conditional trajectory generation,\nSkillDiffuser produces coherent behavior following abstract instructions across\ndiverse tasks. Experiments on multi-task robotic manipulation benchmarks like\nMeta-World and LOReL demonstrate state-of-the-art performance and\nhuman-interpretable skill representations from SkillDiffuser. More\nvisualization results and information could be found on our website.\n","authors":["Zhixuan Liang","Yao Mu","Hengbo Ma","Masayoshi Tomizuka","Mingyu Ding","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2312.11598v2.pdf","comment":"Accepted by CVPR 2024. Camera ready version. Project page:\n  https://skilldiffuser.github.io/"},{"id":"http://arxiv.org/abs/2403.08651v1","updated":"2024-03-13T16:06:07Z","published":"2024-03-13T16:06:07Z","title":"HAIFIT: Human-Centered AI for Fashion Image Translation","summary":"  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications.\n","authors":["Jianan Jiang","Xinglin Li","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08651v1.pdf","comment":"7 pages,8 figures"},{"id":"http://arxiv.org/abs/2403.08650v1","updated":"2024-03-13T16:05:18Z","published":"2024-03-13T16:05:18Z","title":"Data Augmentation in Human-Centric Vision","summary":"  This survey presents a comprehensive analysis of data augmentation techniques\nin human-centric vision tasks, a first of its kind in the field. It delves into\na wide range of research areas including person ReID, human parsing, human pose\nestimation, and pedestrian detection, addressing the significant challenges\nposed by overfitting and limited training data in these domains. Our work\ncategorizes data augmentation methods into two main types: data generation and\ndata perturbation. Data generation covers techniques like graphic engine-based\ngeneration, generative model-based generation, and data recombination, while\ndata perturbation is divided into image-level and human-level perturbations.\nEach method is tailored to the unique requirements of human-centric tasks, with\nsome applicable across multiple areas. Our contributions include an extensive\nliterature review, providing deep insights into the influence of these\naugmentation techniques in human-centric vision and highlighting the nuances of\neach method. We also discuss open issues and future directions, such as the\nintegration of advanced generative models like Latent Diffusion Models, for\ncreating more realistic and diverse training data. This survey not only\nencapsulates the current state of data augmentation in human-centric vision but\nalso charts a course for future research, aiming to develop more robust,\naccurate, and efficient human-centric vision systems.\n","authors":["Wentao Jiang","Yige Zhang","Shaozhong Zheng","Si Liu","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2403.08650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08649v1","updated":"2024-03-13T16:04:29Z","published":"2024-03-13T16:04:29Z","title":"A Causal Inspired Early-Branching Structure for Domain Generalization","summary":"  Learning domain-invariant semantic representations is crucial for achieving\ndomain generalization (DG), where a model is required to perform well on unseen\ntarget domains. One critical challenge is that standard training often results\nin entangled semantic and domain-specific features. Previous works suggest\nformulating the problem from a causal perspective and solving the entanglement\nproblem by enforcing marginal independence between the causal (\\ie semantic)\nand non-causal (\\ie domain-specific) features. Despite its simplicity, the\nbasic marginal independent-based idea alone may be insufficient to identify the\ncausal feature. By d-separation, we observe that the causal feature can be\nfurther characterized by being independent of the domain conditioned on the\nobject, and we propose the following two strategies as complements for the\nbasic framework.\n  First, the observation implicitly implies that for the same object, the\ncausal feature should not be associated with the non-causal feature, revealing\nthat the common practice of obtaining the two features with a shared base\nfeature extractor and two lightweight prediction heads might be inappropriate.\nTo meet the constraint, we propose a simple early-branching structure, where\nthe causal and non-causal feature obtaining branches share the first few blocks\nwhile diverging thereafter, for better structure design; Second, the\nobservation implies that the causal feature remains invariant across different\ndomains for the same object. To this end, we suggest that augmentation should\nbe incorporated into the framework to better characterize the causal feature,\nand we further suggest an effective random domain sampling scheme to fulfill\nthe task. Theoretical and experimental results show that the two strategies are\nbeneficial for the basic marginal independent-based framework. Code is\navailable at \\url{https://github.com/liangchen527/CausEB}.\n","authors":["Liang Chen","Yong Zhang","Yibing Song","Zhen Zhang","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08649v1.pdf","comment":"Accepted by IJCV"},{"id":"http://arxiv.org/abs/2302.08913v4","updated":"2024-03-13T16:04:03Z","published":"2023-02-04T15:55:23Z","title":"Referential communication in heterogeneous communities of pre-trained\n  visual deep networks","summary":"  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of \\textit{referential\ncommunication} in a community of heterogeneous state-of-the-art pre-trained\nvisual networks, showing that they can develop, in a self-supervised way, a\nshared protocol to refer to a target object among a set of candidates. This\nshared protocol can also be used, to some extent, to communicate about\npreviously unseen object categories of different granularity. Moreover, a\nvisual network that was not initially part of an existing community can learn\nthe community's protocol with remarkable ease. Finally, we study, both\nqualitatively and quantitatively, the properties of the emergent protocol,\nproviding some evidence that it is capturing high-level semantic features of\nobjects.\n","authors":["Matéo Mahaut","Francesca Franzon","Roberto Dessì","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2302.08913v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08640v1","updated":"2024-03-13T15:52:20Z","published":"2024-03-13T15:52:20Z","title":"Refractive COLMAP: Refractive Structure-from-Motion Revisited","summary":"  In this paper, we present a complete refractive Structure-from-Motion (RSfM)\nframework for underwater 3D reconstruction using refractive camera setups (for\nboth, flat- and dome-port underwater housings). Despite notable achievements in\nrefractive multi-view geometry over the past decade, a robust, complete and\npublicly available solution for such tasks is not available at present, and\noften practical applications have to resort to approximating refraction effects\nby the intrinsic (distortion) parameters of a pinhole camera model. To fill\nthis gap, we have integrated refraction considerations throughout the entire\nSfM process within the state-of-the-art, open-source SfM framework COLMAP.\nNumerical simulations and reconstruction results on synthetically generated but\nphoto-realistic images with ground truth validate that enabling refraction does\nnot compromise accuracy or robustness as compared to in-air reconstructions.\nFinally, we demonstrate the capability of our approach for large-scale\nrefractive scenarios using a dataset consisting of nearly 6000 images. The\nimplementation is released as open-source at:\nhttps://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.\n","authors":["Mengkun She","Felix Seegräber","David Nakath","Kevin Köser"],"pdf_url":"https://arxiv.org/pdf/2403.08640v1.pdf","comment":"8 pages, 7 figures, the paper is submitted to the 2024 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.08639v1","updated":"2024-03-13T15:51:23Z","published":"2024-03-13T15:51:23Z","title":"HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction","summary":"  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n","authors":["Yi Zhou","Hui Zhang","Jiaqian Yu","Yifan Yang","Sangil Jung","Seung-In Park","ByungIn Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.08639v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08632v1","updated":"2024-03-13T15:46:37Z","published":"2024-03-13T15:46:37Z","title":"A Decade's Battle on Dataset Bias: Are We There Yet?","summary":"  We revisit the \"dataset classification\" experiment suggested by Torralba and\nEfros a decade ago, in the new era with large-scale, diverse, and hopefully\nless biased datasets as well as more capable neural network architectures.\nSurprisingly, we observe that modern neural networks can achieve excellent\naccuracy in classifying which dataset an image is from: e.g., we report 84.7%\naccuracy on held-out validation data for the three-way classification problem\nconsisting of the YFCC, CC, and DataComp datasets. Our further experiments show\nthat such a dataset classifier could learn semantic features that are\ngeneralizable and transferable, which cannot be simply explained by\nmemorization. We hope our discovery will inspire the community to rethink the\nissue involving dataset bias and model capabilities.\n","authors":["Zhuang Liu","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2403.08632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08629v1","updated":"2024-03-13T15:45:04Z","published":"2024-03-13T15:45:04Z","title":"Scaling Up Dynamic Human-Scene Interaction Modeling","summary":"  Confronting the challenges of data scarcity and advanced motion synthesis in\nhuman-scene interaction modeling, we introduce the TRUMANS dataset alongside a\nnovel HSI motion synthesis method. TRUMANS stands as the most comprehensive\nmotion-captured HSI dataset currently available, encompassing over 15 hours of\nhuman interactions across 100 indoor scenes. It intricately captures whole-body\nhuman motions and part-level object dynamics, focusing on the realism of\ncontact. This dataset is further scaled up by transforming physical\nenvironments into exact virtual models and applying extensive augmentations to\nappearance and motion for both humans and objects while maintaining interaction\nfidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model\nthat efficiently generates HSI sequences of any length, taking into account\nboth scene context and intended actions. In experiments, our approach shows\nremarkable zero-shot generalizability on a range of 3D scene datasets (e.g.,\nPROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic\noriginal motion-captured sequences, as confirmed by quantitative experiments\nand human studies.\n","authors":["Nan Jiang","Zhiyuan Zhang","Hongjie Li","Xiaoxuan Ma","Zan Wang","Yixin Chen","Tengyu Liu","Yixin Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.08629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09481v2","updated":"2024-03-13T15:24:19Z","published":"2023-12-15T01:38:26Z","title":"Continual Adversarial Defense","summary":"  In response to the rapidly evolving nature of adversarial attacks against\nvisual classifiers on a monthly basis, numerous defenses have been proposed to\ngeneralize against as many known attacks as possible. However, designing a\ndefense method that generalizes to all types of attacks is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks that emerge as time goes on. The defense\nsystem must gather online few-shot defense feedback to promptly enhance itself,\nleveraging efficient memory utilization. Therefore, we propose the first\ncontinual adversarial defense (CAD) framework that adapts to any attacks in a\ndynamic scenario, where various attacks emerge stage by stage. In practice, CAD\nis modeled under four principles: (1) continual adaptation to new attacks\nwithout catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient\nadaptation, and (4) high accuracy on both clean and adversarial images. We\nexplore and integrate cutting-edge continual learning, few-shot learning, and\nensemble learning techniques to qualify the principles. Experiments conducted\non CIFAR-10 and ImageNet-100 validate the effectiveness of our approach against\nmultiple stages of modern adversarial attacks and demonstrate significant\nimprovements over numerous baseline methods. In particular, CAD is capable of\nquickly adapting with minimal feedback and a low cost of defense failure, while\nmaintaining good performance against previous attacks. Our research sheds light\non a brand-new paradigm for continual defense adaptation against dynamic and\nevolving attacks.\n","authors":["Qian Wang","Yaoyao Liu","Hefei Ling","Yingwei Li","Qihao Liu","Ping Li","Jiazhong Chen","Alan Yuille","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2312.09481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08591v1","updated":"2024-03-13T14:54:04Z","published":"2024-03-13T14:54:04Z","title":"ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning\n  in Instructional Videos","summary":"  We present ActionDiffusion -- a novel diffusion model for procedure planning\nin instructional videos that is the first to take temporal inter-dependencies\nbetween actions into account in a diffusion model for procedure planning. This\napproach is in stark contrast to existing methods that fail to exploit the rich\ninformation content available in the particular order in which actions are\nperformed. Our method unifies the learning of temporal dependencies between\nactions and denoising of the action plan in the diffusion process by projecting\nthe action information into the noise space. This is achieved 1) by adding\naction embeddings in the noise masks in the noise-adding phase and 2) by\nintroducing an attention mechanism in the noise prediction network to learn the\ncorrelations between different action steps. We report extensive experiments on\nthree instructional video benchmark datasets (CrossTask, Coin, and NIV) and\nshow that our method outperforms previous state-of-the-art methods on all\nmetrics on CrossTask and NIV and all metrics except accuracy on Coin dataset.\nWe show that by adding action embeddings into the noise mask the diffusion\nmodel can better learn action temporal dependencies and increase the\nperformances on procedure planning.\n","authors":["Lei Shi","Paul Bürkner","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.08591v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.08586v1","updated":"2024-03-13T14:42:55Z","published":"2024-03-13T14:42:55Z","title":"PRAGO: Differentiable Multi-View Pose Optimization From Objectness\n  Detections","summary":"  Robustly estimating camera poses from a set of images is a fundamental task\nwhich remains challenging for differentiable methods, especially in the case of\nsmall and sparse camera pose graphs. To overcome this challenge, we propose\nPose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of\nobjectness detections on unordered images, our method reconstructs the\nrotational pose, and in turn, the absolute pose, in a differentiable manner\nbenefiting from the optimization of a sequence of geometrical tasks. We show\nhow our objectness pose-refinement module in PRAGO is able to refine the\ninherent ambiguities in pairwise relative pose estimation without removing\nedges and avoiding making early decisions on the viability of graph edges.\nPRAGO then refines the absolute rotations through iterative graph construction,\nreweighting the graph edges to compute the final rotational pose, which can be\nconverted into absolute poses using translation averaging. We show that PRAGO\nis able to outperform non-differentiable solvers on small and sparse scenes\nextracted from 7-Scenes achieving a relative improvement of 21% for rotations\nwhile achieving similar translation estimates.\n","authors":["Matteo Taiana","Matteo Toso","Stuart James","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2403.08586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08580v1","updated":"2024-03-13T14:35:13Z","published":"2024-03-13T14:35:13Z","title":"Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification","summary":"  Classifying videos into distinct categories, such as Sport and Music Video,\nis crucial for multimedia understanding and retrieval, especially when an\nimmense volume of video content is being constantly generated. Traditional\nmethods require video decompression to extract pixel-level features like color,\ntexture, and motion, thereby increasing computational and storage demands.\nMoreover, these methods often suffer from performance degradation in\nlow-quality videos. We present a novel approach that examines only the\npost-compression bitstream of a video to perform classification, eliminating\nthe need for bitstream decoding. To validate our approach, we built a\ncomprehensive data set comprising over 29,000 YouTube video clips, totaling\n6,000 hours and spanning 11 distinct categories. Our evaluations indicate\nprecision, accuracy, and recall rates consistently above 80%, many exceeding\n90%, and some reaching 99%. The algorithm operates approximately 15,000 times\nfaster than real-time for 30fps videos, outperforming traditional Dynamic Time\nWarping (DTW) algorithm by seven orders of magnitude.\n","authors":["Yuxing Han","Yunan Ding","Chen Ye Gan","Jiangtao Wen"],"pdf_url":"https://arxiv.org/pdf/2403.08580v1.pdf","comment":"5 pages, 5 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:2309.07361"},{"id":"http://arxiv.org/abs/2309.15048v4","updated":"2024-03-13T14:24:28Z","published":"2023-09-26T16:25:57Z","title":"Class Incremental Learning via Likelihood Ratio Based Task Prediction","summary":"  Class incremental learning (CIL) is a challenging setting of continual\nlearning, which learns a series of tasks sequentially. Each task consists of a\nset of unique classes. The key feature of CIL is that no task identifier (or\ntask-id) is provided at test time. Predicting the task-id for each test sample\nis a challenging problem. An emerging theory-guided approach (called TIL+OOD)\nis to train a task-specific model for each task in a shared network for all\ntasks based on a task-incremental learning (TIL) method to deal with\ncatastrophic forgetting. The model for each task is an out-of-distribution\n(OOD) detector rather than a conventional classifier. The OOD detector can\nperform both within-task (in-distribution (IND)) class prediction and OOD\ndetection. The OOD detection capability is the key to task-id prediction during\ninference. However, this paper argues that using a traditional OOD detector for\ntask-id prediction is sub-optimal because additional information (e.g., the\nreplay data and the learned tasks) available in CIL can be exploited to design\na better and principled method for task-id prediction. We call the new method\nTPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms\nstrong CIL baselines and has negligible catastrophic forgetting. The code of\nTPL is publicly available at https://github.com/linhaowei1/TPL.\n","authors":["Haowei Lin","Yijia Shao","Weinan Qian","Ningxin Pan","Yiduo Guo","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2309.15048v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08568v1","updated":"2024-03-13T14:24:09Z","published":"2024-03-13T14:24:09Z","title":"Consistent Prompting for Rehearsal-Free Continual Learning","summary":"  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n","authors":["Zhanxin Gao","Jun Cen","Xiaobin Chang"],"pdf_url":"https://arxiv.org/pdf/2403.08568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08566v1","updated":"2024-03-13T14:22:13Z","published":"2024-03-13T14:22:13Z","title":"A Novel Implicit Neural Representation for Volume Data","summary":"  The storage of medical images is one of the challenges in the medical imaging\nfield. There are variable works that use implicit neural representation (INR)\nto compress volumetric medical images. However, there is room to improve the\ncompression rate for volumetric medical images. Most of the INR techniques need\na huge amount of GPU memory and a long training time for high-quality medical\nvolume rendering. In this paper, we present a novel implicit neural\nrepresentation to compress volume data using our proposed architecture, that\nis, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet\nhigh-resolution scheme. Our architecture can effectively reduce training time,\nand gain a high compression rate while retaining the final rendering quality.\nMoreover, it can save GPU memory in comparison with the existing works. The\nexperiments show that the quality of reconstructed images and training speed\nusing our architecture is higher than current works which use the SIREN only.\nBesides, the GPU memory cost is evidently decreased\n","authors":["Armin Sheibanifard","Hongchuan Yu"],"pdf_url":"https://arxiv.org/pdf/2403.08566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02649v2","updated":"2024-03-13T14:20:40Z","published":"2023-04-03T20:19:56Z","title":"Medical Multimodal-Multitask Foundation Model for Superior Chest CT\n  Performance","summary":"  Patient management requires multitasking interaction with multimodal data.\nWhile today's AI, particularly large foundation models, promises unprecedented\nopportunities, progress remains relatively slow in developing medical\nmultimodal multitask foundation models. There are two main challenges along\nthis direction: the data challenge -- the high bar to curate medical multimodal\nmultitask datasets including 3D medical tomographic images in alignment with\nother clinical datasets, and the model challenge -- the unavailability of a\nscalable and adaptable foundation model architecture to synergize multimodal\ndatasets for diverse clinical tasks. Here we propose the first-of-its-kind\nmedical multimodal-multitask foundation model (M3FM) with an emphasis on lung\ncancer screening. To train our M3FM, we first curated a comprehensive\nmultimodal multitask dataset consisting of 163,725 3D chest CT exams, 48\nclinical data types, and 17 medical tasks on lung, heart, and other chest\ndiseases. Then, we created and applied a multimodal question-answering\nframework as a unified training strategy to effectively integrate multimodal\ninformation and naturally perform multiple tasks with free-text prompting.\nExtensive experimental results demonstrate that M3FM consistently outperforms\nthe previous state-of-the-art models. M3FM can identify informative multimodal\ndata elements that are relevant to specific clinical tasks, being instrumental\nin building AI models and gaining insights into correlations among multimodal\ndata and diseases. M3FM can be adapted to boost the performance of new tasks\nwith a small out-of-distribution dataset. M3FM has enabled superior volumetric\nCT imaging performance for lung cancer screening, cardiac disease prediction,\nand other CT-related tasks. M3FM can be extended to incorporate more data types\nand improve other medical tasks, towards AI-empowered precise and efficient\nmedicine.\n","authors":["Chuang Niu","Qing Lyu","Christopher D. Carothers","Parisa Kaviani","Josh Tan","Pingkun Yan","Mannudeep K. Kalra","Christopher T. Whitlow","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2304.02649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18920v4","updated":"2024-03-13T14:13:04Z","published":"2024-02-29T07:26:23Z","title":"Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation","summary":"  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n","authors":["Dongliang Cao","Marvin Eisenberger","Nafie El Amrani","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2402.18920v4.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.08557v1","updated":"2024-03-13T14:08:45Z","published":"2024-03-13T14:08:45Z","title":"Occluded Cloth-Changing Person Re-Identification","summary":"  Cloth-changing person re-identification aims to retrieve and identify\nspe-cific pedestrians by using cloth-irrelevant features in person\ncloth-changing scenarios. However, pedestrian images captured by surveillance\nprobes usually contain occlusions in real-world scenarios. The perfor-mance of\nexisting cloth-changing re-identification methods is significantly degraded due\nto the reduction of discriminative cloth-irrelevant features caused by\nocclusion. We define cloth-changing person re-identification in occlusion\nscenarios as occluded cloth-changing person re-identification (Occ-CC-ReID),\nand to the best of our knowledge, we are the first to pro-pose occluded\ncloth-changing person re-identification as a new task. We constructed two\noccluded cloth-changing person re-identification datasets for different\nocclusion scenarios: Occluded-PRCC and Occluded-LTCC. The datasets can be\nobtained from the following link:\nhttps://github.com/1024AILab/Occluded-Cloth-Changing-Person- Re-Identification.\n","authors":["Zhihao Chen","Yiyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2403.08557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08556v1","updated":"2024-03-13T14:08:25Z","published":"2024-03-13T14:08:25Z","title":"SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple\n  Cameras and Scenes by One Model","summary":"  The generalization of monocular metric depth estimation (MMDE) has been a\nlongstanding challenge. Recent methods made progress by combining relative and\nmetric depth or aligning input image focal length. However, they are still\nbeset by challenges in camera, scene, and data levels: (1) Sensitivity to\ndifferent cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on\nmassive training data. This paper proposes SM4Depth, a seamless MMDE method, to\naddress all the issues above within a single network. First, we reveal that a\nconsistent field of view (FOV) is the key to resolve ``metric ambiguity''\nacross cameras, which guides us to propose a more straightforward preprocessing\nunit. Second, to achieve consistently high accuracy across scenes, we\nexplicitly model the metric scale determination as discretizing the depth\ninterval into bins and propose variation-based unnormalized depth bins. This\nmethod bridges the depth gap of diverse scenes by reducing the ambiguity of the\nconventional metric bin. Third, to reduce the reliance on massive training\ndata, we propose a ``divide and conquer\" solution. Instead of estimating\ndirectly from the vast solution space, the correct metric bins are estimated\nfrom multiple solution sub-spaces for complexity reduction. Finally, with just\n150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves\nstate-of-the-art performance on most previously unseen datasets, especially\nsurpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at\nhttps://github.com/1hao-Liu/SM4Depth.\n","authors":["Yihao Liu","Feng Xue","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2403.08556v1.pdf","comment":"Project Page: xuefeng-cvr.github.io/SM4Depth"},{"id":"http://arxiv.org/abs/2403.08551v1","updated":"2024-03-13T14:02:54Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 1000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08550v1","updated":"2024-03-13T14:02:42Z","published":"2024-03-13T14:02:42Z","title":"CINA: Conditional Implicit Neural Atlas for Spatio-Temporal\n  Representation of Fetal Brains","summary":"  We introduce a conditional implicit neural atlas (CINA) for spatio-temporal\natlas generation from Magnetic Resonance Images (MRI) of the neurotypical and\npathological fetal brain, that is fully independent of affine or non-rigid\nregistration. During training, CINA learns a general representation of the\nfetal brain and encodes subject specific information into latent code. After\ntraining, CINA can construct a faithful atlas with tissue probability maps of\nthe fetal brain for any gestational age (GA) and anatomical variation covered\nwithin the training domain. Thus, CINA is competent to represent both,\nneurotypical and pathological brains. Furthermore, a trained CINA model can be\nfit to brain MRI of unseen subjects via test-time optimization of the latent\ncode. CINA can then produce probabilistic tissue maps tailored to a particular\nsubject. We evaluate our method on a total of 198 T2 weighted MRI of normal and\nabnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's\ncapability to represent a fetal brain atlas that can be flexibly conditioned on\nGA and on anatomical variations like ventricular volume or degree of cortical\nfolding, making it a suitable tool for modeling both neurotypical and\npathological brains. We quantify the fidelity of our atlas by means of tissue\nsegmentation and age prediction and compare it to an established baseline. CINA\ndemonstrates superior accuracy for neurotypical brains and pathological brains\nwith ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23\nweeks in fetal brain age prediction, further confirming an accurate\nrepresentation of fetal brain development.\n","authors":["Maik Dannecker","Vanessa Kyriakopoulou","Lucilio Cordero-Grande","Anthony N. Price","Joseph V. Hajnal","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2403.08550v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.08542v1","updated":"2024-03-13T13:56:34Z","published":"2024-03-13T13:56:34Z","title":"AIGCs Confuse AI Too: Investigating and Explaining Synthetic\n  Image-induced Hallucinations in Large Vision-Language Models","summary":"  The evolution of Artificial Intelligence Generated Contents (AIGCs) is\nadvancing towards higher quality. The growing interactions with AIGCs present a\nnew challenge to the data-driven AI community: While AI-generated contents have\nplayed a crucial role in a wide range of AI models, the potential hidden risks\nthey introduce have not been thoroughly examined. Beyond human-oriented forgery\ndetection, AI-generated content poses potential issues for AI models originally\ndesigned to process natural data. In this study, we underscore the exacerbated\nhallucination phenomena in Large Vision-Language Models (LVLMs) caused by\nAI-synthetic images. Remarkably, our findings shed light on a consistent AIGC\n\\textbf{hallucination bias}: the object hallucinations induced by synthetic\nimages are characterized by a greater quantity and a more uniform position\ndistribution, even these synthetic images do not manifest unrealistic or\nadditional relevant visual features compared to natural images. Moreover, our\ninvestigations on Q-former and Linear projector reveal that synthetic images\nmay present token deviations after visual projection, thereby amplifying the\nhallucination bias.\n","authors":["Yifei Gao","Jiaqi Wang","Zhiyu Lin","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2403.08542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08536v1","updated":"2024-03-13T13:51:02Z","published":"2024-03-13T13:51:02Z","title":"HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional\n  Image Classifiers","summary":"  Convolutional Neural Networks (CNNs) are nowadays the model of choice in\nComputer Vision, thanks to their ability to automatize the feature extraction\nprocess in visual tasks. However, the knowledge acquired during training is\nfully subsymbolic, and hence difficult to understand and explain to end users.\nIn this paper, we propose a new technique called HOLMES (HOLonym-MEronym based\nSemantic inspection) that decomposes a label into a set of related concepts,\nand provides component-level explanations for an image classification model.\nSpecifically, HOLMES leverages ontologies, web scraping and transfer learning\nto automatically construct meronym (parts)-based detectors for a given holonym\n(class). Then, it produces heatmaps at the meronym level and finally, by\nprobing the holonym CNN with occluded images, it highlights the importance of\neach part on the classification output. Compared to state-of-the-art saliency\nmethods, HOLMES takes a step further and provides information about both where\nand what the holonym CNN is looking at, without relying on densely annotated\ndatasets and without forcing concepts to be associated to single computational\nunits. Extensive experimental evaluation on different categories of objects\n(animals, tools and vehicles) shows the feasibility of our approach. On\naverage, HOLMES explanations include at least two meronyms, and the ablation of\na single meronym roughly halves the holonym model confidence. The resulting\nheatmaps were quantitatively evaluated using the\ndeletion/insertion/preservation curves. All metrics were comparable to those\nachieved by GradCAM, while offering the advantage of further decomposing the\nheatmap in human-understandable concepts, thus highlighting both the relevance\nof meronyms to object classification, as well as HOLMES ability to capture it.\nThe code is available at https://github.com/FrancesC0de/HOLMES.\n","authors":["Francesco Dibitonto","Fabio Garcea","André Panisson","Alan Perotti","Lia Morra"],"pdf_url":"https://arxiv.org/pdf/2403.08536v1.pdf","comment":"This work has been accepted to be presented to The 1st World\n  Conference on eXplainable Artificial Intelligence (xAI 2023), July 26-28,\n  2023 - Lisboa, Portugal"},{"id":"http://arxiv.org/abs/2311.11646v2","updated":"2024-03-13T13:42:38Z","published":"2023-11-20T10:26:04Z","title":"Toward Open Vocabulary Aerial Object Detection with CLIP-Activated\n  Student-Teacher Learning","summary":"  An increasingly massive number of remote-sensing images spurs the development\nof extensible object detectors that can detect objects beyond training\ncategories without costly collecting new labeled data. In this paper, we aim to\ndevelop open-vocabulary object detection (OVD) technique in aerial images that\nscales up object vocabulary size beyond training data. The fundamental\nchallenges hinder open vocabulary object detection performance: the qualities\nof the class-agnostic region proposals and the pseudo-labels that can\ngeneralize well to novel object categories. To simultaneously generate\nhigh-quality proposals and pseudo-labels, we propose CastDet, a CLIP-activated\nstudent-teacher open-vocabulary object Detection framework. Our end-to-end\nframework following the student-teacher self-learning mechanism employs the\nRemoteCLIP model as an extra omniscient teacher with rich knowledge. By doing\nso, our approach boosts not only novel object proposals but also\nclassification. Furthermore, we devise a dynamic label queue strategy to\nmaintain high-quality pseudo labels during batch training. We conduct extensive\nexperiments on multiple existing aerial object detection datasets, which are\nset up for the OVD task. Experimental results demonstrate our CastDet achieving\nsuperior open-vocabulary detection performance, e.g., reaching 40.5\\% mAP,\nwhich outperforms previous methods Detic/ViLD by 23.7%/14.9% on the VisDroneZSD\ndataset. To our best knowledge, this is the first work to apply and develop the\nopen-vocabulary object detection technique for aerial images.\n","authors":["Yan Li","Weiwei Guo","Xue Yang","Ning Liao","Dunyun He","Jiaqi Zhou","Wenxian Yu"],"pdf_url":"https://arxiv.org/pdf/2311.11646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04780v7","updated":"2024-03-13T13:39:47Z","published":"2023-10-07T11:45:33Z","title":"IPMix: Label-Preserving Data Augmentation Method for Training Robust\n  Classifiers","summary":"  Data augmentation has been proven effective for training high-accuracy\nconvolutional neural network classifiers by preventing overfitting. However,\nbuilding deep neural networks in real-world scenarios requires not only high\naccuracy on clean data but also robustness when data distributions shift. While\nprior methods have proposed that there is a trade-off between accuracy and\nrobustness, we propose IPMix, a simple data augmentation approach to improve\nrobustness without hurting clean accuracy. IPMix integrates three levels of\ndata augmentation (image-level, patch-level, and pixel-level) into a coherent\nand label-preserving technique to increase the diversity of training data with\nlimited computational overhead. To further improve the robustness, IPMix\nintroduces structural complexity at different levels to generate more diverse\nimages and adopts the random mixing method for multi-scale information fusion.\nExperiments demonstrate that IPMix outperforms state-of-the-art corruption\nrobustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also\nsignificantly improves the other safety measures, including robustness to\nadversarial perturbations, calibration, prediction consistency, and anomaly\ndetection, achieving state-of-the-art or comparable results on several\nbenchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.\n","authors":["Zhenglin Huang","Xiaoan Bao","Na Zhang","Qingqi Zhang","Xiaomei Tu","Biao Wu","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04780v7.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.08528v1","updated":"2024-03-13T13:38:58Z","published":"2024-03-13T13:38:58Z","title":"Pig aggression classification using CNN, Transformers and Recurrent\n  Networks","summary":"  The development of techniques that can be used to analyze and detect animal\nbehavior is a crucial activity for the livestock sector, as it is possible to\nmonitor the stress and animal welfare and contributes to decision making in the\nfarm. Thus, the development of applications can assist breeders in making\ndecisions to improve production performance and reduce costs, once the animal\nbehavior is analyzed by humans and this can lead to susceptible errors and time\nconsumption. Aggressiveness in pigs is an example of behavior that is studied\nto reduce its impact through animal classification and identification. However,\nthis process is laborious and susceptible to errors, which can be reduced\nthrough automation by visually classifying videos captured in controlled\nenvironment. The captured videos can be used for training and, as a result, for\nclassification through computer vision and artificial intelligence, employing\nneural network techniques. The main techniques utilized in this study are\nvariants of transformers: STAM, TimeSformer, and ViViT, as well as techniques\nusing convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These\ntechniques were employed for pig video classification with the objective of\nidentifying aggressive and non-aggressive behaviors. In this work, various\ntechniques were compared to analyze the contribution of using transformers, in\naddition to the effectiveness of the convolution technique in video\nclassification. The performance was evaluated using accuracy, precision, and\nrecall. The TimerSformer technique showed the best results in video\nclassification, with median accuracy of 0.729.\n","authors":["Junior Silva Souza","Eduardo Bedin","Gabriel Toshio Hirokawa Higa","Newton Loebens","Hemerson Pistori"],"pdf_url":"https://arxiv.org/pdf/2403.08528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08512v1","updated":"2024-03-13T13:23:05Z","published":"2024-03-13T13:23:05Z","title":"UniLiDAR: Bridge the domain gap among different LiDARs for continual\n  learning","summary":"  LiDAR-based 3D perception algorithms have evolved rapidly alongside the\nemergence of large datasets. Nonetheless, considerable performance degradation\noften ensues when models trained on a specific dataset are applied to other\ndatasets or real-world scenarios with different LiDAR. This paper aims to\ndevelop a unified model capable of handling different LiDARs, enabling\ncontinual learning across diverse LiDAR datasets and seamless deployment across\nheterogeneous platforms. We observe that the gaps among datasets primarily\nmanifest in geometric disparities (such as variations in beams and point\ncounts) and semantic inconsistencies (taxonomy conflicts). To this end, this\npaper proposes UniLiDAR, an occupancy prediction pipeline that leverages\ngeometric realignment and semantic label mapping to facilitate multiple\ndatasets training and mitigate performance degradation during deployment on\nheterogeneous platforms. Moreover, our method can be easily combined with\nexisting 3D perception models. The efficacy of the proposed approach in\nbridging LiDAR domain gaps is verified by comprehensive experiments on two\nprominent datasets: OpenOccupancy-nuScenes and SemanticKITTI. UniLiDAR elevates\nthe mIoU of occupancy prediction by 15.7% and 12.5%, respectively, compared to\nthe model trained on the directly merged dataset. Moreover, it outperforms\nseveral SOTA methods trained on individual datasets. We expect our research to\nfacilitate further study of 3D generalization, the code will be available soon.\n","authors":["Zikun Xu","Jianqiang Wang","Shaobing Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08511v1","updated":"2024-03-13T13:16:26Z","published":"2024-03-13T13:16:26Z","title":"A Multimodal Fusion Network For Student Emotion Recognition Based on\n  Transformer and Tensor Product","summary":"  In recent years, there have been frequent incidents of foreign objects\nintruding into railway and Airport runways. These objects can include\npedestrians, vehicles, animals, and debris. This paper introduces an improved\nYOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance\nthe detection of foreign objects on railways and Airport runways. This study\nproposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which\ncombines two public datasets for detecting foreign objects in aviation and\nrailway systems. The dataset aims to improve the recognition capabilities of\nforeign object targets. Experimental results on this large dataset have\ndemonstrated significant performance improvements of the proposed model over\nthe baseline YOLOv5 model, reducing computational requirements. improved YOLO\nmodel shows a significant improvement in precision by 1.2%, recall rate by\n1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters\nwere reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%.\nIn the ablation experiment, it is found that the FasterNet module can\nsignificantly reduce the number of parameters of the model, and the reference\nof the attention mechanism can slow down the performance loss caused by\nlightweight.\n","authors":["Ao Xiang","Zongqing Qi","Han Wang","Qin Yang","Danqing Ma"],"pdf_url":"https://arxiv.org/pdf/2403.08511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01650v2","updated":"2024-03-13T13:13:57Z","published":"2024-01-03T10:07:11Z","title":"De-Confusing Pseudo-Labels in Source-Free Domain Adaptation","summary":"  Source-free domain adaptation (SFDA) aims to adapt a source-trained model to\nan unlabeled target domain without access to the source data. SFDA has\nattracted growing attention in recent years, where existing approaches focus on\nself-training that usually includes pseudo-labeling techniques. In this paper,\nwe introduce a novel noise-learning approach tailored to address noise\ndistribution in domain adaptation settings and learn to de-confuse the\npseudo-labels. More specifically, we learn a noise transition matrix of the\npseudo-labels to capture the label corruption of each class and learn the\nunderlying true label distribution. Estimating the noise transition matrix\nenables a better true class-posterior estimation, resulting in better\nprediction accuracy. We demonstrate the effectiveness of our approach when\ncombined with several SFDA methods: SHOT, SHOT++, and AaD. We obtain\nstate-of-the-art results on three domain adaptation datasets: VisDA, DomainNet,\nand OfficeHome.\n","authors":["Idit Diamant","Amir Rosenfeld","Idan Achituve","Jacob Goldberger","Arnon Netzer"],"pdf_url":"https://arxiv.org/pdf/2401.01650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08505v1","updated":"2024-03-13T13:12:57Z","published":"2024-03-13T13:12:57Z","title":"Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08504v1","updated":"2024-03-13T13:12:42Z","published":"2024-03-13T13:12:42Z","title":"OccFiner: Offboard Occupancy Refinement with Hybrid Propagation","summary":"  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion\n(SSC), presents a significant challenge in computer vision. Previous methods,\nconfined to onboard processing, struggle with simultaneous geometric and\nsemantic estimation, continuity across varying viewpoints, and single-view\nocclusion. Our paper introduces OccFiner, a novel offboard framework designed\nto enhance the accuracy of vision-based occupancy predictions. OccFiner\noperates in two hybrid phases: 1) a multi-to-multi local propagation network\nthat implicitly aligns and processes multiple local frames for correcting\nonboard model errors and consistently enhancing occupancy accuracy across all\ndistances. 2) the region-centric global propagation, focuses on refining labels\nusing explicit multi-view geometry and integrating sensor bias, especially to\nincrease the accuracy of distant occupied voxels. Extensive experiments\ndemonstrate that OccFiner improves both geometric and semantic accuracy across\nvarious types of coarse occupancy, setting a new state-of-the-art performance\non the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC\nmodels to a level even surpassing that of LiDAR-based onboard SSC models.\n","authors":["Hao Shi","Song Wang","Jiaming Zhang","Xiaoting Yin","Zhongdao Wang","Zhijian Zhao","Guangming Wang","Jianke Zhu","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08502v1","updated":"2024-03-13T13:10:20Z","published":"2024-03-13T13:10:20Z","title":"Masked Generative Story Transformer with Character Guidance and Caption\n  Augmentation","summary":"  Story Visualization (SV) is a challenging generative vision task, that\nrequires both visual quality and consistency between different frames in\ngenerated image sequences. Previous approaches either employ some kind of\nmemory mechanism to maintain context throughout an auto-regressive generation\nof the image sequence, or model the generation of the characters and their\nbackground separately, to improve the rendering of characters. On the contrary,\nwe embrace a completely parallel transformer-based approach, exclusively\nrelying on Cross-Attention with past and future captions to achieve\nconsistency. Additionally, we propose a Character Guidance technique to focus\non the generation of characters in an implicit manner, by forming a combination\nof text-conditional and character-conditional logits in the logit space. We\nalso employ a caption-augmentation technique, carried out by a Large Language\nModel (LLM), to enhance the robustness of our approach. The combination of\nthese methods culminates into state-of-the-art (SOTA) results over various\nmetrics in the most prominent SV benchmark (Pororo-SV), attained with\nconstraint resources while achieving superior computational complexity compared\nto previous arts. The validity of our quantitative results is supported by a\nhuman survey.\n","authors":["Christos Papadimitriou","Giorgos Filandrianos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2403.08502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08499v1","updated":"2024-03-13T13:07:14Z","published":"2024-03-13T13:07:14Z","title":"Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign\n  Object Detection on Railway and Airway tracks","summary":"  In recent years, there have been frequent incidents of foreign objects\nintruding into railway and Airport runways. These objects can include\npedestrians, vehicles, animals, and debris. This paper introduces an improved\nYOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance\nthe detection of foreign objects on railways and Airport runways. This study\nproposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which\ncombines two public datasets for detecting foreign objects in aviation and\nrailway systems.The dataset aims to improve the recognition capabilities of\nforeign object targets. Experimental results on this large dataset have\ndemonstrated significant performance improvements of the proposed model over\nthe baseline YOLOv5 model, reducing computational requirements.Improved YOLO\nmodel shows a significant improvement in precision by 1.2%, recall rate by\n1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters\nwere reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%.\nIn the ablation experiment, it is found that the FasterNet module can\nsignificantly reduce the number of parameters of the model, and the reference\nof the attention mechanism can slow down the performance loss caused by\nlightweight.\n","authors":["Zongqing Qi","Danqing Ma","Jingyu Xu","Ao Xiang","Hedi Qu"],"pdf_url":"https://arxiv.org/pdf/2403.08499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08498v1","updated":"2024-03-13T13:06:31Z","published":"2024-03-13T13:06:31Z","title":"Gaussian Splatting in Style","summary":"  Scene stylization extends the work of neural style transfer to three spatial\ndimensions. A vital challenge in this problem is to maintain the uniformity of\nthe stylized appearance across a multi-view setting. A vast majority of the\nprevious works achieve this by optimizing the scene with a specific style\nimage. In contrast, we propose a novel architecture trained on a collection of\nstyle images, that at test time produces high quality stylized novel views. Our\nwork builds up on the framework of 3D Gaussian splatting. For a given scene, we\ntake the pretrained Gaussians and process them using a multi resolution hash\ngrid and a tiny MLP to obtain the conditional stylised views. The explicit\nnature of 3D Gaussians give us inherent advantages over NeRF-based methods\nincluding geometric consistency, along with having a fast training and\nrendering regime. This enables our method to be useful for vast practical use\ncases such as in augmented or virtual reality applications. Through our\nexperiments, we show our methods achieve state-of-the-art performance with\nsuperior visual quality on various indoor and outdoor real-world data.\n","authors":["Abhishek Saroha","Mariia Gladkova","Cecilia Curreli","Tarun Yenamandra","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2403.08498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04416v2","updated":"2024-03-13T12:53:24Z","published":"2023-10-06T17:58:26Z","title":"Alice Benchmarks: Connecting Real World Re-Identification with the\n  Synthetic","summary":"  For object re-identification (re-ID), learning from synthetic data has become\na promising strategy to cheaply acquire large-scale annotated datasets and\neffective models, with few privacy concerns. Many interesting research problems\narise from this strategy, e.g., how to reduce the domain gap between synthetic\nsource and real-world target. To facilitate developing more new approaches in\nlearning from synthetic data, we introduce the Alice benchmarks, large-scale\ndatasets providing benchmarks as well as evaluation protocols to the research\ncommunity. Within the Alice benchmarks, two object re-ID tasks are offered:\nperson and vehicle re-ID. We collected and annotated two challenging real-world\ntarget datasets: AlicePerson and AliceVehicle, captured under various\nilluminations, image resolutions, etc. As an important feature of our real\ntarget, the clusterability of its training set is not manually guaranteed to\nmake it closer to a real domain adaptation test scenario. Correspondingly, we\nreuse existing PersonX and VehicleX as synthetic source domains. The primary\ngoal is to train models from synthetic data that can work effectively in the\nreal world. In this paper, we detail the settings of Alice benchmarks, provide\nan analysis of existing commonly-used domain adaptation methods, and discuss\nsome interesting future directions. An online server has been set up for the\ncommunity to evaluate methods conveniently and fairly. Datasets and the online\nserver details are available at https://sites.google.com/view/alice-benchmarks.\n","authors":["Xiaoxiao Sun","Yue Yao","Shengjin Wang","Hongdong Li","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04416v2.pdf","comment":"ICLR 2024. Datasets and the online server details are available at\n  https://sites.google.com/view/alice-benchmarks"},{"id":"http://arxiv.org/abs/2403.08487v1","updated":"2024-03-13T12:52:37Z","published":"2024-03-13T12:52:37Z","title":"Model Will Tell: Training Membership Inference for Diffusion Models","summary":"  Diffusion models pose risks of privacy breaches and copyright disputes,\nprimarily stemming from the potential utilization of unauthorized data during\nthe training phase. The Training Membership Inference (TMI) task aims to\ndetermine whether a specific sample has been used in the training process of a\ntarget model, representing a critical tool for privacy violation verification.\nHowever, the increased stochasticity inherent in diffusion renders traditional\nshadow-model-based or metric-based methods ineffective when applied to\ndiffusion models. Moreover, existing methods only yield binary classification\nlabels which lack necessary comprehensibility in practical applications. In\nthis paper, we explore a novel perspective for the TMI task by leveraging the\nintrinsic generative priors within the diffusion model. Compared with unseen\nsamples, training samples exhibit stronger generative priors within the\ndiffusion model, enabling the successful reconstruction of substantially\ndegraded training images. Consequently, we propose the Degrade Restore Compare\n(DRC) framework. In this framework, an image undergoes sequential degradation\nand restoration, and its membership is determined by comparing it with the\nrestored counterpart. Experimental results verify that our approach not only\nsignificantly outperforms existing methods in terms of accuracy but also\nprovides comprehensible decision criteria, offering evidence for potential\nprivacy violations.\n","authors":["Xiaomeng Fu","Xi Wang","Qiao Li","Jin Liu","Jiao Dai","Jizhong Han"],"pdf_url":"https://arxiv.org/pdf/2403.08487v1.pdf","comment":"18 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2402.03166v2","updated":"2024-03-13T12:52:26Z","published":"2024-02-05T16:35:29Z","title":"RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein\n  Segmentation and Classification","summary":"  The caliber and configuration of retinal blood vessels serve as important\nbiomarkers for various diseases and medical conditions. A thorough analysis of\nthe retinal vasculature requires the segmentation of the blood vessels and\ntheir classification into arteries and veins, typically performed on color\nfundus images obtained by retinography. However, manually performing these\ntasks is labor-intensive and prone to human error. While several automated\nmethods have been proposed to address this task, the current state of art faces\nchallenges due to manifest classification errors affecting the topological\nconsistency of segmentation maps. In this work, we introduce RRWNet, a novel\nend-to-end deep learning framework that addresses this limitation. The\nframework consists of a fully convolutional neural network that recursively\nrefines semantic segmentation maps, correcting manifest classification errors\nand thus improving topological consistency. In particular, RRWNet is composed\nof two specialized subnetworks: a Base subnetwork that generates base\nsegmentation maps from the input images, and a Recursive Refinement subnetwork\nthat iteratively and recursively improves these maps. Evaluation on three\ndifferent public datasets demonstrates the state-of-the-art performance of the\nproposed method, yielding more topologically consistent segmentation maps with\nfewer manifest classification errors than existing approaches. In addition, the\nRecursive Refinement module within RRWNet proves effective in post-processing\nsegmentation maps from other methods, further demonstrating its potential. The\nmodel code, weights, and predictions will be publicly available at\nhttps://github.com/j-morano/rrwnet.\n","authors":["José Morano","Guilherme Aresta","Hrvoje Bogunović"],"pdf_url":"https://arxiv.org/pdf/2402.03166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04414v3","updated":"2024-03-13T12:49:19Z","published":"2023-10-06T17:58:20Z","title":"CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model\n  Generalization Analysis","summary":"  Analyzing model performance in various unseen environments is a critical\nresearch problem in the machine learning community. To study this problem, it\nis important to construct a testbed with out-of-distribution test sets that\nhave broad coverage of environmental discrepancies. However, existing testbeds\ntypically either have a small number of domains or are synthesized by image\ncorruptions, hindering algorithm design that demonstrates real-world\neffectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of\n180 datasets collected by prompting image search engines and diffusion models\nin various ways. Generally sized between 300 and 8,000 images, the datasets\ncontain natural images, cartoons, certain colors, or objects that do not\nnaturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen\nthe understanding of two generalization tasks: domain generalization and model\naccuracy prediction in various out-of-distribution environments. We conduct\nextensive benchmarking and comparison experiments and show that CIFAR-10-W\noffers new and interesting insights inherent to these tasks. We also discuss\nother fields that would benefit from CIFAR-10-W.\n","authors":["Xiaoxiao Sun","Xingjian Leng","Zijian Wang","Yang Yang","Zi Huang","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04414v3.pdf","comment":"ICLR 2024. https://sites.google.com/view/CIFAR-10-warehouse/"},{"id":"http://arxiv.org/abs/2403.08479v1","updated":"2024-03-13T12:46:36Z","published":"2024-03-13T12:46:36Z","title":"MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose\n  Prediction","summary":"  Radiation therapy is crucial in cancer treatment. Experienced experts\ntypically iteratively generate high-quality dose distribution maps, forming the\nbasis for excellent radiation therapy plans. Therefore, automated prediction of\ndose distribution maps is significant in expediting the treatment process and\nproviding a better starting point for developing radiation therapy plans. With\nthe remarkable results of diffusion models in predicting high-frequency regions\nof dose distribution maps, dose prediction methods based on diffusion models\nhave been extensively studied. However, existing methods mainly utilize CNNs or\nTransformers as denoising networks. CNNs lack the capture of global receptive\nfields, resulting in suboptimal prediction performance. Transformers excel in\nglobal modeling but face quadratic complexity with image size, resulting in\nsignificant computational overhead. To tackle these challenges, we introduce a\nnovel diffusion model, MD-Dose, based on the Mamba architecture for predicting\nradiation therapy dose distribution in thoracic cancer patients. In the forward\nprocess, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure\nnoise images. In the backward process, MD-Dose utilizes a noise predictor based\non the Mamba to predict the noise, ultimately outputting the dose distribution\nmaps. Furthermore, We develop a Mamba encoder to extract structural information\nand integrate it into the noise predictor for localizing dose regions in the\nplanning target volume (PTV) and organs at risk (OARs). Through extensive\nexperiments on a dataset of 300 thoracic tumor patients, we showcase the\nsuperiority of MD-Dose in various metrics and time consumption.\n","authors":["Linjie Fu","Xia Li","Xiuding Cai","Yingkai Wang","Xueyao Wang","Yali Shen","Yu Yao"],"pdf_url":"https://arxiv.org/pdf/2403.08479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08477v1","updated":"2024-03-13T12:46:03Z","published":"2024-03-13T12:46:03Z","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through\n  Sparse Interpolated Experts","summary":"  Conventional wisdom suggests parameter-efficient fine-tuning of foundation\nmodels as the state-of-the-art method for transfer learning in vision,\nreplacing the rich literature of alternatives such as meta-learning. In trying\nto harness the best of both worlds, meta-tuning introduces a subsequent\noptimization stage of foundation models but has so far only shown limited\nsuccess and crucially tends to underperform on out-of-domain (OOD) tasks. In\nthis paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse\nmixture-of-experts approaches and trained to isolate subsets of pre-trained\nparameters automatically for meta-tuning on each task. SMAT successfully\novercomes OOD sensitivity and delivers on the promise of enhancing the transfer\nabilities of vision foundation models beyond parameter-efficient finetuning. We\nestablish new state-of-the-art results on a challenging combination of\nMeta-Dataset augmented with additional OOD tasks in both zero-shot and\ngradient-based adaptation settings. In addition, we provide a thorough analysis\nof the superiority of learned over hand-designed sparsity patterns for sparse\nexpert methods and the pivotal importance of the sparsity level in balancing\nbetween in-domain and out-of-domain generalization. Our code is publicly\navailable.\n","authors":["Shengzhuang Chen","Jihoon Tack","Yunqiao Yang","Yee Whye Teh","Jonathan Richard Schwarz","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2403.08477v1.pdf","comment":"18 pages, preprint"},{"id":"http://arxiv.org/abs/2403.06912v2","updated":"2024-03-13T12:41:45Z","published":"2024-03-11T17:02:11Z","title":"DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with\n  Global-Local Depth Normalization","summary":"  Radiance fields have demonstrated impressive performance in synthesizing\nnovel views from sparse input views, yet prevailing methods suffer from high\ntraining costs and slow inference speed. This paper introduces DNGaussian, a\ndepth-regularized framework based on 3D Gaussian radiance fields, offering\nreal-time and high-quality few-shot novel view synthesis at low costs. Our\nmotivation stems from the highly efficient representation and surprising\nquality of the recent 3D Gaussian Splatting, despite it will encounter a\ngeometry degradation when input views decrease. In the Gaussian radiance\nfields, we find this degradation in scene geometry primarily lined to the\npositioning of Gaussian primitives and can be mitigated by depth constraint.\nConsequently, we propose a Hard and Soft Depth Regularization to restore\naccurate scene geometry under coarse monocular depth supervision while\nmaintaining a fine-grained color appearance. To further refine detailed\ngeometry reshaping, we introduce Global-Local Depth Normalization, enhancing\nthe focus on small local depth changes. Extensive experiments on LLFF, DTU, and\nBlender datasets demonstrate that DNGaussian outperforms state-of-the-art\nmethods, achieving comparable or better results with significantly reduced\nmemory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$\nfaster rendering speed.\n","authors":["Jiahe Li","Jiawei Zhang","Xiao Bai","Jin Zheng","Xin Ning","Jun Zhou","Lin Gu"],"pdf_url":"https://arxiv.org/pdf/2403.06912v2.pdf","comment":"Accepted at CVPR 2024. Project page:\n  https://fictionarry.github.io/DNGaussian/"},{"id":"http://arxiv.org/abs/2403.08464v1","updated":"2024-03-13T12:26:55Z","published":"2024-03-13T12:26:55Z","title":"Diffusion Models with Implicit Guidance for Medical Anomaly Detection","summary":"  Diffusion models have advanced unsupervised anomaly detection by improving\nthe transformation of pathological images into pseudo-healthy equivalents.\nNonetheless, standard approaches may compromise critical information during\npathology removal, leading to restorations that do not align with unaffected\nregions in the original scans. Such discrepancies can inadvertently increase\nfalse positive rates and reduce specificity, complicating radiological\nevaluations. This paper introduces Temporal Harmonization for Optimal\nRestoration (THOR), which refines the de-noising process by integrating\nimplicit guidance through temporal anomaly maps. THOR aims to preserve the\nintegrity of healthy tissue in areas unaffected by pathology. Comparative\nevaluations show that THOR surpasses existing diffusion-based methods in\ndetecting and segmenting anomalies in brain MRIs and wrist X-rays. Code:\nhttps://github.com/ci-ber/THOR_DDPM.\n","authors":["Cosmin I. Bercea","Benedikt Wiestler","Daniel Rueckert","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2403.08464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02684v2","updated":"2024-03-13T12:24:06Z","published":"2023-11-05T15:48:29Z","title":"Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE","summary":"  Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/paper_list/Octavius.\n","authors":["Zeren Chen","Ziqin Wang","Zhen Wang","Huayang Liu","Zhenfei Yin","Si Liu","Lu Sheng","Wanli Ouyang","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2311.02684v2.pdf","comment":"22 pages, 12 figures. Accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08460v1","updated":"2024-03-13T12:20:20Z","published":"2024-03-13T12:20:20Z","title":"Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal\n  Diffusion Model","summary":"  Millimeter wave (mmWave) radars have attracted significant attention from\nboth academia and industry due to their capability to operate in extreme\nweather conditions. However, they face challenges in terms of sparsity and\nnoise interference, which hinder their application in the field of micro aerial\nvehicle (MAV) autonomous navigation. To this end, this paper proposes a novel\napproach to dense and accurate mmWave radar point cloud construction via\ncross-modal learning. Specifically, we introduce diffusion models, which\npossess state-of-the-art performance in generative modeling, to predict\nLiDAR-like point clouds from paired raw radar data. We also incorporate the\nmost recent diffusion model inference accelerating techniques to ensure that\nthe proposed method can be implemented on MAVs with limited computing\nresources.We validate the proposed method through extensive benchmark\ncomparisons and real-world experiments, demonstrating its superior performance\nand generalization ability. Code and pretrained models will be available at\nhttps://github.com/ZJU-FAST-Lab/Radar-Diffusion.\n","authors":["Ruibin Zhang","Donglai Xue","Yuhan Wang","Ruixu Geng","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.08460v1.pdf","comment":"8 pages, 6 figures, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2308.10156v2","updated":"2024-03-13T12:16:20Z","published":"2023-08-20T04:09:12Z","title":"SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form\n  Layout-to-Image Generation","summary":"  Despite significant progress in Text-to-Image (T2I) generative models, even\nlengthy and complex text descriptions still struggle to convey detailed\ncontrols. In contrast, Layout-to-Image (L2I) generation, aiming to generate\nrealistic and complex scene images from user-specified layouts, has risen to\nprominence. However, existing methods transform layout information into tokens\nor RGB images for conditional control in the generative process, leading to\ninsufficient spatial and semantic controllability of individual instances. To\naddress these limitations, we propose a novel Spatial-Semantic Map Guided\n(SSMG) diffusion model that adopts the feature map, derived from the layout, as\nguidance. Owing to rich spatial and semantic information encapsulated in\nwell-designed feature maps, SSMG achieves superior generation quality with\nsufficient spatial and semantic controllability compared to previous works.\nAdditionally, we propose the Relation-Sensitive Attention (RSA) and\nLocation-Sensitive Attention (LSA) mechanisms. The former aims to model the\nrelationships among multiple objects within scenes while the latter is designed\nto heighten the model's sensitivity to the spatial information embedded in the\nguidance. Extensive experiments demonstrate that SSMG achieves highly promising\nresults, setting a new state-of-the-art across a range of metrics encompassing\nfidelity, diversity, and controllability.\n","authors":["Chengyou Jia","Minnan Luo","Zhuohang Dang","Guang Dai","Xiaojun Chang","Mengmeng Wang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10156v2.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2311.13717v2","updated":"2024-03-13T12:10:53Z","published":"2023-11-22T22:21:26Z","title":"Feature Extraction for Generative Medical Imaging Evaluation: New\n  Evidence Against an Evolving Trend","summary":"  Fr\\'echet Inception Distance (FID) is a widely used metric for assessing\nsynthetic image quality. It relies on an ImageNet-based feature extractor,\nmaking its applicability to medical imaging unclear. A recent trend is to adapt\nFID to medical imaging through feature extractors trained on medical images.\nOur study challenges this practice by demonstrating that ImageNet-based\nextractors are more consistent and aligned with human judgment than their\nRadImageNet counterparts. We evaluated sixteen StyleGAN2 networks across four\nmedical imaging modalities and four data augmentation techniques with Fr\\'echet\ndistances (FDs) computed using eleven ImageNet or RadImageNet-trained feature\nextractors. Comparison with human judgment via visual Turing tests revealed\nthat ImageNet-based extractors produced rankings consistent with human\njudgment, with the FD derived from the ImageNet-trained SwAV extractor\nsignificantly correlating with expert evaluations. In contrast,\nRadImageNet-based rankings were volatile and inconsistent with human judgment.\nOur findings challenge prevailing assumptions, providing novel evidence that\nmedical image-trained feature extractors do not inherently improve FDs and can\neven compromise their reliability. Our code is available at\nhttps://github.com/mckellwoodland/fid-med-eval.\n","authors":["McKell Woodland","Austin Castelo","Mais Al Taie","Jessica Albuquerque Marques Silva","Mohamed Eltaher","Frank Mohn","Alexander Shieh","Austin Castelo","Suprateek Kundu","Joshua P. Yung","Ankit B. Patel","Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2311.13717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08453v1","updated":"2024-03-13T12:07:14Z","published":"2024-03-13T12:07:14Z","title":"Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on","summary":"  Image-based virtual try-on aims to transfer target in-shop clothing to a\ndressed model image, the objectives of which are totally taking off original\nclothing while preserving the contents outside of the try-on area, naturally\nwearing target clothing and correctly inpainting the gap between target\nclothing and original clothing. Tremendous efforts have been made to facilitate\nthis popular research area, but cannot keep the type of target clothing with\nthe try-on area affected by original clothing. In this paper, we focus on the\nunpaired virtual try-on situation where target clothing and original clothing\non the model are different, i.e., the practical scenario. To break the\ncorrelation between the try-on area and the original clothing and make the\nmodel learn the correct information to inpaint, we propose an adaptive mask\ntraining paradigm that dynamically adjusts training masks. It not only improves\nthe alignment and fit of clothing but also significantly enhances the fidelity\nof virtual try-on experience. Furthermore, we for the first time propose two\nmetrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and\nSkeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the\naccuracy of clothing texture. For unpaired try-on validation, we construct a\ncomprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items\nand model physiques, covering a broad try-on scenarios. Experiments demonstrate\nthe effectiveness of the proposed methods, contributing to the advancement of\nvirtual try-on technology and offering new insights and tools for future\nresearch in the field. The code, model and benchmark will be publicly released.\n","authors":["Xuanpu Zhang","Dan Song","Pengxin Zhan","Qingguo Chen","Kuilong Liu","Anan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11125v2","updated":"2024-03-13T12:04:18Z","published":"2023-09-20T08:16:39Z","title":"PSDiff: Diffusion Model for Person Search with Iterative and\n  Collaborative Refinement","summary":"  Dominant Person Search methods aim to localize and recognize query persons in\na unified network, which jointly optimizes two sub-tasks, \\ie, pedestrian\ndetection and Re-IDentification (ReID). Despite significant progress, current\nmethods face two primary challenges: 1) the pedestrian candidates learned\nwithin detectors are suboptimal for the ReID task. 2) the potential for\ncollaboration between two sub-tasks is overlooked. To address these issues, we\npresent a novel Person Search framework based on the Diffusion model, PSDiff.\nPSDiff formulates the person search as a dual denoising process from noisy\nboxes and ReID embeddings to ground truths. Distinct from the conventional\nDetection-to-ReID approach, our denoising paradigm discards prior pedestrian\ncandidates generated by detectors, thereby avoiding the local optimum problem\nof the ReID task. Following the new paradigm, we further design a new\nCollaborative Denoising Layer (CDL) to optimize detection and ReID sub-tasks in\nan iterative and collaborative way, which makes two sub-tasks mutually\nbeneficial. Extensive experiments on the standard benchmarks show that PSDiff\nachieves state-of-the-art performance with fewer parameters and elastic\ncomputing overhead.\n","authors":["Chengyou Jia","Minnan Luo","Zhuohang Dang","Guang Dai","Xiaojun Chang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2309.11125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08436v1","updated":"2024-03-13T11:39:30Z","published":"2024-03-13T11:39:30Z","title":"PFStorer: Personalized Face Restoration and Super-Resolution","summary":"  Recent developments in face restoration have achieved remarkable results in\nproducing high-quality and lifelike outputs. The stunning results however often\nfail to be faithful with respect to the identity of the person as the models\nlack necessary context. In this paper, we explore the potential of personalized\nface restoration with diffusion models. In our approach a restoration model is\npersonalized using a few images of the identity, leading to tailored\nrestoration with respect to the identity while retaining fine-grained details.\nBy using independent trainable blocks for personalization, the rich prior of a\nbase restoration model can be exploited to its fullest. To avoid the model\nrelying on parts of identity left in the conditioning low-quality images, a\ngenerative regularizer is employed. With a learnable parameter, the model\nlearns to balance between the details generated based on the input image and\nthe degree of personalization. Moreover, we improve the training pipeline of\nface restoration models to enable an alignment-free approach. We showcase the\nrobust capabilities of our approach in several real-world scenarios with\nmultiple identities, demonstrating our method's ability to generate\nfine-grained details with faithful restoration. In the user study we evaluate\nthe perceptual quality and faithfulness of the genereated details, with our\nmethod being voted best 61% of the time compared to the second best with 25% of\nthe votes.\n","authors":["Tuomas Varanka","Tapani Toivonen","Soumya Tripathy","Guoying Zhao","Erman Acar"],"pdf_url":"https://arxiv.org/pdf/2403.08436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05924v2","updated":"2024-03-13T11:36:43Z","published":"2024-03-09T14:18:41Z","title":"CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot\n  Learning","summary":"  Attribute and object (A-O) disentanglement is a fundamental and critical\nproblem for Compositional Zero-shot Learning (CZSL), whose aim is to recognize\nnovel A-O compositions based on foregone knowledge. Existing methods based on\ndisentangled representation learning lose sight of the contextual dependency\nbetween the A-O primitive pairs. Inspired by this, we propose a novel A-O\ndisentangled framework for CZSL, namely Class-specified Cascaded Network\n(CSCNet). The key insight is to firstly classify one primitive and then\nspecifies the predicted class as a priori for guiding another primitive\nrecognition in a cascaded fashion. To this end, CSCNet constructs\nAttribute-to-Object and Object-to-Attribute cascaded branches, in addition to a\ncomposition branch modeling the two primitives as a whole. Notably, we devise a\nparametric classifier (ParamCls) to improve the matching between visual and\nsemantic embeddings. By improving the A-O disentanglement, our framework\nachieves superior results than previous competitive methods.\n","authors":["Yanyi Zhang","Qi Jia","Xin Fan","Yu Liu","Ran He"],"pdf_url":"https://arxiv.org/pdf/2403.05924v2.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.08433v1","updated":"2024-03-13T11:33:38Z","published":"2024-03-13T11:33:38Z","title":"An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language\n  Pre-train Model","summary":"  Recent studies applied Parameter Efficient Fine-Tuning techniques (PEFTs) to\nefficiently narrow the performance gap between pre-training and downstream.\nThere are two important factors for various PEFTs, namely, the accessible data\nsize and fine-tunable parameter size. A natural expectation for PEFTs is that\nthe performance of various PEFTs is positively related to the data size and\nfine-tunable parameter size. However, according to the evaluation of five PEFTs\non two downstream vision-language (VL) tasks, we find that such an intuition\nholds only if the downstream data and task are not consistent with\npre-training. For downstream fine-tuning consistent with pre-training, data\nsize no longer affects the performance, while the influence of fine-tunable\nparameter size is not monotonous. We believe such an observation could guide\nthe choice of training strategy for various PEFTs.\n","authors":["Yuxin Tian","Mouxing Yang","Yunfan Li","Dayiheng Liu","Xingzhang Ren","Xi Peng","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2403.08433v1.pdf","comment":"Accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.08426v1","updated":"2024-03-13T11:23:55Z","published":"2024-03-13T11:23:55Z","title":"Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation","summary":"  The pre-trained vision-language model, exemplified by CLIP, advances\nzero-shot semantic segmentation by aligning visual features with class\nembeddings through a transformer decoder to generate semantic masks. Despite\nits effectiveness, prevailing methods within this paradigm encounter\nchallenges, including overfitting on seen classes and small fragmentation in\nmasks. To mitigate these issues, we propose a Language-Driven Visual Consensus\n(LDVC) approach, fostering improved alignment of semantic and visual\ninformation.Specifically, we leverage class embeddings as anchors due to their\ndiscrete and abstract nature, steering vision features toward class embeddings.\nMoreover, to circumvent noisy alignments from the vision part due to its\nredundant nature, we introduce route attention into self-attention for finding\nvisual consensus, thereby enhancing semantic consistency within the same\nobject. Equipped with a vision-language prompting strategy, our approach\nsignificantly boosts the generalization capacity of segmentation models for\nunseen classes. Experimental results underscore the effectiveness of our\napproach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the\nCOCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.\n","authors":["Zicheng Zhang","Tong Zhang","Yi Zhu","Jianzhuang Liu","Xiaodan Liang","QiXiang Ye","Wei Ke"],"pdf_url":"https://arxiv.org/pdf/2403.08426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09945v2","updated":"2024-03-13T11:21:11Z","published":"2023-08-19T08:41:41Z","title":"Dual Branch Deep Learning Network for Detection and Stage Grading of\n  Diabetic Retinopathy","summary":"  Diabetic retinopathy is a severe complication of diabetes that can lead to\npermanent blindness if not treated promptly. Early and accurate diagnosis of\nthe disease is essential for successful treatment. This paper introduces a deep\nlearning method for the detection and stage grading of diabetic retinopathy,\nusing a single fundus retinal image. Our model utilizes transfer learning,\nemploying two state-of-the-art pre-trained models as feature extractors and\nfine-tuning them on a new dataset. The proposed model is trained on a large\nmulti-center dataset, including the APTOS 2019 dataset, obtained from publicly\navailable sources. It achieves remarkable performance in diabetic retinopathy\ndetection and stage classification on the APTOS 2019, outperforming the\nestablished literature. For binary classification, the proposed approach\nachieves an accuracy of 98.50, a sensitivity of 99.46, and a specificity of\n97.51. In stage grading, it achieves a quadratic weighted kappa of 93.00, an\naccuracy of 89.60, a sensitivity of 89.60, and a specificity of 97.72. The\nproposed approach serves as a reliable screening and stage grading tool for\ndiabetic retinopathy, offering significant potential to enhance clinical\ndecision-making and patient care.\n","authors":["Hossein Shakibania","Sina Raoufi","Behnam Pourafkham","Hassan Khotanlou","Muharram Mansoorizadeh"],"pdf_url":"https://arxiv.org/pdf/2308.09945v2.pdf","comment":"Published in the Biomedical Signal Processing & Control journal, 16\n  pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.07743v2","updated":"2024-03-13T11:20:19Z","published":"2024-03-12T15:22:05Z","title":"Equipping Computational Pathology Systems with Artifact Processing\n  Pipelines: A Showcase for Computation and Performance Trade-offs","summary":"  Histopathology is a gold standard for cancer diagnosis under a microscopic\nexamination. However, histological tissue processing procedures result in\nartifacts, which are ultimately transferred to the digitized version of glass\nslides, known as whole slide images (WSIs). Artifacts are diagnostically\nirrelevant areas and may result in wrong deep learning (DL) algorithms\npredictions. Therefore, detecting and excluding artifacts in the computational\npathology (CPATH) system is essential for reliable automated diagnosis. In this\npaper, we propose a mixture of experts (MoE) scheme for detecting five notable\nartifacts, including damaged tissue, blur, folded tissue, air bubbles, and\nhistologically irrelevant blood from WSIs. First, we train independent binary\nDL models as experts to capture particular artifact morphology. Then, we\nensemble their predictions using a fusion mechanism. We apply probabilistic\nthresholding over the final probability distribution to improve the sensitivity\nof the MoE. We developed DL pipelines using two MoEs and two multiclass models\nof state-of-the-art deep convolutional neural networks (DCNNs) and vision\ntransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed\nsimpler multiclass models and were tested on datasets from different hospitals\nand cancer types, where MoE using DCNNs yielded the best results. The proposed\nMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining\nless computational cost for inference than MoE using ViTs. This best\nperformance of MoEs comes with relatively higher computational trade-offs than\nmulticlass models. The proposed artifact detection pipeline will not only\nensure reliable CPATH predictions but may also provide quality control.\n","authors":["Neel Kanwal","Farbod Khoraminia","Umay Kiraz","Andres Mosquera-Zamudio","Carlos Monteagudo","Emiel A. M. Janssen","Tahlita C. M. Zuiverloon","Chunmig Rong","Kjersti Engan"],"pdf_url":"https://arxiv.org/pdf/2403.07743v2.pdf","comment":"Submitted to BMC Medical Informatics and Decision Making Journal"},{"id":"http://arxiv.org/abs/2403.08420v1","updated":"2024-03-13T11:11:59Z","published":"2024-03-13T11:11:59Z","title":"Low-Cost and Real-Time Industrial Human Action Recognitions Based on\n  Large-Scale Foundation Models","summary":"  Industrial managements, including quality control, cost and safety\noptimization, etc., heavily rely on high quality industrial human action\nrecognitions (IHARs) which were hard to be implemented in large-scale\nindustrial scenes due to their high costs and poor real-time performance. In\nthis paper, we proposed a large-scale foundation model(LSFM)-based IHAR method,\nwherein various LSFMs and lightweight methods were jointly used, for the first\ntime, to fulfill low-cost dataset establishment and real-time IHARs.\nComprehensive tests on in-situ large-scale industrial manufacturing lines\nelucidated that the proposed method realized great reduction on employment\ncosts, superior real-time performance, and satisfactory accuracy and\ngeneralization capabilities, indicating its great potential as a backbone IHAR\nmethod, especially for large-scale industrial applications.\n","authors":["Wensheng Liang","Ruiyan Zhuang","Xianwei Shi","Shuai Li","Zhicheng Wang","Xiaoguang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.08420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11537v3","updated":"2024-03-13T11:11:33Z","published":"2022-03-22T08:28:50Z","title":"Convolutional Neural Network-based Efficient Dense Point Cloud\n  Generation using Unsigned Distance Fields","summary":"  Dense point cloud generation from a sparse or incomplete point cloud is a\ncrucial and challenging problem in 3D computer vision and computer graphics. So\nfar, the existing methods are either computationally too expensive, suffer from\nlimited resolution, or both. In addition, some methods are strictly limited to\nwatertight surfaces -- another major obstacle for a number of applications. To\naddress these issues, we propose a lightweight Convolutional Neural Network\nthat learns and predicts the unsigned distance field for arbitrary 3D shapes\nfor dense point cloud generation using the recently emerged concept of implicit\nfunction learning. Experiments demonstrate that the proposed architecture\noutperforms the state of the art by 7.8x less model parameters, 2.4x faster\ninference time and up to 24.8% improved generation quality compared to the\nstate-of-the-art.\n","authors":["Abol Basher","Jani Boutellier"],"pdf_url":"https://arxiv.org/pdf/2203.11537v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18086v4","updated":"2024-03-13T11:11:18Z","published":"2024-02-28T06:18:33Z","title":"Generalizable Two-Branch Framework for Image Class-Incremental Learning","summary":"  Deep neural networks often severely forget previously learned knowledge when\nlearning new knowledge. Various continual learning (CL) methods have been\nproposed to handle such a catastrophic forgetting issue from different\nperspectives and achieved substantial improvements. In this paper, a novel\ntwo-branch continual learning framework is proposed to further enhance most\nexisting CL methods. Specifically, the main branch can be any existing CL model\nand the newly introduced side branch is a lightweight convolutional network.\nThe output of each main branch block is modulated by the output of the\ncorresponding side branch block. Such a simple two-branch model can then be\neasily implemented and learned with the vanilla optimization setting without\nwhistles and bells. Extensive experiments with various settings on multiple\nimage datasets show that the proposed framework yields consistent improvements\nover state-of-the-art methods.\n","authors":["Chao Wu","Xiaobin Chang","Ruixuan Wang"],"pdf_url":"https://arxiv.org/pdf/2402.18086v4.pdf","comment":"5 pages,3 figures,accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.08417v1","updated":"2024-03-13T11:05:40Z","published":"2024-03-13T11:05:40Z","title":"The Development and Performance of a Machine Learning Based Mobile\n  Platform for Visually Determining the Etiology of Penile Pathology","summary":"  Machine-learning algorithms can facilitate low-cost, user-guided visual\ndiagnostic platforms for addressing disparities in access to sexual health\nservices. We developed a clinical image dataset using original and augmented\nimages for five penile diseases: herpes eruption, syphilitic chancres, penile\ncandidiasis, penile cancer, and genital warts. We used a U-net architecture\nmodel for semantic pixel segmentation into background or subject image, the\nInception-ResNet version 2 neural architecture to classify each pixel as\ndiseased or non-diseased, and a salience map using GradCAM++. We trained the\nmodel on a random 91% sample of the image database using 150 epochs per image,\nand evaluated the model on the remaining 9% of images, assessing recall (or\nsensitivity), precision, specificity, and F1-score (accuracy). Of the 239\nimages in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%)\nwere of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of\npenile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were\nof non-diseased penises. The overall accuracy of the model for correctly\nclassifying the diseased image was 0.944. Between July 1st and October 1st\n2023, there were 2,640 unique users of the mobile platform. Among a random\nsample of submissions (n=437), 271 (62.0%) were from the United States, 64\n(14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United\nKingdom, and 21 (4.8%) from Vietnam. The majority (n=277 [63.4%]) were between\n18 and 30 years old. We report on the development of a machine-learning model\nfor classifying five penile diseases, which demonstrated excellent performance\non a validation dataset. That model is currently in use globally and has the\npotential to improve access to diagnostic services for penile diseases.\n","authors":["Lao-Tzu Allan-Blitz","Sithira Ambepitiya","Raghavendra Tirupathi","Jeffrey D. Klausner","Yudara Kularathne"],"pdf_url":"https://arxiv.org/pdf/2403.08417v1.pdf","comment":"12 pages, 2 figure, 2 tables"},{"id":"http://arxiv.org/abs/2306.16175v3","updated":"2024-03-13T10:57:24Z","published":"2023-06-28T12:52:48Z","title":"$\\mathbf{C}^2$Former: Calibrated and Complementary Transformer for\n  RGB-Infrared Object Detection","summary":"  Object detection on visible (RGB) and infrared (IR) images, as an emerging\nsolution to facilitate robust detection for around-the-clock applications, has\nreceived extensive attention in recent years. With the help of IR images,\nobject detectors have been more reliable and robust in practical applications\nby using RGB-IR combined information. However, existing methods still suffer\nfrom modality miscalibration and fusion imprecision problems. Since transformer\nhas the powerful capability to model the pairwise correlations between\ndifferent features, in this paper, we propose a novel Calibrated and\nComplementary Transformer called $\\mathrm{C}^2$Former to address these two\nproblems simultaneously. In $\\mathrm{C}^2$Former, we design an Inter-modality\nCross-Attention (ICA) module to obtain the calibrated and complementary\nfeatures by learning the cross-attention relationship between the RGB and IR\nmodality. To reduce the computational cost caused by computing the global\nattention in ICA, an Adaptive Feature Sampling (AFS) module is introduced to\ndecrease the dimension of feature maps. Because $\\mathrm{C}^2$Former performs\nin the feature domain, it can be embedded into existed RGB-IR object detectors\nvia the backbone network. Thus, one single-stage and one two-stage object\ndetector both incorporating our $\\mathrm{C}^2$Former are constructed to\nevaluate its effectiveness and versatility. With extensive experiments on the\nDroneVehicle and KAIST RGB-IR datasets, we verify that our method can fully\nutilize the RGB-IR complementary information and achieve robust detection\nresults. The code is available at\nhttps://github.com/yuanmaoxun/Calibrated-and-Complementary-Transformer-for-RGB-Infrared-Object-Detection.git.\n","authors":["Maoxun Yuan","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2306.16175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07042v3","updated":"2024-03-13T10:57:00Z","published":"2023-11-13T02:54:17Z","title":"Open-Vocabulary Video Anomaly Detection","summary":"  Video anomaly detection (VAD) with weak supervision has achieved remarkable\nperformance in utilizing video-level labels to discriminate whether a video\nframe is normal or abnormal. However, current approaches are inherently limited\nto a closed-set setting and may struggle in open-world applications where there\ncan be anomaly categories in the test data unseen during training. A few recent\nstudies attempt to tackle a more realistic setting, open-set VAD, which aims to\ndetect unseen anomalies given seen anomalies and normal videos. However, such a\nsetting focuses on predicting frame anomaly scores, having no ability to\nrecognize the specific categories of anomalies, despite the fact that this\nability is essential for building more informed video surveillance systems.\nThis paper takes a step further and explores open-vocabulary video anomaly\ndetection (OVVAD), in which we aim to leverage pre-trained large models to\ndetect and categorize seen and unseen anomalies. To this end, we propose a\nmodel that decouples OVVAD into two mutually complementary tasks --\nclass-agnostic detection and class-specific classification -- and jointly\noptimizes both tasks. Particularly, we devise a semantic knowledge injection\nmodule to introduce semantic knowledge from large language models for the\ndetection task, and design a novel anomaly synthesis module to generate pseudo\nunseen anomaly videos with the help of large vision generation models for the\nclassification task. These semantic knowledge and synthesis anomalies\nsubstantially extend our model's capability in detecting and categorizing a\nvariety of seen and unseen anomalies. Extensive experiments on three\nwidely-used benchmarks demonstrate our model achieves state-of-the-art\nperformance on OVVAD task.\n","authors":["Peng Wu","Xuerong Zhou","Guansong Pang","Yujia Sun","Jing Liu","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07042v3.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.08407v1","updated":"2024-03-13T10:51:18Z","published":"2024-03-13T10:51:18Z","title":"Iterative Online Image Synthesis via Diffusion Model for Imbalanced\n  Classification","summary":"  Accurate and robust classification of diseases is important for proper\ndiagnosis and treatment. However, medical datasets often face challenges\nrelated to limited sample sizes and inherent imbalanced distributions, due to\ndifficulties in data collection and variations in disease prevalence across\ndifferent types. In this paper, we introduce an Iterative Online Image\nSynthesis (IOIS) framework to address the class imbalance problem in medical\nimage classification. Our framework incorporates two key modules, namely Online\nImage Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively\ntarget the imbalance classification issue at both the instance level and the\nclass level. The OIS module alleviates the data insufficiency problem by\ngenerating representative samples tailored for online training of the\nclassifier. On the other hand, the AAS module dynamically balances the\nsynthesized samples among various classes, targeting those with low training\naccuracy. To evaluate the effectiveness of our proposed method in addressing\nimbalanced classification, we conduct experiments on the HAM10000 and APTOS\ndatasets. The results obtained demonstrate the superiority of our approach over\nstate-of-the-art methods as well as the effectiveness of each component. The\nsource code will be released upon acceptance.\n","authors":["Shuhan Li","Yi Lin","Hao Chen","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.08407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11408v3","updated":"2024-03-13T10:31:21Z","published":"2023-08-22T12:54:48Z","title":"MatFuse: Controllable Material Generation with Diffusion Models","summary":"  Creating high-quality materials in computer graphics is a challenging and\ntime-consuming task, which requires great expertise. To simplify this process,\nwe introduce MatFuse, a unified approach that harnesses the generative power of\ndiffusion models for creation and editing of 3D materials. Our method\nintegrates multiple sources of conditioning, including color palettes,\nsketches, text, and pictures, enhancing creative possibilities and granting\nfine-grained control over material synthesis. Additionally, MatFuse enables\nmap-level material editing capabilities through latent manipulation by means of\na multi-encoder compression model which learns a disentangled latent\nrepresentation for each map. We demonstrate the effectiveness of MatFuse under\nmultiple conditioning settings and explore the potential of material editing.\nFinally, we assess the quality of the generated materials both quantitatively\nin terms of CLIP-IQA and FID scores and qualitatively by conducting a user\nstudy. Source code for training MatFuse and supplemental materials are publicly\navailable at https://gvecchio.com/matfuse.\n","authors":["Giuseppe Vecchio","Renato Sortino","Simone Palazzo","Concetto Spampinato"],"pdf_url":"https://arxiv.org/pdf/2308.11408v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04621v2","updated":"2024-03-13T10:23:45Z","published":"2023-06-07T17:50:59Z","title":"Flexible Distribution Alignment: Towards Long-tailed Semi-supervised\n  Learning with Proper Calibration","summary":"  Long-tailed semi-supervised learning (LTSSL) represents a practical scenario\nfor semi-supervised applications, challenged by skewed labeled distributions\nthat bias classifiers. This problem is often aggravated by discrepancies\nbetween labeled and unlabeled class distributions, leading to biased\npseudo-labels, neglect of rare classes, and poorly calibrated probabilities. To\naddress these issues, we introduce Flexible Distribution Alignment (FlexDA), a\nnovel adaptive logit-adjusted loss framework designed to dynamically estimate\nand align predictions with the actual distribution of unlabeled data and\nachieve a balanced classifier by the end of training. FlexDA is further\nenhanced by a distillation-based consistency loss, promoting fair data usage\nacross classes and effectively leveraging underconfident samples. This method,\nencapsulated in ADELLO (Align and Distill Everything All at Once), proves\nrobust against label shift, significantly improves model calibration in LTSSL\ncontexts, and surpasses previous state-of-of-art approaches across multiple\nbenchmarks, including CIFAR100-LT, STL10-LT, and ImageNet127, addressing class\nimbalance challenges in semi-supervised learning. Our code will be made\navailable upon paper acceptance.\n","authors":["Emanuel Sanchez Aimar","Hannah Helgesen","Yonghao Xu","Marco Kuhlmann","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2306.04621v2.pdf","comment":"Under review, 24 pages"},{"id":"http://arxiv.org/abs/2403.08384v1","updated":"2024-03-13T09:48:11Z","published":"2024-03-13T09:48:11Z","title":"AADNet: Attention aware Demoiréing Network","summary":"  Moire pattern frequently appears in photographs captured with mobile devices\nand digital cameras, potentially degrading image quality. Despite recent\nadvancements in computer vision, image demoire'ing remains a challenging task\ndue to the dynamic textures and variations in colour, shape, and frequency of\nmoire patterns. Most existing methods struggle to generalize to unseen\ndatasets, limiting their effectiveness in removing moire patterns from\nreal-world scenarios. In this paper, we propose a novel lightweight\narchitecture, AADNet (Attention Aware Demoireing Network), for high-resolution\nimage demoire'ing that effectively works across different frequency bands and\ngeneralizes well to unseen datasets. Extensive experiments conducted on the\nUHDM dataset validate the effectiveness of our approach, resulting in\nhigh-fidelity images.\n","authors":["M Rakesh Reddy","Shubham Mandloi","Aman Kumar"],"pdf_url":"https://arxiv.org/pdf/2403.08384v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2403.08383v1","updated":"2024-03-13T09:48:04Z","published":"2024-03-13T09:48:04Z","title":"RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion\n  Attack in Federated Learning","summary":"  Federated learning (FL) empowers privacy-preservation in model training by\nonly exposing users' model gradients. Yet, FL users are susceptible to the\ngradient inversion (GI) attack which can reconstruct ground-truth training data\nsuch as images based on model gradients. However, reconstructing\nhigh-resolution images by existing GI attack works faces two challenges:\ninferior accuracy and slow-convergence, especially when the context is\ncomplicated, e.g., the training batch size is much greater than 1 on each FL\nuser. To address these challenges, we present a Robust, Accurate and\nFast-convergent GI attack algorithm, called RAF-GI, with two components: 1)\nAdditional Convolution Block (ACB) which can restore labels with up to 20%\nimprovement compared with existing works; 2) Total variance, three-channel mEan\nand cAnny edge detection regularization term (TEA), which is a white-box attack\nstrategy to reconstruct images based on labels inferred by ACB. Moreover,\nRAF-GI is robust that can still accurately reconstruct ground-truth data when\nthe users' training batch size is no more than 48. Our experimental results\nmanifest that RAF-GI can diminish 94% time costs while achieving superb\ninversion quality in ImageNet dataset. Notably, with a batch size of 1, RAF-GI\nexhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the\nstate-of-the-art baselines.\n","authors":["Can Liu","Jin Wang","Dongyang Yu"],"pdf_url":"https://arxiv.org/pdf/2403.08383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08381v1","updated":"2024-03-13T09:47:04Z","published":"2024-03-13T09:47:04Z","title":"Tackling the Singularities at the Endpoints of Time Intervals in\n  Diffusion Models","summary":"  Most diffusion models assume that the reverse process adheres to a Gaussian\ndistribution. However, this approximation has not been rigorously validated,\nespecially at singularities, where t=0 and t=1. Improperly dealing with such\nsingularities leads to an average brightness issue in applications, and limits\nthe generation of images with extreme brightness or darkness. We primarily\nfocus on tackling singularities from both theoretical and practical\nperspectives. Initially, we establish the error bounds for the reverse process\napproximation, and showcase its Gaussian characteristics at singularity time\nsteps. Based on this theoretical insight, we confirm the singularity at t=1 is\nconditionally removable while it at t=0 is an inherent property. Upon these\nsignificant conclusions, we propose a novel plug-and-play method SingDiffusion\nto address the initial singular time step sampling, which not only effectively\nresolves the average brightness issue for a wide range of diffusion models\nwithout extra training efforts, but also enhances their generation capability\nin achieving notable lower FID scores. Code and models are released at\nhttps://github.com/PangzeCheung/SingDiffusion.\n","authors":["Pengze Zhang","Hubery Yin","Chen Li","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.08381v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.08380v1","updated":"2024-03-13T09:45:30Z","published":"2024-03-13T09:45:30Z","title":"Mitigate Target-level Insensitivity of Infrared Small Target Detection\n  via Posterior Distribution Modeling","summary":"  Infrared Small Target Detection (IRSTD) aims to segment small targets from\ninfrared clutter background. Existing methods mainly focus on discriminative\napproaches, i.e., a pixel-level front-background binary segmentation. Since\ninfrared small targets are small and low signal-to-clutter ratio, empirical\nrisk has few disturbances when a certain false alarm and missed detection\nexist, which seriously affect the further improvement of such methods.\nMotivated by the dense prediction generative methods, in this paper, we propose\na diffusion model framework for Infrared Small Target Detection which\ncompensates pixel-level discriminant with mask posterior distribution modeling.\nFurthermore, we design a Low-frequency Isolation in the wavelet domain to\nsuppress the interference of intrinsic infrared noise on the diffusion noise\nestimation. This transition from the discriminative paradigm to generative one\nenables us to bypass the target-level insensitivity. Experiments show that the\nproposed method achieves competitive performance gains over state-of-the-art\nmethods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets. Code are available at\nhttps://github.com/Li-Haoqing/IRSTD-Diff.\n","authors":["Haoqing Li","Jinfu Yang","Yifei Xu","Runshi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08378v1","updated":"2024-03-13T09:43:14Z","published":"2024-03-13T09:43:14Z","title":"A Generalized Framework with Adaptive Weighted Soft-Margin for\n  Imbalanced SVM Classification","summary":"  Category imbalance is one of the most popular and important issues in the\ndomain of classification. In this paper, we present a new generalized framework\nwith Adaptive Weight function for soft-margin Weighted SVM (AW-WSVM), which\naims to enhance the issue of imbalance and outlier sensitivity in standard\nsupport vector machine (SVM) for classifying two-class data. The weight\ncoefficient is introduced into the unconstrained soft-margin support vector\nmachines, and the sample weights are updated before each training. The Adaptive\nWeight function (AW function) is constructed from the distance between the\nsamples and the decision hyperplane, assigning different weights to each\nsample. A weight update method is proposed, taking into account the proximity\nof the support vectors to the decision hyperplane. Before training, the weights\nof the corresponding samples are initialized according to different categories.\nSubsequently, the samples close to the decision hyperplane are identified and\nassigned more weights. At the same time, lower weights are assigned to samples\nthat are far from the decision hyperplane. Furthermore, we also put forward an\neffective way to eliminate noise. To evaluate the strength of the proposed\ngeneralized framework, we conducted experiments on standard datasets and\nemotion classification datasets with different imbalanced ratios (IR). The\nexperimental results prove that the proposed generalized framework outperforms\nin terms of accuracy, recall metrics and G-mean, validating the effectiveness\nof the weighted strategy provided in this paper in enhancing support vector\nmachines.\n","authors":["Lu Jiang","Qi Wang","Yuhang Chang","Jianing Song","Haoyue Fu"],"pdf_url":"https://arxiv.org/pdf/2403.08378v1.pdf","comment":"15 pages, 23 figures"},{"id":"http://arxiv.org/abs/2403.07593v2","updated":"2024-03-13T09:39:14Z","published":"2024-03-12T12:25:54Z","title":"MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D\n  Sparse Convolutions","summary":"  This paper presents MinkUNeXt, an effective and efficient architecture for\nplace-recognition from point clouds entirely based on the new 3D MinkNeXt\nBlock, a residual block composed of 3D sparse convolutions that follows the\nphilosophy established by recent Transformers but purely using simple 3D\nconvolutions. Feature extraction is performed at different scales by a U-Net\nencoder-decoder network and the feature aggregation of those features into a\nsingle descriptor is carried out by a Generalized Mean Pooling (GeM). The\nproposed architecture demonstrates that it is possible to surpass the current\nstate-of-the-art by only relying on conventional 3D sparse convolutions without\nmaking use of more complex and sophisticated proposals such as Transformers,\nAttention-Layers or Deformable Convolutions. A thorough assessment of the\nproposal has been carried out using the Oxford RobotCar and the In-house\ndatasets. As a result, MinkUNeXt proves to outperform other methods in the\nstate-of-the-art.\n","authors":["J. J. Cabrera","A. Santo","A. Gil","C. Viegas","L. Payá"],"pdf_url":"https://arxiv.org/pdf/2403.07593v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.08368v1","updated":"2024-03-13T09:30:08Z","published":"2024-03-13T09:30:08Z","title":"METER: a mobile vision transformer architecture for monocular depth\n  estimation","summary":"  Depth estimation is a fundamental knowledge for autonomous systems that need\nto assess their own state and perceive the surrounding environment. Deep\nlearning algorithms for depth estimation have gained significant interest in\nrecent years, owing to the potential benefits of this methodology in overcoming\nthe limitations of active depth sensing systems. Moreover, due to the low cost\nand size of monocular cameras, researchers have focused their attention on\nmonocular depth estimation (MDE), which consists in estimating a dense depth\nmap from a single RGB video frame. State of the art MDE models typically rely\non vision transformers (ViT) architectures that are highly deep and complex,\nmaking them unsuitable for fast inference on devices with hardware constraints.\nPurposely, in this paper, we address the problem of exploiting ViT in MDE on\nembedded devices. Those systems are usually characterized by limited memory\ncapabilities and low-power CPU/GPU. We propose METER, a novel lightweight\nvision transformer architecture capable of achieving state of the art\nestimations and low latency inference performances on the considered embedded\nhardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano. We provide a solution\nconsisting of three alternative configurations of METER, a novel loss function\nto balance pixel estimation and reconstruction of image details, and a new data\naugmentation strategy to improve the overall final predictions. The proposed\nmethod outperforms previous lightweight works over the two benchmark datasets:\nthe indoor NYU Depth v2 and the outdoor KITTI.\n","authors":["L. Papa","P. Russo","I. Amerini"],"pdf_url":"https://arxiv.org/pdf/2403.08368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14230v3","updated":"2024-03-13T09:23:10Z","published":"2023-10-22T08:46:40Z","title":"A comprehensive survey on deep active learning in medical image analysis","summary":"  Deep learning has achieved widespread success in medical image analysis,\nleading to an increasing demand for large-scale expert-annotated medical image\ndatasets. Yet, the high cost of annotating medical images severely hampers the\ndevelopment of deep learning in this field. To reduce annotation costs, active\nlearning aims to select the most informative samples for annotation and train\nhigh-performance models with as few labeled samples as possible. In this\nsurvey, we review the core methods of active learning, including the evaluation\nof informativeness and sampling strategy. For the first time, we provide a\ndetailed summary of the integration of active learning with other\nlabel-efficient techniques, such as semi-supervised, self-supervised learning,\nand so on. We also summarize active learning works that are specifically\ntailored to medical image analysis. Additionally, we conduct a thorough\ncomparative analysis of the performance of different AL methods in medical\nimage analysis with experiments. In the end, we offer our perspectives on the\nfuture trends and challenges of active learning and its applications in medical\nimage analysis.\n","authors":["Haoran Wang","Qiuye Jin","Shiman Li","Siyu Liu","Manning Wang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2310.14230v3.pdf","comment":"More papers and contents of medical image analysis & performance\n  analysis on medical imaging datasets with experiments"},{"id":"http://arxiv.org/abs/2403.08360v1","updated":"2024-03-13T09:20:43Z","published":"2024-03-13T09:20:43Z","title":"Improved Image-based Pose Regressor Models for Underwater Environments","summary":"  We investigate the performance of image-based pose regressor models in\nunderwater environments for relocalization. Leveraging PoseNet and PoseLSTM, we\nregress a 6-degree-of-freedom pose from single RGB images with high accuracy.\nAdditionally, we explore data augmentation with stereo camera images to improve\nmodel accuracy. Experimental results demonstrate that the models achieve high\naccuracy in both simulated and clear waters, promising effective real-world\nunderwater navigation and inspection applications.\n","authors":["Luyuan Peng","Hari Vishnu","Mandar Chitre","Yuen Min Too","Bharath Kalyan","Rajat Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.08360v1.pdf","comment":"Presented at AUV Symposium 2022"},{"id":"http://arxiv.org/abs/2403.08355v1","updated":"2024-03-13T09:12:16Z","published":"2024-03-13T09:12:16Z","title":"NaturalVLM: Leveraging Fine-grained Natural Language for\n  Affordance-Guided Visual Manipulation","summary":"  Enabling home-assistant robots to perceive and manipulate a diverse range of\n3D objects based on human language instructions is a pivotal challenge. Prior\nresearch has predominantly focused on simplistic and task-oriented\ninstructions, i.e., \"Slide the top drawer open\". However, many real-world tasks\ndemand intricate multi-step reasoning, and without human instructions, these\nwill become extremely difficult for robot manipulation. To address these\nchallenges, we introduce a comprehensive benchmark, NrVLM, comprising 15\ndistinct manipulation tasks, containing over 4500 episodes meticulously\nannotated with fine-grained language instructions. We split the long-term task\nprocess into several steps, with each step having a natural language\ninstruction. Moreover, we propose a novel learning framework that completes the\nmanipulation task step-by-step according to the fine-grained instructions.\nSpecifically, we first identify the instruction to execute, taking into account\nvisual observations and the end-effector's current state. Subsequently, our\napproach facilitates explicit learning through action-prompts and\nperception-prompts to promote manipulation-aware cross-modality alignment.\nLeveraging both visual observations and linguistic guidance, our model outputs\na sequence of actionable predictions for manipulation, including contact points\nand end-effector poses. We evaluate our method and baselines using the proposed\nbenchmark NrVLM. The experimental results demonstrate the effectiveness of our\napproach. For additional details, please refer to\nhttps://sites.google.com/view/naturalvlm.\n","authors":["Ran Xu","Yan Shen","Xiaoqi Li","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.08355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08352v1","updated":"2024-03-13T09:00:38Z","published":"2024-03-13T09:00:38Z","title":"Data augmentation with automated machine learning: approaches and\n  performance comparison with classical data augmentation methods","summary":"  Data augmentation is arguably the most important regularization technique\ncommonly used to improve generalization performance of machine learning models.\nIt primarily involves the application of appropriate data transformation\noperations to create new data samples with desired properties. Despite its\neffectiveness, the process is often challenging because of the time-consuming\ntrial and error procedures for creating and testing different candidate\naugmentations and their hyperparameters manually. Automated data augmentation\nmethods aim to automate the process. State-of-the-art approaches typically rely\non automated machine learning (AutoML) principles. This work presents a\ncomprehensive survey of AutoML-based data augmentation techniques. We discuss\nvarious approaches for accomplishing data augmentation with AutoML, including\ndata manipulation, data integration and data synthesis techniques. We present\nextensive discussion of techniques for realizing each of the major subtasks of\nthe data augmentation process: search space design, hyperparameter optimization\nand model evaluation. Finally, we carried out an extensive comparison and\nanalysis of the performance of automated data augmentation techniques and\nstate-of-the-art methods based on classical augmentation approaches. The\nresults show that AutoML methods for data augmentation currently outperform\nstate-of-the-art techniques based on conventional approaches.\n","authors":["Alhassan Mumuni","Fuseini Mumuni"],"pdf_url":"https://arxiv.org/pdf/2403.08352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08350v1","updated":"2024-03-13T08:54:31Z","published":"2024-03-13T08:54:31Z","title":"CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large\n  Language Model","summary":"  Instruction tuning represents a prevalent strategy employed by Multimodal\nLarge Language Models (MLLMs) to align with human instructions and adapt to new\ntasks. Nevertheless, MLLMs encounter the challenge of adapting to users'\nevolving knowledge and demands. Therefore, how to retain existing skills while\nacquiring new knowledge needs to be investigated. In this paper, we present a\ncomprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess\nexisting MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10\ncommonly used datasets spanning 8 task categories, ensuring a diverse range of\ninstructions and tasks. Besides, the trained model is evaluated from two\naspects: Instruction Following and General Knowledge, which assess the\nalignment with human intention and knowledge preserved for reasoning,\nrespectively. Experiments on CoIN demonstrate that current powerful MLLMs still\nsuffer catastrophic forgetting, and the failure in intention alignment assumes\nthe main responsibility, instead of the knowledge forgetting. To this end, we\nintroduce MoELoRA to MLLMs which is effective to retain the previous\ninstruction alignment. Experimental results consistently illustrate the\nforgetting decreased from this method on CoIN.\n","authors":["Cheng Chen","Junchen Zhu","Xu Luo","Hengtao Shen","Lianli Gao","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2403.08350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08344v1","updated":"2024-03-13T08:49:40Z","published":"2024-03-13T08:49:40Z","title":"STMPL: Human Soft-Tissue Simulation","summary":"  In various applications, such as virtual reality and gaming, simulating the\ndeformation of soft tissues in the human body during interactions with external\nobjects is essential. Traditionally, Finite Element Methods (FEM) have been\nemployed for this purpose, but they tend to be slow and resource-intensive. In\nthis paper, we propose a unified representation of human body shape and soft\ntissue with a data-driven simulator of non-rigid deformations. This approach\nenables rapid simulation of realistic interactions.\n  Our method builds upon the SMPL model, which generates human body shapes\nconsidering rigid transformations. We extend SMPL by incorporating a soft\ntissue layer and an intuitive representation of external forces applied to the\nbody during object interactions. Specifically, we mapped the 3D body shape and\nsoft tissue and applied external forces to 2D UV maps. Leveraging a UNET\narchitecture designed for 2D data, our approach achieves high-accuracy\ninference in real time. Our experiment shows that our method achieves plausible\ndeformation of the soft tissue layer, even for unseen scenarios.\n","authors":["Anton Agafonov","Lihi Zelnik-Manor"],"pdf_url":"https://arxiv.org/pdf/2403.08344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18765v3","updated":"2024-03-13T08:47:32Z","published":"2023-11-30T18:05:52Z","title":"MLLMs-Augmented Visual-Language Representation Learning","summary":"  Visual-language pre-training has achieved remarkable success in many\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that Multi-modal Large\nLanguage Models (MLLMs) can enhance visual-language representation learning by\nestablishing richer image-text associations for image-text datasets. Our\napproach is simple, utilizing MLLMs to extend multiple diverse captions for\neach image. To prevent the bias introduced by MLLMs' hallucinations and\nmonotonous language styles, we propose \"text shearing\" to maintain the quality\nand availability of extended captions. In image-text retrieval, without\nintroducing additional training cost, our method consistently obtains 5.6 ~\n35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and\nzero-shot settings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.\n","authors":["Yanqing Liu","Kai Wang","Wenqi Shao","Ping Luo","Yu Qiao","Mike Zheng Shou","Kaipeng Zhang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2311.18765v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08330v1","updated":"2024-03-13T08:29:58Z","published":"2024-03-13T08:29:58Z","title":"Activating Wider Areas in Image Super-Resolution","summary":"  The prevalence of convolution neural networks (CNNs) and vision transformers\n(ViTs) has markedly revolutionized the area of single-image super-resolution\n(SISR). To further boost the SR performances, several techniques, such as\nresidual learning and attention mechanism, are introduced, which can be largely\nattributed to a wider range of activated area, that is, the input pixels that\nstrongly influence the SR results. However, the possibility of further\nimproving SR performance through another versatile vision backbone remains an\nunresolved challenge. To address this issue, in this paper, we unleash the\nrepresentation potential of the modern state space model, i.e., Vision Mamba\n(Vim), in the context of SISR. Specifically, we present three recipes for\nbetter utilization of Vim-based models: 1) Integration into a MetaFormer-style\nblock; 2) Pre-training on a larger and broader dataset; 3) Employing\ncomplementary attention mechanism, upon which we introduce the MMA. The\nresulting network MMA is capable of finding the most relevant and\nrepresentative input pixels to reconstruct the corresponding high-resolution\nimages. Comprehensive experimental analysis reveals that MMA not only achieves\ncompetitive or even superior performance compared to state-of-the-art SISR\nmethods but also maintains relatively low memory and computational overheads\n(e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the\nscale of 2). Furthermore, MMA proves its versatility in lightweight SR\napplications. Through this work, we aim to illuminate the potential\napplications of state space models in the broader realm of image processing\nrather than SISR, encouraging further exploration in this innovative direction.\n","authors":["Cheng Cheng","Hang Wang","Hongbin Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08330v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.19276v3","updated":"2024-03-13T08:24:51Z","published":"2024-02-29T15:44:00Z","title":"Modular Blind Video Quality Assessment","summary":"  Blind video quality assessment (BVQA) plays a pivotal role in evaluating and\nimproving the viewing experience of end-users across a wide range of\nvideo-based platforms and services. Contemporary deep learning-based models\nprimarily analyze the video content in its aggressively downsampled format,\nwhile being blind to the impact of actual spatial resolution and frame rate on\nvideo quality. In this paper, we propose a modular BVQA model, and a method of\ntraining it to improve its modularity. Specifically, our model comprises a base\nquality predictor, a spatial rectifier, and a temporal rectifier, responding to\nthe visual content and distortion, spatial resolution, and frame rate changes\non video quality, respectively. During training, spatial and temporal\nrectifiers are dropped out with some probabilities so as to make the base\nquality predictor a standalone BVQA model, which should work better with the\nrectifiers. Extensive experiments on both professionally-generated content and\nuser generated content video databases show that our quality model achieves\nsuperior or comparable performance to current methods. Furthermore, the\nmodularity of our model offers a great opportunity to analyze existing video\nquality databases in terms of their spatial and temporal complexities. Last,\nour BVQA model is cost-effective to add other quality-relevant video attributes\nsuch as dynamic range and color gamut as additional rectifiers.\n","authors":["Wen Wen","Mu Li","Yabin Zhang","Yiting Liao","Junlin Li","Li Zhang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2402.19276v3.pdf","comment":"Accepted by CVPR 2024; It is not the camera-ready version"},{"id":"http://arxiv.org/abs/2402.15977v2","updated":"2024-03-13T08:18:56Z","published":"2024-02-25T03:43:07Z","title":"An Image Enhancement Method for Improving Small Intestinal Villi Clarity","summary":"  This paper presents, for the first time, an image enhancement methodology\ndesigned to enhance the clarity of small intestinal villi in Wireless Capsule\nEndoscopy (WCE) images. This method first separates the low-frequency and\nhigh-frequency components of small intestinal villi images using guided\nfiltering. Subsequently, an adaptive light gain factor is generated based on\nthe low-frequency component, and an adaptive gradient gain factor is derived\nfrom the convolution results of the Laplacian operator in different regions of\nsmall intestinal villi images. The obtained light gain factor and gradient gain\nfactor are then combined to enhance the high-frequency components. Finally, the\nenhanced high-frequency component is fused with the original image to achieve\nadaptive sharpening of the edges of WCE small intestinal villi images. The\nexperiments affirm that, compared to established WCE image enhancement methods,\nour approach not only accentuates the edge details of WCE small intestine villi\nimages but also skillfully suppresses noise amplification, thereby preventing\nthe occurrence of edge overshooting.\n","authors":["Shaojie Zhang","Yinghui Wang","Peixuan Liu","Yukai Wang","Liangyi Huang","Mingfeng Wang","Ibragim R. Atadjanov"],"pdf_url":"https://arxiv.org/pdf/2402.15977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08321v1","updated":"2024-03-13T08:06:41Z","published":"2024-03-13T08:06:41Z","title":"ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic\n  Manipulation","summary":"  Performing language-conditioned robotic manipulation tasks in unstructured\nenvironments is highly demanded for general intelligent robots. Conventional\nrobotic manipulation methods usually learn semantic representation of the\nobservation for action prediction, which ignores the scene-level spatiotemporal\ndynamics for human goal completion. In this paper, we propose a dynamic\nGaussian Splatting method named ManiGaussian for multi-task robotic\nmanipulation, which mines scene dynamics via future scene reconstruction.\nSpecifically, we first formulate the dynamic Gaussian Splatting framework that\ninfers the semantics propagation in the Gaussian embedding space, where the\nsemantic representation is leveraged to predict the optimal robot action. Then,\nwe build a Gaussian world model to parameterize the distribution in our dynamic\nGaussian Splatting framework, which provides informative supervision in the\ninteractive environment via future scene reconstruction. We evaluate our\nManiGaussian on 10 RLBench tasks with 166 variations, and the results\ndemonstrate our framework can outperform the state-of-the-art methods by 13.1\\%\nin average success rate.\n","authors":["Guanxing Lu","Shiyi Zhang","Ziwei Wang","Changliu Liu","Jiwen Lu","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2403.08321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05854v3","updated":"2024-03-13T08:04:13Z","published":"2024-03-09T09:52:15Z","title":"LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content","summary":"  Long-tail recognition is challenging because it requires the model to learn\ngood representations from tail categories and address imbalances across all\ncategories. In this paper, we propose a novel generative and fine-tuning\nframework, LTGC, to handle long-tail recognition via leveraging generated\ncontent. Firstly, inspired by the rich implicit knowledge in large-scale models\n(e.g., large language models, LLMs), LTGC leverages the power of these models\nto parse and reason over the original tail data to produce diverse tail-class\ncontent. We then propose several novel designs for LTGC to ensure the quality\nof the generated data and to efficiently fine-tune the model using both the\ngenerated and original data. The visualization demonstrates the effectiveness\nof the generation module in LTGC, which produces accurate and diverse tail\ndata. Additionally, the experimental results demonstrate that our LTGC\noutperforms existing state-of-the-art methods on popular long-tailed\nbenchmarks.\n","authors":["Qihao Zhao","Yalun Dai","Hao Li","Wei Hu","Fan Zhang","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05854v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08318v1","updated":"2024-03-13T08:00:07Z","published":"2024-03-13T08:00:07Z","title":"DrFER: Learning Disentangled Representations for 3D Facial Expression\n  Recognition","summary":"  Facial Expression Recognition (FER) has consistently been a focal point in\nthe field of facial analysis. In the context of existing methodologies for 3D\nFER or 2D+3D FER, the extraction of expression features often gets entangled\nwith identity information, compromising the distinctiveness of these features.\nTo tackle this challenge, we introduce the innovative DrFER method, which\nbrings the concept of disentangled representation learning to the field of 3D\nFER. DrFER employs a dual-branch framework to effectively disentangle\nexpression information from identity information. Diverging from prior\ndisentanglement endeavors in the 3D facial domain, we have carefully\nreconfigured both the loss functions and network structure to make the overall\nframework adaptable to point cloud data. This adaptation enhances the\ncapability of the framework in recognizing facial expressions, even in cases\ninvolving varying head poses. Extensive evaluations conducted on the BU-3DFE\nand Bosphorus datasets substantiate that DrFER surpasses the performance of\nother 3D FER methods.\n","authors":["Hebeizi Li","Hongyu Yang","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2403.08318v1.pdf","comment":"Accepted by FG 2024"},{"id":"http://arxiv.org/abs/2402.03246v4","updated":"2024-03-13T07:55:38Z","published":"2024-02-05T18:03:53Z","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","summary":"  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n","authors":["Mingrui Li","Shuhong Liu","Heng Zhou","Guohao Zhu","Na Cheng","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03246v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13764v2","updated":"2024-03-13T07:48:50Z","published":"2023-04-26T18:10:35Z","title":"PhagoStat a scalable and interpretable end to end framework for\n  efficient quantification of cell phagocytosis in neurodegenerative disease\n  studies","summary":"  Quantifying the phagocytosis of dynamic, unstained cells is essential for\nevaluating neurodegenerative diseases. However, measuring rapid cell\ninteractions and distinguishing cells from background make this task very\nchallenging when processing time-lapse phase-contrast video microscopy. In this\nstudy, we introduce an end-to-end, scalable, and versatile real-time framework\nfor quantifying and analyzing phagocytic activity. Our proposed pipeline is\nable to process large data-sets and includes a data quality verification module\nto counteract perturbations such as microscope movements and frame blurring. We\nalso propose an explainable cell segmentation module to improve the\ninterpretability of DL methods compared to black-box algorithms. This includes\ntwo interpretable DL capabilities: visual explanation and model simplification.\nWe demonstrate that interpretability in DL is not the opposite of high\nperformance, by additionally providing essential DL algorithm optimization\ninsights and solutions. Besides, incorporating interpretable modules results in\nan efficient architecture design and optimized execution time. We apply our\npipeline to analyze microglial cell phagocytosis in FTD and obtain\nstatistically reliable results showing that FTD mutant cells are larger and\nmore aggressive than control cells. The method has been tested and validated on\npublic benchmarks by generating state-of-the art performances. To stimulate\ntranslational approaches and future studies, we release an open-source\nend-to-end pipeline and a unique microglial cells phagocytosis dataset for\nimmune system characterization in neurodegenerative diseases research. This\npipeline and the associated dataset will consistently crystallize future\nadvances in this field, promoting the development of interpretable algorithms\ndedicated to the domain of neurodegenerative diseases' characterization.\ngithub.com/ounissimehdi/PhagoStat\n","authors":["Mehdi Ounissi","Morwena Latouche","Daniel Racoceanu"],"pdf_url":"https://arxiv.org/pdf/2304.13764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07359v2","updated":"2024-03-13T07:47:53Z","published":"2024-03-12T06:45:34Z","title":"FSC: Few-point Shape Completion","summary":"  While previous studies have demonstrated successful 3D object shape\ncompletion with a sufficient number of points, they often fail in scenarios\nwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\nanalysis, we find that even a few points, e.g. 64 points, could retain\nsubstantial information to help recover the 3D shape of the object. To address\nthe challenge of shape completion with very sparse point clouds, we then\npropose Few-point Shape Completion (FSC) model, which contains a novel\ndual-branch feature extractor for handling extremely sparse inputs, coupled\nwith an extensive branch for maximal point utilization with a saliency branch\nfor dynamic importance assignment. This model is further bolstered by a\ntwo-stage revision network that refines both the extracted features and the\ndecoder output, enhancing the detail and authenticity of the completed point\ncloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\na few points. The proposed Few-point Shape Completion (FSC) model outperforms\nprevious methods on both few-point inputs and many-point inputs, and shows good\ngeneralizability to different object categories.\n","authors":["Xianzu Wu","Xianfeng Wu","Tianyu Luan","Yajing Bai","Zhongyuan Lai","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.07359v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08310v1","updated":"2024-03-13T07:42:21Z","published":"2024-03-13T07:42:21Z","title":"StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance\n  Fields","summary":"  4D style transfer aims at transferring arbitrary visual style to the\nsynthesized novel views of a dynamic 4D scene with varying viewpoints and\ntimes. Existing efforts on 3D style transfer can effectively combine the visual\nfeatures of style images and neural radiance fields (NeRF) but fail to handle\nthe 4D dynamic scenes limited by the static scene assumption. Consequently, we\naim to handle the novel challenging problem of 4D style transfer for the first\ntime, which further requires the consistency of stylized results on dynamic\nobjects. In this paper, we introduce StyleDyRF, a method that represents the 4D\nfeature space by deforming a canonical feature volume and learns a linear style\ntransformation matrix on the feature volume in a data-driven fashion. To obtain\nthe canonical feature volume, the rays at each time step are deformed with the\ngeometric prior of a pre-trained dynamic NeRF to render the feature map under\nthe supervision of pre-trained visual encoders. With the content and style cues\nin the canonical feature volume and the style image, we can learn the style\ntransformation matrix from their covariance matrices with lightweight neural\nnetworks. The learned style transformation matrix can reflect a direct matching\nof feature covariance from the content volume to the given style pattern, in\nanalogy with the optimization of the Gram matrix in traditional 2D neural style\ntransfer. The experimental results show that our method not only renders 4D\nphotorealistic style transfer results in a zero-shot manner but also\noutperforms existing methods in terms of visual quality and consistency.\n","authors":["Hongbin Xu","Weitao Chen","Feng Xiao","Baigui Sun","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2403.08310v1.pdf","comment":"In submission. The code and model are released at:\n  https://github.com/ToughStoneX/StyleDyRF"},{"id":"http://arxiv.org/abs/2310.11696v2","updated":"2024-03-13T07:39:10Z","published":"2023-10-18T03:57:06Z","title":"MOHO: Learning Single-view Hand-held Object Reconstruction with\n  Multi-view Occlusion-Aware Supervision","summary":"  Previous works concerning single-view hand-held object reconstruction\ntypically rely on supervision from 3D ground-truth models, which are hard to\ncollect in real world. In contrast, readily accessible hand-object videos offer\na promising training data source, but they only give heavily occluded object\nobservations. In this paper, we present a novel synthetic-to-real framework to\nexploit Multi-view Occlusion-aware supervision from hand-object videos for\nHand-held Object reconstruction (MOHO) from a single image, tackling two\npredominant challenges in such setting: hand-induced occlusion and object's\nself-occlusion. First, in the synthetic pre-training stage, we render a\nlarge-scaled synthetic dataset SOMVideo with hand-object images and multi-view\nocclusion-free supervisions, adopted to address hand-induced occlusion in both\n2D and 3D spaces. Second, in the real-world finetuning stage, MOHO leverages\nthe amodal-mask-weighted geometric supervision to mitigate the unfaithful\nguidance caused by the hand-occluded supervising views in real world. Moreover,\ndomain-consistent occlusion-aware features are amalgamated in MOHO to resist\nobject's self-occlusion for inferring the complete object shape. Extensive\nexperiments on HO3D and DexYCB datasets demonstrate 2D-supervised MOHO gains\nsuperior results against 3D-supervised methods by a large margin.\n","authors":["Chenyangguang Zhang","Guanlong Jiao","Yan Di","Gu Wang","Ziqin Huang","Ruida Zhang","Fabian Manhardt","Bowen Fu","Federico Tombari","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2310.11696v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2310.06313v3","updated":"2024-03-13T07:32:06Z","published":"2023-10-10T05:13:17Z","title":"Advancing Pose-Guided Image Synthesis with Progressive Conditional\n  Diffusion Models","summary":"  Recent work has showcased the significant potential of diffusion models in\npose-guided person image synthesis. However, owing to the inconsistency in pose\nbetween the source and target images, synthesizing an image with a distinct\npose, relying exclusively on the source image and target pose information,\nremains a formidable challenge. This paper presents Progressive Conditional\nDiffusion Models (PCDMs) that incrementally bridge the gap between person\nimages under the target and source poses through three stages. Specifically, in\nthe first stage, we design a simple prior conditional diffusion model that\npredicts the global features of the target image by mining the global alignment\nrelationship between pose coordinates and image appearance. Then, the second\nstage establishes a dense correspondence between the source and target images\nusing the global features from the previous stage, and an inpainting\nconditional diffusion model is proposed to further align and enhance the\ncontextual features, generating a coarse-grained person image. In the third\nstage, we propose a refining conditional diffusion model to utilize the\ncoarsely generated image from the previous stage as a condition, achieving\ntexture restoration and enhancing fine-detail consistency. The three-stage\nPCDMs work progressively to generate the final high-quality and high-fidelity\nsynthesized image. Both qualitative and quantitative results demonstrate the\nconsistency and photorealism of our proposed PCDMs under challenging\nscenarios.The code and model will be available at\nhttps://github.com/tencent-ailab/PCDMs.\n","authors":["Fei Shen","Hu Ye","Jun Zhang","Cong Wang","Xiao Han","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06313v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2401.01386v3","updated":"2024-03-13T07:14:16Z","published":"2024-01-01T19:58:36Z","title":"Tissue Artifact Segmentation and Severity Analysis for Automated\n  Diagnosis Using Whole Slide Images","summary":"  Traditionally, pathological analysis and diagnosis are performed by manually\neyeballing glass slide specimens under a microscope by an expert. The whole\nslide image is the digital specimen produced from the glass slide. Whole slide\nimage enabled specimens to be observed on a computer screen and led to\ncomputational pathology where computer vision and artificial intelligence are\nutilized for automated analysis and diagnosis. With the current computational\nadvancement, the entire whole slide image can be analyzed autonomously without\nhuman supervision. However, the analysis could fail or lead to wrong diagnosis\nif the whole slide image is affected by tissue artifacts such as tissue fold or\nair bubbles depending on the severity. Existing artifact detection methods rely\non experts for severity assessment to eliminate artifact affected regions from\nthe analysis. This process is time consuming, exhausting and undermines the\ngoal of automated analysis or removal of artifacts without evaluating their\nseverity, which could result in the loss of diagnostically important data.\nTherefore, it is necessary to detect artifacts and then assess their severity\nautomatically. In this paper, we propose a system that incorporates severity\nevaluation with artifact detection utilizing convolutional neural networks. The\nproposed system uses DoubleUNet to segment artifacts and an ensemble network of\nsix fine tuned convolutional neural network models to determine severity. This\nmethod outperformed current state of the art in accuracy by 9 percent for\nartifact segmentation and achieved a strong correlation of 97 percent with the\nevaluation of pathologists for severity assessment. The robustness of the\nsystem was demonstrated using our proposed heterogeneous dataset and practical\nusability was ensured by integrating it with an automated analysis system.\n","authors":["Galib Muhammad Shahriar Himel"],"pdf_url":"https://arxiv.org/pdf/2401.01386v3.pdf","comment":"Master's thesis, 60 pages, 21 figures, 16 tables"},{"id":"http://arxiv.org/abs/2403.08294v1","updated":"2024-03-13T06:57:23Z","published":"2024-03-13T06:57:23Z","title":"Attack Deterministic Conditional Image Generative Models for Diverse and\n  Controllable Generation","summary":"  Existing generative adversarial network (GAN) based conditional image\ngenerative models typically produce fixed output for the same conditional\ninput, which is unreasonable for highly subjective tasks, such as large-mask\nimage inpainting or style transfer. On the other hand, GAN-based diverse image\ngenerative methods require retraining/fine-tuning the network or designing\ncomplex noise injection functions, which is computationally expensive,\ntask-specific, or struggle to generate high-quality results. Given that many\ndeterministic conditional image generative models have been able to produce\nhigh-quality yet fixed results, we raise an intriguing question: is it possible\nfor pre-trained deterministic conditional image generative models to generate\ndiverse results without changing network structures or parameters? To answer\nthis question, we re-examine the conditional image generation tasks from the\nperspective of adversarial attack and propose a simple and efficient plug-in\nprojected gradient descent (PGD) like method for diverse and controllable image\ngeneration. The key idea is attacking the pre-trained deterministic generative\nmodels by adding a micro perturbation to the input condition. In this way,\ndiverse results can be generated without any adjustment of network structures\nor fine-tuning of the pre-trained models. In addition, we can also control the\ndiverse results to be generated by specifying the attack direction according to\na reference text or image. Our work opens the door to applying adversarial\nattack to low-level vision tasks, and experiments on various conditional image\ngeneration tasks demonstrate the effectiveness and superiority of the proposed\nmethod.\n","authors":["Tianyi Chu","Wei Xing","Jiafu Chen","Zhizhong Wang","Jiakai Sun","Lei Zhao","Haibo Chen","Huaizhong Lin"],"pdf_url":"https://arxiv.org/pdf/2403.08294v1.pdf","comment":"9 pages, 7 figures, accepted by AAAI24"},{"id":"http://arxiv.org/abs/2403.08284v1","updated":"2024-03-13T06:34:49Z","published":"2024-03-13T06:34:49Z","title":"MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge\n  Detection on Federated Learning","summary":"  As a new distributed computing framework that can protect data privacy,\nfederated learning (FL) has attracted more and more attention in recent years.\nIt receives gradients from users to train the global model and releases the\ntrained global model to working users. Nonetheless, the gradient inversion (GI)\nattack reflects the risk of privacy leakage in federated learning. Attackers\nonly need to use gradients through hundreds of thousands of simple iterations\nto obtain relatively accurate private data stored on users' local devices. For\nthis, some works propose simple but effective strategies to obtain user data\nunder a single-label dataset. However, these strategies induce a satisfactory\nvisual effect of the inversion image at the expense of higher time costs. Due\nto the semantic limitation of a single label, the image obtained by gradient\ninversion may have semantic errors. We present a novel gradient inversion\nstrategy based on canny edge detection (MGIC) in both the multi-label and\nsingle-label datasets. To reduce semantic errors caused by a single label, we\nadd new convolution layers' blocks in the trained model to obtain the image's\nmulti-label. Through multi-label representation, serious semantic errors in\ninversion images are reduced. Then, we analyze the impact of parameters on the\ndifficulty of input image reconstruction and discuss how image multi-subjects\naffect the inversion performance. Our proposed strategy has better visual\ninversion image results than the most widely used ones, saving more than 78% of\ntime costs in the ImageNet dataset.\n","authors":["Can Liu","Jin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08283v1","updated":"2024-03-13T06:28:37Z","published":"2024-03-13T06:28:37Z","title":"Optimized Detection and Classification on GTRSB: Advancing Traffic Sign\n  Recognition with Convolutional Neural Networks","summary":"  In the rapidly evolving landscape of transportation, the proliferation of\nautomobiles has made road traffic more complex, necessitating advanced\nvision-assisted technologies for enhanced safety and navigation. These\ntechnologies are imperative for providing critical traffic sign information,\ninfluencing driver behavior, and supporting vehicle control, especially for\ndrivers with disabilities and in the burgeoning field of autonomous vehicles.\nTraffic sign detection and recognition have emerged as key areas of research\ndue to their essential roles in ensuring road safety and compliance with\ntraffic regulations. Traditional computer vision methods have faced challenges\nin achieving optimal accuracy and speed due to real-world variabilities.\nHowever, the advent of deep learning and Convolutional Neural Networks (CNNs)\nhas revolutionized this domain, offering solutions that significantly surpass\nprevious capabilities in terms of speed and reliability. This paper presents an\ninnovative approach leveraging CNNs that achieves an accuracy of nearly 96\\%,\nhighlighting the potential for even greater precision through advanced\nlocalization techniques. Our findings not only contribute to the ongoing\nadvancement of traffic sign recognition technology but also underscore the\ncritical impact of these developments on road safety and the future of\nautonomous driving.\n","authors":["Dhruv Toshniwal","Saurabh Loya","Anuj Khot","Yash Marda"],"pdf_url":"https://arxiv.org/pdf/2403.08283v1.pdf","comment":"8 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.08282v1","updated":"2024-03-13T06:22:17Z","published":"2024-03-13T06:22:17Z","title":"Hierarchical Auto-Organizing System for Open-Ended Multi-Agent\n  Navigation","summary":"  Navigating complex environments in Minecraft poses significant challenges for\nmulti-agent systems due to the game's dynamic and unpredictable open-world\nsetting. Agents need to interact with the environment and coordinate their\nactions with other agents to achieve common objectives. However, traditional\napproaches often struggle to efficiently manage inter-agent communication and\ntask distribution, which are crucial for effective multi-agent navigation.\nFurthermore, processing and integrating multi-modal information (such as\nvisual, textual, and auditory data) is essential for agents to fully comprehend\ntheir goals and navigate the environment successfully. To address this issue,\nwe design the HAS framework to auto-organize groups of LLM-based agents to\ncomplete Navigation tasks. In our approach, we devise a hierarchical\nauto-organizing navigation system, which is characterized by 1) a hierarchical\nsystem for multi-agent organization, ensuring centralized planning and\ndecentralized execution; 2) an auto-organizing and intra-communication\nmechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal\ninformation platform, facilitating multi-modal perception to perform the three\nnavigation tasks with one system. To assess organizational behavior, we design\na series of navigation tasks in the Minecraft environment, which includes\nsearching and exploring. We aim to develop embodied organizations that push the\nboundaries of embodied AI, moving it towards a more human-like organizational\nstructure.\n","authors":["Zhonghan Zhao","Kewei Chen","Dongxu Guo","Wenhao Chai","Tian Ye","Yanting Zhang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08280v1","updated":"2024-03-13T06:18:08Z","published":"2024-03-13T06:18:08Z","title":"Pre-examinations Improve Automated Metastases Detection on Cranial MRI","summary":"  Materials and methods: First, a dual-time approach was assessed, for which\nthe CNN was provided sequences of the MRI that initially depicted new MM\n(diagnosis MRI) as well as of a prediagnosis MRI: inclusion of only\ncontrast-enhanced T1-weighted images (CNNdual_ce) was compared with inclusion\nof also the native T1-weighted images, T2-weighted images, and FLAIR sequences\nof both time points (CNNdual_all).Second, results were compared with the\ncorresponding single time approaches, in which the CNN was provided exclusively\nthe respective sequences of the diagnosis MRI.Casewise diagnostic performance\nparameters were calculated from 5-fold cross-validation.\n  Results: In total, 94 cases with 494 MMs were included. Overall, the highest\ndiagnostic performance was achieved by inclusion of only the contrast-enhanced\nT1-weighted images of the diagnosis and of a prediagnosis MRI (CNNdual_ce,\nsensitivity = 73%, PPV = 25%, F1-score = 36%). Using exclusively\ncontrast-enhanced T1-weighted images as input resulted in significantly less\nfalse-positives (FPs) compared with inclusion of further sequences beyond\ncontrast-enhanced T1-weighted images (FPs = 5/7 for CNNdual_ce/CNNdual_all, P <\n1e-5). Comparison of contrast-enhanced dual and mono time approaches revealed\nthat exclusion of prediagnosis MRI significantly increased FPs (FPs = 5/10 for\nCNNdual_ce/CNNce, P < 1e-9).Approaches with only native sequences were clearly\ninferior to CNNs that were provided contrast-enhanced sequences.\n  Conclusions: Automated MM detection on contrast-enhanced T1-weighted images\nperformed with high sensitivity. Frequent FPs due to artifacts and vessels were\nsignificantly reduced by additional inclusion of prediagnosis MRI, but not by\ninclusion of further sequences beyond contrast-enhanced T1-weighted images.\nFuture studies might investigate different change detection architectures for\ncomputer-aided detection.\n","authors":["Katerina Deike-Hofmann","Dorottya Dancs","Daniel Paech","Heinz-Peter Schlemmer","Klaus Maier-Hein","Philipp Bäumer","Alexander Radbruch","Michael Götz"],"pdf_url":"https://arxiv.org/pdf/2403.08280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08277v1","updated":"2024-03-13T06:11:41Z","published":"2024-03-13T06:11:41Z","title":"VIGFace: Virtual Identity Generation Model for Face Image Synthesis","summary":"  Deep learning-based face recognition continues to face challenges due to its\nreliance on huge datasets obtained from web crawling, which can be costly to\ngather and raise significant real-world privacy concerns. To address this\nissue, we propose VIGFace, a novel framework capable of generating synthetic\nfacial images. Initially, we train the face recognition model using a real face\ndataset and create a feature space for both real and virtual IDs where virtual\nprototypes are orthogonal to other prototypes. Subsequently, we generate\nsynthetic images by using the diffusion model based on the feature space. Our\nproposed framework provides two significant benefits. Firstly, it allows for\ncreating virtual facial images without concerns about portrait rights,\nguaranteeing that the generated virtual face images are clearly differentiated\nfrom existing individuals. Secondly, it serves as an effective augmentation\nmethod by incorporating real existing images. Further experiments demonstrate\nthe efficacy of our framework, achieving state-of-the-art results from both\nperspectives without any external data.\n","authors":["Minsoo Kim","Min-Cheol Sagong","Gi Pyo Nam","Junghyun Cho","Ig-Jae Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.02115v2","updated":"2024-03-13T06:07:52Z","published":"2022-03-04T03:12:15Z","title":"Towards Benchmarking and Evaluating Deepfake Detection","summary":"  Deepfake detection automatically recognizes the manipulated medias through\nthe analysis of the difference between manipulated and non-altered videos. It\nis natural to ask which are the top performers among the existing deepfake\ndetection approaches to identify promising research directions and provide\npractical guidance. Unfortunately, it's difficult to conduct a sound\nbenchmarking comparison of existing detection approaches using the results in\nthe literature because evaluation conditions are inconsistent across studies.\nOur objective is to establish a comprehensive and consistent benchmark, to\ndevelop a repeatable evaluation procedure, and to measure the performance of a\nrange of detection approaches so that the results can be compared soundly. A\nchallenging dataset consisting of the manipulated samples generated by more\nthan 13 different methods has been collected, and 11 popular detection\napproaches (9 algorithms) from the existing literature have been implemented\nand evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92\nmodels have been trained and 644 experiments have been performed for the\nevaluation. The results along with the shared data and evaluation methodology\nconstitute a benchmark for comparing deepfake detection approaches and\nmeasuring progress.\n","authors":["Chenhao Lin","Jingyi Deng","Pengbin Hu","Chao Shen","Qian Wang","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2203.02115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08273v1","updated":"2024-03-13T05:53:25Z","published":"2024-03-13T05:53:25Z","title":"LiqD: A Dynamic Liquid Level Detection Model under Tricky Small\n  Containers","summary":"  In daily life and industrial production, it is crucial to accurately detect\nchanges in liquid level in containers. Traditional contact measurement methods\nhave some limitations, while emerging non-contact image processing technology\nshows good application prospects. This paper proposes a container dynamic\nliquid level detection model based on U^2-Net. This model uses the SAM model to\ngenerate an initial data set, and then evaluates and filters out high-quality\npseudo-label images through the SemiReward framework to build an exclusive data\nset. The model uses U^2-Net to extract mask images of containers from the data\nset, and uses morphological processing to compensate for mask defects.\nSubsequently, the model calculates the grayscale difference between adjacent\nvideo frame images at the same position, segments the liquid level change area\nby setting a difference threshold, and finally uses a lightweight neural\nnetwork to classify the liquid level state. This approach not only mitigates\nthe impact of intricate surroundings, but also reduces the demand for training\ndata, showing strong robustness and versatility. A large number of experimental\nresults show that the proposed model can effectively detect the dynamic liquid\nlevel changes of the liquid in the container, providing a novel and efficient\nsolution for related fields.\n","authors":["Yukun Ma","Zikun Mao"],"pdf_url":"https://arxiv.org/pdf/2403.08273v1.pdf","comment":"7pages, 7 Figures"},{"id":"http://arxiv.org/abs/2403.08271v1","updated":"2024-03-13T05:48:58Z","published":"2024-03-13T05:48:58Z","title":"Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained\n  Ship Classification","summary":"  Fine-grained ship classification in remote sensing (RS-FGSC) poses a\nsignificant challenge due to the high similarity between classes and the\nlimited availability of labeled data, limiting the effectiveness of traditional\nsupervised classification methods. Recent advancements in large pre-trained\nVision-Language Models (VLMs) have demonstrated impressive capabilities in\nfew-shot or zero-shot learning, particularly in understanding image content.\nThis study delves into harnessing the potential of VLMs to enhance\nclassification accuracy for unseen ship categories, which holds considerable\nsignificance in scenarios with restricted data due to cost or privacy\nconstraints. Directly fine-tuning VLMs for RS-FGSC often encounters the\nchallenge of overfitting the seen classes, resulting in suboptimal\ngeneralization to unseen classes, which highlights the difficulty in\ndifferentiating complex backgrounds and capturing distinct ship features. To\naddress these issues, we introduce a novel prompt tuning technique that employs\na hierarchical, multi-granularity prompt design. Our approach integrates remote\nsensing ship priors through bias terms, learned from a small trainable network.\nThis strategy enhances the model's generalization capabilities while improving\nits ability to discern intricate backgrounds and learn discriminative ship\nfeatures. Furthermore, we contribute to the field by introducing a\ncomprehensive dataset, FGSCM-52, significantly expanding existing datasets with\nmore extensive data and detailed annotations for less common ship classes.\nExtensive experimental evaluations demonstrate the superiority of our proposed\nmethod over current state-of-the-art techniques. The source code will be made\npublicly available.\n","authors":["Long Lan","Fengxiang Wang","Shuyan Li","Xiangtao Zheng","Zengmao Wang","Xinwang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08270v1","updated":"2024-03-13T05:46:36Z","published":"2024-03-13T05:46:36Z","title":"Identity-aware Dual-constraint Network for Cloth-Changing Person\n  Re-identification","summary":"  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify\nthe target person in more realistic surveillance scenarios, where pedestrians\nusually change their clothing. Despite great progress, limited cloth-changing\ntraining samples in existing CC-ReID datasets still prevent the model from\nadequately learning cloth-irrelevant features. In addition, due to the absence\nof explicit supervision to keep the model constantly focused on\ncloth-irrelevant areas, existing methods are still hampered by the disruption\nof clothing variations. To solve the above issues, we propose an Identity-aware\nDual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the\nmodel extract cloth-irrelevant clues, we propose a Clothes Diversity\nAugmentation (CDA), which generates more realistic cloth-changing samples by\nenriching the clothing color while preserving the texture. In addition, a\nMulti-scale Constraint Block (MCB) is designed, which extracts fine-grained\nidentity-related features and effectively transfers cloth-irrelevant knowledge.\nMoreover, a Counterfactual-guided Attention Module (CAM) is presented, which\nlearns cloth-irrelevant features from channel and space dimensions and utilizes\nthe counterfactual intervention for supervising the attention map to highlight\nidentity-related regions. Finally, a Semantic Alignment Constraint (SAC) is\ndesigned to facilitate high-level semantic feature interaction. Comprehensive\nexperiments on four CC-ReID datasets indicate that our method outperforms prior\nstate-of-the-art approaches.\n","authors":["Peini Guo","Mengyuan Liu","Hong Liu","Ruijia Fan","Guoquan Wang","Bin He"],"pdf_url":"https://arxiv.org/pdf/2403.08270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08268v1","updated":"2024-03-13T05:44:37Z","published":"2024-03-13T05:44:37Z","title":"Follow-Your-Click: Open-domain Regional Image Animation via Short\n  Prompts","summary":"  Despite recent advances in image-to-video generation, better controllability\nand local animation are less explored. Most existing image-to-video methods are\nnot locally aware and tend to move the entire scene. However, human artists may\nneed to control the movement of different objects or regions. Additionally,\ncurrent I2V methods require users not only to describe the target motion but\nalso to provide redundant detailed descriptions of frame contents. These two\nissues hinder the practical utilization of current I2V tools. In this paper, we\npropose a practical framework, named Follow-Your-Click, to achieve image\nanimation with a simple user click (for specifying what to move) and a short\nmotion prompt (for specifying how to move). Technically, we propose the\nfirst-frame masking strategy, which significantly improves the video generation\nquality, and a motion-augmented module equipped with a short motion prompt\ndataset to improve the short prompt following abilities of our model. To\nfurther control the motion speed, we propose flow-based motion magnitude\ncontrol to control the speed of target movement more precisely. Our framework\nhas simpler yet precise user control and better generation performance than\nprevious methods. Extensive experiments compared with 7 baselines, including\nboth commercial tools and research methods on 8 metrics, suggest the\nsuperiority of our approach. Project Page: https://follow-your-click.github.io/\n","authors":["Yue Ma","Yingqing He","Hongfa Wang","Andong Wang","Chenyang Qi","Chengfei Cai","Xiu Li","Zhifeng Li","Heung-Yeung Shum","Wei Liu","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08268v1.pdf","comment":"Project Page: https://follow-your-click.github.io/ Github Page:\n  https://github.com/mayuelala/FollowYourClick"},{"id":"http://arxiv.org/abs/2310.05737v2","updated":"2024-03-13T05:34:20Z","published":"2023-10-09T14:10:29Z","title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","summary":"  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n","authors":["Lijun Yu","José Lezama","Nitesh B. Gundavarapu","Luca Versari","Kihyuk Sohn","David Minnen","Yong Cheng","Vighnesh Birodkar","Agrim Gupta","Xiuye Gu","Alexander G. Hauptmann","Boqing Gong","Ming-Hsuan Yang","Irfan Essa","David A. Ross","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05737v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08266v1","updated":"2024-03-13T05:33:52Z","published":"2024-03-13T05:33:52Z","title":"Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models","summary":"  While manga is a popular entertainment form, creating manga is tedious,\nespecially adding screentones to the created sketch, namely manga screening.\nUnfortunately, there is no existing method that tailors for automatic manga\nscreening, probably due to the difficulty of generating high-quality shaded\nhigh-frequency screentones. The classic manga screening approaches generally\nrequire user input to provide screentone exemplars or a reference manga image.\nThe recent deep learning models enables the automatic generation by learning\nfrom a large-scale dataset. However, the state-of-the-art models still fail to\ngenerate high-quality shaded screentones due to the lack of a tailored model\nand high-quality manga training data. In this paper, we propose a novel\nsketch-to-manga framework that first generates a color illustration from the\nsketch and then generates a screentoned manga based on the intensity guidance.\nOur method significantly outperforms existing methods in generating\nhigh-quality manga with shaded high-frequency screentones.\n","authors":["Jian Lin","Xueting Liu","Chengze Li","Minshan Xie","Tien-Tsin Wong"],"pdf_url":"https://arxiv.org/pdf/2403.08266v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.08262v1","updated":"2024-03-13T05:25:49Z","published":"2024-03-13T05:25:49Z","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image","summary":"  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1)\\ bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3)\\ the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n","authors":["Minje Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08262v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08261v1","updated":"2024-03-13T05:24:28Z","published":"2024-03-13T05:24:28Z","title":"CoroNetGAN: Controlled Pruning of GANs via Hypernetworks","summary":"  Generative Adversarial Networks (GANs) have proven to exhibit remarkable\nperformance and are widely used across many generative computer vision\napplications. However, the unprecedented demand for the deployment of GANs on\nresource-constrained edge devices still poses a challenge due to huge number of\nparameters involved in the generation process. This has led to focused\nattention on the area of compressing GANs. Most of the existing works use\nknowledge distillation with the overhead of teacher dependency. Moreover, there\nis no ability to control the degree of compression in these methods. Hence, we\npropose CoroNet-GAN for compressing GAN using the combined strength of\ndifferentiable pruning method via hypernetworks. The proposed method provides\nthe advantage of performing controllable compression while training along with\nreducing training time by a substantial factor. Experiments have been done on\nvarious conditional GAN architectures (Pix2Pix and CycleGAN) to signify the\neffectiveness of our approach on multiple benchmark datasets such as\nEdges-to-Shoes, Horse-to-Zebra and Summer-to-Winter. The results obtained\nillustrate that our approach succeeds to outperform the baselines on\nZebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and\n72.3 respectively, yielding high-fidelity images across all the datasets.\nAdditionally, our approach also outperforms the state-of-the-art methods in\nachieving better inference time on various smart-phone chipsets and data-types\nmaking it a feasible solution for deployment on edge devices.\n","authors":["Aman Kumar","Khushboo Anand","Shubham Mandloi","Ashutosh Mishra","Avinash Thakur","Neeraj Kasera","Prathosh A P"],"pdf_url":"https://arxiv.org/pdf/2403.08261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08256v1","updated":"2024-03-13T05:15:43Z","published":"2024-03-13T05:15:43Z","title":"IG-FIQA: Improving Face Image Quality Assessment through Intra-class\n  Variance Guidance robust to Inaccurate Pseudo-Labels","summary":"  In the realm of face image quality assesment (FIQA), method based on sample\nrelative classification have shown impressive performance. However, the quality\nscores used as pseudo-labels assigned from images of classes with low\nintra-class variance could be unrelated to the actual quality in this method.\nTo address this issue, we present IG-FIQA, a novel approach to guide FIQA\ntraining, introducing a weight parameter to alleviate the adverse impact of\nthese classes. This method involves estimating sample intra-class variance at\neach iteration during training, ensuring minimal computational overhead and\nstraightforward implementation. Furthermore, this paper proposes an on-the-fly\ndata augmentation methodology for improved generalization performance in FIQA.\nOn various benchmark datasets, our proposed method, IG-FIQA, achieved novel\nstate-of-the-art (SOTA) performance.\n","authors":["Minsoo Kim","Gi Pyo Nam","Haksub Kim","Haesol Park","Ig-Jae Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08255v1","updated":"2024-03-13T05:13:17Z","published":"2024-03-13T05:13:17Z","title":"Make Me Happier: Evoking Emotions Through Image Diffusion Models","summary":"  Despite the rapid progress in image generation, emotional image editing\nremains under-explored. The semantics, context, and structure of an image can\nevoke emotional responses, making emotional image editing techniques valuable\nfor various real-world applications, including treatment of psychological\ndisorders, commercialization of products, and artistic design. For the first\ntime, we present a novel challenge of emotion-evoked image generation, aiming\nto synthesize images that evoke target emotions while retaining the semantics\nand structures of the original scenes. To address this challenge, we propose a\ndiffusion model capable of effectively understanding and editing source images\nto convey desired emotions and sentiments. Moreover, due to the lack of emotion\nediting datasets, we provide a unique dataset consisting of 340,000 pairs of\nimages and their emotion annotations. Furthermore, we conduct human\npsychophysics experiments and introduce four new evaluation metrics to\nsystematically benchmark all the methods. Experimental results demonstrate that\nour method surpasses all competitive baselines. Our diffusion model is capable\nof identifying emotional cues from original images, editing images that elicit\ndesired emotions, and meanwhile, preserving the semantic structure of the\noriginal images. All code, model, and data will be made public.\n","authors":["Qing Lin","Jingfeng Zhang","Yew Soon Ong","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16510v3","updated":"2024-03-13T05:11:47Z","published":"2023-11-27T12:58:02Z","title":"Source-Free Domain Adaptation with Frozen Multimodal Foundation Model","summary":"  Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a\ntarget domain, with only access to unlabeled target training data and the\nsource model pre-trained on a supervised source domain. Relying on pseudo\nlabeling and/or auxiliary supervision, conventional methods are inevitably\nerror-prone. To mitigate this limitation, in this work we for the first time\nexplore the potentials of off-the-shelf vision-language (ViL) multimodal models\n(e.g.,CLIP) with rich whilst heterogeneous knowledge. We find that directly\napplying the ViL model to the target domain in a zero-shot fashion is\nunsatisfactory, as it is not specialized for this particular task but largely\ngeneric. To make it task specific, we propose a novel Distilling multimodal\nFoundation model(DIFO)approach. Specifically, DIFO alternates between two steps\nduring adaptation: (i) Customizing the ViL model by maximizing the mutual\ninformation with the target model in a prompt learning manner, (ii) Distilling\nthe knowledge of this customized ViL model to the target model. For more\nfine-grained and reliable distillation, we further introduce two effective\nregularization terms, namely most-likely category encouragement and predictive\nconsistency. Extensive experiments show that DIFO significantly outperforms the\nstate-of-the-art alternatives. Code is here\n","authors":["Song Tang","Wenxin Su","Mao Ye","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.16510v3.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08252v1","updated":"2024-03-13T05:08:47Z","published":"2024-03-13T05:08:47Z","title":"PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style\n  Mapping","summary":"  3D scene stylization refers to transform the appearance of a 3D scene to\nmatch a given style image, ensuring that images rendered from different\nviewpoints exhibit the same style as the given style image, while maintaining\nthe 3D consistency of the stylized scene. Several existing methods have\nobtained impressive results in stylizing 3D scenes. However, the models\nproposed by these methods need to be re-trained when applied to a new scene. In\nother words, their models are coupled with a specific scene and cannot adapt to\narbitrary other scenes. To address this issue, we propose a novel 3D scene\nstylization framework to transfer an arbitrary style to an arbitrary scene,\nwithout any style-related or scene-related re-training. Concretely, we first\nmap the appearance of the 3D scene into a 2D style pattern space, which\nrealizes complete disentanglement of the geometry and appearance of the 3D\nscene and makes our model be generalized to arbitrary 3D scenes. Then we\nstylize the appearance of the 3D scene in the 2D style pattern space via a\nprompt-based 2D stylization algorithm. Experimental results demonstrate that\nour proposed framework is superior to SOTA methods in both visual quality and\ngeneralization.\n","authors":["Jiafu Chen","Wei Xing","Jiakai Sun","Tianyi Chu","Yiling Huang","Boyan Ji","Lei Zhao","Huaizhong Lin","Haibo Chen","Zhizhong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08252v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2403.08247v1","updated":"2024-03-13T05:01:37Z","published":"2024-03-13T05:01:37Z","title":"A Dual-domain Regularization Method for Ring Artifact Removal of X-ray\n  CT","summary":"  Ring artifacts in computed tomography images, arising from the undesirable\nresponses of detector units, significantly degrade image quality and diagnostic\nreliability. To address this challenge, we propose a dual-domain regularization\nmodel to effectively remove ring artifacts, while maintaining the integrity of\nthe original CT image. The proposed model corrects the vertical stripe\nartifacts on the sinogram by innovatively updating the response inconsistency\ncompensation coefficients of detector units, which is achieved by employing the\ngroup sparse constraint and the projection-view direction sparse constraint on\nthe stripe artifacts. Simultaneously, we apply the sparse constraint on the\nreconstructed image to further rectified ring artifacts in the image domain.\nThe key advantage of the proposed method lies in considering the relationship\nbetween the response inconsistency compensation coefficients of the detector\nunits and the projection views, which enables a more accurate correction of the\nresponse of the detector units. An alternating minimization method is designed\nto solve the model. Comparative experiments on real photon counting detector\ndata demonstrate that the proposed method not only surpasses existing methods\nin removing ring artifacts but also excels in preserving structural details and\nimage fidelity.\n","authors":["Hongyang Zhu","Xin Lu","Yanwei Qin","Xinran Yu","Tianjiao Sun","Yunsong Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.08247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02279v2","updated":"2024-03-13T04:50:56Z","published":"2023-10-01T05:07:17Z","title":"Consistency Trajectory Models: Learning Probability Flow ODE Trajectory\n  of Diffusion","summary":"  Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion\nmodel sampling at the cost of sample quality but lack a natural way to\ntrade-off quality for speed. To address this limitation, we propose Consistency\nTrajectory Model (CTM), a generalization encompassing CM and score-based models\nas special cases. CTM trains a single neural network that can -- in a single\nforward pass -- output scores (i.e., gradients of log-density) and enables\nunrestricted traversal between any initial and final time along the Probability\nFlow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables\nthe efficient combination of adversarial training and denoising score matching\nloss to enhance performance and achieves new state-of-the-art FIDs for\nsingle-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at\n64x64 resolution (FID 1.92). CTM also enables a new family of sampling schemes,\nboth deterministic and stochastic, involving long jumps along the ODE solution\ntrajectories. It consistently improves sample quality as computational budgets\nincrease, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's\naccess to the score function can streamline the adoption of established\ncontrollable/conditional generation methods from the diffusion community. This\naccess also enables the computation of likelihood. The code is available at\nhttps://github.com/sony/ctm.\n","authors":["Dongjun Kim","Chieh-Hsin Lai","Wei-Hsiang Liao","Naoki Murata","Yuhta Takida","Toshimitsu Uesaka","Yutong He","Yuki Mitsufuji","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2310.02279v2.pdf","comment":"International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2403.08239v1","updated":"2024-03-13T04:45:40Z","published":"2024-03-13T04:45:40Z","title":"Continuous Object State Recognition for Cooking Robots Using Pre-Trained\n  Vision-Language Models and Black-box Optimization","summary":"  The state recognition of the environment and objects by robots is generally\nbased on the judgement of the current state as a classification problem. On the\nother hand, state changes of food in cooking happen continuously and need to be\ncaptured not only at a certain time point but also continuously over time. In\naddition, the state changes of food are complex and cannot be easily described\nby manual programming. Therefore, we propose a method to recognize the\ncontinuous state changes of food for cooking robots through the spoken language\nusing pre-trained large-scale vision-language models. By using models that can\ncompute the similarity between images and texts continuously over time, we can\ncapture the state changes of food while cooking. We also show that by adjusting\nthe weighting of each text prompt based on fitting the similarity changes to a\nsigmoid function and then performing black-box optimization, more accurate and\nrobust continuous state recognition can be achieved. We demonstrate the\neffectiveness and limitations of this method by performing the recognition of\nwater boiling, butter melting, egg cooking, and onion stir-frying.\n","authors":["Kento Kawaharazuka","Naoaki Kanazawa","Yoshiki Obinata","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.08239v1.pdf","comment":"accepted at IEEE Robotics and Automation Letters (RA-L), website -\n  https://haraduka.github.io/continuous-state-recognition/"},{"id":"http://arxiv.org/abs/2403.08236v1","updated":"2024-03-13T04:36:24Z","published":"2024-03-13T04:36:24Z","title":"Point Cloud Compression via Constrained Optimal Transport","summary":"  This paper presents a novel point cloud compression method COT-PCC by\nformulating the task as a constrained optimal transport (COT) problem. COT-PCC\ntakes the bitrate of compressed features as an extra constraint of optimal\ntransport (OT) which learns the distribution transformation between original\nand reconstructed points. Specifically, the formulated COT is implemented with\na generative adversarial network (GAN) and a bitrate loss for training. The\ndiscriminator measures the Wasserstein distance between input and reconstructed\npoints, and a generator calculates the optimal mapping between distributions of\ninput and reconstructed point cloud. Moreover, we introduce a learnable\nsampling module for downsampling in the compression procedure. Extensive\nresults on both sparse and dense point cloud datasets demonstrate that COT-PCC\noutperforms state-of-the-art methods in terms of both CD and PSNR metrics.\nSource codes are available at \\url{https://github.com/cognaclee/PCC-COT}.\n","authors":["Zezeng Li","Weimin Wang","Ziliang Wang","Na Lei"],"pdf_url":"https://arxiv.org/pdf/2403.08236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08227v1","updated":"2024-03-13T04:11:38Z","published":"2024-03-13T04:11:38Z","title":"Matching Non-Identical Objects","summary":"  Not identical but similar objects are everywhere in the world. Examples\ninclude four-legged animals such as dogs and cats, cars of different models,\nakin flowers in various colors, and countless others. In this study, we address\na novel task of matching such non-identical objects. We propose a simple\nweighting scheme of descriptors that enhance various sparse image matching\nmethods, which are originally designed for matching identical objects captured\nfrom different perspectives, and achieve semantically robust matching. The\nexperiments show successful matching between non-identical objects in various\ncases including domain shift. Further, we present a first evaluation of the\nrobustness of the image matching methods under common corruptions, which is a\nsort of domain shift, and the proposed method improves the matching in this\ncase as well.\n","authors":["Yusuke Marumo","Kazuhiko Kawamoto","Hiroshi Kera"],"pdf_url":"https://arxiv.org/pdf/2403.08227v1.pdf","comment":"10+7 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2307.13510v2","updated":"2024-03-13T04:09:04Z","published":"2023-07-25T14:02:02Z","title":"HeightFormer: Explicit Height Modeling without Extra Data for\n  Camera-only 3D Object Detection in Bird's Eye View","summary":"  Vision-based Bird's Eye View (BEV) representation is an emerging perception\nformulation for autonomous driving. The core challenge is to construct BEV\nspace with multi-camera features, which is a one-to-many ill-posed problem.\nDiving into all previous BEV representation generation methods, we found that\nmost of them fall into two types: modeling depths in image views or modeling\nheights in the BEV space, mostly in an implicit way. In this work, we propose\nto explicitly model heights in the BEV space, which needs no extra data like\nLiDAR and can fit arbitrary camera rigs and types compared to modeling depths.\nTheoretically, we give proof of the equivalence between height-based methods\nand depth-based methods. Considering the equivalence and some advantages of\nmodeling heights, we propose HeightFormer, which models heights and\nuncertainties in a self-recursive way. Without any extra data, the proposed\nHeightFormer could estimate heights in BEV accurately. Benchmark results show\nthat the performance of HeightFormer achieves SOTA compared with those\ncamera-only methods.\n","authors":["Yiming Wu","Ruixiang Li","Zequn Qin","Xinhai Zhao","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2307.13510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08224v1","updated":"2024-03-13T04:01:20Z","published":"2024-03-13T04:01:20Z","title":"REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for\n  Noisy Correspondence","summary":"  The presence of noise in acquired data invariably leads to performance\ndegradation in cross-modal matching. Unfortunately, obtaining precise\nannotations in the multimodal field is expensive, which has prompted some\nmethods to tackle the mismatched data pair issue in cross-modal matching\ncontexts, termed as noisy correspondence. However, most of these existing noisy\ncorrespondence methods exhibit the following limitations: a) the problem of\nself-reinforcing error accumulation, and b) improper handling of noisy data\npair. To tackle the two problems, we propose a generalized framework termed as\nRank corrElation and noisy Pair hAlf-replacing wIth memoRy (REPAIR), which\nbenefits from maintaining a memory bank for features of matched pairs.\nSpecifically, we calculate the distances between the features in the memory\nbank and those of the target pair for each respective modality, and use the\nrank correlation of these two sets of distances to estimate the soft\ncorrespondence label of the target pair. Estimating soft correspondence based\non memory bank features rather than using a similarity network can avoid the\naccumulation of errors due to incorrect network identifications. For pairs that\nare completely mismatched, REPAIR searches the memory bank for the most\nmatching feature to replace one feature of one modality, instead of using the\noriginal pair directly or merely discarding the mismatched pair. We conduct\nexperiments on three cross-modal datasets, i.e., Flickr30K, MSCOCO, and CC152K,\nproving the effectiveness and robustness of our REPAIR on synthetic and\nreal-world noise.\n","authors":["Ruochen Zheng","Jiahao Hong","Changxin Gao","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2403.08224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06205v2","updated":"2024-03-13T03:45:29Z","published":"2024-03-10T13:04:01Z","title":"S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes","summary":"  Current 3D stylization methods often assume static scenes, which violates the\ndynamic nature of our real world. To address this limitation, we present\nS-DyRF, a reference-based spatio-temporal stylization method for dynamic neural\nradiance fields. However, stylizing dynamic 3D scenes is inherently challenging\ndue to the limited availability of stylized reference images along the temporal\naxis. Our key insight lies in introducing additional temporal cues besides the\nprovided reference. To this end, we generate temporal pseudo-references from\nthe given stylized reference. These pseudo-references facilitate the\npropagation of style information from the reference to the entire dynamic 3D\nscene. For coarse style transfer, we enforce novel views and times to mimic the\nstyle details present in pseudo-references at the feature level. To preserve\nhigh-frequency details, we create a collection of stylized temporal pseudo-rays\nfrom temporal pseudo-references. These pseudo-rays serve as detailed and\nexplicit stylization guidance for achieving fine style transfer. Experiments on\nboth synthetic and real-world datasets demonstrate that our method yields\nplausible stylized results of space-time view synthesis on dynamic 3D scenes.\n","authors":["Xingyi Li","Zhiguo Cao","Yizheng Wu","Kewei Wang","Ke Xian","Zhe Wang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2403.06205v2.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://xingyi-li.github.io/s-dyrf/"},{"id":"http://arxiv.org/abs/2310.00582v3","updated":"2024-03-13T03:42:31Z","published":"2023-10-01T05:53:15Z","title":"Pink: Unveiling the Power of Referential Comprehension for Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities\nin various multi-modal tasks. Nevertheless, their performance in fine-grained\nimage understanding tasks is still limited. To address this issue, this paper\nproposes a new framework to enhance the fine-grained image understanding\nabilities of MLLMs. Specifically, we present a new method for constructing the\ninstruction tuning dataset at a low cost by leveraging annotations in existing\ndatasets. A self-consistent bootstrapping method is also introduced to extend\nexisting dense object annotations into high-quality\nreferring-expression-bounding-box pairs. These methods enable the generation of\nhigh-quality instruction data which includes a wide range of fundamental\nabilities essential for fine-grained image perception. Moreover, we argue that\nthe visual encoder should be tuned during instruction tuning to mitigate the\ngap between full image perception and fine-grained image perception.\nExperimental results demonstrate the superior performance of our method. For\ninstance, our model exhibits a 5.2% accuracy improvement over Qwen-VL on GQA\nand surpasses the accuracy of Kosmos-2 by 24.7% on RefCOCO_val. We have also\nattained the top rank on the leaderboard of MMBench. This promising performance\nis achieved by training on only publicly available data, making it easily\nreproducible. The models, datasets, and codes are publicly available at\nhttps://github.com/SY-Xuan/Pink.\n","authors":["Shiyu Xuan","Qingpei Guo","Ming Yang","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.00582v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08216v1","updated":"2024-03-13T03:28:39Z","published":"2024-03-13T03:28:39Z","title":"PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise","summary":"  Normalizing flow is a generative modeling approach with efficient sampling.\nHowever, Flow-based models suffer two issues, which are manifold and discrete\ndata. If the target distribution is a manifold, which means the dimension of\nthe latent target distribution and the dimension of the data distribution are\nunmatched, flow-based models might perform badly. Discrete data makes\nflow-based models collapse into a degenerate mixture of point masses. In this\npaper, to sidestep such two issues we propose PaddingFlow, a novel\ndequantization method, which improves normalizing flows with\npadding-dimensional noise. PaddingFlow is easy to implement, computationally\ncheap, widely suitable for various tasks, and generates samples that are\nunbiased estimations of the data. Especially, our method can overcome the\nlimitation of existing dequantization methods that have to change the data\ndistribution, which might degrade performance. We validate our method on the\nmain benchmarks of unconditional density estimation, including five tabular\ndatasets and four image datasets for VAE models, and the IK experiments which\nare conditional density estimation. The results show that PaddingFlow can\nprovide improvement on all tasks in this paper.\n","authors":["Qinglong Meng","Chongkun Xia","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04829v2","updated":"2024-03-13T03:25:32Z","published":"2023-08-09T09:35:16Z","title":"MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner\n  for Open-World Semantic Segmentation","summary":"  Recently, semantic segmentation models trained with image-level text\nsupervision have shown promising results in challenging open-world scenarios.\nHowever, these models still face difficulties in learning fine-grained semantic\nalignment at the pixel level and predicting accurate object masks. To address\nthis issue, we propose MixReorg, a novel and straightforward pre-training\nparadigm for semantic segmentation that enhances a model's ability to\nreorganize patches mixed across images, exploring both local visual relevance\nand global semantic coherence. Our approach involves generating fine-grained\npatch-text pairs data by mixing image patches while preserving the\ncorrespondence between patches and text. The model is then trained to minimize\nthe segmentation loss of the mixed images and the two contrastive losses of the\noriginal and restored features. With MixReorg as a mask learner, conventional\ntext-supervised semantic segmentation models can achieve highly generalizable\npixel-semantic alignment ability, which is crucial for open-world segmentation.\nAfter training with large-scale image-text data, MixReorg models can be applied\ndirectly to segment visual objects of arbitrary categories, without the need\nfor further fine-tuning. Our proposed framework demonstrates strong performance\non popular zero-shot semantic segmentation benchmarks, outperforming GroupViT\nby significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012,\nPASCAL Context, MS COCO, and ADE20K, respectively.\n","authors":["Kaixin Cai","Pengzhen Ren","Yi Zhu","Hang Xu","Jianzhuang Liu","Changlin Li","Guangrun Wang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2308.04829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08215v1","updated":"2024-03-13T03:24:36Z","published":"2024-03-13T03:24:36Z","title":"LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual\n  Semantic Segmentation for Autonomous Driving","summary":"  Despite the impressive performance achieved by data-fusion networks with\nduplex encoders for visual semantic segmentation, they become ineffective when\nspatial geometric data are not available. Implicitly infusing the spatial\ngeometric prior knowledge acquired by a duplex-encoder teacher model into a\nsingle-encoder student model is a practical, albeit less explored research\navenue. This paper delves into this topic and resorts to knowledge distillation\napproaches to address this problem. We introduce the Learning to Infuse \"X\"\n(LIX) framework, with novel contributions in both logit distillation and\nfeature distillation aspects. We present a mathematical proof that underscores\nthe limitation of using a single fixed weight in decoupled knowledge\ndistillation and introduce a logit-wise dynamic weight controller as a solution\nto this issue. Furthermore, we develop an adaptively-recalibrated feature\ndistillation algorithm, including two technical novelties: feature\nrecalibration via kernel regression and in-depth feature consistency\nquantification via centered kernel alignment. Extensive experiments conducted\nwith intermediate-fusion and late-fusion networks across various public\ndatasets provide both quantitative and qualitative evaluations, demonstrating\nthe superior performance of our LIX framework when compared to other\nstate-of-the-art approaches.\n","authors":["Sicen Guo","Zhiyuan Wu","Qijun Chen","Ioannis Pitas","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2403.08215v1.pdf","comment":"13 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.08214v1","updated":"2024-03-13T03:23:50Z","published":"2024-03-13T03:23:50Z","title":"P2LHAP:Wearable sensor-based human activity recognition, segmentation\n  and forecast through Patch-to-Label Seq2Seq Transformer","summary":"  Traditional deep learning methods struggle to simultaneously segment,\nrecognize, and forecast human activities from sensor data. This limits their\nusefulness in many fields such as healthcare and assisted living, where\nreal-time understanding of ongoing and upcoming activities is crucial. This\npaper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles\nall three tasks in a efficient single-task model. P2LHAP divides sensor data\nstreams into a sequence of \"patches\", served as input tokens, and outputs a\nsequence of patch-level activity labels including the predicted future\nactivities. A unique smoothing technique based on surrounding patch labels, is\nproposed to identify activity boundaries accurately. Additionally, P2LHAP\nlearns patch-level representation by sensor signal channel-independent\nTransformer encoders and decoders. All channels share embedding and Transformer\nweights across all sequences. Evaluated on three public datasets, P2LHAP\nsignificantly outperforms the state-of-the-art in all three tasks,\ndemonstrating its effectiveness and potential for real-world applications.\n","authors":["Shuangjian Li","Tao Zhu","Mingxing Nie","Huansheng Ning","Zhenyu Liu","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07420v2","updated":"2024-03-13T03:22:36Z","published":"2024-03-12T08:57:29Z","title":"DragAnything: Motion Control for Anything using Entity Representation","summary":"  We introduce DragAnything, which utilizes a entity representation to achieve\nmotion control for any object in controllable video generation. Comparison to\nexisting motion control methods, DragAnything offers several advantages.\nFirstly, trajectory-based is more userfriendly for interaction, when acquiring\nother guidance signals (e.g., masks, depth maps) is labor-intensive. Users only\nneed to draw a line (trajectory) during interaction. Secondly, our entity\nrepresentation serves as an open-domain embedding capable of representing any\nobject, enabling the control of motion for diverse entities, including\nbackground. Lastly, our entity representation allows simultaneous and distinct\nmotion control for multiple objects. Extensive experiments demonstrate that our\nDragAnything achieves state-of-the-art performance for FVD, FID, and User\nStudy, particularly in terms of object motion control, where our method\nsurpasses the previous methods (e.g., DragNUWA) by 26% in human voting.\n","authors":["Weijia Wu","Zhuang Li","Yuchao Gu","Rui Zhao","Yefei He","David Junhao Zhang","Mike Zheng Shou","Yan Li","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07420v2.pdf","comment":"The project website is at:\n  https://weijiawu.github.io/draganything_page/ . The code is at:\n  https://github.com/showlab/DragAnything"},{"id":"http://arxiv.org/abs/2403.06452v2","updated":"2024-03-13T03:14:53Z","published":"2024-03-11T06:03:31Z","title":"Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for\n  Text-Guided QR Code Generation","summary":"  In the digital era, QR codes serve as a linchpin connecting virtual and\nphysical realms. Their pervasive integration across various applications\nhighlights the demand for aesthetically pleasing codes without compromised\nscannability. However, prevailing methods grapple with the intrinsic challenge\nof balancing customization and scannability. Notably, stable-diffusion models\nhave ushered in an epoch of high-quality, customizable content generation. This\npaper introduces Text2QR, a pioneering approach leveraging these advancements\nto address a fundamental challenge: concurrently achieving user-defined\naesthetics and scanning robustness. To ensure stable generation of aesthetic QR\ncodes, we introduce the QR Aesthetic Blueprint (QAB) module, generating a\nblueprint image exerting control over the entire generation process.\nSubsequently, the Scannability Enhancing Latent Refinement (SELR) process\nrefines the output iteratively in the latent space, enhancing scanning\nrobustness. This approach harnesses the potent generation capabilities of\nstable-diffusion models, navigating the trade-off between image aesthetics and\nQR code scannability. Our experiments demonstrate the seamless fusion of visual\nappeal with the practical utility of aesthetic QR codes, markedly outperforming\nprior methods. Codes are available at \\url{https://github.com/mulns/Text2QR}\n","authors":["Guangyang Wu","Xiaohong Liu","Jun Jia","Xuehao Cui","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2403.06452v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08208v1","updated":"2024-03-13T03:10:11Z","published":"2024-03-13T03:10:11Z","title":"Advancing Security in AI Systems: A Novel Approach to Detecting\n  Backdoors in Deep Neural Networks","summary":"  In the rapidly evolving landscape of communication and network security, the\nincreasing reliance on deep neural networks (DNNs) and cloud services for data\nprocessing presents a significant vulnerability: the potential for backdoors\nthat can be exploited by malicious actors. Our approach leverages advanced\ntensor decomposition algorithms Independent Vector Analysis (IVA), Multiset\nCanonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2)\nto meticulously analyze the weights of pre-trained DNNs and distinguish between\nbackdoored and clean models effectively. The key strengths of our method lie in\nits domain independence, adaptability to various network architectures, and\nability to operate without access to the training data of the scrutinized\nmodels. This not only ensures versatility across different application\nscenarios but also addresses the challenge of identifying backdoors without\nprior knowledge of the specific triggers employed to alter network behavior. We\nhave applied our detection pipeline to three distinct computer vision datasets,\nencompassing both image classification and object detection tasks. The results\ndemonstrate a marked improvement in both accuracy and efficiency over existing\nbackdoor detection methods. This advancement enhances the security of deep\nlearning and AI in networked systems, providing essential cybersecurity against\nevolving threats in emerging technologies.\n","authors":["Khondoker Murad Hossain","Tim Oates"],"pdf_url":"https://arxiv.org/pdf/2403.08208v1.pdf","comment":"6 pages, Accepted at the International Conference on Communications\n  2024. arXiv admin note: text overlap with arXiv:2212.08121"},{"id":"http://arxiv.org/abs/2402.18133v2","updated":"2024-03-13T03:07:08Z","published":"2024-02-28T07:54:50Z","title":"Classes Are Not Equal: An Empirical Study on Image Recognition Fairness","summary":"  In this paper, we present an empirical study on image recognition fairness,\ni.e., extreme class accuracy disparity on balanced data like ImageNet. We\nexperimentally demonstrate that classes are not equal and the fairness issue is\nprevalent for image classification models across various datasets, network\narchitectures, and model capacities. Moreover, several intriguing properties of\nfairness are identified. First, the unfairness lies in problematic\nrepresentation rather than classifier bias. Second, with the proposed concept\nof Model Prediction Bias, we investigate the origins of problematic\nrepresentation during optimization. Our findings reveal that models tend to\nexhibit greater prediction biases for classes that are more challenging to\nrecognize. It means that more other classes will be confused with harder\nclasses. Then the False Positives (FPs) will dominate the learning in\noptimization, thus leading to their poor accuracy. Further, we conclude that\ndata augmentation and representation learning algorithms improve overall\nperformance by promoting fairness to some degree in image classification. The\nCode is available at\nhttps://github.com/dvlab-research/Parametric-Contrastive-Learning.\n","authors":["Jiequan Cui","Beier Zhu","Xin Wen","Xiaojuan Qi","Bei Yu","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.18133v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2310.05375v3","updated":"2024-03-13T02:56:47Z","published":"2023-10-09T03:11:08Z","title":"IPDreamer: Appearance-Controllable 3D Object Generation with Image\n  Prompts","summary":"  Recent advances in 3D generation have been remarkable, with methods such as\nDreamFusion leveraging large-scale text-to-image diffusion-based models to\nsupervise 3D generation. These methods enable the synthesis of detailed and\nphotorealistic textured objects. However, the appearance of 3D objects produced\nby these text-to-3D methods is unpredictable, and it is hard for the\nsingle-image-to-3D methods to deal with complex images, thus posing a challenge\nin generating appearance-controllable 3D objects. To achieve controllable\ncomplex 3D object synthesis, we introduce IPDreamer, a novel approach that\nincorporates $\\textbf{I}$mage $\\textbf{P}$rompts to provide specific and\ncomprehensive appearance information for 3D object generation. Our results\ndemonstrate that IPDreamer effectively generates high-quality 3D objects that\nare consistent with both the provided text and the appearance of complex image\nprompts, demonstrating its promising capability in appearance-controllable 3D\nobject generation. Our code is avaliable at\nhttps://github.com/zengbohan0217/IPDreamer.\n","authors":["Bohan Zeng","Shanglin Li","Yutang Feng","Hong Li","Sicheng Gao","Jiaming Liu","Huaxia Li","Xu Tang","Jianzhuang Liu","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05375v3.pdf","comment":"25 pages, 13 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.08737v1","updated":"2024-03-13T17:38:05Z","published":"2024-03-13T17:38:05Z","title":"ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation","summary":"  Existing Machine Learning approaches for local citation recommendation\ndirectly map or translate a query, which is typically a claim or an entity\nmention, to citation-worthy research papers. Within such a formulation, it is\nchallenging to pinpoint why one should cite a specific research paper for a\nparticular query, leading to limited recommendation interpretability. To\nalleviate this, we introduce the evidence-grounded local citation\nrecommendation task, where the target latent space comprises evidence spans for\nrecommending specific papers. Using a distantly-supervised evidence retrieval\nand multi-step re-ranking framework, our proposed system, ILCiteR, recommends\npapers to cite for a query grounded on similar evidence spans extracted from\nthe existing research literature. Unlike past formulations that simply output\nrecommendations, ILCiteR retrieves ranked lists of evidence span and\nrecommended paper pairs. Secondly, previously proposed neural models for\ncitation recommendation require expensive training on massive labeled data,\nideally after every significant update to the pool of candidate papers. In\ncontrast, ILCiteR relies solely on distant supervision from a dynamic evidence\ndatabase and pre-trained Transformer-based Language Models without any model\ntraining. We contribute a novel dataset for the evidence-grounded local\ncitation recommendation task and demonstrate the efficacy of our proposed\nconditional neural rank-ensembling approach for re-ranking evidence spans.\n","authors":["Sayar Ghosh Roy","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2403.08737v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08475v1","updated":"2024-03-13T12:39:11Z","published":"2024-03-13T12:39:11Z","title":"NLQxform-UI: A Natural Language Interface for Querying DBLP\n  Interactively","summary":"  In recent years, the DBLP computer science bibliography has been prominently\nused for searching scholarly information, such as publications, scholars, and\nvenues. However, its current search service lacks the capability to handle\ncomplex queries, which limits the usability of DBLP. In this paper, we present\nNLQxform-UI, a web-based natural language interface that enables users to query\nDBLP directly with complex natural language questions. NLQxform-UI\nautomatically translates given questions into SPARQL queries and executes the\nqueries over the DBLP knowledge graph to retrieve answers. The querying process\nis presented to users in an interactive manner, which improves the transparency\nof the system and helps examine the returned answers. Also, intermediate\nresults in the querying process can be previewed and manually altered to\nimprove the accuracy of the system. NLQxform-UI has been completely\nopen-sourced: https://github.com/ruijie-wang-uzh/NLQxform-UI.\n","authors":["Ruijie Wang","Zhiruo Zhang","Luca Rossetto","Florian Ruosch","Abraham Bernstein"],"pdf_url":"https://arxiv.org/pdf/2403.08475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08319v1","updated":"2024-03-13T08:02:23Z","published":"2024-03-13T08:02:23Z","title":"Knowledge Conflicts for LLMs: A Survey","summary":"  This survey provides an in-depth analysis of knowledge conflicts for large\nlanguage models (LLMs), highlighting the complex challenges they encounter when\nblending contextual and parametric knowledge. Our focus is on three categories\nof knowledge conflicts: context-memory, inter-context, and intra-memory\nconflict. These conflicts can significantly impact the trustworthiness and\nperformance of LLMs, especially in real-world applications where noise and\nmisinformation are common. By categorizing these conflicts, exploring the\ncauses, examining the behaviors of LLMs under such conflicts, and reviewing\navailable solutions, this survey aims to shed light on strategies for improving\nthe robustness of LLMs, thereby serving as a valuable resource for advancing\nresearch in this evolving area.\n","authors":["Rongwu Xu","Zehan Qi","Cunxiang Wang","Hongru Wang","Yue Zhang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08246v1","updated":"2024-03-13T05:00:42Z","published":"2024-03-13T05:00:42Z","title":"Towards Unified Modeling for Positive and Negative Preferences in\n  Sign-Aware Recommendation","summary":"  Recently, sign-aware graph recommendation has drawn much attention as it will\nlearn users' negative preferences besides positive ones from both positive and\nnegative interactions (i.e., links in a graph) with items. To accommodate the\ndifferent semantics of negative and positive links, existing works utilize two\nindependent encoders to model users' positive and negative preferences,\nrespectively. However, these approaches cannot learn the negative preferences\nfrom high-order heterogeneous interactions between users and items formed by\nmultiple links with different signs, resulting in inaccurate and incomplete\nnegative user preferences. To cope with these intractable issues, we propose a\nnovel \\textbf{L}ight \\textbf{S}igned \\textbf{G}raph Convolution Network\nspecifically for \\textbf{Rec}ommendation (\\textbf{LSGRec}), which adopts a\nunified modeling approach to simultaneously model high-order users' positive\nand negative preferences on a signed user-item interaction graph. Specifically,\nfor the negative preferences within high-order heterogeneous interactions,\nfirst-order negative preferences are captured by the negative links, while\nhigh-order negative preferences are propagated along positive edges. Then,\nrecommendation results are generated based on positive preferences and\noptimized with negative ones. Finally, we train representations of users and\nitems through different auxiliary tasks. Extensive experiments on three\nreal-world datasets demonstrate that our method outperforms existing baselines\nregarding performance and computational efficiency. Our code is available at\n\\url{https://anonymous.4open.science/r/LSGRec-BB95}.\n","authors":["Yuting Liu","Yizhou Dang","Yuliang Liang","Qiang Liu","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08206v1","updated":"2024-03-13T03:03:15Z","published":"2024-03-13T03:03:15Z","title":"Discrete Semantic Tokenization for Deep CTR Prediction","summary":"  Incorporating item content information into click-through rate (CTR)\nprediction models remains a challenge, especially with the time and space\nconstraints of industrial scenarios. The content-encoding paradigm, which\nintegrates user and item encoders directly into CTR models, prioritizes space\nover time. In contrast, the embedding-based paradigm transforms item and user\nsemantics into latent embeddings and then caches them, prioritizes space over\ntime. In this paper, we introduce a new semantic-token paradigm and propose a\ndiscrete semantic tokenization approach, namely UIST, for user and item\nrepresentation. UIST facilitates swift training and inference while maintaining\na conservative memory footprint. Specifically, UIST quantizes dense embedding\nvectors into discrete tokens with shorter lengths and employs a hierarchical\nmixture inference module to weigh the contribution of each user--item token\npair. Our experimental results on news recommendation showcase the\neffectiveness and efficiency (about 200-fold space compression) of UIST for CTR\nprediction.\n","authors":["Qijiong Liu","Hengchang Hu","Jiahao Wu","Jieming Zhu","Min-Yen Kan","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08206v1.pdf","comment":"TheWebConf 2024 accepted paper"},{"id":"http://arxiv.org/abs/2403.06747v3","updated":"2024-03-13T02:44:07Z","published":"2024-03-11T14:13:41Z","title":"MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation","summary":"  Compared to business-to-consumer (B2C) e-commerce systems,\nconsumer-to-consumer (C2C) e-commerce platforms usually encounter the\nlimited-stock problem, that is, a product can only be sold one time in a C2C\nsystem. This poses several unique challenges for click-through rate (CTR)\nprediction. Due to limited user interactions for each product (i.e. item), the\ncorresponding item embedding in the CTR model may not easily converge. This\nmakes the conventional sequence modeling based approaches cannot effectively\nutilize user history information since historical user behaviors contain a\nmixture of items with different volume of stocks. Particularly, the attention\nmechanism in a sequence model tends to assign higher score to products with\nmore accumulated user interactions, making limited-stock products being ignored\nand contribute less to the final output. To this end, we propose the Meta-Split\nNetwork (MSNet) to split user history sequence regarding to the volume of stock\nfor each product, and adopt differentiated modeling approaches for different\nsequences. As for the limited-stock products, a meta-learning approach is\napplied to address the problem of inconvergence, which is achieved by designing\nmeta scaling and shifting networks with ID and side information. In addition,\ntraditional approach can hardly update item embedding once the product is\nconsumed. Thereby, we propose an auxiliary loss that makes the parameters\nupdatable even when the product is no longer in distribution. To the best of\nour knowledge, this is the first solution addressing the recommendation of\nlimited-stock product. Experimental results on the production dataset and\nonline A/B testing demonstrate the effectiveness of our proposed method.\n","authors":["Wenhao Wu","Jialiang Zhou","Ailong He","Shuguang Han","Jufeng Chen","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.06747v3.pdf","comment":"WWW'24 Industry Track"},{"id":"http://arxiv.org/abs/2403.08970v1","updated":"2024-03-13T21:30:01Z","published":"2024-03-13T21:30:01Z","title":"Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval\n  through Self-Supervision by Meticulous Pseudo-Relevance Labeling","summary":"  Recent studies have demonstrated that the ability of dense retrieval models\nto generalize to target domains with different distributions is limited, which\ncontrasts with the results obtained with interaction-based models. Prior\nattempts to mitigate this challenge involved leveraging adversarial learning\nand query generation approaches, but both approaches nevertheless resulted in\nlimited improvements. In this paper, we propose to combine the query-generation\napproach with a self-supervision approach in which pseudo-relevance labels are\nautomatically generated on the target domain. To accomplish this, a T5-3B model\nis utilized for pseudo-positive labeling, and meticulous hard negatives are\nchosen. We also apply this strategy on conversational dense retrieval model for\nconversational search. A similar pseudo-labeling approach is used, but with the\naddition of a query-rewriting module to rewrite conversational queries for\nsubsequent labeling. This proposed approach enables a model's domain adaptation\nwith real queries and documents from the target dataset. Experiments on\nstandard dense retrieval and conversational dense retrieval models both\ndemonstrate improvements on baseline models when they are fine-tuned on the\npseudo-relevance labeled data.\n","authors":["Minghan Li","Eric Gaussier"],"pdf_url":"https://arxiv.org/pdf/2403.08970v1.pdf","comment":"12 pages, accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2206.02631v4","updated":"2024-03-13T20:51:25Z","published":"2022-05-31T01:54:29Z","title":"Contemporary Recommendation Systems on Big Data and Their Applications:\n  A Survey","summary":"  This survey paper conducts a comprehensive analysis of the evolution and\ncontemporary landscape of recommendation systems, which have been extensively\nincorporated across a myriad of web applications. It delves into the\nprogression of personalized recommendation methodologies tailored for online\nproducts or services, organizing the array of recommendation techniques into\nfour main categories: content-based, collaborative filtering, knowledge-based,\nand hybrid approaches, each designed to cater to specific contexts. The\ndocument provides an in-depth review of both the historical underpinnings and\nthe cutting-edge innovations in the domain of recommendation systems, with a\nspecial focus on implementations leveraging big data analytics. The paper also\nhighlights the utilization of prominent datasets such as MovieLens, Amazon\nReviews, Netflix Prize, Last.fm, and Yelp in evaluating recommendation\nalgorithms. It further outlines and explores the predominant challenges\nencountered in the current generation of recommendation systems, including\nissues related to data sparsity, scalability, and the imperative for\ndiversified recommendation outputs. The survey underscores these challenges as\npromising directions for subsequent research endeavors within the discipline.\nAdditionally, the paper examines various real-life applications driven by\nrecommendation systems, addressing the hurdles involved in seamlessly\nintegrating these systems into everyday life. Ultimately, the survey\nunderscores how the advancements in recommendation systems, propelled by big\ndata technologies, have the potential to significantly enhance real-world\nexperiences.\n","authors":["Ziyuan Xia","Anchen Sun","Jingyi Xu","Yuanzhe Peng","Rui Ma","Minghui Cheng"],"pdf_url":"https://arxiv.org/pdf/2206.02631v4.pdf","comment":"34 pages, 8 figures, 2 table"},{"id":"http://arxiv.org/abs/2403.08851v1","updated":"2024-03-13T18:00:00Z","published":"2024-03-13T18:00:00Z","title":"PAPERCLIP: Associating Astronomical Observations and Natural Language\n  with Multi-Modal Models","summary":"  We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation\nfor Contrastive Language-Image Pre-training), a method which associates\nastronomical observations imaged by telescopes with natural language using a\nneural network model. The model is fine-tuned from a pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) model using successful observing proposal\nabstracts and corresponding downstream observations, with the abstracts\noptionally summarized via guided generation using large language models (LLMs).\nUsing observations from the Hubble Space Telescope (HST) as an example, we show\nthat the fine-tuned model embodies a meaningful joint representation between\nobservations and natural language through tests targeting image retrieval\n(i.e., finding the most relevant observations using natural language queries)\nand description retrieval (i.e., querying for astrophysical object classes and\nuse cases most relevant to a given observation). Our study demonstrates the\npotential for using generalist foundation models rather than task-specific\nmodels for interacting with astronomical data by leveraging text as an\ninterface.\n","authors":["Siddharth Mishra-Sharma","Yiding Song","Jesse Thaler"],"pdf_url":"https://arxiv.org/pdf/2403.08851v1.pdf","comment":"17+6 pages, 3+1 figures, 5+2 tables"},{"id":"http://arxiv.org/abs/2403.08844v1","updated":"2024-03-13T15:54:49Z","published":"2024-03-13T15:54:49Z","title":"AcademiaOS: Automating Grounded Theory Development in Qualitative\n  Research with Large Language Models","summary":"  AcademiaOS is a first attempt to automate grounded theory development in\nqualitative research with large language models. Using recent large language\nmodels' language understanding, generation, and reasoning capabilities,\nAcademiaOS codes curated qualitative raw data such as interview transcripts and\ndevelops themes and dimensions to further develop a grounded theoretical model,\naffording novel insights. A user study (n=19) suggests that the system finds\nacceptance in the academic community and exhibits the potential to augment\nhumans in qualitative research. AcademiaOS has been made open-source for others\nto build upon and adapt to their use cases.\n","authors":["Thomas Übellacker"],"pdf_url":"https://arxiv.org/pdf/2403.08844v1.pdf","comment":"Live version: https://academia-os.org Source code:\n  https://github.com/thomasuebi/academia-os"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.08763v1","updated":"2024-03-13T17:58:57Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08757v1","updated":"2024-03-13T17:55:34Z","published":"2024-03-13T17:55:34Z","title":"Efficient Combinatorial Optimization via Heat Diffusion","summary":"  Combinatorial optimization problems are widespread but inherently challenging\ndue to their discrete nature.The primary limitation of existing methods is that\nthey can only access a small fraction of the solution space at each iteration,\nresulting in limited efficiency for searching the global optimal. To overcome\nthis challenge, diverging from conventional efforts of expanding the solver's\nsearch scope, we focus on enabling information to actively propagate to the\nsolver through heat diffusion. By transforming the target function while\npreserving its optima, heat diffusion facilitates information flow from distant\nregions to the solver, providing more efficient navigation. Utilizing heat\ndiffusion, we propose a framework for solving general combinatorial\noptimization problems. The proposed methodology demonstrates superior\nperformance across a range of the most challenging and widely encountered\ncombinatorial optimizations. Echoing recent advancements in harnessing\nthermodynamics for generative artificial intelligence, our study further\nreveals its significant potential in advancing combinatorial optimization.\n","authors":["Hengyuan Ma","Wenlian Lu","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2403.08757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08755v1","updated":"2024-03-13T17:53:47Z","published":"2024-03-13T17:53:47Z","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","summary":"  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n","authors":["Feng Cheng","Ziyang Wang","Yi-Lin Sung","Yan-Bo Lin","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2403.08755v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2403.08750v1","updated":"2024-03-13T17:51:02Z","published":"2024-03-13T17:51:02Z","title":"Neural reproducing kernel Banach spaces and representer theorems for\n  deep networks","summary":"  Studying the function spaces defined by neural networks helps to understand\nthe corresponding learning models and their inductive bias. While in some\nlimits neural networks correspond to function spaces that are reproducing\nkernel Hilbert spaces, these regimes do not capture the properties of the\nnetworks used in practice. In contrast, in this paper we show that deep neural\nnetworks define suitable reproducing kernel Banach spaces.\n  These spaces are equipped with norms that enforce a form of sparsity,\nenabling them to adapt to potential latent structures within the input data and\ntheir representations. In particular, leveraging the theory of reproducing\nkernel Banach spaces, combined with variational results, we derive representer\ntheorems that justify the finite architectures commonly employed in\napplications. Our study extends analogous results for shallow networks and can\nbe seen as a step towards considering more practically plausible neural\narchitectures.\n","authors":["Francesca Bartolucci","Ernesto De Vito","Lorenzo Rosasco","Stefano Vigogna"],"pdf_url":"https://arxiv.org/pdf/2403.08750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08743v1","updated":"2024-03-13T17:46:28Z","published":"2024-03-13T17:46:28Z","title":"Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing\n  Framework","summary":"  Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2212.14511v2","updated":"2024-03-13T17:44:52Z","published":"2022-12-30T01:42:04Z","title":"Can Direct Latent Model Learning Solve Linear Quadratic Gaussian\n  Control?","summary":"  We study the task of learning state representations from potentially\nhigh-dimensional observations, with the goal of controlling an unknown\npartially observable system. We pursue a direct latent model learning approach,\nwhere a dynamic model in some latent state space is learned by predicting\nquantities directly related to planning (e.g., costs) without reconstructing\nthe observations. In particular, we focus on an intuitive cost-driven state\nrepresentation learning method for solving Linear Quadratic Gaussian (LQG)\ncontrol, one of the most fundamental partially observable control problems. As\nour main results, we establish finite-sample guarantees of finding a\nnear-optimal state representation function and a near-optimal controller using\nthe directly learned latent model. To the best of our knowledge, despite\nvarious empirical successes, prior to this work it was unclear if such a\ncost-driven latent model learner enjoys finite-sample guarantees. Our work\nunderscores the value of predicting multi-step costs, an idea that is key to\nour theory, and notably also an idea that is known to be empirically valuable\nfor learning state representations.\n","authors":["Yi Tian","Kaiqing Zhang","Russ Tedrake","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2212.14511v2.pdf","comment":"37 pages; Updated structure and proofs"},{"id":"http://arxiv.org/abs/2403.08741v1","updated":"2024-03-13T17:44:16Z","published":"2024-03-13T17:44:16Z","title":"Learning How to Strategically Disclose Information","summary":"  Strategic information disclosure, in its simplest form, considers a game\nbetween an information provider (sender) who has access to some private\ninformation that an information receiver is interested in. While the receiver\ntakes an action that affects the utilities of both players, the sender can\ndesign information (or modify beliefs) of the receiver through signal\ncommitment, hence posing a Stackelberg game. However, obtaining a Stackelberg\nequilibrium for this game traditionally requires the sender to have access to\nthe receiver's objective. In this work, we consider an online version of\ninformation design where a sender interacts with a receiver of an unknown type\nwho is adversarially chosen at each round. Restricting attention to Gaussian\nprior and quadratic costs for the sender and the receiver, we show that\n$\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback,\nwhere $T$ is the total number of interactions between the sender and the\nreceiver. Further, we propose a novel parametrization that allows the sender to\nachieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function.\nWe then consider the Bayesian Persuasion problem with an additional cost term\nin the objective function, which penalizes signaling policies that are more\ninformative and obtain $\\mathcal{O}(\\log(T))$ regret. Finally, we establish a\nsublinear regret bound for the partial information feedback setting and provide\nsimulations to support our theoretical results.\n","authors":["Raj Kiriti Velicheti","Melih Bastopcu","S. Rasoul Etesami","Tamer Başar"],"pdf_url":"https://arxiv.org/pdf/2403.08741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04475v2","updated":"2024-03-13T17:40:04Z","published":"2023-10-06T05:27:28Z","title":"Demystifying Embedding Spaces using Large Language Models","summary":"  Embeddings have become a pivotal means to represent complex, multi-faceted\ninformation about entities, concepts, and relationships in a condensed and\nuseful format. Nevertheless, they often preclude direct interpretation. While\ndownstream tasks make use of these compressed representations, meaningful\ninterpretation usually requires visualization using dimensionality reduction or\nspecialized machine learning interpretability methods. This paper addresses the\nchallenge of making such embeddings more interpretable and broadly useful, by\nemploying Large Language Models (LLMs) to directly interact with embeddings --\ntransforming abstract vectors into understandable narratives. By injecting\nembeddings into LLMs, we enable querying and exploration of complex embedding\ndata. We demonstrate our approach on a variety of diverse tasks, including:\nenhancing concept activation vectors (CAVs), communicating novel embedded\nentities, and decoding user preferences in recommender systems. Our work\ncouples the immense information potential of embeddings with the interpretative\npower of LLMs.\n","authors":["Guy Tennenholtz","Yinlam Chow","Chih-Wei Hsu","Jihwan Jeong","Lior Shani","Azamat Tulepbergenov","Deepak Ramachandran","Martin Mladenov","Craig Boutilier"],"pdf_url":"https://arxiv.org/pdf/2310.04475v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2312.07511v2","updated":"2024-03-13T17:38:27Z","published":"2023-12-12T18:44:19Z","title":"A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems","summary":"  Recent advances in computational modelling of atomic systems, spanning\nmolecules, proteins, and materials, represent them as geometric graphs with\natoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric\nattributes transform according to the inherent physical symmetries of 3D atomic\nsystems, including rotations and translations in Euclidean space, as well as\nnode permutations. In recent years, Geometric Graph Neural Networks have\nemerged as the preferred machine learning architecture powering applications\nranging from protein structure prediction to molecular simulations and material\ngeneration. Their specificity lies in the inductive biases they leverage - such\nas physical symmetries and chemical properties - to learn informative\nrepresentations of these geometric graphs.\n  In this opinionated paper, we provide a comprehensive and self-contained\noverview of the field of Geometric GNNs for 3D atomic systems. We cover\nfundamental background material and introduce a pedagogical taxonomy of\nGeometric GNN architectures: (1) invariant networks, (2) equivariant networks\nin Cartesian basis, (3) equivariant networks in spherical basis, and (4)\nunconstrained networks. Additionally, we outline key datasets and application\nareas and suggest future research directions. The objective of this work is to\npresent a structured perspective on the field, making it accessible to\nnewcomers and aiding practitioners in gaining an intuition for its mathematical\nabstractions.\n","authors":["Alexandre Duval","Simon V. Mathis","Chaitanya K. Joshi","Victor Schmidt","Santiago Miret","Fragkiskos D. Malliaros","Taco Cohen","Pietro Liò","Yoshua Bengio","Michael Bronstein"],"pdf_url":"https://arxiv.org/pdf/2312.07511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08728v1","updated":"2024-03-13T17:28:20Z","published":"2024-03-13T17:28:20Z","title":"Ambient Diffusion Posterior Sampling: Solving Inverse Problems with\n  Diffusion Models trained on Corrupted Data","summary":"  We provide a framework for solving inverse problems with diffusion models\nlearned from linearly corrupted data. Our method, Ambient Diffusion Posterior\nSampling (A-DPS), leverages a generative model pre-trained on one type of\ncorruption (e.g. image inpainting) to perform posterior sampling conditioned on\nmeasurements from a potentially different forward process (e.g. image\nblurring). We test the efficacy of our approach on standard natural image\ndatasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes\noutperform models trained on clean data for several image restoration tasks in\nboth speed and performance. We further extend the Ambient Diffusion framework\nto train MRI models with access only to Fourier subsampled multi-coil MRI\nmeasurements at various acceleration factors (R=2, 4, 6, 8). We again observe\nthat models trained on highly subsampled data are better priors for solving\ninverse problems in the high acceleration regime than models trained on fully\nsampled data. We open-source our code and the trained Ambient Diffusion MRI\nmodels: https://github.com/utcsilab/ambient-diffusion-mri .\n","authors":["Asad Aali","Giannis Daras","Brett Levac","Sidharth Kumar","Alexandros G. Dimakis","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2403.08728v1.pdf","comment":"Pre-print, work in progress"},{"id":"http://arxiv.org/abs/2306.14306v2","updated":"2024-03-13T17:20:27Z","published":"2023-06-25T18:29:29Z","title":"Adaptive Sharpness-Aware Pruning for Robust Sparse Networks","summary":"  Robustness and compactness are two essential attributes of deep learning\nmodels that are deployed in the real world. The goals of robustness and\ncompactness may seem to be at odds, since robustness requires generalization\nacross domains, while the process of compression exploits specificity in one\ndomain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies\nthese goals through the lens of network sharpness. The AdaSAP method produces\nsparse networks that are robust to input variations which are unseen at\ntraining time. We achieve this by strategically incorporating weight\nperturbations in order to optimize the loss landscape. This allows the model to\nbe both primed for pruning and regularized for improved robustness. AdaSAP\nimproves the robust accuracy of pruned models on image classification by up to\n+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a\ncorrupted Pascal VOC dataset, over a wide range of compression ratios, pruning\ncriteria, and network architectures, outperforming recent pruning art by large\nmargins.\n","authors":["Anna Bair","Hongxu Yin","Maying Shen","Pavlo Molchanov","Jose Alvarez"],"pdf_url":"https://arxiv.org/pdf/2306.14306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14615v2","updated":"2024-03-13T17:11:20Z","published":"2023-02-24T01:26:56Z","title":"Randomized Kaczmarz in Adversarial Distributed Setting","summary":"  Developing large-scale distributed methods that are robust to the presence of\nadversarial or corrupted workers is an important part of making such methods\npractical for real-world problems. In this paper, we propose an iterative\napproach that is adversary-tolerant for convex optimization problems. By\nleveraging simple statistics, our method ensures convergence and is capable of\nadapting to adversarial distributions. Additionally, the efficiency of the\nproposed methods for solving convex problems is shown in simulations with the\npresence of adversaries. Through simulations, we demonstrate the efficiency of\nour approach in the presence of adversaries and its ability to identify\nadversarial workers with high accuracy and tolerate varying levels of adversary\nrates.\n","authors":["Longxiu Huang","Xia Li","Deanna Needell"],"pdf_url":"https://arxiv.org/pdf/2302.14615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07793v4","updated":"2024-03-13T17:10:48Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Yangzhe Li","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v4.pdf","comment":"14 pages, Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.08700v1","updated":"2024-03-13T17:04:56Z","published":"2024-03-13T17:04:56Z","title":"Diffusion-based Iterative Counterfactual Explanations for Fetal\n  Ultrasound Image Quality Assessment","summary":"  Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, producing high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or the fetus dynamics. In this work, we propose using\ndiffusion-based counterfactual explainable AI to generate realistic\nhigh-quality standard planes from low-quality non-standard ones. Through\nquantitative and qualitative evaluation, we demonstrate the effectiveness of\nour method in producing plausible counterfactuals of increased quality. This\nshows future promise both for enhancing training of clinicians by providing\nvisual feedback, as well as for improving image quality and, consequently,\ndownstream diagnosis and monitoring.\n","authors":["Paraskevas Pegios","Manxi Lin","Nina Weng","Morten Bo Søndergaard Svendsen","Zahra Bashir","Siavash Bigdeli","Anders Nymark Christensen","Martin Tolsgaard","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.08700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16664v2","updated":"2024-03-13T17:03:40Z","published":"2024-01-30T01:28:48Z","title":"Fast Dual-Regularized Autoencoder for Sparse Biological Data","summary":"  Relationship inference from sparse data is an important task with\napplications ranging from product recommendation to drug discovery. A recently\nproposed linear model for sparse matrix completion has demonstrated surprising\nadvantage in speed and accuracy over more sophisticated recommender systems\nalgorithms. Here we extend the linear model to develop a shallow autoencoder\nfor the dual neighborhood-regularized matrix completion problem. We demonstrate\nthe speed and accuracy advantage of our approach over the existing\nstate-of-the-art in predicting drug-target interactions and drug-disease\nassociations.\n","authors":["Aleksandar Poleksic"],"pdf_url":"https://arxiv.org/pdf/2401.16664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08699v1","updated":"2024-03-13T17:02:27Z","published":"2024-03-13T17:02:27Z","title":"Implicit Regularization of Gradient Flow on One-Layer Softmax Attention","summary":"  We study gradient flow on the exponential loss for a classification problem\nwith a one-layer softmax attention model, where the key and query weight\nmatrices are trained separately. Under a separability assumption on the data,\nwe show that when gradient flow achieves the minimal loss value, it further\nimplicitly minimizes the nuclear norm of the product of the key and query\nweight matrices. Such implicit regularization can be described by a Support\nVector Machine (SVM) problem with respect to the attention weights. This\nfinding contrasts with prior results showing that the gradient descent induces\nan implicit regularization on the Frobenius norm on the product weight matrix\nwhen the key and query matrices are combined into a single weight matrix for\ntraining. For diagonal key and query matrices, our analysis builds upon the\nreparameterization technique and exploits approximate KKT conditions of the SVM\nassociated with the classification data. Moreover, the results are extended to\ngeneral weights configurations given proper alignment of the weight matrices'\nsingular spaces with the data features at initialization.\n","authors":["Heejune Sheen","Siyu Chen","Tianhao Wang","Harrison H. Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.08699v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2403.06829v2","updated":"2024-03-13T17:01:57Z","published":"2024-03-11T15:44:40Z","title":"Constructing Variables Using Classifiers as an Aid to Regression: An\n  Empirical Assessment","summary":"  This paper proposes a method for the automatic creation of variables (in the\ncase of regression) that complement the information contained in the initial\ninput vector. The method works as a pre-processing step in which the continuous\nvalues of the variable to be regressed are discretized into a set of intervals\nwhich are then used to define value thresholds. Then classifiers are trained to\npredict whether the value to be regressed is less than or equal to each of\nthese thresholds. The different outputs of the classifiers are then\nconcatenated in the form of an additional vector of variables that enriches the\ninitial vector of the regression problem. The implemented system can thus be\nconsidered as a generic pre-processing tool. We tested the proposed enrichment\nmethod with 5 types of regressors and evaluated it in 33 regression datasets.\nOur experimental results confirm the interest of the approach.\n","authors":["Colin Troisemaine","Vincent Lemaire"],"pdf_url":"https://arxiv.org/pdf/2403.06829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01082v2","updated":"2024-03-13T16:48:27Z","published":"2023-10-02T10:48:42Z","title":"Linear attention is (maybe) all you need (to understand transformer\n  optimization)","summary":"  Transformer training is notoriously difficult, requiring a careful design of\noptimizers and use of various heuristics. We make progress towards\nunderstanding the subtleties of training Transformers by carefully studying a\nsimple yet canonical linearized shallow Transformer model. Specifically, we\ntrain linear Transformers to solve regression tasks, inspired by J.~von Oswald\net al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we\nobserve that our proposed linearized models can reproduce several prominent\naspects of Transformer training dynamics. Consequently, the results obtained in\nthis paper suggest that a simple linearized Transformer model could actually be\na valuable, realistic abstraction for understanding Transformer optimization.\n","authors":["Kwangjun Ahn","Xiang Cheng","Minhak Song","Chulhee Yun","Ali Jadbabaie","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2310.01082v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08687v1","updated":"2024-03-13T16:44:36Z","published":"2024-03-13T16:44:36Z","title":"Digital Twin-assisted Reinforcement Learning for Resource-aware\n  Microservice Offloading in Edge Computing","summary":"  Collaborative edge computing (CEC) has emerged as a promising paradigm,\nenabling edge nodes to collaborate and execute microservices from end devices.\nMicroservice offloading, a fundamentally important problem, decides when and\nwhere microservices are executed upon the arrival of services. However, the\ndynamic nature of the real-world CEC environment often leads to inefficient\nmicroservice offloading strategies, resulting in underutilized resources and\nnetwork congestion. To address this challenge, we formulate an online joint\nmicroservice offloading and bandwidth allocation problem, JMOBA, to minimize\nthe average completion time of services. In this paper, we introduce a novel\nmicroservice offloading algorithm, DTDRLMO, which leverages deep reinforcement\nlearning (DRL) and digital twin technology. Specifically, we employ digital\ntwin techniques to predict and adapt to changing edge node loads and network\nconditions of CEC in real-time. Furthermore, this approach enables the\ngeneration of an efficient offloading plan, selecting the most suitable edge\nnode for each microservice. Simulation results on real-world and synthetic\ndatasets demonstrate that DTDRLMO outperforms heuristic and learning-based\nmethods in average service completion time.\n","authors":["Xiangchun Chen","Jiannong Cao","Zhixuan Liang","Yuvraj Sahni","Mingjin Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08687v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.11598v2","updated":"2024-03-13T16:29:50Z","published":"2023-12-18T18:16:52Z","title":"SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution","summary":"  Diffusion models have demonstrated strong potential for robotic trajectory\nplanning. However, generating coherent trajectories from high-level\ninstructions remains challenging, especially for long-range composition tasks\nrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-end\nhierarchical planning framework integrating interpretable skill learning with\nconditional diffusion planning to address this problem. At the higher level,\nthe skill abstraction module learns discrete, human-understandable skill\nrepresentations from visual observations and language instructions. These\nlearned skill embeddings are then used to condition the diffusion model to\ngenerate customized latent trajectories aligned with the skills. This allows\ngenerating diverse state trajectories that adhere to the learnable skills. By\nintegrating skill learning with conditional trajectory generation,\nSkillDiffuser produces coherent behavior following abstract instructions across\ndiverse tasks. Experiments on multi-task robotic manipulation benchmarks like\nMeta-World and LOReL demonstrate state-of-the-art performance and\nhuman-interpretable skill representations from SkillDiffuser. More\nvisualization results and information could be found on our website.\n","authors":["Zhixuan Liang","Yao Mu","Hengbo Ma","Masayoshi Tomizuka","Mingyu Ding","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2312.11598v2.pdf","comment":"Accepted by CVPR 2024. Camera ready version. Project page:\n  https://skilldiffuser.github.io/"},{"id":"http://arxiv.org/abs/2403.08673v1","updated":"2024-03-13T16:25:55Z","published":"2024-03-13T16:25:55Z","title":"When can we Approximate Wide Contrastive Models with Neural Tangent\n  Kernels and Principal Component Analysis?","summary":"  Contrastive learning is a paradigm for learning representations from\nunlabelled data that has been highly successful for image and text data.\nSeveral recent works have examined contrastive losses to claim that contrastive\nmodels effectively learn spectral embeddings, while few works show relations\nbetween (wide) contrastive models and kernel principal component analysis\n(PCA). However, it is not known if trained contrastive models indeed correspond\nto kernel methods or PCA. In this work, we analyze the training dynamics of\ntwo-layer contrastive models, with non-linear activation, and answer when these\nmodels are close to PCA or kernel methods. It is well known in the supervised\nsetting that neural networks are equivalent to neural tangent kernel (NTK)\nmachines, and that the NTK of infinitely wide networks remains constant during\ntraining. We provide the first convergence results of NTK for contrastive\nlosses, and present a nuanced picture: NTK of wide networks remains almost\nconstant for cosine similarity based contrastive losses, but not for losses\nbased on dot product similarity. We further study the training dynamics of\ncontrastive models with orthogonality constraints on output layer, which is\nimplicitly assumed in works relating contrastive learning to spectral\nembedding. Our deviation bounds suggest that representations learned by\ncontrastive models are close to the principal components of a certain matrix\ncomputed from random features. We empirically show that our theoretical results\npossibly hold beyond two-layer networks.\n","authors":["Gautham Govind Anil","Pascal Esser","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2403.08673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v1","updated":"2024-03-13T16:17:09Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.08662v1","updated":"2024-03-13T16:16:20Z","published":"2024-03-13T16:16:20Z","title":"Self-Supervised Learning for Covariance Estimation","summary":"  We consider the use of deep learning for covariance estimation. We propose to\nglobally learn a neural network that will then be applied locally at inference\ntime. Leveraging recent advancements in self-supervised foundational models, we\ntrain the network without any labeling by simply masking different samples and\nlearning to predict their covariance given their surrounding neighbors. The\narchitecture is based on the popular attention mechanism. Its main advantage\nover classical methods is the automatic exploitation of global characteristics\nwithout any distributional assumptions or regularization. It can be pre-trained\nas a foundation model and then be repurposed for various downstream tasks,\ne.g., adaptive target detection in radar or hyperspectral imagery.\n","authors":["Tzvi Diskin","Ami Wiesel"],"pdf_url":"https://arxiv.org/pdf/2403.08662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10294v2","updated":"2024-03-13T16:08:01Z","published":"2024-01-17T23:07:59Z","title":"Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of\n  Gaussians Mechanisms","summary":"  We give a procedure for computing group-level $(\\epsilon, \\delta)$-DP\nguarantees for DP-SGD, when using Poisson sampling or fixed batch size\nsampling. Up to discretization errors in the implementation, the DP guarantees\ncomputed by this procedure are tight (assuming we release every intermediate\niterate).\n","authors":["Arun Ganesh"],"pdf_url":"https://arxiv.org/pdf/2401.10294v2.pdf","comment":"v2: Added links to open-source implementation of PLD accounting for\n  MoG mechanisms"},{"id":"http://arxiv.org/abs/2403.08652v1","updated":"2024-03-13T16:06:26Z","published":"2024-03-13T16:06:26Z","title":"Extracting Explanations, Justification, and Uncertainty from Black-Box\n  Deep Neural Networks","summary":"  Deep Neural Networks (DNNs) do not inherently compute or exhibit\nempirically-justified task confidence. In mission critical applications, it is\nimportant to both understand associated DNN reasoning and its supporting\nevidence. In this paper, we propose a novel Bayesian approach to extract\nexplanations, justifications, and uncertainty estimates from DNNs. Our approach\nis efficient both in terms of memory and computation, and can be applied to any\nblack box DNN without any retraining, including applications to anomaly\ndetection and out-of-distribution detection tasks. We validate our approach on\nthe CIFAR-10 dataset, and show that it can significantly improve the\ninterpretability and reliability of DNNs.\n","authors":["Paul Ardis","Arjuna Flenner"],"pdf_url":"https://arxiv.org/pdf/2403.08652v1.pdf","comment":"8 pages, 5 figures, SPIE DCS 2024"},{"id":"http://arxiv.org/abs/2302.08913v4","updated":"2024-03-13T16:04:03Z","published":"2023-02-04T15:55:23Z","title":"Referential communication in heterogeneous communities of pre-trained\n  visual deep networks","summary":"  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of \\textit{referential\ncommunication} in a community of heterogeneous state-of-the-art pre-trained\nvisual networks, showing that they can develop, in a self-supervised way, a\nshared protocol to refer to a target object among a set of candidates. This\nshared protocol can also be used, to some extent, to communicate about\npreviously unseen object categories of different granularity. Moreover, a\nvisual network that was not initially part of an existing community can learn\nthe community's protocol with remarkable ease. Finally, we study, both\nqualitatively and quantitatively, the properties of the emergent protocol,\nproviding some evidence that it is capturing high-level semantic features of\nobjects.\n","authors":["Matéo Mahaut","Francesca Franzon","Roberto Dessì","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2302.08913v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08638v1","updated":"2024-03-13T15:51:03Z","published":"2024-03-13T15:51:03Z","title":"Disparate Effect Of Missing Mediators On Transportability of Causal\n  Effects","summary":"  Transported mediation effects provide an avenue to understand how upstream\ninterventions (such as improved neighborhood conditions like green spaces)\nwould work differently when applied to different populations as a result of\nfactors that mediate the effects. However, when mediators are missing in the\npopulation where the effect is to be transported, these estimates could be\nbiased. We study this issue of missing mediators, motivated by challenges in\npublic health, wherein mediators can be missing, not at random. We propose a\nsensitivity analysis framework that quantifies the impact of missing mediator\ndata on transported mediation effects. This framework enables us to identify\nthe settings under which the conditional transported mediation effect is\nrendered insignificant for the subgroup with missing mediator data.\nSpecifically, we provide the bounds on the transported mediation effect as a\nfunction of missingness. We then apply the framework to longitudinal data from\nthe Moving to Opportunity Study, a large-scale housing voucher experiment, to\nquantify the effect of missing mediators on transport effect estimates of\nvoucher receipt, an upstream intervention on living location, in childhood on\nsubsequent risk of mental health or substance use disorder mediated through\nparental health across sites. Our findings provide a tangible understanding of\nhow much missing data can be withstood for unbiased effect estimates.\n","authors":["Vishwali Mhasawade","Rumi Chunara"],"pdf_url":"https://arxiv.org/pdf/2403.08638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08635v1","updated":"2024-03-13T15:47:26Z","published":"2024-03-13T15:47:26Z","title":"Human Alignment of Large Language Models through Online Preference\n  Optimisation","summary":"  Ensuring alignment of language models' outputs with human preferences is\ncritical to guarantee a useful, safe, and pleasant user experience. Thus, human\nalignment has been extensively studied recently and several methods such as\nReinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation\n(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,\nour contribution is two-fold. First, we show the equivalence between two recent\nalignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror\nDescent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,\nthat leverages the regularised sampling approach proposed by Nash-MD.\n  This equivalence may seem surprising at first sight, since IPO is an offline\nmethod whereas Nash-MD is an online method using a preference model. However,\nthis equivalence can be proven when we consider the online version of IPO, that\nis when both generations are sampled by the online policy and annotated by a\ntrained preference model. Optimising the IPO loss with such a stream of data\nbecomes then equivalent to finding the Nash equilibrium of the preference model\nthrough self-play. Building on this equivalence, we introduce the IPO-MD\nalgorithm that generates data with a mixture policy (between the online and\nreference policy) similarly as the general Nash-MD algorithm. We compare\nonline-IPO and IPO-MD to different online versions of existing losses on\npreference data such as DPO and SLiC on a summarisation task.\n","authors":["Daniele Calandriello","Daniel Guo","Remi Munos","Mark Rowland","Yunhao Tang","Bernardo Avila Pires","Pierre Harvey Richemond","Charline Le Lan","Michal Valko","Tianqi Liu","Rishabh Joshi","Zeyu Zheng","Bilal Piot"],"pdf_url":"https://arxiv.org/pdf/2403.08635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08632v1","updated":"2024-03-13T15:46:37Z","published":"2024-03-13T15:46:37Z","title":"A Decade's Battle on Dataset Bias: Are We There Yet?","summary":"  We revisit the \"dataset classification\" experiment suggested by Torralba and\nEfros a decade ago, in the new era with large-scale, diverse, and hopefully\nless biased datasets as well as more capable neural network architectures.\nSurprisingly, we observe that modern neural networks can achieve excellent\naccuracy in classifying which dataset an image is from: e.g., we report 84.7%\naccuracy on held-out validation data for the three-way classification problem\nconsisting of the YFCC, CC, and DataComp datasets. Our further experiments show\nthat such a dataset classifier could learn semantic features that are\ngeneralizable and transferable, which cannot be simply explained by\nmemorization. We hope our discovery will inspire the community to rethink the\nissue involving dataset bias and model capabilities.\n","authors":["Zhuang Liu","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2403.08632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08630v1","updated":"2024-03-13T15:45:29Z","published":"2024-03-13T15:45:29Z","title":"Leveraging Non-Decimated Wavelet Packet Features and Transformer Models\n  for Time Series Forecasting","summary":"  This article combines wavelet analysis techniques with machine learning\nmethods for univariate time series forecasting, focusing on three main\ncontributions. Firstly, we consider the use of Daubechies wavelets with\ndifferent numbers of vanishing moments as input features to both non-temporal\nand temporal forecasting methods, by selecting these numbers during the\ncross-validation phase. Secondly, we compare the use of both the non-decimated\nwavelet transform and the non-decimated wavelet packet transform for computing\nthese features, the latter providing a much larger set of potentially useful\ncoefficient vectors. The wavelet coefficients are computed using a shifted\nversion of the typical pyramidal algorithm to ensure no leakage of future\ninformation into these inputs. Thirdly, we evaluate the use of these wavelet\nfeatures on a significantly wider set of forecasting methods than previous\nstudies, including both temporal and non-temporal models, and both statistical\nand deep learning-based methods. The latter include state-of-the-art\ntransformer-based neural network architectures. Our experiments suggest\nsignificant benefit in replacing higher-order lagged features with wavelet\nfeatures across all examined non-temporal methods for one-step-forward\nforecasting, and modest benefit when used as inputs for temporal deep\nlearning-based models for long-horizon forecasting.\n","authors":["Guy P Nason","James L. Wei"],"pdf_url":"https://arxiv.org/pdf/2403.08630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08627v1","updated":"2024-03-13T15:40:17Z","published":"2024-03-13T15:40:17Z","title":"Multifidelity linear regression for scientific machine learning from\n  scarce data","summary":"  Machine learning (ML) methods, which fit to data the parameters of a given\nparameterized model class, have garnered significant interest as potential\nmethods for learning surrogate models for complex engineering systems for which\ntraditional simulation is expensive. However, in many scientific and\nengineering settings, generating high-fidelity data on which to train ML models\nis expensive, and the available budget for generating training data is limited.\nML models trained on the resulting scarce high-fidelity data have high variance\nand are sensitive to vagaries of the training data set. We propose a new\nmultifidelity training approach for scientific machine learning that exploits\nthe scientific context where data of varying fidelities and costs are\navailable; for example high-fidelity data may be generated by an expensive\nfully resolved physics simulation whereas lower-fidelity data may arise from a\ncheaper model based on simplifying assumptions. We use the multifidelity data\nto define new multifidelity Monte Carlo estimators for the unknown parameters\nof linear regression models, and provide theoretical analyses that guarantee\nthe approach's accuracy and improved robustness to small training budgets.\nNumerical results verify the theoretical analysis and demonstrate that\nmultifidelity learned models trained on scarce high-fidelity data and\nadditional low-fidelity data achieve order-of-magnitude lower model variance\nthan standard models trained on only high-fidelity data of comparable cost.\nThis illustrates that in the scarce data regime, our multifidelity training\nstrategy yields models with lower expected error than standard training\napproaches.\n","authors":["Elizabeth Qian","Anirban Chaudhuri","Dayoung Kang","Vignesh Sella"],"pdf_url":"https://arxiv.org/pdf/2403.08627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08618v1","updated":"2024-03-13T15:32:08Z","published":"2024-03-13T15:32:08Z","title":"Verifix: Post-Training Correction to Improve Label Noise Robustness with\n  Verified Samples","summary":"  Label corruption, where training samples have incorrect labels, can\nsignificantly degrade the performance of machine learning models. This\ncorruption often arises from non-expert labeling or adversarial attacks.\nAcquiring large, perfectly labeled datasets is costly, and retraining large\nmodels from scratch when a clean dataset becomes available is computationally\nexpensive. To address this challenge, we propose Post-Training Correction, a\nnew paradigm that adjusts model parameters after initial training to mitigate\nlabel noise, eliminating the need for retraining. We introduce Verifix, a novel\nSingular Value Decomposition (SVD) based algorithm that leverages a small,\nverified dataset to correct the model weights using a single update. Verifix\nuses SVD to estimate a Clean Activation Space and then projects the model's\nweights onto this space to suppress activations corresponding to corrupted\ndata. We demonstrate Verifix's effectiveness on both synthetic and real-world\nlabel noise. Experiments on the CIFAR dataset with 25% synthetic corruption\nshow 7.36% generalization improvements on average. Additionally, we observe\ngeneralization improvements of up to 2.63% on naturally corrupted datasets like\nWebVision1.0 and Clothing1M.\n","authors":["Sangamesh Kodge","Deepak Ravikumar","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2403.08618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09481v2","updated":"2024-03-13T15:24:19Z","published":"2023-12-15T01:38:26Z","title":"Continual Adversarial Defense","summary":"  In response to the rapidly evolving nature of adversarial attacks against\nvisual classifiers on a monthly basis, numerous defenses have been proposed to\ngeneralize against as many known attacks as possible. However, designing a\ndefense method that generalizes to all types of attacks is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks that emerge as time goes on. The defense\nsystem must gather online few-shot defense feedback to promptly enhance itself,\nleveraging efficient memory utilization. Therefore, we propose the first\ncontinual adversarial defense (CAD) framework that adapts to any attacks in a\ndynamic scenario, where various attacks emerge stage by stage. In practice, CAD\nis modeled under four principles: (1) continual adaptation to new attacks\nwithout catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient\nadaptation, and (4) high accuracy on both clean and adversarial images. We\nexplore and integrate cutting-edge continual learning, few-shot learning, and\nensemble learning techniques to qualify the principles. Experiments conducted\non CIFAR-10 and ImageNet-100 validate the effectiveness of our approach against\nmultiple stages of modern adversarial attacks and demonstrate significant\nimprovements over numerous baseline methods. In particular, CAD is capable of\nquickly adapting with minimal feedback and a low cost of defense failure, while\nmaintaining good performance against previous attacks. Our research sheds light\non a brand-new paradigm for continual defense adaptation against dynamic and\nevolving attacks.\n","authors":["Qian Wang","Yaoyao Liu","Hefei Ling","Yingwei Li","Qihao Liu","Ping Li","Jiazhong Chen","Alan Yuille","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2312.09481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08613v1","updated":"2024-03-13T15:23:55Z","published":"2024-03-13T15:23:55Z","title":"Link Prediction for Social Networks using Representation Learning and\n  Heuristic-based Features","summary":"  The exponential growth in scale and relevance of social networks enable them\nto provide expansive insights. Predicting missing links in social networks\nefficiently can help in various modern-day business applications ranging from\ngenerating recommendations to influence analysis. Several categories of\nsolutions exist for the same. Here, we explore various feature extraction\ntechniques to generate representations of nodes and edges in a social network\nthat allow us to predict missing links. We compare the results of using ten\nfeature extraction techniques categorized across Structural embeddings,\nNeighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,\nfollowed by modeling with ensemble classifiers and custom Neural Networks.\nFurther, we propose combining heuristic-based features and learned\nrepresentations that demonstrate improved performance for the link prediction\ntask on social network datasets. Using this method to generate accurate\nrecommendations for many applications is a matter of further study that appears\nvery promising. The code for all the experiments has been made public.\n","authors":["Samarth Khanna","Sree Bhattacharyya","Sudipto Ghosh","Kushagra Agarwal","Asit Kumar Das"],"pdf_url":"https://arxiv.org/pdf/2403.08613v1.pdf","comment":"Accepted to the MAISoN Workshop at IJCAI 2023"},{"id":"http://arxiv.org/abs/2403.08609v1","updated":"2024-03-13T15:21:14Z","published":"2024-03-13T15:21:14Z","title":"On the Convergence of Locally Adaptive and Scalable Diffusion-Based\n  Sampling Methods for Deep Bayesian Neural Network Posteriors","summary":"  Achieving robust uncertainty quantification for deep neural networks\nrepresents an important requirement in many real-world applications of deep\nlearning such as medical imaging where it is necessary to assess the\nreliability of a neural network's prediction. Bayesian neural networks are a\npromising approach for modeling uncertainties in deep neural networks.\nUnfortunately, generating samples from the posterior distribution of neural\nnetworks is a major challenge. One significant advance in that direction would\nbe the incorporation of adaptive step sizes, similar to modern neural network\noptimizers, into Monte Carlo Markov chain sampling algorithms without\nsignificantly increasing computational demand. Over the past years, several\npapers have introduced sampling algorithms with claims that they achieve this\nproperty. However, do they indeed converge to the correct distribution? In this\npaper, we demonstrate that these methods can have a substantial bias in the\ndistribution they sample, even in the limit of vanishing step sizes and at full\nbatch size.\n","authors":["Tim Rensmeyer","Oliver Niggemann"],"pdf_url":"https://arxiv.org/pdf/2403.08609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12375v4","updated":"2024-03-13T15:00:20Z","published":"2023-07-23T16:54:41Z","title":"In-Context Learning Learns Label Relationships but Is Not Conventional\n  Learning","summary":"  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n","authors":["Jannik Kossen","Yarin Gal","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2307.12375v4.pdf","comment":"Accepted for publication at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08592v1","updated":"2024-03-13T14:57:10Z","published":"2024-03-13T14:57:10Z","title":"Data-Efficient Sleep Staging with Synthetic Time Series Pretraining","summary":"  Analyzing electroencephalographic (EEG) time series can be challenging,\nespecially with deep neural networks, due to the large variability among human\nsubjects and often small datasets. To address these challenges, various\nstrategies, such as self-supervised learning, have been suggested, but they\ntypically rely on extensive empirical datasets. Inspired by recent advances in\ncomputer vision, we propose a pretraining task termed \"frequency pretraining\"\nto pretrain a neural network for sleep staging by predicting the frequency\ncontent of randomly generated synthetic time series. Our experiments\ndemonstrate that our method surpasses fully supervised learning in scenarios\nwith limited data and few subjects, and matches its performance in regimes with\nmany subjects. Furthermore, our results underline the relevance of frequency\ninformation for sleep stage scoring, while also demonstrating that deep neural\nnetworks utilize information beyond frequencies to enhance sleep staging\nperformance, which is consistent with previous research. We anticipate that our\napproach will be advantageous across a broad spectrum of applications where EEG\ndata is limited or derived from a small number of subjects, including the\ndomain of brain-computer interfaces.\n","authors":["Niklas Grieger","Siamak Mehrkanoon","Stephan Bialonski"],"pdf_url":"https://arxiv.org/pdf/2403.08592v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.02895v2","updated":"2024-03-13T14:56:11Z","published":"2023-10-04T15:32:27Z","title":"CoLiDE: Concomitant Linear DAG Estimation","summary":"  We deal with the combinatorial problem of learning directed acyclic graph\n(DAG) structure from observational data adhering to a linear structural\nequation model (SEM). Leveraging advances in differentiable, nonconvex\ncharacterizations of acyclicity, recent efforts have advocated a continuous\nconstrained optimization paradigm to efficiently explore the space of DAGs.\nMost existing methods employ lasso-type score functions to guide this search,\nwhich (i) require expensive penalty parameter retuning when the\n$\\textit{unknown}$ SEM noise variances change across problem instances; and\n(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we\npropose a new convex score function for sparsity-aware learning of linear DAGs,\nwhich incorporates concomitant estimation of scale and thus effectively\ndecouples the sparsity parameter from the exogenous noise levels.\nRegularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE\n($\\textbf{Co}$ncomitant $\\textbf{Li}$near $\\textbf{D}$AG\n$\\textbf{E}$stimation), a regression-based criterion amenable to efficient\ngradient computation and closed-form estimation of noise variances in\nheteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods\nwithout incurring added complexity, especially when the DAGs are larger and the\nnoise level profile is heterogeneous. We also find CoLiDE exhibits enhanced\nstability manifested via reduced standard deviations in several domain-specific\nmetrics, underscoring the robustness of our novel linear DAG estimator.\n","authors":["Seyed Saman Saboksayr","Gonzalo Mateos","Mariano Tepper"],"pdf_url":"https://arxiv.org/pdf/2310.02895v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03853v2","updated":"2024-03-13T14:52:47Z","published":"2023-12-06T19:07:38Z","title":"Dr. Jekyll and Mr. Hyde: Two Faces of LLMs","summary":"  Only a year ago, we witnessed a rise in the use of Large Language Models\n(LLMs), especially when combined with applications like chatbot assistants.\nSafety mechanisms and specialized training procedures are implemented to\nprevent improper responses from these assistants. In this work, we bypass these\nmeasures for ChatGPT and Bard (and, to some extent, Bing chat) by making them\nimpersonate complex personas with opposite characteristics as those of the\ntruthful assistants they are supposed to be. We start by creating elaborate\nbiographies of these personas, which we then use in a new session with the same\nchatbots. Our conversation followed a role-play style to get the response the\nassistant was not allowed to provide. By making use of personas, we show that\nthe response that is prohibited is actually provided, making it possible to\nobtain unauthorized, illegal, or harmful information. This work shows that by\nusing adversarial personas, one can overcome safety mechanisms set out by\nChatGPT and Bard. We also introduce several ways of activating such adversarial\npersonas, altogether showing that both chatbots are vulnerable to this kind of\nattack. With the same principle, we introduce two defenses that push the model\nto interpret trustworthy personalities and make it more robust against such\nattacks.\n","authors":["Matteo Gioele Collu","Tom Janssen-Groesbeek","Stefanos Koffas","Mauro Conti","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2312.03853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08589v1","updated":"2024-03-13T14:51:16Z","published":"2024-03-13T14:51:16Z","title":"Can physical information aid the generalization ability of Neural\n  Networks for hydraulic modeling?","summary":"  Application of Neural Networks to river hydraulics is fledgling, despite the\nfield suffering from data scarcity, a challenge for machine learning\ntechniques. Consequently, many purely data-driven Neural Networks proved to\nlack predictive capabilities. In this work, we propose to mitigate such problem\nby introducing physical information into the training phase. The idea is\nborrowed from Physics-Informed Neural Networks which have been recently\nproposed in other contexts. Physics-Informed Neural Networks embed physical\ninformation in the form of the residual of the Partial Differential Equations\n(PDEs) governing the phenomenon and, as such, are conceived as neural solvers,\ni.e. an alternative to traditional numerical solvers. Such approach is seldom\nsuitable for environmental hydraulics, where epistemic uncertainties are large,\nand computing residuals of PDEs exhibits difficulties similar to those faced by\nclassical numerical methods. Instead, we envisaged the employment of Neural\nNetworks as neural operators, featuring physical constraints formulated without\nresorting to PDEs. The proposed novel methodology shares similarities with data\naugmentation and regularization. We show that incorporating such soft physical\ninformation can improve predictive capabilities.\n","authors":["Gianmarco Guglielmo","Andrea Montessori","Jean-Michel Tucny","Michele La Rocca","Pietro Prestininzi"],"pdf_url":"https://arxiv.org/pdf/2403.08589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10385v3","updated":"2024-03-13T14:48:36Z","published":"2023-12-16T08:48:46Z","title":"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning","summary":"  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n","authors":["Huy Hoang","Tien Mai","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2312.10385v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11155v5","updated":"2024-03-13T14:46:05Z","published":"2022-03-16T12:23:25Z","title":"A New Quantum CNN Model for Image Classification","summary":"  Quantum density matrix represents all the information of the entire quantum\nsystem, and novel models of meaning employing density matrices naturally model\nlinguistic phenomena such as hyponymy and linguistic ambiguity, among others in\nquantum question answering tasks. Naturally, we argue that the quantum density\nmatrix can enhance the image feature information and the relationship between\nthe features for the classical image classification. Specifically, we (i)\ncombine density matrices and CNN to design a new mechanism; (ii) apply the new\nmechanism to some representative classical image classification tasks. A series\nof experiments show that the application of quantum density matrix in image\nclassification has the generalization and high efficiency on different\ndatasets. The application of quantum density matrix both in classical question\nanswering tasks and classical image classification tasks show more effective\nperformance.\n","authors":["X. Q. Zhao","T. L. Chen"],"pdf_url":"https://arxiv.org/pdf/2203.11155v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08585v1","updated":"2024-03-13T14:42:06Z","published":"2024-03-13T14:42:06Z","title":"Improving Implicit Regularization of SGD with Preconditioning for Least\n  Square Problems","summary":"  Stochastic gradient descent (SGD) exhibits strong algorithmic regularization\neffects in practice and plays an important role in the generalization of modern\nmachine learning. However, prior research has revealed instances where the\ngeneralization performance of SGD is worse than ridge regression due to uneven\noptimization along different dimensions. Preconditioning offers a natural\nsolution to this issue by rebalancing optimization across different directions.\nYet, the extent to which preconditioning can enhance the generalization\nperformance of SGD and whether it can bridge the existing gap with ridge\nregression remains uncertain. In this paper, we study the generalization\nperformance of SGD with preconditioning for the least squared problem. We make\na comprehensive comparison between preconditioned SGD and (standard \\&\npreconditioned) ridge regression. Our study makes several key contributions\ntoward understanding and improving SGD with preconditioning. First, we\nestablish excess risk bounds (generalization performance) for preconditioned\nSGD and ridge regression under an arbitrary preconditions matrix. Second,\nleveraging the excessive risk characterization of preconditioned SGD and ridge\nregression, we show that (through construction) there exists a simple\npreconditioned matrix that can outperform (standard \\& preconditioned) ridge\nregression. Finally, we show that our proposed preconditioning matrix is\nstraightforward enough to allow robust estimation from finite samples while\nmaintaining a theoretical advantage over ridge regression. Our empirical\nresults align with our theoretical findings, collectively showcasing the\nenhanced regularization effect of preconditioned SGD.\n","authors":["Junwei Su","Difan Zou","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08584v1","updated":"2024-03-13T14:37:00Z","published":"2024-03-13T14:37:00Z","title":"Local Binary and Multiclass SVMs Trained on a Quantum Annealer","summary":"  Support vector machines (SVMs) are widely used machine learning models (e.g.,\nin remote sensing), with formulations for both classification and regression\ntasks. In the last years, with the advent of working quantum annealers, hybrid\nSVM models characterised by quantum training and classical execution have been\nintroduced. These models have demonstrated comparable performance to their\nclassical counterparts. However, they are limited in the training set size due\nto the restricted connectivity of the current quantum annealers. Hence, to take\nadvantage of large datasets (like those related to Earth observation), a\nstrategy is required. In the classical domain, local SVMs, namely, SVMs trained\non the data samples selected by a k-nearest neighbors model, have already\nproven successful. Here, the local application of quantum-trained SVM models is\nproposed and empirically assessed. In particular, this approach allows\novercoming the constraints on the training set size of the quantum-trained\nmodels while enhancing their performance. In practice, the FaLK-SVM method,\ndesigned for efficient local SVMs, has been combined with quantum-trained SVM\nmodels for binary and multiclass classification. In addition, for comparison,\nFaLK-SVM has been interfaced for the first time with a classical single-step\nmulticlass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's\nquantum annealers and real-world datasets taken from the remote sensing domain\nhave been employed. The results have shown the effectiveness and scalability of\nthe proposed approach, but also its practical applicability in a real-world\nlarge-scale scenario.\n","authors":["Enrico Zardini","Amer Delilbasic","Enrico Blanzieri","Gabriele Cavallaro","Davide Pastorello"],"pdf_url":"https://arxiv.org/pdf/2403.08584v1.pdf","comment":"12 pages, 1 figure, 11 tables"},{"id":"http://arxiv.org/abs/2403.08579v1","updated":"2024-03-13T14:34:34Z","published":"2024-03-13T14:34:34Z","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation","summary":"  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2403.08579v1.pdf","comment":"Submitted to LION18"},{"id":"http://arxiv.org/abs/2403.08572v1","updated":"2024-03-13T14:28:02Z","published":"2024-03-13T14:28:02Z","title":"Caformer: Rethinking Time Series Analysis from Causal Perspective","summary":"  Time series analysis is a vital task with broad applications in various\ndomains. However, effectively capturing cross-dimension and cross-time\ndependencies in non-stationary time series poses significant challenges,\nparticularly in the context of environmental factors. The spurious correlation\ninduced by the environment confounds the causal relationships between\ncross-dimension and cross-time dependencies. In this paper, we introduce a\nnovel framework called Caformer (\\underline{\\textbf{Ca}}usal\nTrans\\underline{\\textbf{former}}) for time series analysis from a causal\nperspective. Specifically, our framework comprises three components: Dynamic\nLearner, Environment Learner, and Dependency Learner. The Dynamic Learner\nunveils dynamic interactions among dimensions, the Environment Learner\nmitigates spurious correlations caused by environment with a back-door\nadjustment, and the Dependency Learner aims to infer robust interactions across\nboth time and dimensions. Our Caformer demonstrates consistent state-of-the-art\nperformance across five mainstream time series analysis tasks, including long-\nand short-term forecasting, imputation, classification, and anomaly detection,\nwith proper interpretability.\n","authors":["Kexuan Zhang","Xiaobei Zou","Yang Tang"],"pdf_url":"https://arxiv.org/pdf/2403.08572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00902v3","updated":"2024-03-13T14:27:46Z","published":"2023-10-02T04:59:19Z","title":"DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and\n  Diffusion Models","summary":"  Quantifying the impact of training data points is crucial for understanding\nthe outputs of machine learning models and for improving the transparency of\nthe AI pipeline. The influence function is a principled and popular data\nattribution method, but its computational cost often makes it challenging to\nuse. This issue becomes more pronounced in the setting of large language models\nand text-to-image models. In this work, we propose DataInf, an efficient\ninfluence approximation method that is practical for large-scale generative AI\nmodels. Leveraging an easy-to-compute closed-form expression, DataInf\noutperforms existing influence computation algorithms in terms of computational\nand memory efficiency. Our theoretical analysis shows that DataInf is\nparticularly well-suited for parameter-efficient fine-tuning techniques such as\nLoRA. Through systematic empirical evaluations, we show that DataInf accurately\napproximates influence scores and is orders of magnitude faster than existing\nmethods. In applications to RoBERTa-large, Llama-2-13B-chat, and\nstable-diffusion-v1.5 models, DataInf effectively identifies the most\ninfluential fine-tuning examples better than other approximate influence\nscores. Moreover, it can help to identify which data points are mislabeled.\n","authors":["Yongchan Kwon","Eric Wu","Kevin Wu","James Zou"],"pdf_url":"https://arxiv.org/pdf/2310.00902v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08569v1","updated":"2024-03-13T14:25:15Z","published":"2024-03-13T14:25:15Z","title":"A Physics-driven GraphSAGE Method for Physical Process Simulations\n  Described by Partial Differential Equations","summary":"  Physics-informed neural networks (PINNs) have successfully addressed various\ncomputational physics problems based on partial differential equations (PDEs).\nHowever, while tackling issues related to irregularities like singularities and\noscillations, trained solutions usually suffer low accuracy. In addition, most\ncurrent works only offer the trained solution for predetermined input\nparameters. If any change occurs in input parameters, transfer learning or\nretraining is required, and traditional numerical techniques also need an\nindependent simulation. In this work, a physics-driven GraphSAGE approach\n(PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal\nbasis functions is presented to solve computational problems governed by\nirregular PDEs and to develop parametric PDE surrogate models. This approach\nemploys graph representations of physical domains, thereby reducing the demands\nfor evaluated points due to local refinement. A distance-related edge feature\nand a feature mapping strategy are devised to help training and convergence for\nsingularity and oscillation situations, respectively. The merits of the\nproposed method are demonstrated through a couple of cases. Moreover, the\nrobust PDE surrogate model for heat conduction problems parameterized by the\nGaussian random field source is successfully established, which not only\nprovides the solution accurately but is several times faster than the finite\nelement method in our experiments.\n","authors":["Hang Hu","Sidi Wu","Guoxiong Cai","Na Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08569v1.pdf","comment":"18 pages,11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2309.15048v4","updated":"2024-03-13T14:24:28Z","published":"2023-09-26T16:25:57Z","title":"Class Incremental Learning via Likelihood Ratio Based Task Prediction","summary":"  Class incremental learning (CIL) is a challenging setting of continual\nlearning, which learns a series of tasks sequentially. Each task consists of a\nset of unique classes. The key feature of CIL is that no task identifier (or\ntask-id) is provided at test time. Predicting the task-id for each test sample\nis a challenging problem. An emerging theory-guided approach (called TIL+OOD)\nis to train a task-specific model for each task in a shared network for all\ntasks based on a task-incremental learning (TIL) method to deal with\ncatastrophic forgetting. The model for each task is an out-of-distribution\n(OOD) detector rather than a conventional classifier. The OOD detector can\nperform both within-task (in-distribution (IND)) class prediction and OOD\ndetection. The OOD detection capability is the key to task-id prediction during\ninference. However, this paper argues that using a traditional OOD detector for\ntask-id prediction is sub-optimal because additional information (e.g., the\nreplay data and the learned tasks) available in CIL can be exploited to design\na better and principled method for task-id prediction. We call the new method\nTPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms\nstrong CIL baselines and has negligible catastrophic forgetting. The code of\nTPL is publicly available at https://github.com/linhaowei1/TPL.\n","authors":["Haowei Lin","Yijia Shao","Weinan Qian","Ningxin Pan","Yiduo Guo","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2309.15048v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08568v1","updated":"2024-03-13T14:24:09Z","published":"2024-03-13T14:24:09Z","title":"Consistent Prompting for Rehearsal-Free Continual Learning","summary":"  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n","authors":["Zhanxin Gao","Jun Cen","Xiaobin Chang"],"pdf_url":"https://arxiv.org/pdf/2403.08568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08509v2","updated":"2024-03-13T14:18:59Z","published":"2023-07-17T14:10:01Z","title":"Kernel-Based Testing for Single-Cell Differential Analysis","summary":"  Single-cell technologies offer insights into molecular feature distributions,\nbut comparing them poses challenges. We propose a kernel-testing framework for\nnon-linear cell-wise distribution comparison, analyzing gene expression and\nepigenomic modifications. Our method allows feature-wise and global\ntranscriptome/epigenome comparisons, revealing cell population heterogeneities.\nUsing a classifier based on embedding variability, we identify transitions in\ncell states, overcoming limitations of traditional single-cell analysis.\nApplied to single-cell ChIP-Seq data, our approach identifies untreated breast\ncancer cells with an epigenomic profile resembling persister cells. This\ndemonstrates the effectiveness of kernel testing in uncovering subtle\npopulation variations that might be missed by other methods.\n","authors":["Anthony Ozier-Lafontaine","Camille Fourneaux","Ghislain Durif","Céline Vallot","Olivier Gandrillon","Sandrine Giraud","Bertrand Michel","Franck Picard"],"pdf_url":"https://arxiv.org/pdf/2307.08509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16456v2","updated":"2024-03-13T14:16:54Z","published":"2023-09-28T14:06:17Z","title":"Resisting Backdoor Attacks in Federated Learning via Bidirectional\n  Elections and Individual Perspective","summary":"  Existing approaches defend against backdoor attacks in federated learning\n(FL) mainly through a) mitigating the impact of infected models, or b)\nexcluding infected models. The former negatively impacts model accuracy, while\nthe latter usually relies on globally clear boundaries between benign and\ninfected model updates. However, model updates are easy to be mixed and\nscattered throughout in reality due to the diverse distributions of local data.\nThis work focuses on excluding infected models in FL. Unlike previous\nperspectives from a global view, we propose Snowball, a novel anti-backdoor FL\nframework through bidirectional elections from an individual perspective\ninspired by one principle deduced by us and two principles in FL and deep\nlearning. It is characterized by a) bottom-up election, where each candidate\nmodel update votes to several peer ones such that a few model updates are\nelected as selectees for aggregation; and b) top-down election, where selectees\nprogressively enlarge themselves through picking up from the candidates. We\ncompare Snowball with state-of-the-art defenses to backdoor attacks in FL on\nfive real-world datasets, demonstrating its superior resistance to backdoor\nattacks and slight impact on the accuracy of the global model.\n","authors":["Zhen Qin","Feiyi Chen","Chen Zhi","Xueqiang Yan","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2309.16456v2.pdf","comment":"Accepted by AAAI 2024. Codes are publicly available at\n  https://github.com/zhenqincn/Snowball"},{"id":"http://arxiv.org/abs/2403.08562v1","updated":"2024-03-13T14:14:47Z","published":"2024-03-13T14:14:47Z","title":"Structural perspective on constraint-based learning of Markov networks","summary":"  Markov networks are probabilistic graphical models that employ undirected\ngraphs to depict conditional independence relationships among variables. Our\nfocus lies in constraint-based structure learning, which entails learning the\nundirected graph from data through the execution of conditional independence\ntests. We establish theoretical limits concerning two critical aspects of\nconstraint-based learning of Markov networks: the number of tests and the sizes\nof the conditioning sets. These bounds uncover an exciting interplay between\nthe structural properties of the graph and the amount of tests required to\nlearn a Markov network. The starting point of our work is that the graph\nparameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number\nof vertex-disjoint paths connecting a pair of vertices in the graph, is\nresponsible for the sizes of independence tests required to learn the graph. On\none hand, we show that at least one test with the size of the conditioning set\nat least $\\kappa$ is always necessary. On the other hand, we prove that any\ngraph can be learned by performing tests of size at most $\\kappa$. This\ncompletely resolves the question of the minimum size of conditioning sets\nrequired to learn the graph. When it comes to the number of tests, our upper\nbound on the sizes of conditioning sets implies that every $n$-vertex graph can\nbe learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at\nmost $\\kappa$. We show that for any upper bound $q$ on the sizes of the\nconditioning sets, there exist graphs with $O(n q)$ vertices that require at\nleast $n^{\\Omega(\\kappa)}$ tests to learn. This lower bound holds even when the\ntreewidth and the maximum degree of the graph are at most $\\kappa+2$. On the\npositive side, we prove that every graph of bounded treewidth can be learned by\na polynomial number of tests with conditioning sets of sizes at most $2\\kappa$.\n","authors":["Tuukka Korhonen","Fedor V. Fomin","Pekka Parviainen"],"pdf_url":"https://arxiv.org/pdf/2403.08562v1.pdf","comment":"AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.08554v1","updated":"2024-03-13T14:06:51Z","published":"2024-03-13T14:06:51Z","title":"Federated Knowledge Graph Unlearning via Diffusion Model","summary":"  Federated learning (FL) promotes the development and application of\nartificial intelligence technologies by enabling model sharing and\ncollaboration while safeguarding data privacy. Knowledge graph (KG) embedding\nrepresentation provides a foundation for knowledge reasoning and applications\nby mapping entities and relations into vector space. Federated KG embedding\nenables the utilization of knowledge from diverse client sources while\nsafeguarding the privacy of local data. However, due to demands such as privacy\nprotection and the need to adapt to dynamic data changes, investigations into\nmachine unlearning (MU) have been sparked. However, it is challenging to\nmaintain the performance of KG embedding models while forgetting the influence\nof specific forgotten data on the model. In this paper, we propose FedDM, a\nnovel framework tailored for machine unlearning in federated knowledge graphs.\nLeveraging diffusion models, we generate noisy data to sensibly mitigate the\ninfluence of specific knowledge on FL models while preserving the overall\nperformance concerning the remaining data. We conduct experimental evaluations\non benchmark datasets to assess the efficacy of the proposed model. Extensive\nexperiments demonstrate that FedDM yields promising results in knowledge\nforgetting.\n","authors":["Bingchen Liu","Yuanyuan Fang"],"pdf_url":"https://arxiv.org/pdf/2403.08554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08553v1","updated":"2024-03-13T14:06:18Z","published":"2024-03-13T14:06:18Z","title":"Regret Analysis of Policy Optimization over Submanifolds for Linearly\n  Constrained Online LQG","summary":"  Recent advancement in online optimization and control has provided novel\ntools to study online linear quadratic regulator (LQR) problems, where cost\nmatrices are varying adversarially over time. However, the controller\nparameterization of existing works may not satisfy practical conditions like\nsparsity due to physical connections. In this work, we study online linear\nquadratic Gaussian problems with a given linear constraint imposed on the\ncontroller. Inspired by the recent work of [1] which proposed, for a linearly\nconstrained policy optimization of an offline LQR, a second order method\nequipped with a Riemannian metric that emerges naturally in the context of\noptimal control problems, we propose online optimistic Newton on manifold\n(OONM) which provides an online controller based on the prediction on the first\nand second order information of the function sequence. To quantify the proposed\nalgorithm, we leverage the notion of regret defined as the sub-optimality of\nits cumulative cost to that of a (locally) minimizing controller sequence and\nprovide the regret bound in terms of the path-length of the minimizer sequence.\nSimulation results are also provided to verify the property of OONM.\n","authors":["Ting-Jui Chang","Shahin Shahrampour"],"pdf_url":"https://arxiv.org/pdf/2403.08553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08550v1","updated":"2024-03-13T14:02:42Z","published":"2024-03-13T14:02:42Z","title":"CINA: Conditional Implicit Neural Atlas for Spatio-Temporal\n  Representation of Fetal Brains","summary":"  We introduce a conditional implicit neural atlas (CINA) for spatio-temporal\natlas generation from Magnetic Resonance Images (MRI) of the neurotypical and\npathological fetal brain, that is fully independent of affine or non-rigid\nregistration. During training, CINA learns a general representation of the\nfetal brain and encodes subject specific information into latent code. After\ntraining, CINA can construct a faithful atlas with tissue probability maps of\nthe fetal brain for any gestational age (GA) and anatomical variation covered\nwithin the training domain. Thus, CINA is competent to represent both,\nneurotypical and pathological brains. Furthermore, a trained CINA model can be\nfit to brain MRI of unseen subjects via test-time optimization of the latent\ncode. CINA can then produce probabilistic tissue maps tailored to a particular\nsubject. We evaluate our method on a total of 198 T2 weighted MRI of normal and\nabnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's\ncapability to represent a fetal brain atlas that can be flexibly conditioned on\nGA and on anatomical variations like ventricular volume or degree of cortical\nfolding, making it a suitable tool for modeling both neurotypical and\npathological brains. We quantify the fidelity of our atlas by means of tissue\nsegmentation and age prediction and compare it to an established baseline. CINA\ndemonstrates superior accuracy for neurotypical brains and pathological brains\nwith ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23\nweeks in fetal brain age prediction, further confirming an accurate\nrepresentation of fetal brain development.\n","authors":["Maik Dannecker","Vanessa Kyriakopoulou","Lucilio Cordero-Grande","Anthony N. Price","Joseph V. Hajnal","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2403.08550v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.06757v2","updated":"2024-03-13T13:57:42Z","published":"2024-03-11T14:29:56Z","title":"Koopman Ensembles for Probabilistic Time Series Forecasting","summary":"  In the context of an increasing popularity of data-driven models to represent\ndynamical systems, many machine learning-based implementations of the Koopman\noperator have recently been proposed. However, the vast majority of those works\nare limited to deterministic predictions, while the knowledge of uncertainty is\ncritical in fields like meteorology and climatology. In this work, we\ninvestigate the training of ensembles of models to produce stochastic outputs.\nWe show through experiments on real remote sensing image time series that\nensembles of independently trained models are highly overconfident and that\nusing a training criterion that explicitly encourages the members to produce\npredictions with high inter-model variances greatly improves the uncertainty\nquantification of the ensembles.\n","authors":["Anthony Frion","Lucas Drumetz","Guillaume Tochon","Mauro Dalla Mura","Albdeldjalil Aïssa El Bey"],"pdf_url":"https://arxiv.org/pdf/2403.06757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08540v1","updated":"2024-03-13T13:54:00Z","published":"2024-03-13T13:54:00Z","title":"Language models scale reliably with over-training and on downstream\n  tasks","summary":"  Scaling laws are useful guides for developing language models, but there are\nstill gaps between current scaling studies and how language models are\nultimately trained and evaluated. For instance, scaling is usually studied in\nthe compute-optimal training regime (i.e., \"Chinchilla optimal\" regime);\nhowever, in practice, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but\nultimately models are compared based on downstream task performance. In this\npaper, we address both shortcomings. To do so, we create a testbed of 104\nmodels with 0.011B to 6.9B parameters trained with various numbers of tokens on\nthree data distributions. First, we investigate scaling in the over-trained\nregime. We fit scaling laws that extrapolate in both the number of model\nparameters and the ratio of training tokens to parameters. This enables us to\npredict the validation loss of a 1.4B parameter, 900B token run (i.e.,\n32$\\times$ over-trained) and a 6.9B parameter, 138B token\nrun$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.\nSecond, we relate the perplexity of a language model to its downstream task\nperformance via a power law. We use this law to predict top-1 error averaged\nover downstream tasks for the two aforementioned models using experiments that\ntake 20$\\times$ less compute. Our experiments are available at\nhttps://github.com/mlfoundations/scaling.\n","authors":["Samir Yitzhak Gadre","Georgios Smyrnis","Vaishaal Shankar","Suchin Gururangan","Mitchell Wortsman","Rulin Shao","Jean Mercat","Alex Fang","Jeffrey Li","Sedrick Keh","Rui Xin","Marianna Nezhurina","Igor Vasiljevic","Jenia Jitsev","Alexandros G. Dimakis","Gabriel Ilharco","Shuran Song","Thomas Kollar","Yair Carmon","Achal Dave","Reinhard Heckel","Niklas Muennighoff","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2403.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08536v1","updated":"2024-03-13T13:51:02Z","published":"2024-03-13T13:51:02Z","title":"HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional\n  Image Classifiers","summary":"  Convolutional Neural Networks (CNNs) are nowadays the model of choice in\nComputer Vision, thanks to their ability to automatize the feature extraction\nprocess in visual tasks. However, the knowledge acquired during training is\nfully subsymbolic, and hence difficult to understand and explain to end users.\nIn this paper, we propose a new technique called HOLMES (HOLonym-MEronym based\nSemantic inspection) that decomposes a label into a set of related concepts,\nand provides component-level explanations for an image classification model.\nSpecifically, HOLMES leverages ontologies, web scraping and transfer learning\nto automatically construct meronym (parts)-based detectors for a given holonym\n(class). Then, it produces heatmaps at the meronym level and finally, by\nprobing the holonym CNN with occluded images, it highlights the importance of\neach part on the classification output. Compared to state-of-the-art saliency\nmethods, HOLMES takes a step further and provides information about both where\nand what the holonym CNN is looking at, without relying on densely annotated\ndatasets and without forcing concepts to be associated to single computational\nunits. Extensive experimental evaluation on different categories of objects\n(animals, tools and vehicles) shows the feasibility of our approach. On\naverage, HOLMES explanations include at least two meronyms, and the ablation of\na single meronym roughly halves the holonym model confidence. The resulting\nheatmaps were quantitatively evaluated using the\ndeletion/insertion/preservation curves. All metrics were comparable to those\nachieved by GradCAM, while offering the advantage of further decomposing the\nheatmap in human-understandable concepts, thus highlighting both the relevance\nof meronyms to object classification, as well as HOLMES ability to capture it.\nThe code is available at https://github.com/FrancesC0de/HOLMES.\n","authors":["Francesco Dibitonto","Fabio Garcea","André Panisson","Alan Perotti","Lia Morra"],"pdf_url":"https://arxiv.org/pdf/2403.08536v1.pdf","comment":"This work has been accepted to be presented to The 1st World\n  Conference on eXplainable Artificial Intelligence (xAI 2023), July 26-28,\n  2023 - Lisboa, Portugal"},{"id":"http://arxiv.org/abs/2310.04264v3","updated":"2024-03-13T13:42:40Z","published":"2023-10-06T14:11:21Z","title":"Deep learning modelling of manufacturing and build variations on\n  multi-stage axial compressors aerodynamics","summary":"  Application of deep learning methods to physical simulations such as CFD\n(Computational Fluid Dynamics) for turbomachinery applications, have been so\nfar of limited industrial relevance. This paper demonstrates the development\nand application of a deep learning framework for real-time predictions of the\nimpact of manufacturing and build variations, such as tip clearance and surface\nroughness, on the flow field and aerodynamic performance of multi-stage axial\ncompressors in gas turbines. The associated scatter in compressor efficiency is\nknown to have a significant impact on the corresponding overall performance and\nemissions of the gas turbine, therefore posing a challenge of great industrial\nand environmental relevance. The proposed architecture is proven to achieve an\naccuracy comparable to that of the CFD benchmark, in real-time, for an\nindustrially relevant application. The deployed model, is readily integrated\nwithin the manufacturing and build process of gas turbines, thus providing the\nopportunity to analytically assess the impact on performance and potentially\nreduce requirements for expensive physical tests.\n","authors":["Giuseppe Bruni","Sepehr Maleki","Senthil K. Krishnababu"],"pdf_url":"https://arxiv.org/pdf/2310.04264v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08525v1","updated":"2024-03-13T13:33:35Z","published":"2024-03-13T13:33:35Z","title":"From Weak to Strong Sound Event Labels using Adaptive Change-Point\n  Detection and Active Learning","summary":"  In this work we propose an audio recording segmentation method based on an\nadaptive change point detection (A-CPD) for machine guided weak label\nannotation of audio recording segments. The goal is to maximize the amount of\ninformation gained about the temporal activation's of the target sounds. For\neach unlabeled audio recording, we use a prediction model to derive a\nprobability curve used to guide annotation. The prediction model is initially\npre-trained on available annotated sound event data with classes that are\ndisjoint from the classes in the unlabeled dataset. The prediction model then\ngradually adapts to the annotations provided by the annotator in an active\nlearning loop. The queries used to guide the weak label annotator towards\nstrong labels are derived using change point detection on these probabilities.\nWe show that it is possible to derive strong labels of high quality even with a\nlimited annotation budget, and show favorable results for A-CPD when compared\nto two baseline query strategies.\n","authors":["John Martinsson","Olof Mogren","Maria Sandsten","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2403.08525v1.pdf","comment":"Under review at EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2308.09262v3","updated":"2024-03-13T13:15:11Z","published":"2023-08-18T02:36:21Z","title":"Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality\n  Assessment Model","summary":"  This study proposes a multi-task pseudo-label learning (MPL)-based\nnon-intrusive speech quality assessment model called MTQ-Net. MPL consists of\ntwo stages: obtaining pseudo-label scores from a pretrained model and\nperforming multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS),\nNoise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The\npretrained MOSA-Net model is utilized to estimate three pseudo labels:\nperceptual evaluation of speech quality (PESQ), short-time objective\nintelligibility (STOI), and speech distortion index (SDI). Multi-task learning\nis then employed to train MTQ-Net by combining a supervised loss (derived from\nthe difference between the estimated score and the ground-truth label) and a\nsemi-supervised loss (derived from the difference between the estimated score\nand the pseudo label), where the Huber loss is employed as the loss function.\nExperimental results first demonstrate the advantages of MPL compared to\ntraining a model from scratch and using a direct knowledge transfer mechanism.\nSecond, the benefit of the Huber loss for improving the predictive ability of\nMTQ-Net is verified. Finally, the MTQ-Net with the MPL approach exhibits higher\noverall predictive power compared to other SSL-based speech assessment models.\n","authors":["Ryandhimas E. Zezario","Bo-Ren Brian Bai","Chiou-Shann Fuh","Hsin-Min Wang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2308.09262v3.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2311.14864v2","updated":"2024-03-13T13:12:10Z","published":"2023-11-24T22:42:37Z","title":"Effective Structural Encodings via Local Curvature Profiles","summary":"  Structural and Positional Encodings can significantly improve the performance\nof Graph Neural Networks in downstream tasks. Recent literature has begun to\nsystematically investigate differences in the structural properties that these\napproaches encode, as well as performance trade-offs between them. However, the\nquestion of which structural properties yield the most effective encoding\nremains open. In this paper, we investigate this question from a geometric\nperspective. We propose a novel structural encoding based on discrete Ricci\ncurvature (Local Curvature Profiles, short LCP) and show that it significantly\noutperforms existing encoding approaches. We further show that combining local\nstructural encodings, such as LCP, with global positional encodings improves\ndownstream performance, suggesting that they capture complementary geometric\ninformation. Finally, we compare different encoding types with\n(curvature-based) rewiring techniques. Rewiring has recently received a surge\nof interest due to its ability to improve the performance of Graph Neural\nNetworks by mitigating over-smoothing and over-squashing effects. Our results\nsuggest that utilizing curvature information for structural encodings delivers\nsignificantly larger performance increases than rewiring.\n","authors":["Lukas Fesser","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2311.14864v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01678v4","updated":"2024-03-13T13:02:57Z","published":"2023-12-04T07:01:54Z","title":"Jellyfish: A Large Language Model for Data Preprocessing","summary":"  This paper explores the utilization of LLMs for data preprocessing (DP), a\ncrucial step in the data mining pipeline that transforms raw data into a clean\nformat conducive to easy processing. Whereas the use of LLMs has sparked\ninterest in devising universal solutions to DP, recent initiatives in this\ndomain typically rely on GPT APIs, raising inevitable data breach concerns.\nUnlike these approaches, we consider instruction-tuning local LLMs (7 - 13B\nmodels) as universal DP ask solver. We select a collection of datasets across\nfour representative DP tasks and construct instruction-tuning data using\nserialization and knowledge injection techniques tailored to DP. As such, the\ninstruction-tuned LLMs empower users to manually craft instructions for DP.\nMeanwhile, they can operate on a local, single, and low-priced GPU, ensuring\ndata security and enabling further tuning. Our experiments show that our\ndataset constructed for DP instruction tuning, namely Jellyfish, effectively\nenhances LLMs' DP performances and barely compromises their abilities in NLP\ntasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the\nmodels deliver competitiveness compared to state-of-the-art DP methods and\nstrong generalizability to unseen tasks. The models' performance rivals that of\nGPT series models, and the interpretation offers enhanced reasoning\ncapabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available\nat Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B\nhttps://huggingface.co/NECOUDBFM/Jellyfish-13B\n","authors":["Haochen Zhang","Yuyang Dong","Chuan Xiao","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2312.01678v4.pdf","comment":"a.k.a. \"Jellyfish: Instruction-Tuning Local Large Language Models for\n  Data Preprocessing''"},{"id":"http://arxiv.org/abs/2303.16521v4","updated":"2024-03-13T12:53:31Z","published":"2023-03-29T08:23:26Z","title":"Hard Regularization to Prevent Deep Online Clustering Collapse without\n  Data Augmentation","summary":"  Online deep clustering refers to the joint use of a feature extraction\nnetwork and a clustering model to assign cluster labels to each new data point\nor batch as it is processed. While faster and more versatile than offline\nmethods, online clustering can easily reach the collapsed solution where the\nencoder maps all inputs to the same point and all are put into a single\ncluster. Successful existing models have employed various techniques to avoid\nthis problem, most of which require data augmentation or which aim to make the\naverage soft assignment across the dataset the same for each cluster. We\npropose a method that does not require data augmentation, and that, differently\nfrom existing methods, regularizes the hard assignments. Using a Bayesian\nframework, we derive an intuitive optimization objective that can be\nstraightforwardly included in the training of the encoder network. Tested on\nfour image datasets and one human-activity recognition dataset, it consistently\navoids collapse more robustly than other methods and leads to more accurate\nclustering. We also conduct further experiments and analyses justifying our\nchoice to regularize the hard cluster assignments. Code is available at\nhttps://github.com/Lou1sM/online_hard_clustering.\n","authors":["Louis Mahon","Thomas Lukasiewicz"],"pdf_url":"https://arxiv.org/pdf/2303.16521v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08481v1","updated":"2024-03-13T12:46:51Z","published":"2024-03-13T12:46:51Z","title":"SoK: Reducing the Vulnerability of Fine-tuned Language Models to\n  Membership Inference Attacks","summary":"  Natural language processing models have experienced a significant upsurge in\nrecent years, with numerous applications being built upon them. Many of these\napplications require fine-tuning generic base models on customized, proprietary\ndatasets. This fine-tuning data is especially likely to contain personal or\nsensitive information about individuals, resulting in increased privacy risk.\nMembership inference attacks are the most commonly employed attack to assess\nthe privacy leakage of a machine learning model. However, limited research is\navailable on the factors that affect the vulnerability of language models to\nthis kind of attack, or on the applicability of different defense strategies in\nthe language domain. We provide the first systematic review of the\nvulnerability of fine-tuned large language models to membership inference\nattacks, the various factors that come into play, and the effectiveness of\ndifferent defense strategies. We find that some training methods provide\nsignificantly reduced privacy risk, with the combination of differential\nprivacy and low-rank adaptors achieving the best privacy protection against\nthese attacks.\n","authors":["Guy Amit","Abigail Goldsteen","Ariel Farkash"],"pdf_url":"https://arxiv.org/pdf/2403.08481v1.pdf","comment":"preliminary version"},{"id":"http://arxiv.org/abs/2403.08477v1","updated":"2024-03-13T12:46:03Z","published":"2024-03-13T12:46:03Z","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through\n  Sparse Interpolated Experts","summary":"  Conventional wisdom suggests parameter-efficient fine-tuning of foundation\nmodels as the state-of-the-art method for transfer learning in vision,\nreplacing the rich literature of alternatives such as meta-learning. In trying\nto harness the best of both worlds, meta-tuning introduces a subsequent\noptimization stage of foundation models but has so far only shown limited\nsuccess and crucially tends to underperform on out-of-domain (OOD) tasks. In\nthis paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse\nmixture-of-experts approaches and trained to isolate subsets of pre-trained\nparameters automatically for meta-tuning on each task. SMAT successfully\novercomes OOD sensitivity and delivers on the promise of enhancing the transfer\nabilities of vision foundation models beyond parameter-efficient finetuning. We\nestablish new state-of-the-art results on a challenging combination of\nMeta-Dataset augmented with additional OOD tasks in both zero-shot and\ngradient-based adaptation settings. In addition, we provide a thorough analysis\nof the superiority of learned over hand-designed sparsity patterns for sparse\nexpert methods and the pivotal importance of the sparsity level in balancing\nbetween in-domain and out-of-domain generalization. Our code is publicly\navailable.\n","authors":["Shengzhuang Chen","Jihoon Tack","Yunqiao Yang","Yee Whye Teh","Jonathan Richard Schwarz","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2403.08477v1.pdf","comment":"18 pages, preprint"},{"id":"http://arxiv.org/abs/2403.08469v1","updated":"2024-03-13T12:31:08Z","published":"2024-03-13T12:31:08Z","title":"An Analysis of Human Alignment of Latent Diffusion Models","summary":"  Diffusion models, trained on large amounts of data, showed remarkable\nperformance for image synthesis. They have high error consistency with humans\nand low texture bias when used for classification. Furthermore, prior work\ndemonstrated the decomposability of their bottleneck layer representations into\nsemantic directions. In this work, we analyze how well such representations are\naligned to human responses on a triplet odd-one-out task. We find that despite\nthe aforementioned observations: I) The representational alignment with humans\nis comparable to that of models trained only on ImageNet-1k. II) The most\naligned layers of the denoiser U-Net are intermediate layers and not the\nbottleneck. III) Text conditioning greatly improves alignment at high noise\nlevels, hinting at the importance of abstract textual information, especially\nin the early stage of generation.\n","authors":["Lorenz Linhardt","Marco Morik","Sidney Bender","Naima Elosegui Borras"],"pdf_url":"https://arxiv.org/pdf/2403.08469v1.pdf","comment":"Accepted at the ICLR 2024 Workshop on Representational Alignment"},{"id":"http://arxiv.org/abs/2403.08464v1","updated":"2024-03-13T12:26:55Z","published":"2024-03-13T12:26:55Z","title":"Diffusion Models with Implicit Guidance for Medical Anomaly Detection","summary":"  Diffusion models have advanced unsupervised anomaly detection by improving\nthe transformation of pathological images into pseudo-healthy equivalents.\nNonetheless, standard approaches may compromise critical information during\npathology removal, leading to restorations that do not align with unaffected\nregions in the original scans. Such discrepancies can inadvertently increase\nfalse positive rates and reduce specificity, complicating radiological\nevaluations. This paper introduces Temporal Harmonization for Optimal\nRestoration (THOR), which refines the de-noising process by integrating\nimplicit guidance through temporal anomaly maps. THOR aims to preserve the\nintegrity of healthy tissue in areas unaffected by pathology. Comparative\nevaluations show that THOR surpasses existing diffusion-based methods in\ndetecting and segmenting anomalies in brain MRIs and wrist X-rays. Code:\nhttps://github.com/ci-ber/THOR_DDPM.\n","authors":["Cosmin I. Bercea","Benedikt Wiestler","Daniel Rueckert","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2403.08464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08462v1","updated":"2024-03-13T12:25:47Z","published":"2024-03-13T12:25:47Z","title":"Authorship Verification based on the Likelihood Ratio of Grammar Models","summary":"  Authorship Verification (AV) is the process of analyzing a set of documents\nto determine whether they were written by a specific author. This problem often\narises in forensic scenarios, e.g., in cases where the documents in question\nconstitute evidence for a crime. Existing state-of-the-art AV methods use\ncomputational solutions that are not supported by a plausible scientific\nexplanation for their functioning and that are often difficult for analysts to\ninterpret. To address this, we propose a method relying on calculating a\nquantity we call $\\lambda_G$ (LambdaG): the ratio between the likelihood of a\ndocument given a model of the Grammar for the candidate author and the\nlikelihood of the same document given a model of the Grammar for a reference\npopulation. These Grammar Models are estimated using $n$-gram language models\nthat are trained solely on grammatical features. Despite not needing large\namounts of data for training, LambdaG still outperforms other established AV\nmethods with higher computational complexity, including a fine-tuned Siamese\nTransformer network. Our empirical evaluation based on four baseline methods\napplied to twelve datasets shows that LambdaG leads to better results in terms\nof both accuracy and AUC in eleven cases and in all twelve cases if considering\nonly topic-agnostic methods. The algorithm is also highly robust to important\nvariations in the genre of the reference population in many cross-genre\ncomparisons. In addition to these properties, we demonstrate how LambdaG is\neasier to interpret than the current state-of-the-art. We argue that the\nadvantage of LambdaG over other methods is due to fact that it is compatible\nwith Cognitive Linguistic theories of language processing.\n","authors":["Andrea Nini","Oren Halvani","Lukas Graner","Valerio Gherardi","Shunichi Ishihara"],"pdf_url":"https://arxiv.org/pdf/2403.08462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01225v4","updated":"2024-03-13T12:21:06Z","published":"2023-10-02T14:12:53Z","title":"A path-norm toolkit for modern networks: consequences, promises and\n  challenges","summary":"  This work introduces the first toolkit around path-norms that fully\nencompasses general DAG ReLU networks with biases, skip connections and any\noperation based on the extraction of order statistics: max pooling, GroupSort\netc. This toolkit notably allows us to establish generalization bounds for\nmodern neural networks that are not only the most widely applicable path-norm\nbased ones, but also recover or beat the sharpest known bounds of this type.\nThese extended path-norms further enjoy the usual benefits of path-norms: ease\nof computation, invariance under the symmetries of the network, and improved\nsharpness on layered fully-connected networks compared to the product of\noperator norms, another complexity measure most commonly used.\n  The versatility of the toolkit and its ease of implementation allow us to\nchallenge the concrete promises of path-norm-based generalization bounds, by\nnumerically evaluating the sharpest known bounds for ResNets on ImageNet.\n","authors":["Antoine Gonon","Nicolas Brisebarre","Elisa Riccietti","Rémi Gribonval"],"pdf_url":"https://arxiv.org/pdf/2310.01225v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03690v2","updated":"2024-03-13T12:04:36Z","published":"2023-12-06T18:53:45Z","title":"Inverse Design of Vitrimeric Polymers by Molecular Dynamics and\n  Generative Modeling","summary":"  Vitrimer is a new class of sustainable polymers with the ability of\nself-healing through rearrangement of dynamic covalent adaptive networks.\nHowever, a limited choice of constituent molecules restricts their property\nspace, prohibiting full realization of their potential applications. Through a\ncombination of molecular dynamics (MD) simulations and machine learning (ML),\nparticularly a novel graph variational autoencoder (VAE) model, we establish a\nmethod for generating novel vitrimers and guide their inverse design based on\ndesired glass transition temperature (Tg). We build the first vitrimer dataset\nof one million and calculate Tg on 8,424 of them by high-throughput MD\nsimulations calibrated by a Gaussian process model. The proposed VAE employs\ndual graph encoders and a latent dimension overlapping scheme which allows for\nindividual representation of multi-component vitrimers. By constructing a\ncontinuous latent space containing necessary information of vitrimers, we\ndemonstrate high accuracy and efficiency of our framework in discovering novel\nvitrimers with desirable Tg beyond the training regime. The proposed vitrimers\nwith reasonable synthesizability cover a wide range of Tg and broaden the\npotential widespread usage of vitrimeric materials.\n","authors":["Yiwen Zheng","Prakash Thakolkaran","Jake A. Smith","Ziheng Lu","Shuxin Zheng","Bichlien H. Nguyen","Siddhant Kumar","Aniruddh Vashisth"],"pdf_url":"https://arxiv.org/pdf/2312.03690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08448v1","updated":"2024-03-13T12:03:27Z","published":"2024-03-13T12:03:27Z","title":"Actor-Critic Physics-informed Neural Lyapunov Control","summary":"  Designing control policies for stabilization tasks with provable guarantees\nis a long-standing problem in nonlinear control. A crucial performance metric\nis the size of the resulting region of attraction, which essentially serves as\na robustness \"margin\" of the closed-loop system against uncertainties. In this\npaper, we propose a new method to train a stabilizing neural network controller\nalong with its corresponding Lyapunov certificate, aiming to maximize the\nresulting region of attraction while respecting the actuation constraints.\nCrucial to our approach is the use of Zubov's Partial Differential Equation\n(PDE), which precisely characterizes the true region of attraction of a given\ncontrol policy. Our framework follows an actor-critic pattern where we\nalternate between improving the control policy (actor) and learning a Zubov\nfunction (critic). Finally, we compute the largest certifiable region of\nattraction by invoking an SMT solver after the training procedure. Our\nnumerical experiments on several design problems show consistent and\nsignificant improvements in the size of the resulting region of attraction.\n","authors":["Jiarui Wang","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2403.08448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17453v2","updated":"2024-03-13T12:02:25Z","published":"2024-02-27T12:26:07Z","title":"DS-Agent: Automated Data Science by Empowering Large Language Models\n  with Case-Based Reasoning","summary":"  In this work, we investigate the potential of large language models (LLMs)\nbased agents to automate data science tasks, with the goal of comprehending\ntask requirements, then building and training the best-fit machine learning\nmodels. Despite their widespread success, existing LLM agents are hindered by\ngenerating unreasonable experiment plans within this scenario. To this end, we\npresent DS-Agent, a novel automatic framework that harnesses LLM agent and\ncase-based reasoning (CBR). In the development stage, DS-Agent follows the CBR\nframework to structure an automatic iteration pipeline, which can flexibly\ncapitalize on the expert knowledge from Kaggle, and facilitate consistent\nperformance improvement through the feedback mechanism. Moreover, DS-Agent\nimplements a low-resource deployment stage with a simplified CBR paradigm to\nadapt past successful solutions from the development stage for direct code\ngeneration, significantly reducing the demand on foundational capabilities of\nLLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success\nrate in the development stage, while attaining 36% improvement on average one\npass rate across alternative LLMs in the deployment stage. In both stages,\nDS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per\nrun with GPT-4, respectively. Our code is open-sourced at\nhttps://github.com/guosyjlu/DS-Agent.\n","authors":["Siyuan Guo","Cheng Deng","Ying Wen","Hechang Chen","Yi Chang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04431v4","updated":"2024-03-13T12:01:29Z","published":"2023-01-11T12:19:28Z","title":"Adaptive proximal algorithms for convex optimization under local\n  Lipschitz continuity of the gradient","summary":"  Backtracking linesearch is the de facto approach for minimizing continuously\ndifferentiable functions with locally Lipschitz gradient. In recent years, it\nhas been shown that in the convex setting it is possible to avoid linesearch\naltogether, and to allow the stepsize to adapt based on a local smoothness\nestimate without any backtracks or evaluations of the function value. In this\nwork we propose an adaptive proximal gradient method, adaPG, that uses novel\nestimates of the local smoothness modulus which leads to less conservative\nstepsize updates and that can additionally cope with nonsmooth terms. This idea\nis extended to the primal-dual setting where an adaptive three-term primal-dual\nalgorithm, adaPD, is proposed which can be viewed as an extension of the PDHG\nmethod. Moreover, in this setting the \"essentially\" fully adaptive variant\nadaPD$^+$ is proposed that avoids evaluating the linear operator norm by\ninvoking a backtracking procedure, that, remarkably, does not require extra\ngradient evaluations. Numerical simulations demonstrate the effectiveness of\nthe proposed algorithms compared to the state of the art.\n","authors":["Puya Latafat","Andreas Themelis","Lorenzo Stella","Panagiotis Patrinos"],"pdf_url":"https://arxiv.org/pdf/2301.04431v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08444v1","updated":"2024-03-13T11:56:10Z","published":"2024-03-13T11:56:10Z","title":"COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud\n  Environments","summary":"  In this work, we present COSTREAM, a novel learned cost model for Distributed\nStream Processing Systems that provides accurate predictions of the execution\ncosts of a streaming query in an edge-cloud environment. The cost model can be\nused to find an initial placement of operators across heterogeneous hardware,\nwhich is particularly important in these environments. In our evaluation, we\ndemonstrate that COSTREAM can produce highly accurate cost estimates for the\ninitial operator placement and even generalize to unseen placements, queries,\nand hardware. When using COSTREAM to optimize the placements of streaming\noperators, a median speed-up of around 21x can be achieved compared to\nbaselines.\n","authors":["Roman Heinrich","Carsten Binnig","Harald Kornmayer","Manisha Luthra"],"pdf_url":"https://arxiv.org/pdf/2403.08444v1.pdf","comment":"This paper has been accepted by IEEE ICDE 2024"},{"id":"http://arxiv.org/abs/2403.08438v1","updated":"2024-03-13T11:44:30Z","published":"2024-03-13T11:44:30Z","title":"Reproducibility and Geometric Intrinsic Dimensionality: An Investigation\n  on Graph Neural Network Research","summary":"  Difficulties in replication and reproducibility of empirical evidences in\nmachine learning research have become a prominent topic in recent years.\nEnsuring that machine learning research results are sound and reliable requires\nreproducibility, which verifies the reliability of research findings using the\nsame code and data. This promotes open and accessible research, robust\nexperimental workflows, and the rapid integration of new findings. Evaluating\nthe degree to which research publications support these different aspects of\nreproducibility is one goal of the present work. For this we introduce an\nontology of reproducibility in machine learning and apply it to methods for\ngraph neural networks. Building on these efforts we turn towards another\ncritical challenge in machine learning, namely the curse of dimensionality,\nwhich poses challenges in data collection, representation, and analysis, making\nit harder to find representative data and impeding the training and inference\nprocesses. Using the closely linked concept of geometric intrinsic dimension we\ninvestigate to which extend the used machine learning models are influenced by\nthe intrinsic dimension of the data sets they are trained on.\n","authors":["Tobias Hille","Maximilian Stubbemann","Tom Hanika"],"pdf_url":"https://arxiv.org/pdf/2403.08438v1.pdf","comment":"39 pages, 9 figures"},{"id":"http://arxiv.org/abs/2301.10164v2","updated":"2024-03-13T11:43:22Z","published":"2023-01-17T15:06:33Z","title":"Lowering Detection in Sport Climbing Based on Orientation of the Sensor\n  Enhanced Quickdraw","summary":"  Tracking climbers' activity to improve services and make the best use of\ntheir infrastructure is a concern for climbing gyms. Each climbing session must\nbe analyzed from beginning till lowering of the climber. Therefore, spotting\nthe climbers descending is crucial since it indicates when the ascent has come\nto an end. This problem must be addressed while preserving privacy and\nconvenience of the climbers and the costs of the gyms. To this aim, a hardware\nprototype is developed to collect data using accelerometer sensors attached to\na piece of climbing equipment mounted on the wall, called quickdraw, that\nconnects the climbing rope to the bolt anchors. The corresponding sensors are\nconfigured to be energy-efficient, hence become practical in terms of expenses\nand time consumption for replacement when using in large quantity in a climbing\ngym. This paper describes hardware specifications, studies data measured by the\nsensors in ultra-low power mode, detect sensors' orientation patterns during\nlowering different routes, and develop an supervised approach to identify\nlowering.\n","authors":["Sadaf Moaveninejad","Andrea Janes","Camillo Porcaro"],"pdf_url":"https://arxiv.org/pdf/2301.10164v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2211.02680"},{"id":"http://arxiv.org/abs/2403.08428v1","updated":"2024-03-13T11:26:43Z","published":"2024-03-13T11:26:43Z","title":"DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued\n  Neural Networks","summary":"  Deep Neural Networks are widely used in academy as well as corporate and\npublic applications, including safety critical applications such as health care\nand autonomous driving. The ability to explain their output is critical for\nsafety reasons as well as acceptance among applicants. A multitude of methods\nhave been proposed to explain real-valued neural networks. Recently,\ncomplex-valued neural networks have emerged as a new class of neural networks\ndealing with complex-valued input data without the necessity of projecting them\nonto $\\mathbb{R}^2$. This brings up the need to develop explanation algorithms\nfor this kind of neural networks. In this paper we provide these developments.\nWhile we focus on adapting the widely used DeepSHAP algorithm to the complex\ndomain, we also present versions of four gradient based explanation methods\nsuitable for use in complex-valued neural networks. We evaluate the explanation\nquality of all presented algorithms and provide all of them as an open source\nlibrary adaptable to most recent complex-valued neural network architectures.\n","authors":["Florian Eilers","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.08428v1.pdf","comment":"14 Pages plus 4 Pages Appendix"},{"id":"http://arxiv.org/abs/2308.09945v2","updated":"2024-03-13T11:21:11Z","published":"2023-08-19T08:41:41Z","title":"Dual Branch Deep Learning Network for Detection and Stage Grading of\n  Diabetic Retinopathy","summary":"  Diabetic retinopathy is a severe complication of diabetes that can lead to\npermanent blindness if not treated promptly. Early and accurate diagnosis of\nthe disease is essential for successful treatment. This paper introduces a deep\nlearning method for the detection and stage grading of diabetic retinopathy,\nusing a single fundus retinal image. Our model utilizes transfer learning,\nemploying two state-of-the-art pre-trained models as feature extractors and\nfine-tuning them on a new dataset. The proposed model is trained on a large\nmulti-center dataset, including the APTOS 2019 dataset, obtained from publicly\navailable sources. It achieves remarkable performance in diabetic retinopathy\ndetection and stage classification on the APTOS 2019, outperforming the\nestablished literature. For binary classification, the proposed approach\nachieves an accuracy of 98.50, a sensitivity of 99.46, and a specificity of\n97.51. In stage grading, it achieves a quadratic weighted kappa of 93.00, an\naccuracy of 89.60, a sensitivity of 89.60, and a specificity of 97.72. The\nproposed approach serves as a reliable screening and stage grading tool for\ndiabetic retinopathy, offering significant potential to enhance clinical\ndecision-making and patient care.\n","authors":["Hossein Shakibania","Sina Raoufi","Behnam Pourafkham","Hassan Khotanlou","Muharram Mansoorizadeh"],"pdf_url":"https://arxiv.org/pdf/2308.09945v2.pdf","comment":"Published in the Biomedical Signal Processing & Control journal, 16\n  pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.07743v2","updated":"2024-03-13T11:20:19Z","published":"2024-03-12T15:22:05Z","title":"Equipping Computational Pathology Systems with Artifact Processing\n  Pipelines: A Showcase for Computation and Performance Trade-offs","summary":"  Histopathology is a gold standard for cancer diagnosis under a microscopic\nexamination. However, histological tissue processing procedures result in\nartifacts, which are ultimately transferred to the digitized version of glass\nslides, known as whole slide images (WSIs). Artifacts are diagnostically\nirrelevant areas and may result in wrong deep learning (DL) algorithms\npredictions. Therefore, detecting and excluding artifacts in the computational\npathology (CPATH) system is essential for reliable automated diagnosis. In this\npaper, we propose a mixture of experts (MoE) scheme for detecting five notable\nartifacts, including damaged tissue, blur, folded tissue, air bubbles, and\nhistologically irrelevant blood from WSIs. First, we train independent binary\nDL models as experts to capture particular artifact morphology. Then, we\nensemble their predictions using a fusion mechanism. We apply probabilistic\nthresholding over the final probability distribution to improve the sensitivity\nof the MoE. We developed DL pipelines using two MoEs and two multiclass models\nof state-of-the-art deep convolutional neural networks (DCNNs) and vision\ntransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed\nsimpler multiclass models and were tested on datasets from different hospitals\nand cancer types, where MoE using DCNNs yielded the best results. The proposed\nMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining\nless computational cost for inference than MoE using ViTs. This best\nperformance of MoEs comes with relatively higher computational trade-offs than\nmulticlass models. The proposed artifact detection pipeline will not only\nensure reliable CPATH predictions but may also provide quality control.\n","authors":["Neel Kanwal","Farbod Khoraminia","Umay Kiraz","Andres Mosquera-Zamudio","Carlos Monteagudo","Emiel A. M. Janssen","Tahlita C. M. Zuiverloon","Chunmig Rong","Kjersti Engan"],"pdf_url":"https://arxiv.org/pdf/2403.07743v2.pdf","comment":"Submitted to BMC Medical Informatics and Decision Making Journal"},{"id":"http://arxiv.org/abs/2312.05720v3","updated":"2024-03-13T11:19:24Z","published":"2023-12-10T01:19:59Z","title":"Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer\n  Inputs of Language Models in Federated Learning","summary":"  Language models trained via federated learning (FL) demonstrate impressive\ncapabilities in handling complex tasks while protecting user privacy. Recent\nstudies indicate that leveraging gradient information and prior knowledge can\npotentially reveal training samples within FL setting. However, these\ninvestigations have overlooked the potential privacy risks tied to the\nintrinsic architecture of the models. This paper presents a two-stage privacy\nattack strategy that targets the vulnerabilities in the architecture of\ncontemporary language models, significantly enhancing attack performance by\ninitially recovering certain feature directions as additional supervisory\nsignals. Our comparative experiments demonstrate superior attack performance\nacross various datasets and scenarios, highlighting the privacy leakage risk\nassociated with the increasingly complex architectures of language models. We\ncall for the community to recognize and address these potential privacy risks\nin designing large language models.\n","authors":["Jianwei Li","Sheng Liu","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2312.05720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08417v1","updated":"2024-03-13T11:05:40Z","published":"2024-03-13T11:05:40Z","title":"The Development and Performance of a Machine Learning Based Mobile\n  Platform for Visually Determining the Etiology of Penile Pathology","summary":"  Machine-learning algorithms can facilitate low-cost, user-guided visual\ndiagnostic platforms for addressing disparities in access to sexual health\nservices. We developed a clinical image dataset using original and augmented\nimages for five penile diseases: herpes eruption, syphilitic chancres, penile\ncandidiasis, penile cancer, and genital warts. We used a U-net architecture\nmodel for semantic pixel segmentation into background or subject image, the\nInception-ResNet version 2 neural architecture to classify each pixel as\ndiseased or non-diseased, and a salience map using GradCAM++. We trained the\nmodel on a random 91% sample of the image database using 150 epochs per image,\nand evaluated the model on the remaining 9% of images, assessing recall (or\nsensitivity), precision, specificity, and F1-score (accuracy). Of the 239\nimages in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%)\nwere of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of\npenile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were\nof non-diseased penises. The overall accuracy of the model for correctly\nclassifying the diseased image was 0.944. Between July 1st and October 1st\n2023, there were 2,640 unique users of the mobile platform. Among a random\nsample of submissions (n=437), 271 (62.0%) were from the United States, 64\n(14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United\nKingdom, and 21 (4.8%) from Vietnam. The majority (n=277 [63.4%]) were between\n18 and 30 years old. We report on the development of a machine-learning model\nfor classifying five penile diseases, which demonstrated excellent performance\non a validation dataset. That model is currently in use globally and has the\npotential to improve access to diagnostic services for penile diseases.\n","authors":["Lao-Tzu Allan-Blitz","Sithira Ambepitiya","Raghavendra Tirupathi","Jeffrey D. Klausner","Yudara Kularathne"],"pdf_url":"https://arxiv.org/pdf/2403.08417v1.pdf","comment":"12 pages, 2 figure, 2 tables"},{"id":"http://arxiv.org/abs/2403.08414v1","updated":"2024-03-13T10:58:55Z","published":"2024-03-13T10:58:55Z","title":"Causal Graph Neural Networks for Wildfire Danger Prediction","summary":"  Wildfire forecasting is notoriously hard due to the complex interplay of\ndifferent factors such as weather conditions, vegetation types and human\nactivities. Deep learning models show promise in dealing with this complexity\nby learning directly from data. However, to inform critical decision making, we\nargue that we need models that are right for the right reasons; that is, the\nimplicit rules learned should be grounded by the underlying processes driving\nwildfires. In that direction, we propose integrating causality with Graph\nNeural Networks (GNNs) that explicitly model the causal mechanism among complex\nvariables via graph learning. The causal adjacency matrix considers the\nsynergistic effect among variables and removes the spurious links from highly\ncorrelated impacts. Our methodology's effectiveness is demonstrated through\nsuperior performance forecasting wildfire patterns in the European boreal and\nmediterranean biome. The gain is especially prominent in a highly imbalanced\ndataset, showcasing an enhanced robustness of the model to adapt to regime\nshifts in functional relationships. Furthermore, SHAP values from our trained\nmodel further enhance our understanding of the model's inner workings.\n","authors":["Shan Zhao","Ioannis Prapas","Ilektra Karasante","Zhitong Xiong","Ioannis Papoutsis","Gustau Camps-Valls","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.08414v1.pdf","comment":"Accepted by ICLR 2024 Machine Learning for Remote Sensing (ML4RS)\n  Workshop"},{"id":"http://arxiv.org/abs/2311.05657v2","updated":"2024-03-13T10:54:21Z","published":"2023-11-09T00:30:13Z","title":"Agent Lumos: Unified and Modular Training for Open-Source Language\n  Agents","summary":"  Closed-source agents suffer from several issues such as a lack of\naffordability, transparency, and reproducibility, particularly on complex\ninteractive tasks. This motivates the development of open-source alternatives.\nWe introduce LUMOS, one of the first frameworks for training open-source\nLLM-based agents. LUMOS features a learnable, unified, and modular architecture\nwith a planning module that learns high-level subgoal generation, and a\ngrounding module trained to translate these into actions using various tools in\nthe execution module. The design allows for modular upgrades and wider\napplicability to diverse interactive tasks. To foster generalizable agent\nlearning, we collect large-scale, unified, and high-quality training\nannotations derived from diverse ground-truth reasoning rationales across\nvarious complex interactive tasks. On 9 datasets, LUMOS exhibits several key\nadvantages: (1) LUMOS excels multiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type. LUMOS even surpasses GPT\nagents on QA and web tasks; (2) LUMOS outperforms open-source agents produced\nby chain-of-thoughts and unmodularized integrated training; and (3) LUMOS\neffectively generalizes to unseen tasks, outperforming 33B-scale agents and\ndomain-specific agents.\n","authors":["Da Yin","Faeze Brahman","Abhilasha Ravichander","Khyathi Chandu","Kai-Wei Chang","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2311.05657v2.pdf","comment":"Project website: https://allenai.github.io/lumos/"},{"id":"http://arxiv.org/abs/2403.08408v1","updated":"2024-03-13T10:51:38Z","published":"2024-03-13T10:51:38Z","title":"Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve\n  Generalization Performance of Deep Classification Models","summary":"  The generalization performance of deep neural networks in classification\ntasks is a major concern in machine learning research. Despite widespread\ntechniques used to diminish the over-fitting issue such as data augmentation,\npseudo-labeling, regularization, and ensemble learning, this performance still\nneeds to be enhanced with other approaches. In recent years, it has been\ntheoretically demonstrated that the loss function characteristics i.e. its\nLipschitzness and maximum value affect the generalization performance of deep\nneural networks which can be utilized as a guidance to propose novel distance\nmeasures. In this paper, by analyzing the aforementioned characteristics, we\nintroduce a distance called Reduced Jeffries-Matusita as a loss function for\ntraining deep classification models to reduce the over-fitting issue. In our\nexperiments, we evaluate the new loss function in two different problems: image\nclassification in computer vision and node classification in the context of\ngraph learning. The results show that the new distance measure stabilizes the\ntraining process significantly, enhances the generalization ability, and\nimproves the performance of the models in the Accuracy and F1-score metrics,\neven if the training set size is small.\n","authors":["Mohammad Lashkari","Amin Gheibi"],"pdf_url":"https://arxiv.org/pdf/2403.08408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08403v1","updated":"2024-03-13T10:37:52Z","published":"2024-03-13T10:37:52Z","title":"FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo\n  Time-Series Data using Discrete Relaxation","summary":"  Conventional feature selection algorithms applied to Pseudo Time-Series (PTS)\ndata, which consists of observations arranged in sequential order without\nadhering to a conventional temporal dimension, often exhibit impractical\ncomputational complexities with high dimensional data. To address this\nchallenge, we introduce a Deep Learning (DL)-based feature selection algorithm:\nFeature Selection through Discrete Relaxation (FSDR), tailored for PTS data.\nUnlike the existing feature selection algorithms, FSDR learns the important\nfeatures as model parameters using discrete relaxation, which refers to the\nprocess of approximating a discrete optimisation problem with a continuous one.\nFSDR is capable of accommodating a high number of feature dimensions, a\ncapability beyond the reach of existing DL-based or traditional methods.\nThrough testing on a hyperspectral dataset (i.e., a type of PTS data), our\nexperimental results demonstrate that FSDR outperforms three commonly used\nfeature selection algorithms, taking into account a balance among execution\ntime, $R^2$, and $RMSE$.\n","authors":["Mohammad Rahman","Manzur Murshed","Shyh Wei Teng","Manoranjan Paul"],"pdf_url":"https://arxiv.org/pdf/2403.08403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04621v2","updated":"2024-03-13T10:23:45Z","published":"2023-06-07T17:50:59Z","title":"Flexible Distribution Alignment: Towards Long-tailed Semi-supervised\n  Learning with Proper Calibration","summary":"  Long-tailed semi-supervised learning (LTSSL) represents a practical scenario\nfor semi-supervised applications, challenged by skewed labeled distributions\nthat bias classifiers. This problem is often aggravated by discrepancies\nbetween labeled and unlabeled class distributions, leading to biased\npseudo-labels, neglect of rare classes, and poorly calibrated probabilities. To\naddress these issues, we introduce Flexible Distribution Alignment (FlexDA), a\nnovel adaptive logit-adjusted loss framework designed to dynamically estimate\nand align predictions with the actual distribution of unlabeled data and\nachieve a balanced classifier by the end of training. FlexDA is further\nenhanced by a distillation-based consistency loss, promoting fair data usage\nacross classes and effectively leveraging underconfident samples. This method,\nencapsulated in ADELLO (Align and Distill Everything All at Once), proves\nrobust against label shift, significantly improves model calibration in LTSSL\ncontexts, and surpasses previous state-of-of-art approaches across multiple\nbenchmarks, including CIFAR100-LT, STL10-LT, and ImageNet127, addressing class\nimbalance challenges in semi-supervised learning. Our code will be made\navailable upon paper acceptance.\n","authors":["Emanuel Sanchez Aimar","Hannah Helgesen","Yonghao Xu","Marco Kuhlmann","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2306.04621v2.pdf","comment":"Under review, 24 pages"},{"id":"http://arxiv.org/abs/2312.04142v2","updated":"2024-03-13T10:23:10Z","published":"2023-12-07T08:56:44Z","title":"TimeDRL: Disentangled Representation Learning for Multivariate\n  Time-Series","summary":"  Multivariate time-series data in numerous real-world applications (e.g.,\nhealthcare and industry) are informative but challenging due to the lack of\nlabels and high dimensionality. Recent studies in self-supervised learning have\nshown their potential in learning rich representations without relying on\nlabels, yet they fall short in learning disentangled embeddings and addressing\nissues of inductive bias (e.g., transformation-invariance). To tackle these\nchallenges, we propose TimeDRL, a generic multivariate time-series\nrepresentation learning framework with disentangled dual-level embeddings.\nTimeDRL is characterized by three novel features: (i) disentangled derivation\nof timestamp-level and instance-level embeddings from patched time-series data\nusing a [CLS] token strategy; (ii) utilization of timestamp-predictive and\ninstance-contrastive tasks for disentangled representation learning, with the\nformer optimizing timestamp-level embeddings with predictive loss, and the\nlatter optimizing instance-level embeddings with contrastive loss; and (iii)\navoidance of augmentation methods to eliminate inductive biases, such as\ntransformation-invariance from cropping and masking. Comprehensive experiments\non 6 time-series forecasting datasets and 5 time-series classification datasets\nhave shown that TimeDRL consistently surpasses existing representation learning\napproaches, achieving an average improvement of forecasting by 58.02% in MSE\nand classification by 1.48% in accuracy. Furthermore, extensive ablation\nstudies confirmed the relative contribution of each component in TimeDRL's\narchitecture, and semi-supervised learning evaluations demonstrated its\neffectiveness in real-world scenarios, even with limited labeled data. The code\nis available at https://github.com/blacksnail789521/TimeDRL.\n","authors":["Ching Chang","Chiao-Tung Chan","Wei-Yao Wang","Wen-Chih Peng","Tien-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2312.04142v2.pdf","comment":"This paper has been accepted by the International Conference on Data\n  Engineering (ICDE) 2024"},{"id":"http://arxiv.org/abs/2111.07058v4","updated":"2024-03-13T10:20:14Z","published":"2021-11-13T06:54:36Z","title":"Bolstering Stochastic Gradient Descent with Model Building","summary":"  Stochastic gradient descent method and its variants constitute the core\noptimization algorithms that achieve good convergence rates for solving machine\nlearning problems. These rates are obtained especially when these algorithms\nare fine-tuned for the application at hand. Although this tuning process can\nrequire large computational costs, recent work has shown that these costs can\nbe reduced by line search methods that iteratively adjust the step length. We\npropose an alternative approach to stochastic line search by using a new\nalgorithm based on forward step model building. This model building step\nincorporates second-order information that allows adjusting not only the step\nlength but also the search direction. Noting that deep learning model\nparameters come in groups (layers of tensors), our method builds its model and\ncalculates a new step for each parameter group. This novel diagonalization\napproach makes the selected step lengths adaptive. We provide convergence rate\nanalysis, and experimentally show that the proposed algorithm achieves faster\nconvergence and better generalization in well-known test problems. More\nprecisely, SMB requires less tuning, and shows comparable performance to other\nadaptive methods.\n","authors":["S. Ilker Birbil","Ozgur Martin","Gonenc Onay","Figen Oztoprak"],"pdf_url":"https://arxiv.org/pdf/2111.07058v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03522v2","updated":"2024-03-13T09:50:40Z","published":"2024-03-06T08:03:05Z","title":"Non-verbal information in spontaneous speech -- towards a new framework\n  of analysis","summary":"  Non-verbal signals in speech are encoded by prosody and carry information\nthat ranges from conversation action to attitude and emotion. Despite its\nimportance, the principles that govern prosodic structure are not yet\nadequately understood. This paper offers an analytical schema and a\ntechnological proof-of-concept for the categorization of prosodic signals and\ntheir association with meaning. The schema interprets surface-representations\nof multi-layered prosodic events. As a first step towards implementation, we\npresent a classification process that disentangles prosodic phenomena of three\norders. It relies on fine-tuning a pre-trained speech recognition model,\nenabling the simultaneous multi-class/multi-label detection. It generalizes\nover a large variety of spontaneous data, performing on a par with, or superior\nto, human annotation. In addition to a standardized formalization of prosody,\ndisentangling prosodic patterns can direct a theory of communication and speech\norganization. A welcome by-product is an interpretation of prosody that will\nenhance speech- and language-related technologies.\n","authors":["Tirza Biron","Moshe Barboy","Eran Ben-Artzy","Alona Golubchik","Yanir Marmor","Smadar Szekely","Yaron Winter","David Harel"],"pdf_url":"https://arxiv.org/pdf/2403.03522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08386v1","updated":"2024-03-13T09:49:26Z","published":"2024-03-13T09:49:26Z","title":"Optimizing Risk-averse Human-AI Hybrid Teams","summary":"  We anticipate increased instances of humans and AI systems working together\nin what we refer to as a hybrid team. The increase in collaboration is expected\nas AI systems gain proficiency and their adoption becomes more widespread.\nHowever, their behavior is not error-free, making hybrid teams a very suitable\nsolution. As such, we consider methods for improving performance for these\nteams of humans and AI systems. For hybrid teams, we will refer to both the\nhumans and AI systems as agents. To improve team performance over that seen for\nagents operating individually, we propose a manager which learns, through a\nstandard Reinforcement Learning scheme, how to best delegate, over time, the\nresponsibility of taking a decision to any of the agents. We further guide the\nmanager's learning so they also minimize how many changes in delegation are\nmade resulting from undesirable team behavior. We demonstrate the optimality of\nour manager's performance in several grid environments which include failure\nstates which terminate an episode and should be avoided. We perform our\nexperiments with teams of agents with varying degrees of acceptable risk, in\nthe form of proximity to a failure state, and measure the manager's ability to\nmake effective delegation decisions with respect to its own risk-based\nconstraints, then compare these to the optimal decisions. Our results show our\nmanager can successfully learn desirable delegations which result in team paths\nnear/exactly optimal with respect to path length and number of delegations.\n","authors":["Andrew Fuchs","Andrea Passarella","Marco Conti"],"pdf_url":"https://arxiv.org/pdf/2403.08386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08376v1","updated":"2024-03-13T09:39:15Z","published":"2024-03-13T09:39:15Z","title":"Nonlinear Manifold Learning Determines Microgel Size from Raman\n  Spectroscopy","summary":"  Polymer particle size constitutes a crucial characteristic of product quality\nin polymerization. Raman spectroscopy is an established and reliable process\nanalytical technology for in-line concentration monitoring. Recent approaches\nand some theoretical considerations show a correlation between Raman signals\nand particle sizes but do not determine polymer size from Raman spectroscopic\nmeasurements accurately and reliably. With this in mind, we propose three\nalternative machine learning workflows to perform this task, all involving\ndiffusion maps, a nonlinear manifold learning technique for dimensionality\nreduction: (i) directly from diffusion maps, (ii) alternating diffusion maps,\nand (iii) conformal autoencoder neural networks. We apply the workflows to a\ndata set of Raman spectra with associated size measured via dynamic light\nscattering of 47 microgel (cross-linked polymer) samples in a diameter range of\n208nm to 483 nm. The conformal autoencoders substantially outperform\nstate-of-the-art methods and results for the first time in a promising\nprediction of polymer size from Raman spectra.\n","authors":["Eleni D. Koronaki","Luise F. Kaven","Johannes M. M. Faust","Ioannis G. Kevrekidis","Alexander Mitsos"],"pdf_url":"https://arxiv.org/pdf/2403.08376v1.pdf","comment":"51 pages, 12 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.08370v1","updated":"2024-03-13T09:31:50Z","published":"2024-03-13T09:31:50Z","title":"SMART: Submodular Data Mixture Strategy for Instruction Tuning","summary":"  Instruction Tuning involves finetuning a language model on a collection of\ninstruction-formatted datasets in order to enhance the generalizability of the\nmodel to unseen tasks. Studies have shown the importance of balancing different\ntask proportions during finetuning, but finding the right balance remains\nchallenging. Unfortunately, there's currently no systematic method beyond\nmanual tuning or relying on practitioners' intuition. In this paper, we\nintroduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a\nnovel data mixture strategy which makes use of a submodular function to assign\nimportance scores to tasks which are then used to determine the mixture\nweights. Given a fine-tuning budget, SMART redistributes the budget among tasks\nand selects non-redundant samples from each task. Experimental results\ndemonstrate that SMART significantly outperforms traditional methods such as\nexamples proportional mixing and equal mixing. Furthermore, SMART facilitates\nthe creation of data mixtures based on a few representative subsets of tasks\nalone and through task pruning analysis, we reveal that in a limited budget\nsetting, allocating budget among a subset of representative tasks yields\nsuperior performance compared to distributing the budget among all tasks. The\ncode for reproducing our results is open-sourced at\nhttps://github.com/kowndinya-renduchintala/SMART.\n","authors":["H S V N S Kowndinya Renduchintala","Sumit Bhatia","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08364v1","updated":"2024-03-13T09:24:59Z","published":"2024-03-13T09:24:59Z","title":"Decoupled Federated Learning on Long-Tailed and Non-IID data with\n  Feature Statistics","summary":"  Federated learning is designed to enhance data security and privacy, but\nfaces challenges when dealing with heterogeneous data in long-tailed and\nnon-IID distributions. This paper explores an overlooked scenario where tail\nclasses are sparsely distributed over a few clients, causing the models trained\nwith these classes to have a lower probability of being selected during client\naggregation, leading to slower convergence rates and poorer model performance.\nTo address this issue, we propose a two-stage Decoupled Federated learning\nframework using Feature Statistics (DFL-FS). In the first stage, the server\nestimates the client's class coverage distributions through masked local\nfeature statistics clustering to select models for aggregation to accelerate\nconvergence and enhance feature learning without privacy leakage. In the second\nstage, DFL-FS employs federated feature regeneration based on global feature\nstatistics and utilizes resampling and weighted covariance to calibrate the\nglobal classifier to enhance the model's adaptability to long-tailed data\ndistributions. We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets\nwith various long-tailed rates. The results demonstrate that our method\noutperforms state-of-the-art methods in both accuracy and convergence rate.\n","authors":["Zhuoxin Chen","Zhenyu Wu","Yang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08362v1","updated":"2024-03-13T09:22:30Z","published":"2024-03-13T09:22:30Z","title":"Mean-Field Microcanonical Gradient Descent","summary":"  Microcanonical gradient descent is a sampling procedure for energy-based\nmodels allowing for efficient sampling of distributions in high dimension. It\nworks by transporting samples from a high-entropy distribution, such as\nGaussian white noise, to a low-energy region using gradient descent. We put\nthis model in the framework of normalizing flows, showing how it can often\noverfit by losing an unnecessary amount of entropy in the descent. As a remedy,\nwe propose a mean-field microcanonical gradient descent that samples several\nweakly coupled data points simultaneously, allowing for better control of the\nentropy loss while paying little in terms of likelihood fit. We study these\nmodels in the context of financial time series, illustrating the improvements\non both synthetic and real data.\n","authors":["Marcus Häggbom","Morten Karlsmark","Joakim andén"],"pdf_url":"https://arxiv.org/pdf/2403.08362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08352v1","updated":"2024-03-13T09:00:38Z","published":"2024-03-13T09:00:38Z","title":"Data augmentation with automated machine learning: approaches and\n  performance comparison with classical data augmentation methods","summary":"  Data augmentation is arguably the most important regularization technique\ncommonly used to improve generalization performance of machine learning models.\nIt primarily involves the application of appropriate data transformation\noperations to create new data samples with desired properties. Despite its\neffectiveness, the process is often challenging because of the time-consuming\ntrial and error procedures for creating and testing different candidate\naugmentations and their hyperparameters manually. Automated data augmentation\nmethods aim to automate the process. State-of-the-art approaches typically rely\non automated machine learning (AutoML) principles. This work presents a\ncomprehensive survey of AutoML-based data augmentation techniques. We discuss\nvarious approaches for accomplishing data augmentation with AutoML, including\ndata manipulation, data integration and data synthesis techniques. We present\nextensive discussion of techniques for realizing each of the major subtasks of\nthe data augmentation process: search space design, hyperparameter optimization\nand model evaluation. Finally, we carried out an extensive comparison and\nanalysis of the performance of automated data augmentation techniques and\nstate-of-the-art methods based on classical augmentation approaches. The\nresults show that AutoML methods for data augmentation currently outperform\nstate-of-the-art techniques based on conventional approaches.\n","authors":["Alhassan Mumuni","Fuseini Mumuni"],"pdf_url":"https://arxiv.org/pdf/2403.08352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08344v1","updated":"2024-03-13T08:49:40Z","published":"2024-03-13T08:49:40Z","title":"STMPL: Human Soft-Tissue Simulation","summary":"  In various applications, such as virtual reality and gaming, simulating the\ndeformation of soft tissues in the human body during interactions with external\nobjects is essential. Traditionally, Finite Element Methods (FEM) have been\nemployed for this purpose, but they tend to be slow and resource-intensive. In\nthis paper, we propose a unified representation of human body shape and soft\ntissue with a data-driven simulator of non-rigid deformations. This approach\nenables rapid simulation of realistic interactions.\n  Our method builds upon the SMPL model, which generates human body shapes\nconsidering rigid transformations. We extend SMPL by incorporating a soft\ntissue layer and an intuitive representation of external forces applied to the\nbody during object interactions. Specifically, we mapped the 3D body shape and\nsoft tissue and applied external forces to 2D UV maps. Leveraging a UNET\narchitecture designed for 2D data, our approach achieves high-accuracy\ninference in real time. Our experiment shows that our method achieves plausible\ndeformation of the soft tissue layer, even for unseen scenarios.\n","authors":["Anton Agafonov","Lihi Zelnik-Manor"],"pdf_url":"https://arxiv.org/pdf/2403.08344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18765v3","updated":"2024-03-13T08:47:32Z","published":"2023-11-30T18:05:52Z","title":"MLLMs-Augmented Visual-Language Representation Learning","summary":"  Visual-language pre-training has achieved remarkable success in many\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that Multi-modal Large\nLanguage Models (MLLMs) can enhance visual-language representation learning by\nestablishing richer image-text associations for image-text datasets. Our\napproach is simple, utilizing MLLMs to extend multiple diverse captions for\neach image. To prevent the bias introduced by MLLMs' hallucinations and\nmonotonous language styles, we propose \"text shearing\" to maintain the quality\nand availability of extended captions. In image-text retrieval, without\nintroducing additional training cost, our method consistently obtains 5.6 ~\n35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and\nzero-shot settings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.\n","authors":["Yanqing Liu","Kai Wang","Wenqi Shao","Ping Luo","Yu Qiao","Mike Zheng Shou","Kaipeng Zhang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2311.18765v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01188v2","updated":"2024-03-13T08:45:53Z","published":"2023-10-02T13:26:43Z","title":"Quantifying the Plausibility of Context Reliance in Neural Machine\n  Translation","summary":"  Establishing whether language models can use contextual information in a\nhuman-plausible way is important to ensure their trustworthiness in real-world\nsettings. However, the questions of when and which parts of the context affect\nmodel generations are typically tackled separately, with current plausibility\nevaluations being practically limited to a handful of artificial benchmarks. To\naddress this, we introduce Plausibility Evaluation of Context Reliance\n(PECoRe), an end-to-end interpretability framework designed to quantify context\nusage in language models' generations. Our approach leverages model internals\nto (i) contrastively identify context-sensitive target tokens in generated\ntexts and (ii) link them to contextual cues justifying their prediction. We use\n\\pecore to quantify the plausibility of context-aware machine translation\nmodels, comparing model rationales with human annotations across several\ndiscourse-level phenomena. Finally, we apply our method to unannotated model\ntranslations to identify context-mediated predictions and highlight instances\nof (im)plausible context usage throughout generation.\n","authors":["Gabriele Sarti","Grzegorz Chrupała","Malvina Nissim","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2310.01188v2.pdf","comment":"ICLR 2024 Camera Ready. Code: https://github.com/gsarti/pecore.\n  Artifacts:\n  https://huggingface.co/collections/gsarti/pecore-iclr-2024-65edab42e28439e21b612c2e"},{"id":"http://arxiv.org/abs/2403.08337v1","updated":"2024-03-13T08:41:55Z","published":"2024-03-13T08:41:55Z","title":"LLM-Assisted Light: Leveraging Large Language Model Capabilities for\n  Human-Mimetic Traffic Signal Control in Complex Urban Environments","summary":"  Traffic congestion in metropolitan areas presents a formidable challenge with\nfar-reaching economic, environmental, and societal ramifications. Therefore,\neffective congestion management is imperative, with traffic signal control\n(TSC) systems being pivotal in this endeavor. Conventional TSC systems,\ndesigned upon rule-based algorithms or reinforcement learning (RL), frequently\nexhibit deficiencies in managing the complexities and variabilities of urban\ntraffic flows, constrained by their limited capacity for adaptation to\nunfamiliar scenarios. In response to these limitations, this work introduces an\ninnovative approach that integrates Large Language Models (LLMs) into TSC,\nharnessing their advanced reasoning and decision-making faculties.\nSpecifically, a hybrid framework that augments LLMs with a suite of perception\nand decision-making tools is proposed, facilitating the interrogation of both\nthe static and dynamic traffic information. This design places the LLM at the\ncenter of the decision-making process, combining external traffic data with\nestablished TSC methods. Moreover, a simulation platform is developed to\ncorroborate the efficacy of the proposed framework. The findings from our\nsimulations attest to the system's adeptness in adjusting to a multiplicity of\ntraffic environments without the need for additional training. Notably, in\ncases of Sensor Outage (SO), our approach surpasses conventional RL-based\nsystems by reducing the average waiting time by $20.4\\%$. This research\nsignifies a notable advance in TSC strategies and paves the way for the\nintegration of LLMs into real-world, dynamic scenarios, highlighting their\npotential to revolutionize traffic management. The related code is available at\n\\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.\n","authors":["Maonan Wang","Aoyu Pang","Yuheng Kan","Man-On Pun","Chung Shue Chen","Bo Huang"],"pdf_url":"https://arxiv.org/pdf/2403.08337v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2403.08335v1","updated":"2024-03-13T08:40:49Z","published":"2024-03-13T08:40:49Z","title":"A Sparsity Principle for Partially Observable Causal Representation\n  Learning","summary":"  Causal representation learning aims at identifying high-level causal\nvariables from perceptual data. Most methods assume that all latent causal\nvariables are captured in the high-dimensional observations. We instead\nconsider a partially observed setting, in which each measurement only provides\ninformation about a subset of the underlying causal state. Prior work has\nstudied this setting with multiple domains or views, each depending on a fixed\nsubset of latents. Here, we focus on learning from unpaired observations from a\ndataset with an instance-dependent partial observability pattern. Our main\ncontribution is to establish two identifiability results for this setting: one\nfor linear mixing functions without parametric assumptions on the underlying\ncausal model, and one for piecewise linear mixing functions with Gaussian\nlatent causal variables. Based on these insights, we propose two methods for\nestimating the underlying causal variables by enforcing sparsity in the\ninferred representation. Experiments on different simulated datasets and\nestablished benchmarks highlight the effectiveness of our approach in\nrecovering the ground-truth latents.\n","authors":["Danru Xu","Dingling Yao","Sébastien Lachapelle","Perouz Taslakian","Julius von Kügelgen","Francesco Locatello","Sara Magliacane"],"pdf_url":"https://arxiv.org/pdf/2403.08335v1.pdf","comment":"33 pages, 18 figures, 9 tables"},{"id":"http://arxiv.org/abs/2403.08333v1","updated":"2024-03-13T08:37:31Z","published":"2024-03-13T08:37:31Z","title":"Fast Inference of Removal-Based Node Influence","summary":"  Graph neural networks (GNNs) are widely utilized to capture the information\nspreading patterns in graphs. While remarkable performance has been achieved,\nthere is a new trending topic of evaluating node influence. We propose a new\nmethod of evaluating node influence, which measures the prediction change of a\ntrained GNN model caused by removing a node. A real-world application is, \"In\nthe task of predicting Twitter accounts' polarity, had a particular account\nbeen removed, how would others' polarity change?\". We use the GNN as a\nsurrogate model whose prediction could simulate the change of nodes or edges\ncaused by node removal. To obtain the influence for every node, a\nstraightforward way is to alternately remove every node and apply the trained\nGNN on the modified graph. It is reliable but time-consuming, so we need an\nefficient method. The related lines of work, such as graph adversarial attack\nand counterfactual explanation, cannot directly satisfy our needs, since they\ndo not focus on the global influence score for every node. We propose an\nefficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA),\nwhich uses the gradient to approximate the node-removal influence. It only\ncosts one forward propagation and one backpropagation to approximate the\ninfluence score for all nodes. Extensive experiments on six datasets and six\nGNN models verify the effectiveness of NORA. Our code is available at\nhttps://github.com/weikai-li/NORA.git.\n","authors":["Weikai Li","Zhiping Xiao","Xiao Luo","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08333v1.pdf","comment":"To be published in the Web Conference 2024"},{"id":"http://arxiv.org/abs/2308.16207v2","updated":"2024-03-13T08:35:00Z","published":"2023-08-30T04:49:24Z","title":"MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural\n  Networks for Continuous and Discrete EEG Emotion Recognition","summary":"  Emotion recognition using electroencephalogram (EEG) mainly has two\nscenarios: classification of the discrete labels and regression of the\ncontinuously tagged labels. Although many algorithms were proposed for\nclassification tasks, there are only a few methods for regression tasks. For\nemotion regression, the label is continuous in time. A natural method is to\nlearn the temporal dynamic patterns. In previous studies, long short-term\nmemory (LSTM) and temporal convolutional neural networks (TCN) were utilized to\nlearn the temporal contextual information from feature vectors of EEG. However,\nthe spatial patterns of EEG were not effectively extracted. To enable the\nspatial learning ability of TCN towards better regression and classification\nperformances, we propose a novel unified model, named MASA-TCN, for EEG emotion\nregression and classification tasks. The space-aware temporal layer enables TCN\nto additionally learn from spatial relations among EEG electrodes. Besides, a\nnovel multi-anchor block with attentive fusion is proposed to learn dynamic\ntemporal dependencies. Experiments on two publicly available datasets show\nMASA-TCN achieves higher results than the state-of-the-art methods for both EEG\nemotion regression and classification tasks. The code is available at\nhttps://github.com/yi-ding-cs/MASA-TCN.\n","authors":["Yi Ding","Su Zhang","Chuangao Tang","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2308.16207v2.pdf","comment":"12 pages, 4 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2402.15281v3","updated":"2024-03-13T08:34:47Z","published":"2024-02-23T12:06:48Z","title":"Neural Implicit Swept Volume Models for Fast Collision Detection","summary":"  Collision detection is one of the most time-consuming operations during\nmotion planning. Thus, there is an increasing interest in exploring machine\nlearning techniques to speed up collision detection and sampling-based motion\nplanning. A recent line of research focuses on utilizing neural signed distance\nfunctions of either the robot geometry or the swept volume of the robot motion.\nBuilding on this, we present a novel neural implicit swept volume model to\ncontinuously represent arbitrary motions parameterized by their start and goal\nconfigurations. This allows to quickly compute signed distances for any point\nin the task space to the robot motion. Further, we present an algorithm\ncombining the speed of the deep learning-based signed distance computations\nwith the strong accuracy guarantees of geometric collision checkers. We\nvalidate our approach in simulated and real-world robotic experiments, and\ndemonstrate that it is able to speed up a commercial bin picking application.\n","authors":["Dominik Joho","Jonas Schwinn","Kirill Safronov"],"pdf_url":"https://arxiv.org/pdf/2402.15281v3.pdf","comment":"To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have\n  equal contribution"},{"id":"http://arxiv.org/abs/2403.08331v1","updated":"2024-03-13T08:34:40Z","published":"2024-03-13T08:34:40Z","title":"Bayesian Optimization that Limits Search Region to Lower Dimensions\n  Utilizing Local GPR","summary":"  Optimization of product and system characteristics is required in many\nfields, including design and control. Bayesian optimization (BO) is often used\nwhen there are high observing costs, because BO theoretically guarantees an\nupper bound on regret. However, computational costs increase exponentially with\nthe number of parameters to be optimized, decreasing search efficiency. We\npropose a BO that limits the search region to lower dimensions and utilizes\nlocal Gaussian process regression (LGPR) to scale the BO to higher dimensions.\nLGPR treats the low-dimensional search region as \"local,\" improving prediction\naccuracies there. The LGPR model is trained on a local subset of data specific\nto that region. This improves prediction accuracy and search efficiency and\nreduces the time complexity of matrix inversion in the Gaussian process\nregression. In evaluations with 20D Ackley and Rosenbrock functions, search\nefficiencies are equal to or higher than those of the compared methods,\nimproved by about 69% and 40% from the case without LGPR. We apply our method\nto an automatic design task for a power semiconductor device. We successfully\nreduce the specific on-resistance to 25% better than a conventional method and\n3.4% better than without LGPR.\n","authors":["Yasunori Taguchi","Hiro Gangi"],"pdf_url":"https://arxiv.org/pdf/2403.08331v1.pdf","comment":"8 pages, 13 figures, 22nd International Conference on Machine\n  Learning and Applications (ICMLA2023)"},{"id":"http://arxiv.org/abs/2307.09249v2","updated":"2024-03-13T08:20:34Z","published":"2023-07-18T13:28:31Z","title":"UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model\n  in Data Science","summary":"  Recent advancements in NLP have witnessed the groundbreaking impact of\npretrained models, yielding impressive outcomes across various tasks. This\nstudy seeks to extend the power of pretraining methodologies to facilitating\nthe prediction over tables in data science, a domain traditionally overlooked,\nyet inherently challenging due to the plethora of table schemas intrinsic to\ndifferent tasks. The primary research questions underpinning this work revolve\naround the establishment of a universal pretraining protocol for tables with\nvaried structures, the generalizability and transferability of learned\nknowledge across tasks, the adaptation to diverse downstream applications, and\nthe incorporation of incremental columns over time. In response to these\nchallenges, we introduce UniTabE, a straightforward yet effective method\ndesigned to process tables in a uniform manner, devoid of constraints imposed\nby specific table structures. UniTabE's core concept relies on representing\neach basic table element with a module, termed TabUnit. This is subsequently\nfollowed by a Transformer encoder to refine the representation. Moreover, our\nmodel is designed to facilitate pretraining and finetuning through the\nutilization of free-form prompts. In order to implement the pretraining phase,\nwe curated an expansive tabular dataset comprising approximately 13B samples,\nmeticulously gathered from the Kaggle platform. This research primarily centers\non classification and regression tasks involving tabular data, and conducts\nrigorous experimental testing and analyses to validate the effectiveness of our\nmethodology. The experimental results demonstrate UniTabE's superior\nperformance against several baselines across massive benchmarks. This,\ntherefore, underscores UniTabE's potential to significantly enhance the\nsemantic representation of tabular data, thereby marking a significant stride\nfor tabular data analysis.\n","authors":["Yazheng Yang","Yuqi Wang","Guang Liu","Ledell Wu","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2307.09249v2.pdf","comment":"ICLR 2024, 9 pages"},{"id":"http://arxiv.org/abs/2304.10151v3","updated":"2024-03-13T08:14:02Z","published":"2023-04-20T08:23:58Z","title":"Flexible K Nearest Neighbors Classifier: Derivation and Application for\n  Ion-mobility Spectrometry-based Indoor Localization","summary":"  The K Nearest Neighbors (KNN) classifier is widely used in many fields such\nas fingerprint-based localization or medicine. It determines the class\nmembership of unlabelled sample based on the class memberships of the K\nlabelled samples, the so-called nearest neighbors, that are closest to the\nunlabelled sample. The choice of K has been the topic of various studies and\nproposed KNN-variants. Yet no variant has been proven to outperform all other\nvariants. In this paper a KNN-variant is discussed which ensures that the K\nnearest neighbors are indeed close to the unlabelled sample and finds K along\nthe way. The algorithm is tested and compared to the standard KNN in\ntheoretical scenarios and for indoor localization based on ion-mobility\nspectrometry fingerprints. It achieves a higher classification accuracy than\nthe KNN in the tests, while having the same computational demand.\n","authors":["Philipp Müller"],"pdf_url":"https://arxiv.org/pdf/2304.10151v3.pdf","comment":"11 pages, 3 figures, paper presented at the 2023 International\n  Conference on Indoor Positioning and Indoor Navigation (IPIN)"},{"id":"http://arxiv.org/abs/2307.02037v3","updated":"2024-03-13T08:11:18Z","published":"2023-07-05T05:42:03Z","title":"Reverse Diffusion Monte Carlo","summary":"  We propose a Monte Carlo sampler from the reverse diffusion process. Unlike\nthe practice of diffusion models, where the intermediary updates -- the score\nfunctions -- are learned with a neural network, we transform the score matching\nproblem into a mean estimation one. By estimating the means of the regularized\nposterior distributions, we derive a novel Monte Carlo sampling algorithm\ncalled reverse diffusion Monte Carlo (rdMC), which is distinct from the Markov\nchain Monte Carlo (MCMC) methods. We determine the sample size from the error\ntolerance and the properties of the posterior distribution to yield an\nalgorithm that can approximately sample the target distribution with any\ndesired accuracy. Additionally, we demonstrate and prove under suitable\nconditions that sampling with rdMC can be significantly faster than that with\nMCMC. For multi-modal target distributions such as those in Gaussian mixture\nmodels, rdMC greatly improves over the Langevin-style MCMC sampling methods\nboth theoretically and in practice. The proposed rdMC method offers a new\nperspective and solution beyond classical MCMC algorithms for the challenging\ncomplex distributions.\n","authors":["Xunpeng Huang","Hanze Dong","Yifan Hao","Yi-An Ma","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.02037v3.pdf","comment":"44 pages, 16 figures, ICLR 2024"},{"id":"http://arxiv.org/abs/2307.02040v3","updated":"2024-03-13T08:06:37Z","published":"2023-07-05T05:55:08Z","title":"VertiBench: Advancing Feature Distribution Diversity in Vertical\n  Federated Learning Benchmarks","summary":"  Vertical Federated Learning (VFL) is a crucial paradigm for training machine\nlearning models on feature-partitioned, distributed data. However, due to\nprivacy restrictions, few public real-world VFL datasets exist for algorithm\nevaluation, and these represent a limited array of feature distributions.\nExisting benchmarks often resort to synthetic datasets, derived from arbitrary\nfeature splits from a global set, which only capture a subset of feature\ndistributions, leading to inadequate algorithm performance assessment. This\npaper addresses these shortcomings by introducing two key factors affecting VFL\nperformance - feature importance and feature correlation - and proposing\nassociated evaluation metrics and dataset splitting methods. Additionally, we\nintroduce a real VFL dataset to address the deficit in image-image VFL\nscenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides\nvaluable insights for future research in the field.\n","authors":["Zhaomin Wu","Junyi Hou","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2307.02040v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07628v5","updated":"2024-03-13T08:02:51Z","published":"2023-01-18T16:12:04Z","title":"Universal Neural-Cracking-Machines: Self-Configurable Password Models\n  from Auxiliary Data","summary":"  We introduce the concept of \"universal password model\" -- a password model\nthat, once pre-trained, can automatically adapt its guessing strategy based on\nthe target system. To achieve this, the model does not need to access any\nplaintext passwords from the target credentials. Instead, it exploits users'\nauxiliary information, such as email addresses, as a proxy signal to predict\nthe underlying password distribution. Specifically, the model uses deep\nlearning to capture the correlation between the auxiliary data of a group of\nusers (e.g., users of a web application) and their passwords. It then exploits\nthose patterns to create a tailored password model for the target system at\ninference time. No further training steps, targeted data collection, or prior\nknowledge of the community's password distribution is required. Besides\nimproving over current password strength estimation techniques and attacks, the\nmodel enables any end-user (e.g., system administrators) to autonomously\ngenerate tailored password models for their systems without the often\nunworkable requirements of collecting suitable training data and fitting the\nunderlying machine learning model. Ultimately, our framework enables the\ndemocratization of well-calibrated password models to the community, addressing\na major challenge in the deployment of password security solutions at scale.\n","authors":["Dario Pasquini","Giuseppe Ateniese","Carmela Troncoso"],"pdf_url":"https://arxiv.org/pdf/2301.07628v5.pdf","comment":"Appearing in the proceedings of the 45th IEEE Symposium on Security\n  and Privacy S&P 2024"},{"id":"http://arxiv.org/abs/2403.08319v1","updated":"2024-03-13T08:02:23Z","published":"2024-03-13T08:02:23Z","title":"Knowledge Conflicts for LLMs: A Survey","summary":"  This survey provides an in-depth analysis of knowledge conflicts for large\nlanguage models (LLMs), highlighting the complex challenges they encounter when\nblending contextual and parametric knowledge. Our focus is on three categories\nof knowledge conflicts: context-memory, inter-context, and intra-memory\nconflict. These conflicts can significantly impact the trustworthiness and\nperformance of LLMs, especially in real-world applications where noise and\nmisinformation are common. By categorizing these conflicts, exploring the\ncauses, examining the behaviors of LLMs under such conflicts, and reviewing\navailable solutions, this survey aims to shed light on strategies for improving\nthe robustness of LLMs, thereby serving as a valuable resource for advancing\nresearch in this evolving area.\n","authors":["Rongwu Xu","Zehan Qi","Cunxiang Wang","Hongru Wang","Yue Zhang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.06388v3","updated":"2024-03-13T07:50:44Z","published":"2022-09-14T03:02:22Z","title":"TSFool: Crafting Highly-Imperceptible Adversarial Time Series through\n  Multi-Objective Attack","summary":"  Recent years have witnessed the success of recurrent neural network (RNN)\nmodels in time series classification (TSC). However, neural networks (NNs) are\nvulnerable to adversarial samples, which cause real-life adversarial attacks\nthat undermine the robustness of AI models. To date, most existing attacks\ntarget at feed-forward NNs and image recognition tasks, but they cannot perform\nwell on RNN-based TSC. This is due to the cyclical computation of RNN, which\nprevents direct model differentiation. In addition, the high visual sensitivity\nof time series to perturbations also poses challenges to local objective\noptimization of adversarial samples. In this paper, we propose an efficient\nmethod called TSFool to craft highly-imperceptible adversarial time series for\nRNN-based TSC. The core idea is a new global optimization objective known as\n\"Camouflage Coefficient\" that captures the imperceptibility of adversarial\nsamples from the class distribution. Based on this, we reduce the adversarial\nattack problem to a multi-objective optimization problem that enhances the\nperturbation quality. Furthermore, to speed up the optimization process, we\npropose to use a representation model for RNN to capture deeply embedded\nvulnerable samples whose features deviate from the latent manifold. Experiments\non 11 UCR and UEA datasets showcase that TSFool significantly outperforms six\nwhite-box and three black-box benchmark attacks in terms of effectiveness,\nefficiency and imperceptibility from various perspectives including standard\nmeasure, human study and real-world defense.\n","authors":["Yanyun Wang","Dehui Du","Haibo Hu","Zi Liang","Yuanhao Liu"],"pdf_url":"https://arxiv.org/pdf/2209.06388v3.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2304.13764v2","updated":"2024-03-13T07:48:50Z","published":"2023-04-26T18:10:35Z","title":"PhagoStat a scalable and interpretable end to end framework for\n  efficient quantification of cell phagocytosis in neurodegenerative disease\n  studies","summary":"  Quantifying the phagocytosis of dynamic, unstained cells is essential for\nevaluating neurodegenerative diseases. However, measuring rapid cell\ninteractions and distinguishing cells from background make this task very\nchallenging when processing time-lapse phase-contrast video microscopy. In this\nstudy, we introduce an end-to-end, scalable, and versatile real-time framework\nfor quantifying and analyzing phagocytic activity. Our proposed pipeline is\nable to process large data-sets and includes a data quality verification module\nto counteract perturbations such as microscope movements and frame blurring. We\nalso propose an explainable cell segmentation module to improve the\ninterpretability of DL methods compared to black-box algorithms. This includes\ntwo interpretable DL capabilities: visual explanation and model simplification.\nWe demonstrate that interpretability in DL is not the opposite of high\nperformance, by additionally providing essential DL algorithm optimization\ninsights and solutions. Besides, incorporating interpretable modules results in\nan efficient architecture design and optimized execution time. We apply our\npipeline to analyze microglial cell phagocytosis in FTD and obtain\nstatistically reliable results showing that FTD mutant cells are larger and\nmore aggressive than control cells. The method has been tested and validated on\npublic benchmarks by generating state-of-the art performances. To stimulate\ntranslational approaches and future studies, we release an open-source\nend-to-end pipeline and a unique microglial cells phagocytosis dataset for\nimmune system characterization in neurodegenerative diseases research. This\npipeline and the associated dataset will consistently crystallize future\nadvances in this field, promoting the development of interpretable algorithms\ndedicated to the domain of neurodegenerative diseases' characterization.\ngithub.com/ounissimehdi/PhagoStat\n","authors":["Mehdi Ounissi","Morwena Latouche","Daniel Racoceanu"],"pdf_url":"https://arxiv.org/pdf/2304.13764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08309v1","updated":"2024-03-13T07:38:20Z","published":"2024-03-13T07:38:20Z","title":"HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain\n  Reinforcement Learning From AI Feedback","summary":"  Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter\nannotation cycles and lower costs over Reinforcement Learning from Human\nFeedback (RLHF), making it highly efficient during the rapid strategy iteration\nperiods of large language model (LLM) training. Using ChatGPT as a labeler to\nprovide feedback on open-domain prompts in RLAIF training, we observe an\nincrease in human evaluators' preference win ratio for model responses, but a\ndecrease in evaluators' satisfaction rate. Analysis suggests that the decrease\nin satisfaction rate is mainly due to some responses becoming less helpful,\nparticularly in terms of correctness and truthfulness, highlighting practical\nlimitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement\nLearning from AI Feedback (HRLAIF). This method enhances the accuracy of AI\nannotations for responses, making the model's helpfulness more robust in\ntraining process. Additionally, it employs AI for Red Teaming, further\nimproving the model's harmlessness. Human evaluation results show that HRLAIF\ninherits the ability of RLAIF to enhance human preference for outcomes at a low\ncost while also improving the satisfaction rate of responses. Compared to the\npolicy model before Reinforcement Learning (RL), it achieves an increase of\n2.08\\% in satisfaction rate, effectively addressing the issue of a decrease of\n4.58\\% in satisfaction rate after basic RLAIF.\n","authors":["Ang Li","Qiugen Xiao","Peng Cao","Jian Tang","Yi Yuan","Zijie Zhao","Xiaoyuan Chen","Liang Zhang","Xiangyang Li","Kaitong Yang","Weidong Guo","Yukang Gan","Daniell Wang","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2403.08309v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.14989v2","updated":"2024-03-13T07:27:47Z","published":"2024-02-22T22:00:03Z","title":"Stable Neural Stochastic Differential Equations in Analyzing Irregular\n  Time Series Data","summary":"  Irregular sampling intervals and missing values in real-world time series\ndata present challenges for conventional methods that assume consistent\nintervals and complete data. Neural Ordinary Differential Equations (Neural\nODEs) offer an alternative approach, utilizing neural networks combined with\nODE solvers to learn continuous latent representations through parameterized\nvector fields. Neural Stochastic Differential Equations (Neural SDEs) extend\nNeural ODEs by incorporating a diffusion term, although this addition is not\ntrivial, particularly when addressing irregular intervals and missing values.\nConsequently, careful design of drift and diffusion functions is crucial for\nmaintaining stability and enhancing performance, while incautious choices can\nresult in adverse properties such as the absence of strong solutions,\nstochastic destabilization, or unstable Euler discretizations, significantly\naffecting Neural SDEs' performance. In this study, we propose three stable\nclasses of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE.\nThen, we rigorously demonstrate their robustness in maintaining excellent\nperformance under distribution shift, while effectively preventing overfitting.\nTo assess the effectiveness of our approach, we conduct extensive experiments\non four benchmark datasets for interpolation, forecasting, and classification\ntasks, and analyze the robustness of our methods with 30 public datasets under\ndifferent missing rates. Our results demonstrate the efficacy of the proposed\nmethod in handling real-world irregular time series data.\n","authors":["YongKyung Oh","Dongyoung Lim","Sungil Kim"],"pdf_url":"https://arxiv.org/pdf/2402.14989v2.pdf","comment":"Accepted at ICLR 2024, Spotlight presentation (Notable Top 5%).\n  https://openreview.net/forum?id=4VIgNuQ1pY"},{"id":"http://arxiv.org/abs/2403.02619v2","updated":"2024-03-13T07:19:06Z","published":"2024-03-05T03:18:43Z","title":"Training Machine Learning models at the Edge: A Survey","summary":"  Edge Computing (EC) has gained significant traction in recent years,\npromising enhanced efficiency by integrating Artificial Intelligence (AI)\ncapabilities at the edge. While the focus has primarily been on the deployment\nand inference of Machine Learning (ML) models at the edge, the training aspect\nremains less explored. This survey delves into Edge Learning (EL), specifically\nthe optimization of ML model training at the edge. The objective is to\ncomprehensively explore diverse approaches and methodologies in EL, synthesize\nexisting knowledge, identify challenges, and highlight future trends. Utilizing\nScopus' advanced search, relevant literature on EL was identified, revealing a\nconcentration of research efforts in distributed learning methods, particularly\nFederated Learning (FL). This survey further provides a guideline for comparing\ntechniques used to optimize ML for edge learning, along with an exploration of\ndifferent frameworks, libraries, and simulation tools available for EL. In\ndoing so, the paper contributes to a holistic understanding of the current\nlandscape and future directions in the intersection of edge computing and\nmachine learning, paving the way for informed comparisons between optimization\nmethods and techniques designed for edge learning.\n","authors":["Aymen Rayane Khouas","Mohamed Reda Bouadjenek","Hakim Hacid","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2403.02619v2.pdf","comment":"30 pages, 7 figures, submitted to IEEE Communications Surveys &\n  Tutorials"},{"id":"http://arxiv.org/abs/2401.01386v3","updated":"2024-03-13T07:14:16Z","published":"2024-01-01T19:58:36Z","title":"Tissue Artifact Segmentation and Severity Analysis for Automated\n  Diagnosis Using Whole Slide Images","summary":"  Traditionally, pathological analysis and diagnosis are performed by manually\neyeballing glass slide specimens under a microscope by an expert. The whole\nslide image is the digital specimen produced from the glass slide. Whole slide\nimage enabled specimens to be observed on a computer screen and led to\ncomputational pathology where computer vision and artificial intelligence are\nutilized for automated analysis and diagnosis. With the current computational\nadvancement, the entire whole slide image can be analyzed autonomously without\nhuman supervision. However, the analysis could fail or lead to wrong diagnosis\nif the whole slide image is affected by tissue artifacts such as tissue fold or\nair bubbles depending on the severity. Existing artifact detection methods rely\non experts for severity assessment to eliminate artifact affected regions from\nthe analysis. This process is time consuming, exhausting and undermines the\ngoal of automated analysis or removal of artifacts without evaluating their\nseverity, which could result in the loss of diagnostically important data.\nTherefore, it is necessary to detect artifacts and then assess their severity\nautomatically. In this paper, we propose a system that incorporates severity\nevaluation with artifact detection utilizing convolutional neural networks. The\nproposed system uses DoubleUNet to segment artifacts and an ensemble network of\nsix fine tuned convolutional neural network models to determine severity. This\nmethod outperformed current state of the art in accuracy by 9 percent for\nartifact segmentation and achieved a strong correlation of 97 percent with the\nevaluation of pathologists for severity assessment. The robustness of the\nsystem was demonstrated using our proposed heterogeneous dataset and practical\nusability was ensured by integrating it with an automated analysis system.\n","authors":["Galib Muhammad Shahriar Himel"],"pdf_url":"https://arxiv.org/pdf/2401.01386v3.pdf","comment":"Master's thesis, 60 pages, 21 figures, 16 tables"},{"id":"http://arxiv.org/abs/2212.12989v4","updated":"2024-03-13T07:07:58Z","published":"2022-12-26T02:32:20Z","title":"Improved Kernel Alignment Regret Bound for Online Kernel Learning","summary":"  In this paper, we improve the kernel alignment regret bound for online kernel\nlearning in the regime of the Hinge loss function. Previous algorithm achieves\na regret of $O((\\mathcal{A}_TT\\ln{T})^{\\frac{1}{4}})$ at a computational\ncomplexity (space and per-round time) of $O(\\sqrt{\\mathcal{A}_TT\\ln{T}})$,\nwhere $\\mathcal{A}_T$ is called \\textit{kernel alignment}. We propose an\nalgorithm whose regret bound and computational complexity are better than\nprevious results. Our results depend on the decay rate of eigenvalues of the\nkernel matrix. If the eigenvalues of the kernel matrix decay exponentially,\nthen our algorithm enjoys a regret of $O(\\sqrt{\\mathcal{A}_T})$ at a\ncomputational complexity of $O(\\ln^2{T})$. Otherwise, our algorithm enjoys a\nregret of $O((\\mathcal{A}_TT)^{\\frac{1}{4}})$ at a computational complexity of\n$O(\\sqrt{\\mathcal{A}_TT})$. We extend our algorithm to batch learning and\nobtain a $O(\\frac{1}{T}\\sqrt{\\mathbb{E}[\\mathcal{A}_T]})$ excess risk bound\nwhich improves the previous $O(1/\\sqrt{T})$ bound.\n","authors":["Junfan Li","Shizhong Liao"],"pdf_url":"https://arxiv.org/pdf/2212.12989v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08291v1","updated":"2024-03-13T06:54:15Z","published":"2024-03-13T06:54:15Z","title":"CleanAgent: Automating Data Standardization with LLM-based Agents","summary":"  Data standardization is a crucial part in data science life cycle. While\ntools like Pandas offer robust functionalities, their complexity and the manual\neffort required for customizing code to diverse column types pose significant\nchallenges. Although large language models (LLMs) like ChatGPT have shown\npromise in automating this process through natural language understanding and\ncode generation, it still demands expert-level programming knowledge and\ncontinuous interaction for prompt refinement. To solve these challenges, our\nkey idea is to propose a Python library with declarative, unified APIs for\nstandardizing column types, simplifying the code generation of LLM with concise\nAPI calls. We first propose Dataprep.Clean which is written as a component of\nthe Dataprep Library, offers a significant reduction in complexity by enabling\nthe standardization of specific column types with a single line of code. Then\nwe introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based\nagents to automate the data standardization process. With CleanAgent, data\nscientists need only provide their requirements once, allowing for a\nhands-free, automatic standardization process.\n","authors":["Danrui Qi","Jiannan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04945v2","updated":"2024-03-13T06:20:47Z","published":"2024-03-07T23:20:56Z","title":"Electrocardiogram Instruction Tuning for Report Generation","summary":"  Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool\nfor cardiac conditions monitoring, are crucial in assisting clinicians. Recent\nstudies have concentrated on classifying cardiac conditions using ECG data but\nhave overlooked ECG report generation, which is not only time-consuming but\nalso requires clinical expertise. To automate ECG report generation and ensure\nits versatility, we propose the Multimodal ECG Instruction Tuning (MEIT)\nframework, the \\textit{first} attempt to tackle ECG report generation with LLMs\nand multimodal instructions. To facilitate future research, we establish a\nbenchmark to evaluate MEIT with various LLMs backbones across two large-scale\nECG datasets. Our approach uniquely aligns the representations of the ECG\nsignal and the report, and we conduct extensive experiments to benchmark MEIT\nwith nine open source LLMs, using more than 800,000 ECG reports. MEIT's results\nunderscore the superior performance of instruction-tuned LLMs, showcasing their\nproficiency in quality report generation, zero-shot capabilities, and\nresilience to signal perturbation. These findings emphasize the efficacy of our\nMEIT framework and its potential for real-world clinical application.\n","authors":["Zhongwei Wan","Che Liu","Xin Wang","Chaofan Tao","Hui Shen","Zhenwu Peng","Jie Fu","Rossella Arcucci","Huaxiu Yao","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.04945v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.07240v2","updated":"2024-03-13T05:55:39Z","published":"2023-10-11T07:08:20Z","title":"CacheGen: Fast Context Loading for Language Model Applications via KV\n  Cache Streaming","summary":"  As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge or\nuser-specific information. Yet using long contexts poses a challenge for\nresponsive LLM systems, as nothing can be generated until the whole context is\nprocessed by the LLM. While the context-processing delay can be reduced by\nreusing the KV cache of a context across different inputs, fetching the KV\ncache, which contains large tensors, over the network can cause extra network\ndelays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, which embraces KV cache's distributional\nproperties, to encode a KV cache into more compact bitstream representations\nwith negligible encoding/decoding overhead. This reduces the bandwidth demand\nto fetch the KV cache. Second, to maintain low context-loading delay and high\ngeneration quality, CacheGen adapts the streaming strategies to cope with\nchanges in available bandwidth. When available bandwidth drops, CacheGen may\nraise the compression level for a part of the context or choose to recompute\nits KV cache on the fly. We test CacheGen on four popular LLMs of various sizes\nand four datasets (662 contexts in total). Compared to the recent systems that\nreuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the\ntotal delay in fetching and processing contexts by 2.7-3.2x while having\nnegligible impact on the LLM response quality in accuracy or perplexity.\n","authors":["Yuhan Liu","Hanchen Li","Yihua Cheng","Siddhant Ray","Yuyang Huang","Qizheng Zhang","Kuntai Du","Jiayi Yao","Shan Lu","Ganesh Ananthanarayanan","Michael Maire","Henry Hoffmann","Ari Holtzman","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.07240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18511v3","updated":"2024-03-13T05:42:44Z","published":"2023-05-29T16:18:28Z","title":"Contextual Bandits with Budgeted Information Reveal","summary":"  Contextual bandit algorithms are commonly used in digital health to recommend\npersonalized treatments. However, to ensure the effectiveness of the\ntreatments, patients are often requested to take actions that have no immediate\nbenefit to them, which we refer to as pro-treatment actions. In practice,\nclinicians have a limited budget to encourage patients to take these actions\nand collect additional information. We introduce a novel optimization and\nlearning algorithm to address this problem. This algorithm effectively combines\nthe strengths of two algorithmic approaches in a seamless manner, including 1)\nan online primal-dual algorithm for deciding the optimal timing to reach out to\npatients, and 2) a contextual bandit learning algorithm to deliver personalized\ntreatment to the patient. We prove that this algorithm admits a sub-linear\nregret bound. We illustrate the usefulness of this algorithm on both synthetic\nand real-world data.\n","authors":["Kyra Gan","Esmaeil Keyvanshokooh","Xueqing Liu","Susan Murphy"],"pdf_url":"https://arxiv.org/pdf/2305.18511v3.pdf","comment":"International Conference on Artificial Intelligence and Statistics,\n  2024"},{"id":"http://arxiv.org/abs/2309.17428v2","updated":"2024-03-13T05:39:25Z","published":"2023-09-29T17:40:26Z","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized\n  Toolsets","summary":"  Large language models (LLMs) are often augmented with tools to solve complex\ntasks. By generating code snippets and executing them through task-specific\nApplication Programming Interfaces (APIs), they can offload certain functions\nto dedicated external modules, such as image encoding and performing\ncalculations. However, most existing approaches to augment LLMs with tools are\nconstrained by general-purpose APIs and lack the flexibility for tailoring them\nto specific tasks. In this work, we present CRAFT, a general tool creation and\nretrieval framework for LLMs. It creates toolsets specifically curated for the\ntasks and equips LLMs with a component that retrieves tools from these sets to\nenhance their capability to solve complex tasks. For each task, we collect\nspecific code solutions by prompting GPT-4 to solve the training examples.\nFollowing a validation step ensuring the correctness, these solutions are\nabstracted into code snippets to enhance reusability, and deduplicated for\nhigher quality. At inference time, the language model retrieves snippets from\nthe toolsets and then executes them or generates the output conditioning on the\nretrieved snippets. Our method is designed to be flexible and offers a\nplug-and-play approach to adapt off-the-shelf LLMs to unseen domains and\nmodalities, without any finetuning. Experiments on vision-language, tabular\nprocessing, and mathematical reasoning tasks show that our approach achieves\nsubstantial improvements compared to strong baselines. In addition, our\nin-depth analysis reveals that: (1) consistent performance improvement can be\nachieved by scaling up the number of tools and the capability of the backbone\nmodels; (2) each component of our approach contributes to the performance\ngains; (3) the created tools are well-structured and reliable with low\ncomplexity and atomicity. The code is available at\nhttps://github.com/lifan-yuan/CRAFT.\n","authors":["Lifan Yuan","Yangyi Chen","Xingyao Wang","Yi R. Fung","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.17428v2.pdf","comment":"Accepted to ICLR 2024. Code is available at\n  https://github.com/lifan-yuan/CRAFT"},{"id":"http://arxiv.org/abs/2403.08267v1","updated":"2024-03-13T05:35:55Z","published":"2024-03-13T05:35:55Z","title":"SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V","summary":"  This paper presents SNOW-SCA, the first power side-channel analysis (SCA)\nattack of a 5G mobile communication security standard candidate, SNOW-V,\nrunning on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic\nknown-key correlation (KKC) analysis to identify the leakage points. Next, a\ncorrelation power analysis (CPA) attack is performed, which reduces the attack\ncomplexity to two key guesses for each key byte. The correct secret key is then\nuniquely identified utilizing linear discriminant analysis (LDA). The profiled\nSCA attack with LDA achieves 100% accuracy after training with $<200$ traces,\nwhich means the attack succeeds with just a single trace. Overall, using the\n\\textit{combined CPA and LDA attack} model, the correct secret key byte is\nrecovered with <50 traces collected using the ChipWhisperer platform. The\nentire 256-bit secret key of SNOW-V can be recovered incrementally using the\nproposed SCA attack. Finally, we suggest low-overhead countermeasures that can\nbe used to prevent these SCA attacks.\n","authors":["Harshit Saurabh","Anupam Golder","Samarth Shivakumar Titti","Suparna Kundu","Chaoyun Li","Angshuman Karmakar","Debayan Das"],"pdf_url":"https://arxiv.org/pdf/2403.08267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08265v1","updated":"2024-03-13T05:32:13Z","published":"2024-03-13T05:32:13Z","title":"Random Search as a Baseline for Sparse Neural Network Architecture\n  Search","summary":"  Sparse neural networks have shown similar or better generalization\nperformance than their dense counterparts while having higher parameter\nefficiency. This has motivated a number of works to learn, induce, or search\nfor high performing sparse networks. While reports of quality or efficiency\ngains are impressive, standard baselines are lacking, therefore hindering\nhaving reliable comparability and reproducibility across methods. In this work,\nwe provide an evaluation approach and a naive Random Search baseline method for\nfinding good sparse configurations. We apply Random Search on the node space of\nan overparameterized network with the goal of finding better initialized sparse\nsub-networks that are positioned more advantageously in the loss landscape. We\nrecord sparse network post-training performances at various levels of sparsity\nand compare against both their fully connected parent networks and random\nsparse configurations at the same sparsity levels. We observe that for this\narchitecture search task, initialized sparse networks found by Random Search\nneither perform better nor converge more efficiently than their random\ncounterparts. Thus we conclude that Random Search may be viewed as a suitable\nneutral baseline for sparsity search methods.\n","authors":["Rezsa Farahani"],"pdf_url":"https://arxiv.org/pdf/2403.08265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08258v1","updated":"2024-03-13T05:20:45Z","published":"2024-03-13T05:20:45Z","title":"Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition","summary":"  Conformer-based attention models have become the de facto backbone model for\nAutomatic Speech Recognition tasks. A blank symbol is usually introduced to\nalign the input and output sequences for CTC or RNN-T models. Unfortunately,\nthe long input length overloads computational budget and memory consumption\nquadratically by attention mechanism. In this work, we propose a\n\"Skip-and-Recover\" Conformer architecture, named Skipformer, to squeeze\nsequence input length dynamically and inhomogeneously. Skipformer uses an\nintermediate CTC output as criteria to split frames into three groups: crucial,\nskipping and ignoring. The crucial group feeds into next conformer blocks and\nits output joint with skipping group by original temporal order as the final\nencoder output. Experiments show that our model reduces the input sequence\nlength by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile,\nthe model can achieve better recognition accuracy and faster inference speed\nthan recent baseline models. Our code is open-sourced and available online.\n","authors":["Wenjing Zhu","Sining Sun","Changhao Shan","Peng Fan","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2403.08258v1.pdf","comment":"Accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.08254v1","updated":"2024-03-13T05:11:24Z","published":"2024-03-13T05:11:24Z","title":"Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and\n  Prospects","summary":"  Personal digital data is a critical asset, and governments worldwide have\nenforced laws and regulations to protect data privacy. Data users have been\nendowed with the right to be forgotten of their data. In the course of machine\nlearning (ML), the forgotten right requires a model provider to delete user\ndata and its subsequent impact on ML models upon user requests. Machine\nunlearning emerges to address this, which has garnered ever-increasing\nattention from both industry and academia. While the area has developed\nrapidly, there is a lack of comprehensive surveys to capture the latest\nadvancements. Recognizing this shortage, we conduct an extensive exploration to\nmap the landscape of machine unlearning including the (fine-grained) taxonomy\nof unlearning algorithms under centralized and distributed settings, debate on\napproximate unlearning, verification and evaluation metrics, challenges and\nsolutions for unlearning under different applications, as well as attacks\ntargeting machine unlearning. The survey concludes by outlining potential\ndirections for future research, hoping to serve as a guide for interested\nscholars.\n","authors":["Na Li","Chunyi Zhou","Yansong Gao","Hui Chen","Anmin Fu","Zhi Zhang","Yu Shui"],"pdf_url":"https://arxiv.org/pdf/2403.08254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05115v3","updated":"2024-03-13T05:02:32Z","published":"2023-08-08T07:50:38Z","title":"PTransIPs: Identification of phosphorylation sites enhanced by protein\n  PLM embeddings","summary":"  Phosphorylation is pivotal in numerous fundamental cellular processes and\nplays a significant role in the onset and progression of various diseases. The\naccurate identification of these phosphorylation sites is crucial for\nunraveling the molecular mechanisms within cells and during viral infections,\npotentially leading to the discovery of novel therapeutic targets. In this\nstudy, we develop PTransIPs, a new deep learning framework for the\nidentification of phosphorylation sites. Independent testing results\ndemonstrate that PTransIPs outperforms existing state-of-the-art (SOTA)\nmethods, achieving AUCs of 0.9232 and 0.9660 for the identification of\nphosphorylated S/T and Y sites, respectively. PTransIPs contributes from three\naspects. 1) PTransIPs is the first to apply protein pre-trained language model\n(PLM) embeddings to this task. It utilizes ProtTrans and EMBER2 to extract\nsequence and structure embeddings, respectively, as additional inputs into the\nmodel, effectively addressing issues of dataset size and overfitting, thus\nenhancing model performance; 2) PTransIPs is based on Transformer architecture,\noptimized through the integration of convolutional neural networks and TIM loss\nfunction, providing practical insights for model design and training; 3) The\nencoding of amino acids in PTransIPs enables it to serve as a universal\nframework for other peptide bioactivity tasks, with its excellent performance\nshown in extended experiments of this paper. Our code, data and models are\npublicly available at https://github.com/StatXzy7/PTransIPs.\n","authors":["Ziyang Xu","Haitian Zhong","Bingrui He","Xueying Wang","Tianchi Lu"],"pdf_url":"https://arxiv.org/pdf/2308.05115v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08246v1","updated":"2024-03-13T05:00:42Z","published":"2024-03-13T05:00:42Z","title":"Towards Unified Modeling for Positive and Negative Preferences in\n  Sign-Aware Recommendation","summary":"  Recently, sign-aware graph recommendation has drawn much attention as it will\nlearn users' negative preferences besides positive ones from both positive and\nnegative interactions (i.e., links in a graph) with items. To accommodate the\ndifferent semantics of negative and positive links, existing works utilize two\nindependent encoders to model users' positive and negative preferences,\nrespectively. However, these approaches cannot learn the negative preferences\nfrom high-order heterogeneous interactions between users and items formed by\nmultiple links with different signs, resulting in inaccurate and incomplete\nnegative user preferences. To cope with these intractable issues, we propose a\nnovel \\textbf{L}ight \\textbf{S}igned \\textbf{G}raph Convolution Network\nspecifically for \\textbf{Rec}ommendation (\\textbf{LSGRec}), which adopts a\nunified modeling approach to simultaneously model high-order users' positive\nand negative preferences on a signed user-item interaction graph. Specifically,\nfor the negative preferences within high-order heterogeneous interactions,\nfirst-order negative preferences are captured by the negative links, while\nhigh-order negative preferences are propagated along positive edges. Then,\nrecommendation results are generated based on positive preferences and\noptimized with negative ones. Finally, we train representations of users and\nitems through different auxiliary tasks. Extensive experiments on three\nreal-world datasets demonstrate that our method outperforms existing baselines\nregarding performance and computational efficiency. Our code is available at\n\\url{https://anonymous.4open.science/r/LSGRec-BB95}.\n","authors":["Yuting Liu","Yizhou Dang","Yuliang Liang","Qiang Liu","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08245v1","updated":"2024-03-13T05:00:23Z","published":"2024-03-13T05:00:23Z","title":"Scattered Mixture-of-Experts Implementation","summary":"  We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE)\non GPUs. ScatterMoE builds upon existing implementations, and overcoming some\nof the limitations to improve inference and training speed, and memory\nfootprint. This implementation achieves this by avoiding padding and making\nexcessive copies of the input. We introduce ParallelLinear, the main component\nwe use to build our implementation and the various kernels used to speed up the\noperation. We benchmark our implementation against Megablocks, and show that it\nenables a higher throughput and lower memory footprint. We also show how\nParallelLinear enables extension of the Mixture-of-Experts concept by\ndemonstrating with an implementation of Mixture of Attention.\n","authors":["Shawn Tan","Yikang Shen","Rameswar Panda","Aaron Courville"],"pdf_url":"https://arxiv.org/pdf/2403.08245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02279v2","updated":"2024-03-13T04:50:56Z","published":"2023-10-01T05:07:17Z","title":"Consistency Trajectory Models: Learning Probability Flow ODE Trajectory\n  of Diffusion","summary":"  Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion\nmodel sampling at the cost of sample quality but lack a natural way to\ntrade-off quality for speed. To address this limitation, we propose Consistency\nTrajectory Model (CTM), a generalization encompassing CM and score-based models\nas special cases. CTM trains a single neural network that can -- in a single\nforward pass -- output scores (i.e., gradients of log-density) and enables\nunrestricted traversal between any initial and final time along the Probability\nFlow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables\nthe efficient combination of adversarial training and denoising score matching\nloss to enhance performance and achieves new state-of-the-art FIDs for\nsingle-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at\n64x64 resolution (FID 1.92). CTM also enables a new family of sampling schemes,\nboth deterministic and stochastic, involving long jumps along the ODE solution\ntrajectories. It consistently improves sample quality as computational budgets\nincrease, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's\naccess to the score function can streamline the adoption of established\ncontrollable/conditional generation methods from the diffusion community. This\naccess also enables the computation of likelihood. The code is available at\nhttps://github.com/sony/ctm.\n","authors":["Dongjun Kim","Chieh-Hsin Lai","Wei-Hsiang Liao","Naoki Murata","Yuhta Takida","Toshimitsu Uesaka","Yutong He","Yuki Mitsufuji","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2310.02279v2.pdf","comment":"International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2403.08239v1","updated":"2024-03-13T04:45:40Z","published":"2024-03-13T04:45:40Z","title":"Continuous Object State Recognition for Cooking Robots Using Pre-Trained\n  Vision-Language Models and Black-box Optimization","summary":"  The state recognition of the environment and objects by robots is generally\nbased on the judgement of the current state as a classification problem. On the\nother hand, state changes of food in cooking happen continuously and need to be\ncaptured not only at a certain time point but also continuously over time. In\naddition, the state changes of food are complex and cannot be easily described\nby manual programming. Therefore, we propose a method to recognize the\ncontinuous state changes of food for cooking robots through the spoken language\nusing pre-trained large-scale vision-language models. By using models that can\ncompute the similarity between images and texts continuously over time, we can\ncapture the state changes of food while cooking. We also show that by adjusting\nthe weighting of each text prompt based on fitting the similarity changes to a\nsigmoid function and then performing black-box optimization, more accurate and\nrobust continuous state recognition can be achieved. We demonstrate the\neffectiveness and limitations of this method by performing the recognition of\nwater boiling, butter melting, egg cooking, and onion stir-frying.\n","authors":["Kento Kawaharazuka","Naoaki Kanazawa","Yoshiki Obinata","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.08239v1.pdf","comment":"accepted at IEEE Robotics and Automation Letters (RA-L), website -\n  https://haraduka.github.io/continuous-state-recognition/"},{"id":"http://arxiv.org/abs/2110.08691v3","updated":"2024-03-13T04:45:31Z","published":"2021-10-17T00:50:52Z","title":"Terminal Embeddings in Sublinear Time","summary":"  Recently (Elkin, Filtser, Neiman 2017) introduced the concept of a {\\it\nterminal embedding} from one metric space $(X,d_X)$ to another $(Y,d_Y)$ with a\nset of designated terminals $T\\subset X$. Such an embedding $f$ is said to have\ndistortion $\\rho\\ge 1$ if $\\rho$ is the smallest value such that there exists a\nconstant $C>0$ satisfying\n  \\begin{equation*}\n  \\forall x\\in T\\ \\forall q\\in X,\\ C d_X(x, q) \\le d_Y(f(x), f(q)) \\le C \\rho\nd_X(x, q) .\n  \\end{equation*}\n  When $X,Y$ are both Euclidean metrics with $Y$ being $m$-dimensional,\nrecently (Narayanan, Nelson 2019), following work of (Mahabadi, Makarychev,\nMakarychev, Razenshteyn 2018), showed that distortion $1+\\epsilon$ is\nachievable via such a terminal embedding with $m = O(\\epsilon^{-2}\\log n)$ for\n$n := |T|$. This generalizes the Johnson-Lindenstrauss lemma, which only\npreserves distances within $T$ and not to $T$ from the rest of space. The\ndownside of prior work is that evaluating their embedding on some $q\\in\n\\mathbb{R}^d$ required solving a semidefinite program with $\\Theta(n)$\nconstraints in~$m$ variables and thus required some superlinear\n$\\mathrm{poly}(n)$ runtime. Our main contribution in this work is to give a new\ndata structure for computing terminal embeddings. We show how to pre-process\n$T$ to obtain an almost linear-space data structure that supports computing the\nterminal embedding image of any $q\\in\\mathbb{R}^d$ in sublinear time $O^*\n(n^{1-\\Theta(\\epsilon^2)} + d)$. To accomplish this, we leverage tools\ndeveloped in the context of approximate nearest neighbor search.\n","authors":["Yeshwanth Cherapanamjeri","Jelani Nelson"],"pdf_url":"https://arxiv.org/pdf/2110.08691v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07353v2","updated":"2024-03-13T04:43:23Z","published":"2024-03-12T06:22:10Z","title":"Graph Unlearning with Efficient Partial Retraining","summary":"  Graph Neural Networks (GNNs) have achieved remarkable success in various\nreal-world applications. However, GNNs may be trained on undesirable graph\ndata, which can degrade their performance and reliability. To enable trained\nGNNs to efficiently unlearn unwanted data, a desirable solution is\nretraining-based graph unlearning, which partitions the training graph into\nsubgraphs and trains sub-models on them, allowing fast unlearning through\npartial retraining. However, the graph partition process causes information\nloss in the training graph, resulting in the low model utility of sub-GNN\nmodels. In this paper, we propose GraphRevoker, a novel graph unlearning\nframework that better maintains the model utility of unlearnable GNNs.\nSpecifically, we preserve the graph property with graph property-aware sharding\nand effectively aggregate the sub-GNN models for prediction with graph\ncontrastive sub-model aggregation. We conduct extensive experiments to\ndemonstrate the superiority of our proposed approach.\n","authors":["Jiahao Zhang","Lin Wang","Shijie Wang","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2403.07353v2.pdf","comment":"8 pages, 3 figures, accepted by The Web Conference 2024 (PhD\n  Symposium Track)"},{"id":"http://arxiv.org/abs/2310.11762v4","updated":"2024-03-13T04:14:52Z","published":"2023-10-18T07:39:05Z","title":"A Quasi-Wasserstein Loss for Learning Graph Neural Networks","summary":"  When learning graph neural networks (GNNs) in node-level prediction tasks,\nmost existing loss functions are applied for each node independently, even if\nnode embeddings and their labels are non-i.i.d. because of their graph\nstructures. To eliminate such inconsistency, in this study we propose a novel\nQuasi-Wasserstein (QW) loss with the help of the optimal transport defined on\ngraphs, leading to new learning and prediction paradigms of GNNs. In\nparticular, we design a ``Quasi-Wasserstein'' distance between the observed\nmulti-dimensional node labels and their estimations, optimizing the label\ntransport defined on graph edges. The estimations are parameterized by a GNN in\nwhich the optimal label transport may determine the graph edge weights\noptionally. By reformulating the strict constraint of the label transport to a\nBregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein\nloss associated with two efficient solvers learning the GNN together with\noptimal label transport. When predicting node labels, our model combines the\noutput of the GNN with the residual component provided by the optimal label\ntransport, leading to a new transductive prediction paradigm. Experiments show\nthat the proposed QW loss applies to various GNNs and helps to improve their\nperformance in node-level classification and regression tasks. The code of this\nwork can be found at \\url{https://github.com/SDS-Lab/QW_Loss}.\n","authors":["Minjie Cheng","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.11762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17279v2","updated":"2024-03-13T03:58:56Z","published":"2023-06-29T19:41:56Z","title":"Probabilistic Constraint for Safety-Critical Reinforcement Learning","summary":"  In this paper, we consider the problem of learning safe policies for\nprobabilistic-constrained reinforcement learning (RL). Specifically, a safe\npolicy or controller is one that, with high probability, maintains the\ntrajectory of the agent in a given safe set. We establish a connection between\nthis probabilistic-constrained setting and the cumulative-constrained\nformulation that is frequently explored in the existing literature. We provide\ntheoretical bounds elucidating that the probabilistic-constrained setting\noffers a better trade-off in terms of optimality and safety (constraint\nsatisfaction). The challenge encountered when dealing with the probabilistic\nconstraints, as explored in this work, arises from the absence of explicit\nexpressions for their gradients. Our prior work provides such an explicit\ngradient expression for probabilistic constraints which we term Safe Policy\nGradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved\ngradient SPG-Actor-Critic that leads to a lower variance than SPG-REINFORCE,\nwhich is substantiated by our theoretical results. A noteworthy aspect of both\nSPGs is their inherent algorithm independence, rendering them versatile for\napplication across a range of policy-based algorithms. Furthermore, we propose\na Safe Primal-Dual algorithm that can leverage both SPGs to learn safe\npolicies. It is subsequently followed by theoretical analyses that encompass\nthe convergence of the algorithm, as well as the near-optimality and\nfeasibility on average. In addition, we test the proposed approaches by a\nseries of empirical experiments. These experiments aim to examine and analyze\nthe inherent trade-offs between the optimality and safety, and serve to\nsubstantiate the efficacy of two SPGs, as well as our theoretical\ncontributions.\n","authors":["Weiqin Chen","Dharmashankar Subramanian","Santiago Paternain"],"pdf_url":"https://arxiv.org/pdf/2306.17279v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2210.00596"},{"id":"http://arxiv.org/abs/2403.08222v1","updated":"2024-03-13T03:47:08Z","published":"2024-03-13T03:47:08Z","title":"Robust Decision Aggregation with Adversarial Experts","summary":"  We consider a binary decision aggregation problem in the presence of both\ntruthful and adversarial experts. The truthful experts will report their\nprivate signals truthfully with proper incentive, while the adversarial experts\ncan report arbitrarily. The decision maker needs to design a robust aggregator\nto forecast the true state of the world based on the reports of experts. The\ndecision maker does not know the specific information structure, which is a\njoint distribution of signals, states, and strategies of adversarial experts.\nWe want to find the optimal aggregator minimizing regret under the worst\ninformation structure. The regret is defined by the difference in expected loss\nbetween the aggregator and a benchmark who makes the optimal decision given the\njoint distribution and reports of truthful experts.\n  We prove that when the truthful experts are symmetric and adversarial experts\nare not too numerous, the truncated mean is optimal, which means that we remove\nsome lowest reports and highest reports and take averaging among the left\nreports. Moreover, for many settings, the optimal aggregators are in the family\nof piecewise linear functions. The regret is independent of the total number of\nexperts but only depends on the ratio of adversaries. We evaluate our\naggregators by numerical experiment in an ensemble learning task. We also\nobtain some negative results for the aggregation problem with adversarial\nexperts under some more general information structures and experts' report\nspace.\n","authors":["Yongkang Guo","Yuqing Kong"],"pdf_url":"https://arxiv.org/pdf/2403.08222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08220v1","updated":"2024-03-13T03:45:14Z","published":"2024-03-13T03:45:14Z","title":"Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian\n  inversion enabled by derivative-informed neural operators","summary":"  We propose an operator learning approach to accelerate geometric Markov chain\nMonte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse\nproblems. While geometric MCMC employs high-quality proposals that adapt to\nposterior local geometry, it requires computing local gradient and Hessian\ninformation of the log-likelihood, incurring a high cost when the\nparameter-to-observable (PtO) map is defined through expensive model\nsimulations. We consider a delayed-acceptance geometric MCMC method driven by a\nneural operator surrogate of the PtO map, where the proposal is designed to\nexploit fast surrogate approximations of the log-likelihood and,\nsimultaneously, its gradient and Hessian. To achieve a substantial speedup, the\nsurrogate needs to be accurate in predicting both the observable and its\nparametric derivative (the derivative of the observable with respect to the\nparameter). Training such a surrogate via conventional operator learning using\ninput--output samples often demands a prohibitively large number of model\nsimulations. In this work, we present an extension of derivative-informed\noperator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)]\nusing input--output--derivative training samples. Such a learning method leads\nto derivative-informed neural operator (DINO) surrogates that accurately\npredict the observable and its parametric derivative at a significantly lower\ntraining cost than the conventional method. Cost and error analysis for reduced\nbasis DINO surrogates are provided. Numerical studies on PDE-constrained\nBayesian inversion demonstrate that DINO-driven MCMC generates effective\nposterior samples 3--9 times faster than geometric MCMC and 60--97 times faster\nthan prior geometry-based MCMC. Furthermore, the training cost of DINO\nsurrogates breaks even after collecting merely 10--25 effective posterior\nsamples compared to geometric MCMC.\n","authors":["Lianghao Cao","Thomas O'Leary-Roseberry","Omar Ghattas"],"pdf_url":"https://arxiv.org/pdf/2403.08220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16583v3","updated":"2024-03-13T03:36:44Z","published":"2023-05-26T02:15:26Z","title":"Detecting Errors in a Numerical Response via any Regression Model","summary":"  Noise plagues many numerical datasets, where the recorded values in the data\nmay fail to match the true underlying values due to reasons including:\nerroneous sensors, data entry/processing mistakes, or imperfect human\nestimates. We consider general regression settings with covariates and a\npotentially corrupted response whose observed values may contain errors. By\naccounting for various uncertainties, we introduced veracity scores that\ndistinguish between genuine errors and natural data fluctuations, conditioned\non the available covariate information in the dataset. We propose a simple yet\nefficient filtering procedure for eliminating potential errors, and establish\ntheoretical guarantees for our method. We also contribute a new error detection\nbenchmark involving 5 regression datasets with real-world numerical errors (for\nwhich the true values are also known). In this benchmark and additional\nsimulation studies, our method identifies incorrect values with better\nprecision/recall than other approaches.\n","authors":["Hang Zhou","Jonas Mueller","Mayank Kumar","Jane-Ling Wang","Jing Lei"],"pdf_url":"https://arxiv.org/pdf/2305.16583v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08217v1","updated":"2024-03-13T03:31:26Z","published":"2024-03-13T03:31:26Z","title":"Research on the Application of Deep Learning-based BERT Model in\n  Sentiment Analysis","summary":"  This paper explores the application of deep learning techniques, particularly\nfocusing on BERT models, in sentiment analysis. It begins by introducing the\nfundamental concept of sentiment analysis and how deep learning methods are\nutilized in this domain. Subsequently, it delves into the architecture and\ncharacteristics of BERT models. Through detailed explanation, it elucidates the\napplication effects and optimization strategies of BERT models in sentiment\nanalysis, supported by experimental validation. The experimental findings\nindicate that BERT models exhibit robust performance in sentiment analysis\ntasks, with notable enhancements post fine-tuning. Lastly, the paper concludes\nby summarizing the potential applications of BERT models in sentiment analysis\nand suggests directions for future research and practical implementations.\n","authors":["Yichao Wu","Zhengyu Jin","Chenxi Shi","Penghao Liang","Tong Zhan"],"pdf_url":"https://arxiv.org/pdf/2403.08217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08216v1","updated":"2024-03-13T03:28:39Z","published":"2024-03-13T03:28:39Z","title":"PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise","summary":"  Normalizing flow is a generative modeling approach with efficient sampling.\nHowever, Flow-based models suffer two issues, which are manifold and discrete\ndata. If the target distribution is a manifold, which means the dimension of\nthe latent target distribution and the dimension of the data distribution are\nunmatched, flow-based models might perform badly. Discrete data makes\nflow-based models collapse into a degenerate mixture of point masses. In this\npaper, to sidestep such two issues we propose PaddingFlow, a novel\ndequantization method, which improves normalizing flows with\npadding-dimensional noise. PaddingFlow is easy to implement, computationally\ncheap, widely suitable for various tasks, and generates samples that are\nunbiased estimations of the data. Especially, our method can overcome the\nlimitation of existing dequantization methods that have to change the data\ndistribution, which might degrade performance. We validate our method on the\nmain benchmarks of unconditional density estimation, including five tabular\ndatasets and four image datasets for VAE models, and the IK experiments which\nare conditional density estimation. The results show that PaddingFlow can\nprovide improvement on all tasks in this paper.\n","authors":["Qinglong Meng","Chongkun Xia","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08215v1","updated":"2024-03-13T03:24:36Z","published":"2024-03-13T03:24:36Z","title":"LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual\n  Semantic Segmentation for Autonomous Driving","summary":"  Despite the impressive performance achieved by data-fusion networks with\nduplex encoders for visual semantic segmentation, they become ineffective when\nspatial geometric data are not available. Implicitly infusing the spatial\ngeometric prior knowledge acquired by a duplex-encoder teacher model into a\nsingle-encoder student model is a practical, albeit less explored research\navenue. This paper delves into this topic and resorts to knowledge distillation\napproaches to address this problem. We introduce the Learning to Infuse \"X\"\n(LIX) framework, with novel contributions in both logit distillation and\nfeature distillation aspects. We present a mathematical proof that underscores\nthe limitation of using a single fixed weight in decoupled knowledge\ndistillation and introduce a logit-wise dynamic weight controller as a solution\nto this issue. Furthermore, we develop an adaptively-recalibrated feature\ndistillation algorithm, including two technical novelties: feature\nrecalibration via kernel regression and in-depth feature consistency\nquantification via centered kernel alignment. Extensive experiments conducted\nwith intermediate-fusion and late-fusion networks across various public\ndatasets provide both quantitative and qualitative evaluations, demonstrating\nthe superior performance of our LIX framework when compared to other\nstate-of-the-art approaches.\n","authors":["Sicen Guo","Zhiyuan Wu","Qijun Chen","Ioannis Pitas","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2403.08215v1.pdf","comment":"13 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2209.06408v3","updated":"2024-03-13T03:10:25Z","published":"2022-09-14T04:28:15Z","title":"Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values\n  for Multi-classifiers","summary":"  While advanced classifiers have been increasingly used in real-world\nsafety-critical applications, how to properly evaluate the black-box models\ngiven specific human values remains a concern in the community. Such human\nvalues include punishing error cases of different severity in varying degrees\nand making compromises in general performance to reduce specific dangerous\ncases. In this paper, we propose a novel evaluation measure named Meta Pattern\nConcern Score based on the abstract representation of probabilistic prediction\nand the adjustable threshold for the concession in prediction confidence, to\nintroduce the human values into multi-classifiers. Technically, we learn from\nthe advantages and disadvantages of two kinds of common metrics, namely the\nconfusion matrix-based evaluation measures and the loss values, so that our\nmeasure is effective as them even under general tasks, and the cross entropy\nloss becomes a special case of our measure in the limit. Besides, our measure\ncan also be used to refine the model training by dynamically adjusting the\nlearning rate. The experiments on four kinds of models and six datasets confirm\nthe effectiveness and efficiency of our measure. And a case study shows it can\nnot only find the ideal model reducing 0.53% of dangerous cases by only\nsacrificing 0.04% of training accuracy, but also refine the learning rate to\ntrain a new model averagely outperforming the original one with a 1.62% lower\nvalue of itself and 0.36% fewer number of dangerous cases.\n","authors":["Yanyun Wang","Dehui Du","Yuanhao Liu"],"pdf_url":"https://arxiv.org/pdf/2209.06408v3.pdf","comment":"Published at the 2023 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC); 9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.18133v2","updated":"2024-03-13T03:07:08Z","published":"2024-02-28T07:54:50Z","title":"Classes Are Not Equal: An Empirical Study on Image Recognition Fairness","summary":"  In this paper, we present an empirical study on image recognition fairness,\ni.e., extreme class accuracy disparity on balanced data like ImageNet. We\nexperimentally demonstrate that classes are not equal and the fairness issue is\nprevalent for image classification models across various datasets, network\narchitectures, and model capacities. Moreover, several intriguing properties of\nfairness are identified. First, the unfairness lies in problematic\nrepresentation rather than classifier bias. Second, with the proposed concept\nof Model Prediction Bias, we investigate the origins of problematic\nrepresentation during optimization. Our findings reveal that models tend to\nexhibit greater prediction biases for classes that are more challenging to\nrecognize. It means that more other classes will be confused with harder\nclasses. Then the False Positives (FPs) will dominate the learning in\noptimization, thus leading to their poor accuracy. Further, we conclude that\ndata augmentation and representation learning algorithms improve overall\nperformance by promoting fairness to some degree in image classification. The\nCode is available at\nhttps://github.com/dvlab-research/Parametric-Contrastive-Learning.\n","authors":["Jiequan Cui","Beier Zhu","Xin Wen","Xiaojuan Qi","Bei Yu","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.18133v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08207v1","updated":"2024-03-13T03:03:40Z","published":"2024-03-13T03:03:40Z","title":"BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural\n  Network","summary":"  Many computer vision and machine learning problems are modelled as learning\ntasks on heterogeneous graphs, featuring a wide array of relations from diverse\ntypes of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out\nas a promising neural model class designed for heterogeneous graphs. Built on\ntraditional GNNs, existing HGNNs employ different parameter spaces to model the\nvaried relationships. However, the practical effectiveness of existing HGNNs is\noften limited to simple heterogeneous graphs with few relation types. This\npaper first highlights and demonstrates that the standard approach employed by\nexisting HGNNs inevitably leads to parameter explosion and relation collapse,\nmaking HGNNs less effective or impractical for complex heterogeneous graphs\nwith numerous relation types. To overcome this issue, we introduce a novel\nframework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges\nby carefully integrating different relations into a unified feature space\nmanageable by a single set of parameters. This results in a refined HGNN method\nthat is more efficient and effective in learning from heterogeneous graphs,\nespecially when the number of relations grows. Our empirical studies illustrate\nthat BG-HGNN significantly surpasses existing HGNNs in terms of parameter\nefficiency (up to 28.96 $\\times$), training throughput (up to 8.12 $\\times$),\nand accuracy (up to 1.07 $\\times$).\n","authors":["Junwei Su","Lingjun Mao","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18554v3","updated":"2024-03-13T02:58:07Z","published":"2023-10-28T01:27:52Z","title":"Improved Regret Bounds of (Multinomial) Logistic Bandits via\n  Regret-to-Confidence-Set Conversion","summary":"  Logistic bandit is a ubiquitous framework of modeling users' choices, e.g.,\nclick vs. no click for advertisement recommender system. We observe that the\nprior works overlook or neglect dependencies in $S \\geq \\lVert \\theta_\\star\n\\rVert_2$, where $\\theta_\\star \\in \\mathbb{R}^d$ is the unknown parameter\nvector, which is particularly problematic when $S$ is large, e.g., $S \\geq d$.\nIn this work, we improve the dependency on $S$ via a novel approach called {\\it\nregret-to-confidence set conversion (R2CS)}, which allows us to construct a\nconvex confidence set based on only the \\textit{existence} of an online\nlearning algorithm with a regret guarantee. Using R2CS, we obtain a strict\nimprovement in the regret bound w.r.t. $S$ in logistic bandits while retaining\ncomputational feasibility and the dependence on other factors such as $d$ and\n$T$. We apply our new confidence set to the regret analyses of logistic bandits\nwith a new martingale concentration step that circumvents an additional factor\nof $S$. We then extend this analysis to multinomial logistic bandits and obtain\nsimilar improvements in the regret, showing the efficacy of R2CS. While we\napplied R2CS to the (multinomial) logistic model, R2CS is a generic approach\nfor developing confidence sets that can be used for various models, which can\nbe of independent interest.\n","authors":["Junghyun Lee","Se-Young Yun","Kwang-Sung Jun"],"pdf_url":"https://arxiv.org/pdf/2310.18554v3.pdf","comment":"39 pages, 1 figure, 1 table; Accepted to the 27th International\n  Conference on Artificial Intelligence and Statistics (AISTATS 2024) (ver2:\n  fixed some errors and significantly expanded discussions on various parts,\n  such as related work. ver3: fixed some minor typos)"},{"id":"http://arxiv.org/abs/2403.08204v1","updated":"2024-03-13T02:56:31Z","published":"2024-03-13T02:56:31Z","title":"AutoDFP: Automatic Data-Free Pruning via Channel Similarity\n  Reconstruction","summary":"  Structured pruning methods are developed to bridge the gap between the\nmassive scale of neural networks and the limited hardware resources. Most\ncurrent structured pruning methods rely on training datasets to fine-tune the\ncompressed model, resulting in high computational burdens and being\ninapplicable for scenarios with stringent requirements on privacy and security.\nAs an alternative, some data-free methods have been proposed, however, these\nmethods often require handcraft parameter tuning and can only achieve\ninflexible reconstruction. In this paper, we propose the Automatic Data-Free\nPruning (AutoDFP) method that achieves automatic pruning and reconstruction\nwithout fine-tuning. Our approach is based on the assumption that the loss of\ninformation can be partially compensated by retaining focused information from\nsimilar channels. Specifically, We formulate data-free pruning as an\noptimization problem, which can be effectively addressed through reinforcement\nlearning. AutoDFP assesses the similarity of channels for each layer and\nprovides this information to the reinforcement learning agent, guiding the\npruning and reconstruction process of the network. We evaluate AutoDFP with\nmultiple networks on multiple datasets, achieving impressive compression\nresults. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\\%\nreduction in accuracy loss compared to the recently proposed data-free pruning\nmethod DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset,\nAutoDFP achieves 43.17\\% higher accuracy than the SOTA method with the same\n80\\% preserved ratio on MobileNet-V1.\n","authors":["Siqi Li","Jun Chen","Jingyang Xiang","Chengrui Zhu","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08204v1.pdf","comment":"11 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.08203v1","updated":"2024-03-13T02:55:27Z","published":"2024-03-13T02:55:27Z","title":"Learnable Community-Aware Transformer for Brain Connectome Analysis with\n  Token Clustering","summary":"  Neuroscientific research has revealed that the complex brain network can be\norganized into distinct functional communities, each characterized by a\ncohesive group of regions of interest (ROIs) with strong interconnections.\nThese communities play a crucial role in comprehending the functional\norganization of the brain and its implications for neurological conditions,\nincluding Autism Spectrum Disorder (ASD) and biological differences, such as in\ngender. Traditional models have been constrained by the necessity of predefined\ncommunity clusters, limiting their flexibility and adaptability in deciphering\nthe brain's functional organization. Furthermore, these models were restricted\nby a fixed number of communities, hindering their ability to accurately\nrepresent the brain's dynamic nature. In this study, we present a token\nclustering brain transformer-based model ($\\texttt{TC-BrainTF}$) for joint\ncommunity clustering and classification. Our approach proposes a novel token\nclustering (TC) module based on the transformer architecture, which utilizes\nlearnable prompt tokens with orthogonal loss where each ROI embedding is\nprojected onto the prompt embedding space, effectively clustering ROIs into\ncommunities and reducing the dimensions of the node representation via merging\nwith communities. Our results demonstrate that our learnable community-aware\nmodel $\\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and\nclassifying genders through rigorous testing on ABIDE and HCP datasets.\nAdditionally, the qualitative analysis on $\\texttt{TC-BrainTF}$ has\ndemonstrated the effectiveness of the designed TC module and its relevance to\nneuroscience interpretations.\n","authors":["Yanting Yang","Beidi Zhao","Zhuohao Ni","Yize Zhao","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2403.08203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08199v1","updated":"2024-03-13T02:53:52Z","published":"2024-03-13T02:53:52Z","title":"Deep Submodular Peripteral Network","summary":"  Submodular functions, crucial for various applications, often lack practical\nlearning methods for their acquisition. Seemingly unrelated, learning a scaling\nfrom oracles offering graded pairwise preferences (GPC) is underexplored,\ndespite a rich history in psychometrics. In this paper, we introduce deep\nsubmodular peripteral networks (DSPNs), a novel parametric family of submodular\nfunctions, and methods for their training using a contrastive-learning inspired\nGPC-ready strategy to connect and then tackle both of the above challenges. We\nintroduce newly devised GPC-style \"peripteral\" loss which leverages numerically\ngraded relationships between pairs of objects (sets in our case). Unlike\ntraditional contrastive learning, our method utilizes graded comparisons,\nextracting more nuanced information than just binary-outcome comparisons, and\ncontrasts sets of any size (not just two). We also define a novel suite of\nautomatic sampling strategies for training, including active-learning inspired\nsubmodular feedback. We demonstrate DSPNs' efficacy in learning submodularity\nfrom a costly target submodular function showing superiority in downstream\ntasks such as experimental design and streaming applications.\n","authors":["Gantavya Bhatt","Arnav Das","Jeff Bilmes"],"pdf_url":"https://arxiv.org/pdf/2403.08199v1.pdf","comment":"Preprint"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.08580v1","updated":"2024-03-13T14:35:13Z","published":"2024-03-13T14:35:13Z","title":"Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification","summary":"  Classifying videos into distinct categories, such as Sport and Music Video,\nis crucial for multimedia understanding and retrieval, especially when an\nimmense volume of video content is being constantly generated. Traditional\nmethods require video decompression to extract pixel-level features like color,\ntexture, and motion, thereby increasing computational and storage demands.\nMoreover, these methods often suffer from performance degradation in\nlow-quality videos. We present a novel approach that examines only the\npost-compression bitstream of a video to perform classification, eliminating\nthe need for bitstream decoding. To validate our approach, we built a\ncomprehensive data set comprising over 29,000 YouTube video clips, totaling\n6,000 hours and spanning 11 distinct categories. Our evaluations indicate\nprecision, accuracy, and recall rates consistently above 80%, many exceeding\n90%, and some reaching 99%. The algorithm operates approximately 15,000 times\nfaster than real-time for 30fps videos, outperforming traditional Dynamic Time\nWarping (DTW) algorithm by seven orders of magnitude.\n","authors":["Yuxing Han","Yunan Ding","Chen Ye Gan","Jiangtao Wen"],"pdf_url":"https://arxiv.org/pdf/2403.08580v1.pdf","comment":"5 pages, 5 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:2309.07361"},{"id":"http://arxiv.org/abs/2403.08551v1","updated":"2024-03-13T14:02:54Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 1000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08505v1","updated":"2024-03-13T13:12:57Z","published":"2024-03-13T13:12:57Z","title":"Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16175v3","updated":"2024-03-13T10:57:24Z","published":"2023-06-28T12:52:48Z","title":"$\\mathbf{C}^2$Former: Calibrated and Complementary Transformer for\n  RGB-Infrared Object Detection","summary":"  Object detection on visible (RGB) and infrared (IR) images, as an emerging\nsolution to facilitate robust detection for around-the-clock applications, has\nreceived extensive attention in recent years. With the help of IR images,\nobject detectors have been more reliable and robust in practical applications\nby using RGB-IR combined information. However, existing methods still suffer\nfrom modality miscalibration and fusion imprecision problems. Since transformer\nhas the powerful capability to model the pairwise correlations between\ndifferent features, in this paper, we propose a novel Calibrated and\nComplementary Transformer called $\\mathrm{C}^2$Former to address these two\nproblems simultaneously. In $\\mathrm{C}^2$Former, we design an Inter-modality\nCross-Attention (ICA) module to obtain the calibrated and complementary\nfeatures by learning the cross-attention relationship between the RGB and IR\nmodality. To reduce the computational cost caused by computing the global\nattention in ICA, an Adaptive Feature Sampling (AFS) module is introduced to\ndecrease the dimension of feature maps. Because $\\mathrm{C}^2$Former performs\nin the feature domain, it can be embedded into existed RGB-IR object detectors\nvia the backbone network. Thus, one single-stage and one two-stage object\ndetector both incorporating our $\\mathrm{C}^2$Former are constructed to\nevaluate its effectiveness and versatility. With extensive experiments on the\nDroneVehicle and KAIST RGB-IR datasets, we verify that our method can fully\nutilize the RGB-IR complementary information and achieve robust detection\nresults. The code is available at\nhttps://github.com/yuanmaoxun/Calibrated-and-Complementary-Transformer-for-RGB-Infrared-Object-Detection.git.\n","authors":["Maoxun Yuan","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2306.16175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05737v2","updated":"2024-03-13T05:34:20Z","published":"2023-10-09T14:10:29Z","title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","summary":"  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n","authors":["Lijun Yu","José Lezama","Nitesh B. Gundavarapu","Luca Versari","Kihyuk Sohn","David Minnen","Yong Cheng","Vighnesh Birodkar","Agrim Gupta","Xiuye Gu","Alexander G. Hauptmann","Boqing Gong","Ming-Hsuan Yang","Irfan Essa","David A. Ross","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05737v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07338v2","updated":"2024-03-13T04:45:41Z","published":"2024-03-12T05:43:16Z","title":"D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic\n  Communications","summary":"  Semantic communications (SemCom) have emerged as a new paradigm for\nsupporting sixth-generation applications, where semantic features of data are\ntransmitted using artificial intelligence algorithms to attain high\ncommunication efficiencies. Most existing SemCom techniques utilize deep neural\nnetworks (DNNs) to implement analog source-channel mappings, which are\nincompatible with existing digital communication architectures. To address this\nissue, this paper proposes a novel framework of digital deep joint\nsource-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The\nframework features digital source and channel codings that are jointly\noptimized to reduce the end-to-end (E2E) distortion. First, deep source coding\nwith an adaptive density model is designed to encode semantic features\naccording to their distributions. Second, digital channel coding is employed to\nprotect encoded features against channel distortion. To facilitate their joint\ndesign, the E2E distortion is characterized as a function of the source and\nchannel rates via the analysis of the Bayesian model and Lipschitz assumption\non the DNNs. Then to minimize the E2E distortion, a two-step algorithm is\nproposed to control the source-channel rates for a given channel\nsignal-to-noise ratio. Simulation results reveal that the proposed framework\noutperforms classic deep JSCC and mitigates the cliff and leveling-off effects,\nwhich commonly exist for separation-based approaches.\n","authors":["Jianhao Huang","Kai Yuan","Chuan Huang","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.07338v2.pdf","comment":null}]},"2024-03-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.09636v1","updated":"2024-03-14T17:59:26Z","published":"2024-03-14T17:59:26Z","title":"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference","summary":"  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for on-line key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression rates in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to ~3.7x throughput increase in auto-regressive inference on a\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. We find\nthat DMC preserves the original downstream performance with up to 4x cache\ncompression, outperforming up-trained grouped-query attention (GQA). GQA and\nDMC can be even combined to obtain compounded gains. As a result DMC fits\nlonger contexts and larger batches within any given memory budget.\n","authors":["Piotr Nawrot","Adrian Łańcucki","Marcin Chochowski","David Tarjan","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2403.09636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09635v1","updated":"2024-03-14T17:59:14Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v1.pdf","comment":"Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.\n  Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable"},{"id":"http://arxiv.org/abs/2403.09631v1","updated":"2024-03-14T17:58:41Z","published":"2024-03-14T17:58:41Z","title":"3D-VLA: A 3D Vision-Language-Action Generative World Model","summary":"  Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.\n","authors":["Haoyu Zhen","Xiaowen Qiu","Peihao Chen","Jincheng Yang","Xin Yan","Yilun Du","Yining Hong","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09631v1.pdf","comment":"Project page: https://vis-www.cs.umass.edu/3dvla/"},{"id":"http://arxiv.org/abs/2403.09629v1","updated":"2024-03-14T17:58:16Z","published":"2024-03-14T17:58:16Z","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking","summary":"  When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.\n","authors":["Eric Zelikman","Georges Harik","Yijia Shao","Varuna Jayasiri","Nick Haber","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.09629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09613v1","updated":"2024-03-14T17:51:54Z","published":"2024-03-14T17:51:54Z","title":"Reawakening knowledge: Anticipatory recovery from catastrophic\n  interference via structured training","summary":"  We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.\n","authors":["Yanlai Yang","Matt Jones","Michael C. Mozer","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09613v1.pdf","comment":"19 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.09611v1","updated":"2024-03-14T17:51:32Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, consisting of\nboth dense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09606v1","updated":"2024-03-14T17:47:20Z","published":"2024-03-14T17:47:20Z","title":"Large Language Models and Causal Inference in Collaboration: A\n  Comprehensive Survey","summary":"  Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.\n","authors":["Xiaoyu Liu","Paiheng Xu","Junda Wu","Jiaxin Yuan","Yifan Yang","Yuhang Zhou","Fuxiao Liu","Tianrui Guan","Haoliang Wang","Tong Yu","Julian McAuley","Wei Ai","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2403.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07769v2","updated":"2024-03-14T17:16:18Z","published":"2024-03-12T15:56:10Z","title":"Transforming Competition into Collaboration: The Revolutionary Role of\n  Multi-Agent Systems and Language Models in Modern Organizations","summary":"  This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.\n","authors":["Carlos Jose Xavier Cruz"],"pdf_url":"https://arxiv.org/pdf/2403.07769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07865v2","updated":"2024-03-14T16:57:37Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Yu Qiao","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09559v1","updated":"2024-03-14T16:47:25Z","published":"2024-03-14T16:47:25Z","title":"Less is More: Data Value Estimation for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01308v2","updated":"2024-03-14T16:37:37Z","published":"2024-03-02T20:40:11Z","title":"VBART: The Turkish LLM","summary":"  We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.\n","authors":["Meliksah Turker","Mehmet Erdi Ari","Aydin Han"],"pdf_url":"https://arxiv.org/pdf/2403.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09539v1","updated":"2024-03-14T16:27:49Z","published":"2024-03-14T16:27:49Z","title":"Logits of API-Protected LLMs Leak Proprietary Information","summary":"  The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.\n","authors":["Matthew Finlayson","Swabha Swayamdipta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12749v3","updated":"2024-03-14T16:13:36Z","published":"2024-02-20T06:37:31Z","title":"Me LLaMA: Foundation Large Language Models for Medical Applications","summary":"  Recent large language models (LLMs) such as ChatGPT and LLaMA have shown\ngreat promise in many AI applications. However, their performance on medical\ntasks is suboptimal and can be improved by training on extensive\ndomain-specific datasets. This study introduces Me LLaMA, a medical LLM family\nthat includes foundation models - Me LLaMA 13/70B, along with their\nchat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual\npre-training and instruction tuning of LLaMA2 using large medical datasets. Our\ndomain-specific data suite for training and evaluation includes a large-scale,\ncontinual pre-training dataset with 129B tokens, an instruction tuning dataset\nwith 214k samples, and a new medical evaluation benchmark (MIBE) across six\ntasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me\nLLaMA models achieve overall better performance than existing open-source\nmedical LLMs in zero-shot, few-shot and supervised learning abilities. Their\nzero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,\nwith a slight variance of within 3%, and yet falls short when compared to\nGPT-4. In addition, we investigated the catastrophic forgetting problem, and\nour results show that Me LLaMA models outperform other open-source medical LLMs\nin mitigating this issue. Me LLaMA is one of the largest open-source medical\nfoundation LLMs that use both biomedical and clinical data. It exhibits\nsuperior performance across both general and medical tasks compared to other\nopen-source medical LLMs, rendering it an attractive choice for medical AI\napplications. We release our models, datasets, and evaluation scripts at:\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\n","authors":["Qianqian Xie","Qingyu Chen","Aokun Chen","Cheng Peng","Yan Hu","Fongci Lin","Xueqing Peng","Jimin Huang","Jeffrey Zhang","Vipina Keloth","Xingyu Zhou","Huan He","Lucila Ohno-Machado","Yonghui Wu","Hua Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2402.12749v3.pdf","comment":"21 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.09530v1","updated":"2024-03-14T16:13:00Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v1.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.09522v1","updated":"2024-03-14T16:07:39Z","published":"2024-03-14T16:07:39Z","title":"MT-PATCHER: Selective and Extendable Knowledge Distillation from Large\n  Language Models for Machine Translation","summary":"  Large Language Models (LLM) have demonstrated their strong ability in the\nfield of machine translation (MT), yet they suffer from high computational cost\nand latency. Therefore, transferring translation knowledge from giant LLMs to\nmedium-sized machine translation models is a promising research direction.\nHowever, traditional knowledge distillation methods do not take the capability\nof student and teacher models into consideration, therefore repeatedly teaching\nstudent models on the knowledge they have learned, and failing to extend to\nnovel contexts and knowledge. In this paper, we propose a framework called\nMT-Patcher, which transfers knowledge from LLMs to existing MT models in a\nselective, comprehensive and proactive manner. Considering the current\ntranslation ability of student MT models, we only identify and correct their\ntranslation errors, instead of distilling the whole translation from the\nteacher. Leveraging the strong language abilities of LLMs, we instruct LLM\nteachers to synthesize diverse contexts and anticipate more potential errors\nfor the student. Experiment results on translating both specific language\nphenomena and general MT benchmarks demonstrate that finetuning the student MT\nmodel on about 10% examples can achieve comparable results to the traditional\nknowledge distillation method, and synthesized potential errors and diverse\ncontexts further improve translation performances on unseen contexts and words.\n","authors":["Jiahuan Li","Shanbo Cheng","Shujian Huang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.09522v1.pdf","comment":"Accepted to NAACL-2024"},{"id":"http://arxiv.org/abs/2403.09516v1","updated":"2024-03-14T15:58:36Z","published":"2024-03-14T15:58:36Z","title":"Leveraging Prototypical Representations for Mitigating Social Bias\n  without Demographic Information","summary":"  Mitigating social biases typically requires identifying the social groups\nassociated with each data sample. In this paper, we present DAFair, a novel\napproach to address social bias in language models. Unlike traditional methods\nthat rely on explicit demographic labels, our approach does not require any\nsuch information. Instead, we leverage predefined prototypical demographic\ntexts and incorporate a regularization term during the fine-tuning process to\nmitigate bias in the model's representations. Our empirical results across two\ntasks and two models demonstrate the effectiveness of our method compared to\nprevious approaches that do not rely on labeled data. Moreover, with limited\ndemographic-annotated data, our approach outperforms common debiasing\napproaches.\n","authors":["Shadi Iskander","Kira Radinsky","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.09516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v2","updated":"2024-03-14T15:57:59Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.09498v1","updated":"2024-03-14T15:40:13Z","published":"2024-03-14T15:40:13Z","title":"From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\n  Fake News","summary":"  In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news.\n","authors":["Yuhan Liu","Xiuying Chen","Xiaoqing Zhang","Xing Gao","Ji Zhang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.09498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11093v2","updated":"2024-03-14T15:36:17Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.01980v2","updated":"2024-03-14T15:30:41Z","published":"2024-02-03T01:33:16Z","title":"SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks","summary":"  Social science NLP tasks, such as emotion or humor detection, are required to\ncapture the semantics along with the implicit pragmatics from text, often with\nlimited amounts of training data. Instruction tuning has been shown to improve\nthe many capabilities of large language models (LLMs) such as commonsense\nreasoning, reading comprehension, and computer programming. However, little is\nknown about the effectiveness of instruction tuning on the social domain where\nimplicit pragmatic cues are often needed to be captured. We explore the use of\ninstruction tuning for social science NLP tasks and introduce Socialite-Llama\n-- an open-source, instruction-tuned Llama. On a suite of 20 social science\ntasks, Socialite-Llama improves upon the performance of Llama as well as\nmatches or improves upon the performance of a state-of-the-art, multi-task\nfinetuned model on a majority of them. Further, Socialite-Llama also leads to\nimprovement on 5 out of 6 related social tasks as compared to Llama, suggesting\ninstruction tuning can lead to generalized social understanding. All resources\nincluding our code, model and dataset can be found through\nbit.ly/socialitellama.\n","authors":["Gourab Dey","Adithya V Ganesan","Yash Kumar Lal","Manal Shah","Shreyashee Sinha","Matthew Matero","Salvatore Giorgi","Vivek Kulkarni","H. Andrew Schwartz"],"pdf_url":"https://arxiv.org/pdf/2402.01980v2.pdf","comment":"Short paper accepted to EACL 2024. 4 pgs, 2 tables"},{"id":"http://arxiv.org/abs/2403.09490v1","updated":"2024-03-14T15:30:25Z","published":"2024-03-14T15:30:25Z","title":"Hyper-CL: Conditioning Sentence Representations with Hypernetworks","summary":"  While the introduction of contrastive learning frameworks in sentence\nrepresentation learning has significantly contributed to advancements in the\nfield, it still remains unclear whether state-of-the-art sentence embeddings\ncan capture the fine-grained semantics of sentences, particularly when\nconditioned on specific perspectives. In this paper, we introduce Hyper-CL, an\nefficient methodology that integrates hypernetworks with contrastive learning\nto compute conditioned sentence representations. In our proposed approach, the\nhypernetwork is responsible for transforming pre-computed condition embeddings\ninto corresponding projection layers. This enables the same sentence embeddings\nto be projected differently according to various conditions. Evaluation on two\nrepresentative conditioning benchmarks, namely conditional semantic text\nsimilarity and knowledge graph completion, demonstrates that Hyper-CL is\neffective in flexibly conditioning sentence representations, showcasing its\ncomputational efficiency at the same time. We also provide a comprehensive\nanalysis of the inner workings of our approach, leading to a better\ninterpretation of its mechanisms.\n","authors":["Young Hyun Yoo","Jii Cha","Changhyeon Kim","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.09490v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.09488v1","updated":"2024-03-14T15:30:14Z","published":"2024-03-14T15:30:14Z","title":"Rectifying Demonstration Shortcut in In-Context Learning","summary":"  Large language models (LLMs) are able to solve various tasks with only a few\ndemonstrations utilizing their in-context learning (ICL) abilities. However,\nLLMs often rely on their pre-trained semantic priors of demonstrations rather\nthan on the input-label relationships to proceed with ICL prediction. In this\nwork, we term this phenomenon as the `Demonstration Shortcut'. While previous\nworks have primarily focused on improving ICL prediction results for predefined\ntasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM\nto effectively learn new input-label relationships from demonstrations. To\nachieve this, we introduce In-Context Calibration, a demonstration-aware\ncalibration method. We evaluate the effectiveness of the proposed method in two\nsettings: (1) the Original ICL Task using the standard label space and (2) the\nTask Learning setting, where the label space is replaced with semantically\nunrelated tokens. In both settings, In-Context Calibration demonstrates\nsubstantial improvements, with results generalized across three LLM families\n(OPT, GPT, and Llama2) under various configurations.\n","authors":["Joonwon Jang","Sanghwan Jang","Wonbin Kweon","Minjin Jeon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09472v1","updated":"2024-03-14T15:12:38Z","published":"2024-03-14T15:12:38Z","title":"Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision","summary":"  Current AI alignment methodologies rely on human-provided demonstrations or\njudgments, and the learned capabilities of AI systems would be upper-bounded by\nhuman capabilities as a result. This raises a challenging research question:\nHow can we keep improving the systems when their capabilities have surpassed\nthe levels of humans? This paper answers this question in the context of\ntackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from\nhuman annotations on easier tasks (e.g., level 1-3 MATH problems), which we\nterm as \\textit{easy-to-hard generalization}. Our key insight is that an\nevaluator (reward model) trained on supervisions for easier tasks can be\neffectively used for scoring candidate solutions of harder tasks and hence\nfacilitating easy-to-hard generalization over different levels of tasks. Based\non this insight, we propose a novel approach to scalable alignment, which\nfirstly trains the process-supervised reward models on easy problems (e.g.,\nlevel 1-3), and then uses them to evaluate the performance of policy models on\nhard problems. We show that such \\textit{easy-to-hard generalization from\nevaluators} can enable \\textit{easy-to-hard generalizations in generators}\neither through re-ranking or reinforcement learning (RL). Notably, our\nprocess-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500,\ndespite only using human supervision on easy problems. Our approach suggests a\npromising path toward AI systems that advance beyond the frontier of human\nsupervision.\n","authors":["Zhiqing Sun","Longhui Yu","Yikang Shen","Weiyang Liu","Yiming Yang","Sean Welleck","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13718v5","updated":"2024-03-14T15:05:08Z","published":"2023-05-23T06:13:10Z","title":"Exploring Self-supervised Logic-enhanced Training for Large Language\n  Models","summary":"  Existing efforts to improve logical reasoning ability of language models have\npredominantly relied on supervised fine-tuning, hindering generalization to new\ndomains and/or tasks. The development of Large Langauge Models (LLMs) has\ndemonstrated the capacity of compressing abundant knowledge into a single\nproxy, enabling them to tackle multiple tasks effectively. Our preliminary\nexperiments, nevertheless, show that LLMs do not show capability on logical\nreasoning. The performance of LLMs on logical reasoning benchmarks is far\nbehind the existing state-of-the-art baselines. In this paper, we make the\nfirst attempt to investigate the feasibility of incorporating logical knowledge\nthrough self-supervised post-training, and activating it via in-context\nlearning, which we termed as LogicLLM. Specifically, we devise an\nauto-regressive objective variant of MERIt and integrate it with two LLM\nseries, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to\n13 billion. The results on two challenging logical reasoning benchmarks\ndemonstrate the effectiveness of LogicLLM. Besides, we conduct extensive\nablation studies to analyze the key factors in designing logic-oriented proxy\ntasks.\n","authors":["Fangkai Jiao","Zhiyang Teng","Bosheng Ding","Zhengyuan Liu","Nancy F. Chen","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2305.13718v5.pdf","comment":"16 pages, NAACL 2024"},{"id":"http://arxiv.org/abs/2210.17437v3","updated":"2024-03-14T14:55:48Z","published":"2022-10-31T16:06:48Z","title":"Learning New Tasks from a Few Examples with Soft-Label Prototypes","summary":"  Existing approaches to few-shot learning in NLP rely on large language models\nand fine-tuning of these to generalise on out-of-distribution data. In this\nwork, we propose a simple yet powerful approach to \"extreme\" few-shot learning,\nwherein models are exposed to as little as 4 examples per class, based on\nsoft-label prototypes that collectively capture the distribution of different\nclasses across the input domain space. Inspired by previous work (Sucholutsky\net al., 2021) on univariate or simple multivariate (synthetic) data, we propose\na novel approach that is effective on large, high-dimensional and real-world\ndatasets. We learn soft-label prototypes within a neural framework (DeepSLP)\nand we experimentally demonstrate that it achieves superior performance on\n31/48 tested tasks and few-shot settings while closely matching the performance\nof strong baselines on the rest. We focus on learning previously unseen NLP\ntasks from very few examples (4, 8, 16) per label and present an in-depth\nanalysis of the effectiveness of our approach.\n","authors":["Avyav Kumar Singh","Ekaterina Shutova","Helen Yannakoudakis"],"pdf_url":"https://arxiv.org/pdf/2210.17437v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19791v3","updated":"2024-03-14T14:54:54Z","published":"2023-10-30T17:55:02Z","title":"LILO: Learning Interpretable Libraries by Compressing and Documenting\n  Code","summary":"  While large language models (LLMs) now excel at code generation, a key aspect\nof software development is the art of refactoring: consolidating code into\nlibraries of reusable and readable programs. In this paper, we introduce LILO,\na neurosymbolic framework that iteratively synthesizes, compresses, and\ndocuments code to build libraries tailored to particular problem domains. LILO\ncombines LLM-guided program synthesis with recent algorithmic advances in\nautomated refactoring from Stitch: a symbolic compression system that\nefficiently identifies optimal lambda abstractions across large code corpora.\nTo make these abstractions interpretable, we introduce an auto-documentation\n(AutoDoc) procedure that infers natural language names and docstrings based on\ncontextual examples of usage. In addition to improving human readability, we\nfind that AutoDoc boosts performance by helping LILO's synthesizer to interpret\nand deploy learned abstractions. We evaluate LILO on three inductive program\nsynthesis benchmarks for string editing, scene reasoning, and graphics\ncomposition. Compared to existing neural and symbolic methods - including the\nstate-of-the-art library learning algorithm DreamCoder - LILO solves more\ncomplex tasks and learns richer libraries that are grounded in linguistic\nknowledge.\n","authors":["Gabriel Grand","Lionel Wong","Matthew Bowers","Theo X. Olausson","Muxin Liu","Joshua B. Tenenbaum","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2310.19791v3.pdf","comment":"ICLR 2024 camera-ready"},{"id":"http://arxiv.org/abs/2306.10322v3","updated":"2024-03-14T14:33:51Z","published":"2023-06-17T11:44:04Z","title":"CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot\n  Vision-and-Language Navigation","summary":"  Understanding and following natural language instructions while navigating\nthrough complex, real-world environments poses a significant challenge for\ngeneral-purpose robots. These environments often include obstacles and\npedestrians, making it essential for autonomous agents to possess the\ncapability of self-corrected planning to adjust their actions based on feedback\nfrom the surroundings. However, the majority of existing vision-and-language\nnavigation (VLN) methods primarily operate in less realistic simulator settings\nand do not incorporate environmental feedback into their decision-making\nprocesses. To address this gap, we introduce a novel zero-shot framework called\nCorNav, utilizing a large language model for decision-making and comprising two\nkey components: 1) incorporating environmental feedback for refining future\nplans and adjusting its actions, and 2) multiple domain experts for parsing\ninstructions, scene understanding, and refining predicted actions. In addition\nto the framework, we develop a 3D simulator that renders realistic scenarios\nusing Unreal Engine 5. To evaluate the effectiveness and generalization of\nnavigation agents in a zero-shot multi-task setting, we create a benchmark\ncalled NavBench. Extensive experiments demonstrate that CorNav consistently\noutperforms all baselines by a significant margin across all tasks. On average,\nCorNav achieves a success rate of 28.1\\%, surpassing the best baseline's\nperformance of 20.5\\%.\n","authors":["Xiwen Liang","Liang Ma","Shanshan Guo","Jianhua Han","Hang Xu","Shikui Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.10322v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.00795v2","updated":"2024-03-14T14:25:13Z","published":"2024-02-23T05:31:36Z","title":"Executing Natural Language-Described Algorithms with Large Language\n  Models: An Investigation","summary":"  Executing computer programs described in natural language has long been a\npursuit of computer science. With the advent of enhanced natural language\nunderstanding capabilities exhibited by large language models (LLMs), the path\ntoward this goal has been illuminated. In this paper, we seek to examine the\ncapacity of present-day LLMs to comprehend and execute algorithms outlined in\nnatural language. We established an algorithm test set sourced from\nIntroduction to Algorithm, a well-known textbook that contains many\nrepresentative widely-used algorithms. To systematically assess LLMs' code\nexecution abilities, we selected 30 algorithms, generated 300 random-sampled\ninstances in total, and evaluated whether popular LLMs can understand and\nexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\neffectively execute programs described in natural language, as long as no heavy\nnumeric computation is involved. We believe our findings contribute to\nevaluating LLMs' code execution abilities and would encourage further\ninvestigation and application for the computation power of LLMs.\n","authors":["Xin Zheng","Qiming Zhu","Hongyu Lin","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2403.00795v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09409v1","updated":"2024-03-14T14:01:26Z","published":"2024-03-14T14:01:26Z","title":"\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS\n  Students using Large Language Models","summary":"  Grasping complex computing concepts often poses a challenge for students who\nstruggle to anchor these new ideas to familiar experiences and understandings.\nTo help with this, a good analogy can bridge the gap between unfamiliar\nconcepts and familiar ones, providing an engaging way to aid understanding.\nHowever, creating effective educational analogies is difficult even for\nexperienced instructors. We investigate to what extent large language models\n(LLMs), specifically ChatGPT, can provide access to personally relevant\nanalogies on demand. Focusing on recursion, a challenging threshold concept, we\nconducted an investigation analyzing the analogies generated by more than 350\nfirst-year computing students. They were provided with a code snippet and\ntasked to generate their own recursion-based analogies using ChatGPT,\noptionally including personally relevant topics in their prompts. We observed a\ngreat deal of diversity in the analogies produced with student-prescribed\ntopics, in contrast to the otherwise generic analogies, highlighting the value\nof student creativity when working with LLMs. Not only did students enjoy the\nactivity and report an improved understanding of recursion, but they described\nmore easily remembering analogies that were personally and culturally relevant.\n","authors":["Seth Bernstein","Paul Denny","Juho Leinonen","Lauren Kan","Arto Hellas","Matt Littlefield Sami Sarsa","Stephen MacNeil"],"pdf_url":"https://arxiv.org/pdf/2403.09409v1.pdf","comment":"7 pages, 2 figures, ITiCSE 2024 preprint"},{"id":"http://arxiv.org/abs/2201.12191v5","updated":"2024-03-14T13:46:29Z","published":"2022-01-28T15:45:13Z","title":"Kernelized Concept Erasure","summary":"  The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n","authors":["Shauli Ravfogel","Francisco Vargas","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12191v5.pdf","comment":"Accepted as a long paper in EMNLP22"},{"id":"http://arxiv.org/abs/2210.10012v4","updated":"2024-03-14T13:39:35Z","published":"2022-10-18T17:30:02Z","title":"Log-linear Guardedness and its Implications","summary":"  Methods for erasing human-interpretable concepts from neural representations\nthat assume linearity have been found to be tractable and useful. However, the\nimpact of this removal on the behavior of downstream classifiers trained on the\nmodified representations is not fully understood. In this work, we formally\ndefine the notion of log-linear guardedness as the inability of an adversary to\npredict the concept directly from the representation, and study its\nimplications. We show that, in the binary case, under certain assumptions, a\ndownstream log-linear model cannot recover the erased concept. However, we\ndemonstrate that a multiclass log-linear model \\emph{can} be constructed that\nindirectly recovers the concept in some cases, pointing to the inherent\nlimitations of log-linear guardedness as a downstream bias mitigation\ntechnique. These findings shed light on the theoretical limitations of linear\nerasure methods and highlight the need for further research on the connections\nbetween intrinsic and extrinsic bias in neural models.\n","authors":["Shauli Ravfogel","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2210.10012v4.pdf","comment":"Accepted as a long paper in ACL 2023"},{"id":"http://arxiv.org/abs/2403.04369v2","updated":"2024-03-14T13:25:48Z","published":"2024-03-07T09:57:42Z","title":"From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge\n  Prediction","summary":"  Confusing charge prediction is a challenging task in legal AI, which involves\npredicting confusing charges based on fact descriptions. While existing charge\nprediction methods have shown impressive performance, they face significant\nchallenges when dealing with confusing charges, such as Snatch and Robbery. In\nthe legal domain, constituent elements play a pivotal role in distinguishing\nconfusing charges. Constituent elements are fundamental behaviors underlying\ncriminal punishment and have subtle distinctions among charges. In this paper,\nwe introduce a novel From Graph to Word Bag (FWGB) approach, which introduces\ndomain knowledge regarding constituent elements to guide the model in making\njudgments on confusing charges, much like a judge's reasoning process.\nSpecifically, we first construct a legal knowledge graph containing constituent\nelements to help select keywords for each charge, forming a word bag.\nSubsequently, to guide the model's attention towards the differentiating\ninformation for each charge within the context, we expand the attention\nmechanism and introduce a new loss function with attention supervision through\nwords in the word bag. We construct the confusing charges dataset from\nreal-world judicial documents. Experiments demonstrate the effectiveness of our\nmethod, especially in maintaining exceptional performance in imbalanced label\ndistributions.\n","authors":["Ang Li","Qiangchao Chen","Yiquan Wu","Ming Cai","Xiang Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2403.04369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09362v1","updated":"2024-03-14T13:12:21Z","published":"2024-03-14T13:12:21Z","title":"Komodo: A Linguistic Expedition into Indonesia's Regional Languages","summary":"  The recent breakthroughs in Large Language Models (LLMs) have mostly focused\non languages with easily available and sufficient resources, such as English.\nHowever, there remains a significant gap for languages that lack sufficient\nlinguistic resources in the public domain. Our work introduces Komodo-7B,\n7-billion-parameter Large Language Models designed to address this gap by\nseamlessly operating across Indonesian, English, and 11 regional languages in\nIndonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and\nKomodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art\nperformance in various tasks and languages, outperforming the benchmarks set by\nOpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B,\nMixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only\ndemonstrates superior performance in both language-specific and overall\nassessments but also highlights its capability to excel in linguistic\ndiversity. Our commitment to advancing language models extends beyond\nwell-resourced languages, aiming to bridge the gap for those with limited\nlinguistic assets. Additionally, Komodo-7B-Instruct's better cross-language\nunderstanding contributes to addressing educational disparities in Indonesia,\noffering direct translations from English to 11 regional languages, a\nsignificant improvement compared to existing language translation services.\nKomodo-7B represents a crucial step towards inclusivity and effectiveness in\nlanguage models, providing to the linguistic needs of diverse communities.\n","authors":["Louis Owen","Vishesh Tripathi","Abhay Kumar","Biddwan Ahmed"],"pdf_url":"https://arxiv.org/pdf/2403.09362v1.pdf","comment":"30 Pages, 8 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2305.15083v3","updated":"2024-03-14T13:04:49Z","published":"2023-05-24T12:00:24Z","title":"Eliciting the Translation Ability of Large Language Models via\n  Multilingual Finetuning with Translation Instructions","summary":"  Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have\nshown strong abilities in multilingual translations, without being explicitly\ntrained on parallel corpora. It is interesting how the LLMs obtain their\nability to carry out translation instructions for different languages. In this\npaper, we present a detailed analysis by finetuning a multilingual pretrained\nlanguage model, XGLM-7B, to perform multilingual translation following given\ninstructions. Firstly, we show that multilingual LLMs have stronger translation\nabilities than previously demonstrated. For a certain language, the performance\ndepends on its similarity to English and the amount of data used in the\npretraining phase. Secondly, we find that LLMs' ability to carry out\ntranslation instructions relies on the understanding of translation\ninstructions and the alignment among different languages. With multilingual\nfinetuning, LLMs could learn to perform the translation task well even for\nthose language pairs unseen during the instruction tuning phase.\n","authors":["Jiahuan Li","Hao Zhou","Shujian Huang","Shanbo Cheng","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2305.15083v3.pdf","comment":"accepted by Transaction of ACL, pre-MIT version"},{"id":"http://arxiv.org/abs/2310.05116v3","updated":"2024-03-14T12:39:14Z","published":"2023-10-08T11:09:16Z","title":"Utilizing Contextual Clues and Role Correlations for Enhancing\n  Document-level Event Argument Extraction","summary":"  Document-level event argument extraction is a crucial yet challenging task\nwithin the field of information extraction. Current mainstream approaches\nprimarily focus on the information interaction between event triggers and their\narguments, facing two limitations: insufficient context interaction and the\nignorance of event correlations. Here, we introduce a novel framework named\nCARLG (Contextual Aggregation of clues and Role-based Latent Guidance),\ncomprising two innovative components: the Contextual Clues Aggregation (CCA)\nand the Role-based Latent Information Guidance (RLIG). The CCA module leverages\nthe attention weights derived from a pre-trained encoder to adaptively\nassimilates broader contextual information, while the RLIG module aims to\ncapture the semantic correlations among event roles. We then instantiate the\nCARLG framework into two variants based on two types of current mainstream EAE\napproaches. Notably, our CARLG framework introduces less than 1% new parameters\nyet significantly improving the performance. Comprehensive experiments across\nthe RAMS, WikiEvents, and MLEE datasets confirm the superiority of CARLG,\nshowing significant superiority in terms of both performance and inference\nspeed compared to major benchmarks. Further analyses demonstrate the\neffectiveness of the proposed modules.\n","authors":["Wanlong Liu","Dingyi Zeng","Li Zhou","Yichen Xiao","Weishan Kong","Malu Zhang","Shaohuan Cheng","Hongyang Zhao","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05116v3.pdf","comment":"pre-submission"},{"id":"http://arxiv.org/abs/2403.03750v2","updated":"2024-03-14T12:30:54Z","published":"2024-03-06T14:37:30Z","title":"German also Hallucinates! Inconsistency Detection in News Summaries with\n  the Absinth Dataset","summary":"  The advent of Large Language Models (LLMs) has led to remarkable progress on\na wide range of natural language processing tasks. Despite the advances, these\nlarge-sized models still suffer from hallucinating information in their output,\nwhich poses a major issue in automatic text summarization, as we must guarantee\nthat the generated summary is consistent with the content of the source\ndocument. Previous research addresses the challenging task of detecting\nhallucinations in the output (i.e. inconsistency detection) in order to\nevaluate the faithfulness of the generated summaries. However, these works\nprimarily focus on English and recent multilingual approaches lack German data.\nThis work presents absinth, a manually annotated dataset for hallucination\ndetection in German news summarization and explores the capabilities of novel\nopen-source LLMs on this task in both fine-tuning and in-context learning\nsettings. We open-source and release the absinth dataset to foster further\nresearch on hallucination detection in German.\n","authors":["Laura Mascarell","Ribin Chalumattu","Annette Rios"],"pdf_url":"https://arxiv.org/pdf/2403.03750v2.pdf","comment":"11 pages, 2 figures, 7 tables, conference: Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024), Turin, Italy, May 20-25, 2024"},{"id":"http://arxiv.org/abs/2402.18603v4","updated":"2024-03-14T12:10:43Z","published":"2024-02-28T08:29:42Z","title":"MMSR: Symbolic Regression is a Multimodal Task","summary":"  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n","authors":["Yanjie Li","Jingyi Liu","Weijun Li","Lina Yu","Min Wu","Wenqiang Li","Meilan Hao","Su Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2402.18603v4.pdf","comment":"12 page"},{"id":"http://arxiv.org/abs/2403.09298v1","updated":"2024-03-14T11:37:02Z","published":"2024-03-14T11:37:02Z","title":"More than words: Advancements and challenges in speech recognition for\n  singing","summary":"  This paper addresses the challenges and advancements in speech recognition\nfor singing, a domain distinctly different from standard speech recognition.\nSinging encompasses unique challenges, including extensive pitch variations,\ndiverse vocal styles, and background music interference. We explore key areas\nsuch as phoneme recognition, language identification in songs, keyword\nspotting, and full lyrics transcription. I will describe some of my own\nexperiences when performing research on these tasks just as they were starting\nto gain traction, but will also show how recent developments in deep learning\nand large-scale datasets have propelled progress in this field. My goal is to\nilluminate the complexities of applying speech recognition to singing, evaluate\ncurrent capabilities, and outline future research directions.\n","authors":["Anna Kruspe"],"pdf_url":"https://arxiv.org/pdf/2403.09298v1.pdf","comment":"Conference on Electronic Speech Signal Processing (ESSV) 2024,\n  Keynote"},{"id":"http://arxiv.org/abs/2403.00774v2","updated":"2024-03-14T11:34:28Z","published":"2024-02-14T02:33:17Z","title":"Regional inflation analysis using social network data","summary":"  Inflation is one of the most important macroeconomic indicators that have a\ngreat impact on the population of any country and region. Inflation is\ninfluenced by range of factors, one of which is inflation expectations. Many\ncentral banks take this factor into consideration while implementing monetary\npolicy within the inflation targeting regime. Nowadays, a lot of people are\nactive users of the Internet, especially social networks. There is a hypothesis\nthat people search, read, and discuss mainly only those issues that are of\nparticular interest to them. It is logical to assume that the dynamics of\nprices may also be in the focus of user discussions. So, such discussions could\nbe regarded as an alternative source of more rapid information about inflation\nexpectations. This study is based on unstructured data from Vkontakte social\nnetwork to analyze upward and downward inflationary trends (on the example of\nthe Omsk region). The sample of more than 8.5 million posts was collected\nbetween January 2010 and May 2022. The authors used BERT neural networks to\nsolve the problem. These models demonstrated better results than the benchmarks\n(e.g., logistic regression, decision tree classifier, etc.). It makes possible\nto define pro-inflationary and disinflationary types of keywords in different\ncontexts and get their visualization with SHAP method. This analysis provides\nadditional operational information about inflationary processes at the regional\nlevel The proposed approach can be scaled for other regions. At the same time\nthe limitation of the work is the time and power costs for the initial training\nof similar models for all regions of Russia.\n","authors":["Vasilii Chsherbakov","Ilia Karpov"],"pdf_url":"https://arxiv.org/pdf/2403.00774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09294v1","updated":"2024-03-14T11:29:47Z","published":"2024-03-14T11:29:47Z","title":"Anatomical Structure-Guided Medical Vision-Language Pre-training","summary":"  Learning medical visual representations through vision-language pre-training\nhas reached remarkable progress. Despite the promising performance, it still\nfaces challenges, i.e., local alignment lacks interpretability and clinical\nrelevance, and the insufficient internal and external representation learning\nof image-report pairs. To address these issues, we propose an Anatomical\nStructure-Guided (ASG) framework. Specifically, we parse raw reports into\ntriplets <anatomical region, finding, existence>, and fully utilize each\nelement as supervision to enhance representation learning. For anatomical\nregion, we design an automatic anatomical region-sentence alignment paradigm in\ncollaboration with radiologists, considering them as the minimum semantic units\nto explore fine-grained local alignment. For finding and existence, we regard\nthem as image tags, applying an image-tag recognition decoder to associate\nimage features with their respective tags within each sample and constructing\nsoft labels for contrastive learning to improve the semantic association of\ndifferent image-report pairs. We evaluate the proposed ASG framework on two\ndownstream tasks, including five public benchmarks. Experimental results\ndemonstrate that our method outperforms the state-of-the-art methods.\n","authors":["Qingqiu Li","Xiaohan Yan","Jilan Xu","Runtian Yuan","Yuejie Zhang","Rui Feng","Quanli Shen","Xiaobo Zhang","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01334v2","updated":"2024-03-14T11:01:15Z","published":"2023-10-02T16:51:32Z","title":"Merge, Then Compress: Demystify Efficient SMoE with Hints from Its\n  Routing Policy","summary":"  Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up\nthe learning capacity of neural networks, however, they have issues like (a)\nHigh Memory Usage, due to duplication of the network layers into multiple\ncopies as experts; and (b) Redundancy in Experts, as common learning-based\nrouting policies suffer from representational collapse. Therefore, vanilla SMoE\nmodels are memory inefficient and non-scalable, especially for\nresource-constrained downstream scenarios. In this paper, we ask: Can we craft\na compact SMoE model by consolidating expert information? What is the best\nrecipe to merge multiple experts into fewer but more knowledgeable experts? Our\npilot investigation reveals that conventional model merging methods fail to be\neffective in such expert merging for SMoE. The potential reasons are: (1)\nredundant information overshadows critical experts; (2) appropriate neuron\npermutation for each expert is missing to bring all of them in alignment. To\naddress this, we propose M-SMoE, which leverages routing statistics to guide\nexpert merging. Specifically, it starts with neuron permutation alignment for\nexperts; then, dominant experts and their \"group members\" are formed; lastly,\nevery expert group is merged into a single expert by utilizing each expert's\nactivation frequency as their weight for merging, thus diminishing the impact\nof insignificant experts. Moreover, we observed that our proposed merging\npromotes a low dimensionality in the merged expert's weight space, naturally\npaving the way for additional compression. Hence, our final method, MC-SMoE\n(i.e., Merge, then Compress SMoE), further decomposes the merged experts into\nlow-rank and structural sparse alternatives. Extensive experiments across 8\nbenchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE\nachieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in\nperformance.\n","authors":["Pingzhi Li","Zhenyu Zhang","Prateek Yadav","Yi-Lin Sung","Yu Cheng","Mohit Bansal","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2310.01334v2.pdf","comment":"This paper is accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2311.17371v2","updated":"2024-03-14T10:56:50Z","published":"2023-11-29T05:54:41Z","title":"Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs","summary":"  Recent advancements in large language models (LLMs) underscore their\npotential for responding to inquiries in various domains. However, ensuring\nthat generative agents provide accurate and reliable answers remains an ongoing\nchallenge. In this context, multi-agent debate (MAD) has emerged as a promising\nstrategy for enhancing the truthfulness of LLMs. We benchmark a range of\ndebating and prompting strategies to explore the trade-offs between cost, time,\nand accuracy. Importantly, we find that multi-agent debating systems, in their\ncurrent form, do not reliably outperform other proposed prompting strategies,\nsuch as self-consistency and ensembling using multiple reasoning paths.\nHowever, when performing hyperparameter tuning, several MAD systems, such as\nMulti-Persona, perform better. This suggests that MAD protocols might not be\ninherently worse than other approaches, but that they are more sensitive to\ndifferent hyperparameter settings and difficult to optimize. We build on these\nresults to offer insights into improving debating strategies, such as adjusting\nagent agreement levels, which can significantly enhance performance and even\nsurpass all other non-debate protocols we evaluated. We provide an open-source\nrepository to the community with several state-of-the-art protocols together\nwith evaluation scripts to benchmark across popular research datasets.\n","authors":["Andries Smit","Paul Duckworth","Nathan Grinsztajn","Thomas D. Barrett","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2311.17371v2.pdf","comment":"2 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.08103v2","updated":"2024-03-14T10:47:32Z","published":"2024-03-12T22:23:08Z","title":"Contextual Clarity: Generating Sentences with Transformer Models using\n  Context-Reverso Data","summary":"  In the age of information abundance, the ability to provide users with\ncontextually relevant and concise information is crucial. Keyword in Context\n(KIC) generation is a task that plays a vital role in and generation\napplications, such as search engines, personal assistants, and content\nsummarization. In this paper, we present a novel approach to generating\nunambiguous and brief sentence-contexts for given keywords using the T5\ntransformer model, leveraging data obtained from the Context-Reverso API. The\ncode is available at https://github.com/Rusamus/word2context/tree/main .\n","authors":["Ruslan Musaev"],"pdf_url":"https://arxiv.org/pdf/2403.08103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14000v2","updated":"2024-03-14T10:35:57Z","published":"2022-07-28T10:44:46Z","title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation","summary":"  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gate attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention\ncan achieve higher test accuracy than DeepLogic and other RNN baseline models.\nOur model achieves better out-of-distribution generalisation than RoBERTa-Large\nwhen the rules have been shuffled. Furthermore, to address the issue of\nunbalanced distribution of reasoning depths in the current multi-step reasoning\ndatasets, we develop PARARULE-Plus, a large dataset with more examples that\nrequire deeper reasoning steps. Experimental results show that the addition of\nPARARULE-Plus can increase the model's performance on examples requiring deeper\nreasoning depths. The source code and data are available at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Tim Hartill","Neset Tan","Zhenyun Deng","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2207.14000v2.pdf","comment":"10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)"},{"id":"http://arxiv.org/abs/2403.09259v1","updated":"2024-03-14T10:33:28Z","published":"2024-03-14T10:33:28Z","title":"To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation","summary":"  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines\nuncertainty and diversity for sentence selection. HUDS computes uncertainty\nscores for unlabeled sentences and subsequently stratifies them. It then\nclusters sentence embeddings within each stratum using k-MEANS and computes\ndiversity scores by distance to the centroid. A weighted hybrid score that\ncombines uncertainty and diversity is then used to select the top instances for\nannotation in each AL iteration. Experiments on multi-domain German-English\ndatasets demonstrate the better performance of HUDS over other strong AL\nbaselines. We analyze the sentence selection with HUDS and show that it\nprioritizes diverse instances having high model uncertainty for annotation in\nearly AL iterations.\n","authors":["Abdul Hameed Azeemi","Ihsan Ayyub Qazi","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2403.09259v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2309.17167v3","updated":"2024-03-14T09:52:16Z","published":"2023-09-29T12:04:14Z","title":"DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks","summary":"  Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns are raised about potential data\ncontamination in their considerable volume of training corpus. Moreover, the\nstatic nature and fixed complexity of current benchmarks may inadequately gauge\nthe advancing capabilities of LLMs. In this paper, we introduce DyVal, a\ngeneral and flexible protocol for dynamic evaluation of LLMs. Based on our\nframework, we build graph-informed DyVal by leveraging the structural advantage\nof directed acyclic graphs to dynamically generate evaluation samples with\ncontrollable complexities. DyVal generates challenging evaluation sets on\nreasoning tasks including mathematics, logical reasoning, and algorithm\nproblems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo\nand GPT-4. Experiments show that LLMs perform worse in DyVal-generated\nevaluation samples with different complexities, highlighting the significance\nof dynamic evaluation. We also analyze the failure cases and results of\ndifferent prompting methods. Moreover, DyVal-generated samples are not only\nevaluation sets, but also helpful data for fine-tuning to improve the\nperformance of LLMs on existing benchmarks. We hope that DyVal can shed light\non future evaluation research of LLMs. Code is available at:\nhttps://github.com/microsoft/promptbench.\n","authors":["Kaijie Zhu","Jiaao Chen","Jindong Wang","Neil Zhenqiang Gong","Diyi Yang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2309.17167v3.pdf","comment":"ICLR 2024 spotlight; 38 pages; code is at aka.ms/dyval"},{"id":"http://arxiv.org/abs/2403.09226v1","updated":"2024-03-14T09:45:05Z","published":"2024-03-14T09:45:05Z","title":"Retrieval augmented text-to-SQL generation for epidemiological question\n  answering using electronic health records","summary":"  Electronic health records (EHR) and claims data are rich sources of\nreal-world data that reflect patient health status and healthcare utilization.\nQuerying these databases to answer epidemiological questions is challenging due\nto the intricacy of medical terminology and the need for complex SQL queries.\nHere, we introduce an end-to-end methodology that combines text-to-SQL\ngeneration with retrieval augmented generation (RAG) to answer epidemiological\nquestions using EHR and claims data. We show that our approach, which\nintegrates a medical coding step into the text-to-SQL process, significantly\nimproves the performance over simple prompting. Our findings indicate that\nalthough current language models are not yet sufficiently accurate for\nunsupervised use, RAG offers a promising direction for improving their\ncapabilities, as shown in a realistic industry setting.\n","authors":["Angelo Ziletti","Leonardo D'Ambrosi"],"pdf_url":"https://arxiv.org/pdf/2403.09226v1.pdf","comment":"6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2309.10272v2","updated":"2024-03-14T09:32:16Z","published":"2023-09-19T02:59:41Z","title":"Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and\n  Hindi","summary":"  One of the most popular downstream tasks in the field of Natural Language\nProcessing is text classification. Text classification tasks have become more\ndaunting when the texts are code-mixed. Though they are not exposed to such\ntext during pre-training, different BERT models have demonstrated success in\ntackling Code-Mixed NLP challenges. Again, in order to enhance their\nperformance, Code-Mixed NLP models have depended on combining synthetic data\nwith real-world data. It is crucial to understand how the BERT models'\nperformance is impacted when they are pretrained using corresponding code-mixed\nlanguages. In this paper, we introduce Tri-Distil-BERT, a multilingual model\npre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model\nfine-tuned on code-mixed data. Both models are evaluated across multiple NLP\ntasks and demonstrate competitive performance against larger models like mBERT\nand XLM-R. Our two-tiered pre-training approach offers efficient alternatives\nfor multilingual and code-mixed language understanding, contributing to\nadvancements in the field.\n","authors":["Md Nishat Raihan","Dhiman Goswami","Antara Mahmud"],"pdf_url":"https://arxiv.org/pdf/2309.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09207v1","updated":"2024-03-14T09:21:25Z","published":"2024-03-14T09:21:25Z","title":"TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic\n  Tasks","summary":"  In this paper, we explore the capabilities of LLMs in capturing\nlexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model\nand test it on multiple lexical semantic tasks. As the outcome of our\nexperiments, we present TaxoLLaMA, the everything-in-one model, lightweight due\nto 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results\nout of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy\nConstruction, and Lexical Entailment tasks. Moreover, it demonstrates very\nstrong zero-shot performance on Lexical Entailment and Taxonomy Construction\nwith no fine-tuning. We also explore its hidden multilingual and domain\nadaptation capabilities with a little tuning or few-shot learning. All\ndatasets, code, and model are available online at\nhttps://github.com/VityaVitalich/TaxoLLaMA\n","authors":["Viktor Moskvoretskii","Ekaterina Neminova","Alina Lobanova","Alexander Panchenko","Irina Nikishina"],"pdf_url":"https://arxiv.org/pdf/2403.09207v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.09167v1","updated":"2024-03-14T08:27:32Z","published":"2024-03-14T08:27:32Z","title":"Dial-insight: Fine-tuning Large Language Models with High-Quality\n  Domain-Specific Data Preventing Capability Collapse","summary":"  The efficacy of large language models (LLMs) is heavily dependent on the\nquality of the underlying data, particularly within specialized domains. A\ncommon challenge when fine-tuning LLMs for domain-specific applications is the\npotential degradation of the model's generalization capabilities. To address\nthese issues, we propose a two-stage approach for the construction of\nproduction prompts designed to yield high-quality data. This method involves\nthe generation of a diverse array of prompts that encompass a broad spectrum of\ntasks and exhibit a rich variety of expressions. Furthermore, we introduce a\ncost-effective, multi-dimensional quality assessment framework to ensure the\nintegrity of the generated labeling data. Utilizing a dataset comprised of\nservice provider and customer interactions from the real estate sector, we\ndemonstrate a positive correlation between data quality and model performance.\nNotably, our findings indicate that the domain-specific proficiency of general\nLLMs can be enhanced through fine-tuning with data produced via our proposed\nmethod, without compromising their overall generalization abilities, even when\nexclusively domain-specific data is employed for fine-tuning.\n","authors":["Jianwei Sun","Chaoyang Mei","Linlin Wei","Kaiyu Zheng","Na Liu","Ming Cui","Tianyi Li"],"pdf_url":"https://arxiv.org/pdf/2403.09167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09164v1","updated":"2024-03-14T08:20:40Z","published":"2024-03-14T08:20:40Z","title":"Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine\n  Knowledge","summary":"  No previous work has studied the performance of Large Language Models (LLMs)\nin the context of Traditional Chinese Medicine (TCM), an essential and distinct\nbranch of medical knowledge with a rich history. To bridge this gap, we present\na TCM question dataset named TCM-QA, which comprises three question types:\nsingle choice, multiple choice, and true or false, to examine the LLM's\ncapacity for knowledge recall and comprehensive reasoning within the TCM\ndomain. In our study, we evaluate two settings of the LLM, zero-shot and\nfew-shot settings, while concurrently discussing the differences between\nEnglish and Chinese prompts. Our results indicate that ChatGPT performs best in\ntrue or false questions, achieving the highest precision of 0.688 while scoring\nthe lowest precision is 0.241 in multiple-choice questions. Furthermore, we\nobserved that Chinese prompts outperformed English prompts in our evaluations.\nAdditionally, we assess the quality of explanations generated by ChatGPT and\ntheir potential contribution to TCM knowledge comprehension. This paper offers\nvaluable insights into the applicability of LLMs in specialized domains and\npaves the way for future research in leveraging these powerful models to\nadvance TCM.\n","authors":["Li Yizhen","Huang Shaohan","Qi Jiaxing","Quan Lei","Han Dongran","Luan Zhongzhi"],"pdf_url":"https://arxiv.org/pdf/2403.09164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09163v1","updated":"2024-03-14T08:19:41Z","published":"2024-03-14T08:19:41Z","title":"Caveat Lector: Large Language Models in Legal Practice","summary":"  The current fascination with large language models, or LLMs, derives from the\nfact that many users lack the expertise to evaluate the quality of the\ngenerated text. LLMs may therefore appear more capable than they actually are.\nThe dangerous combination of fluency and superficial plausibility leads to the\ntemptation to trust the generated text and creates the risk of overreliance.\nWho would not trust perfect legalese? Relying recent findings in both technical\nand legal scholarship, this Article counterbalances the overly optimistic\npredictions as to the role of LLMs in legal practice. Integrating LLMs into\nlegal workstreams without a better comprehension of their limitations, will\ncreate inefficiencies if not outright risks. Notwithstanding their\nunprecedented ability to generate text, LLMs do not understand text. Without\nthe ability to understand meaning, LLMs will remain unable to use language, to\nacquire knowledge and to perform complex reasoning tasks. Trained to model\nlanguage on the basis of stochastic word predictions, LLMs cannot distinguish\nfact from fiction. Their knowledge of the law is limited to word strings\nmemorized in their parameters. It is also incomplete and largely incorrect.\nLLMs operate at the level of word distributions, not at the level of verified\nfacts. The resulting propensity to hallucinate, to produce statements that are\nincorrect but appear helpful and relevant, is alarming in high-risk areas like\nlegal services. At present, lawyers should beware of relying on text generated\nby LLMs.\n","authors":["Eliza Mik"],"pdf_url":"https://arxiv.org/pdf/2403.09163v1.pdf","comment":"Vol 19 Rutgers Bus L R 2 2024 (forthcoming)"},{"id":"http://arxiv.org/abs/2403.09162v1","updated":"2024-03-14T08:18:59Z","published":"2024-03-14T08:18:59Z","title":"Unveiling the Generalization Power of Fine-Tuned Large Language Models","summary":"  While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.\n","authors":["Haoran Yang","Yumeng Zhang","Jiaqi Xu","Hongyuan Lu","Pheng Ann Heng","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2403.09162v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2305.13068v3","updated":"2024-03-14T08:15:27Z","published":"2023-05-22T14:37:05Z","title":"Making Language Models Better Tool Learners with Execution Feedback","summary":"  Tools serve as pivotal interfaces that enable humans to understand and\nreshape the environment. With the advent of foundation models, AI systems can\nutilize tools to expand their capabilities and interact with the real world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce large language models to utilize\ntools indiscriminately, as complex tasks often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the large language model\nselectively use tools by improving the accuracy of tool usage while enhancing\ninsufficient tool learning and mitigating excessive reliance on tools. Code is\navailable at https://github.com/zjunlp/TRICE.\n","authors":["Shuofei Qiao","Honghao Gui","Chengfei Lv","Qianghuai Jia","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13068v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09159v1","updated":"2024-03-14T08:12:47Z","published":"2024-03-14T08:12:47Z","title":"Basque and Spanish Counter Narrative Generation: Data Creation and\n  Evaluation","summary":"  Counter Narratives (CNs) are non-negative textual responses to Hate Speech\n(HS) aiming at defusing online hatred and mitigating its spreading across\nmedia. Despite the recent increase in HS content posted online, research on\nautomatic CN generation has been relatively scarce and predominantly focused on\nEnglish. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset\nfor CN generation developed by means of Machine Translation (MT) and\nprofessional post-edition. Being a parallel corpus, also with respect to the\noriginal English CONAN, it allows to perform novel research on multilingual and\ncrosslingual automatic generation of CNs. Our experiments on CN generation with\nmT5, a multilingual encoder-decoder model, show that generation greatly\nbenefits from training on post-edited data, as opposed to relying on silver MT\ndata only. These results are confirmed by their correlation with a qualitative\nmanual evaluation, demonstrating that manually revised training data remains\ncrucial for the quality of the generated CNs. Furthermore, multilingual data\naugmentation improves results over monolingual settings for structurally\nsimilar languages such as English and Spanish, while being detrimental for\nBasque, a language isolate. Similar findings occur in zero-shot crosslingual\nevaluations, where model transfer (fine-tuning in English and generating in a\ndifferent target language) outperforms fine-tuning mT5 on machine translated\ndata for Spanish but not for Basque. This provides an interesting insight into\nthe asymmetry in the multilinguality of generative models, a challenging topic\nwhich is still open to research.\n","authors":["Jaione Bengoetxea","Yi-Ling Chung","Marco Guerini","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2403.09159v1.pdf","comment":"Accepted for the Joint International Conference on Computational\n  Linguistics, Language Resources and Evaluation (LREC-COLING) 2024"},{"id":"http://arxiv.org/abs/2310.20703v3","updated":"2024-03-14T08:05:18Z","published":"2023-10-31T17:59:05Z","title":"Vanishing Gradients in Reinforcement Finetuning of Language Models","summary":"  Pretrained language models are commonly aligned with human preferences and\ndownstream tasks via reinforcement finetuning (RFT), which refers to maximizing\na (possibly learned) reward function using policy gradient algorithms. This\nwork identifies a fundamental optimization obstacle in RFT: we prove that the\nexpected gradient for an input vanishes when its reward standard deviation\nunder the model is small, even if the expected reward is far from optimal.\nThrough experiments on an RFT benchmark and controlled environments, as well as\na theoretical analysis, we then demonstrate that vanishing gradients due to\nsmall reward standard deviation are prevalent and detrimental, leading to\nextremely slow reward maximization. Lastly, we explore ways to overcome\nvanishing gradients in RFT. We find the common practice of an initial\nsupervised finetuning (SFT) phase to be the most promising candidate, which\nsheds light on its importance in an RFT pipeline. Moreover, we show that a\nrelatively small number of SFT optimization steps on as few as 1% of the input\nsamples can suffice, indicating that the initial SFT phase need not be\nexpensive in terms of compute and data labeling efforts. Overall, our results\nemphasize that being mindful for inputs whose expected gradient vanishes, as\nmeasured by the reward standard deviation, is crucial for successful execution\nof RFT.\n","authors":["Noam Razin","Hattie Zhou","Omid Saremi","Vimal Thilak","Arwen Bradley","Preetum Nakkiran","Joshua Susskind","Etai Littwin"],"pdf_url":"https://arxiv.org/pdf/2310.20703v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08495v2","updated":"2024-03-14T08:05:08Z","published":"2024-03-13T13:04:58Z","title":"Automatic Interactive Evaluation for Large Language Models with State\n  Aware Patient Simulator","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.\n","authors":["Yusheng Liao","Yutong Meng","Yuhao Wang","Hongcheng Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08495v2.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.05296v2","updated":"2024-03-14T08:04:17Z","published":"2023-11-09T11:53:52Z","title":"BeLLM: Backward Dependency Enhanced Large Language Model for Sentence\n  Embeddings","summary":"  Sentence embeddings are crucial in measuring semantic similarity. Most recent\nstudies employed large language models (LLMs) to learn sentence embeddings.\nExisting LLMs mainly adopted autoregressive architecture without explicit\nbackward dependency modeling. Therefore, we examined the effects of backward\ndependencies in LLMs for semantic similarity measurements. Concretely, we\npropose a novel model: backward dependency enhanced large language model\n(BeLLM). It learns sentence embeddings via transforming specific attention\nlayers from uni- to bi-directional. We extensively experiment across various\nsemantic textual similarity (STS) tasks and downstream applications. BeLLM\nachieves state-of-the-art performance in varying scenarios. It shows that\nauto-regressive LLMs benefit from backward dependencies for sentence\nembeddings.\n","authors":["Xianming Li","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2311.05296v2.pdf","comment":"Accepted by NAACL24 Main Conference"},{"id":"http://arxiv.org/abs/2403.04321v2","updated":"2024-03-14T08:02:29Z","published":"2024-03-07T08:37:33Z","title":"Discriminative Probing and Tuning for Text-to-Image Generation","summary":"  Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.\n","authors":["Leigang Qu","Wenjie Wang","Yongqi Li","Hanwang Zhang","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2403.04321v2.pdf","comment":"CVPR 2024; project page: https://dpt-t2i.github.io/"},{"id":"http://arxiv.org/abs/2403.05101v3","updated":"2024-03-14T08:00:51Z","published":"2024-03-08T07:06:43Z","title":"Rule-driven News Captioning","summary":"  News captioning task aims to generate sentences by describing named entities\nor concrete events for an image with its news article. Existing methods have\nachieved remarkable results by relying on the large-scale pre-trained models,\nwhich primarily focus on the correlations between the input news content and\nthe output predictions. However, the news captioning requires adhering to some\nfundamental rules of news reporting, such as accurately describing the\nindividuals and actions associated with the event. In this paper, we propose\nthe rule-driven news captioning method, which can generate image descriptions\nfollowing designated rule signal. Specifically, we first design the news-aware\nsemantic rule for the descriptions. This rule incorporates the primary action\ndepicted in the image (e.g., \"performing\") and the roles played by named\nentities involved in the action (e.g., \"Agent\" and \"Place\"). Second, we inject\nthis semantic rule into the large-scale pre-trained model, BART, with the\nprefix-tuning strategy, where multiple encoder layers are embedded with\nnews-aware semantic rule. Finally, we can effectively guide BART to generate\nnews sentences that comply with the designated rule. Extensive experiments on\ntwo widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the\neffectiveness of our method.\n","authors":["Ning Xu","Tingting Zhang","Hongshuo Tian","An-An Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05101v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09148v1","updated":"2024-03-14T07:58:27Z","published":"2024-03-14T07:58:27Z","title":"Evaluating LLMs for Gender Disparities in Notable Persons","summary":"  This study examines the use of Large Language Models (LLMs) for retrieving\nfactual information, addressing concerns over their propensity to produce\nfactually incorrect \"hallucinated\" responses or to altogether decline to even\nanswer prompt at all. Specifically, it investigates the presence of\ngender-based biases in LLMs' responses to factual inquiries. This paper takes a\nmulti-pronged approach to evaluating GPT models by evaluating fairness across\nmultiple dimensions of recall, hallucinations and declinations. Our findings\nreveal discernible gender disparities in the responses generated by GPT-3.5.\nWhile advancements in GPT-4 have led to improvements in performance, they have\nnot fully eradicated these gender disparities, notably in instances where\nresponses are declined. The study further explores the origins of these\ndisparities by examining the influence of gender associations in prompts and\nthe homogeneity in the responses.\n","authors":["Lauren Rhue","Sofie Goethals","Arun Sundararajan"],"pdf_url":"https://arxiv.org/pdf/2403.09148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07691v2","updated":"2024-03-14T07:47:08Z","published":"2024-03-12T14:34:08Z","title":"ORPO: Monolithic Preference Optimization without Reference Model","summary":"  While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level\nloose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model\ncheckpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).\n","authors":["Jiwoo Hong","Noah Lee","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2403.07691v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.09131v1","updated":"2024-03-14T06:49:16Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.05502v2","updated":"2024-03-14T05:55:44Z","published":"2023-10-09T08:07:04Z","title":"XAL: EXplainable Active Learning Makes Classifiers Better Low-resource\n  Learners","summary":"  Active learning (AL), which aims to construct an effective training set by\niteratively curating the most formative unlabeled data for annotation, has been\nwidely used in low-resource tasks. Most active learning techniques in\nclassification rely on the model's uncertainty or disagreement to choose\nunlabeled data, suffering from the problem of over-confidence in superficial\npatterns and a lack of exploration. Inspired by the cognitive processes in\nwhich humans deduce and predict through causal information, we take an initial\nattempt towards integrating rationales into AL and propose a novel Explainable\nActive Learning framework (XAL) for low-resource text classification, which\naims to encourage classifiers to justify their inferences and delve into\nunlabeled data for which they cannot provide reasonable explanations.\nSpecifically, besides using a pre-trained bi-directional encoder for\nclassification, we employ a pre-trained uni-directional decoder to generate and\nscore the explanation. We further facilitate the alignment of the model with\nhuman reasoning preference through a proposed ranking loss. During the\nselection of unlabeled data, the predicted uncertainty of the encoder and the\nexplanation score of the decoder complement each other as the final metric to\nacquire informative data. Extensive experiments on six datasets show that XAL\nachieves consistent improvement over 9 strong baselines. Analysis indicates\nthat the proposed method can generate corresponding explanations for its\npredictions.\n","authors":["Yun Luo","Zhen Yang","Fandong Meng","Yingjie Li","Fang Guo","Qinglin Qi","Jie Zhou","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05502v2.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09113v1","updated":"2024-03-14T05:29:35Z","published":"2024-03-14T05:29:35Z","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based\n  on Meta Learning","summary":"  Large-scale pretraining followed by task-specific finetuning has achieved\ngreat success in various NLP tasks. Since finetuning all parameters of large\npretrained models poses substantial computational and memory challenges,\nseveral efficient finetuning methods have been developed. Among them, low-rank\nadaptation (LoRA), which finetunes low-rank incremental update matrices on top\nof frozen pretrained weights, has proven particularly effective. Nonetheless,\nLoRA's uniform rank assignment across all layers, along with its reliance on an\nexhaustive search to find the best rank, leads to high computation costs and\nsuboptimal finetuning performance. To address these limitations, we introduce\nAutoLoRA, a meta learning based framework for automatically identifying the\noptimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a\nlow-rank update matrix with a selection variable, which determines whether the\nrank-1 matrix should be discarded. A meta learning based method is developed to\nlearn these selection variables. The optimal rank is determined by thresholding\nthe values of these variables. Our comprehensive experiments on natural\nlanguage understanding, generation, and sequence labeling demonstrate the\neffectiveness of AutoLoRA.\n","authors":["Ruiyi Zhang","Rushi Qiang","Sai Ashish Somayajula","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2403.09113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09097v1","updated":"2024-03-14T04:43:02Z","published":"2024-03-14T04:43:02Z","title":"AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI\n  Publications","summary":"  Identifying scientific publications that are within a dynamic field of\nresearch often requires costly annotation by subject-matter experts. Resources\nlike widely-accepted classification criteria or field taxonomies are\nunavailable for a domain like artificial intelligence (AI), which spans\nemerging topics and technologies. We address these challenges by inferring a\nfunctional definition of AI research from existing expert labels, and then\nevaluating state-of-the-art chatbot models on the task of expert data\nannotation. Using the arXiv publication database as ground-truth, we experiment\nwith prompt engineering for GPT chatbot models to identify an alternative,\nautomated expert annotation pipeline that assigns AI labels with 94% accuracy.\nFor comparison, we fine-tune SPECTER, a transformer language model pre-trained\non scientific publications, that achieves 96% accuracy (only 2% higher than\nGPT) on classifying AI publications. Our results indicate that with effective\nprompt engineering, chatbots can be used as reliable data annotators even where\nsubject-area expertise is required. To evaluate the utility of\nchatbot-annotated datasets on downstream classification tasks, we train a new\nclassifier on GPT-labeled data and compare its performance to the arXiv-trained\nmodel. The classifier trained on GPT-labeled data outperforms the arXiv-trained\nmodel by nine percentage points, achieving 82% accuracy.\n","authors":["Autumn Toney-Wails","Christian Schoeberl","James Dunham"],"pdf_url":"https://arxiv.org/pdf/2403.09097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09092v1","updated":"2024-03-14T04:32:13Z","published":"2024-03-14T04:32:13Z","title":"MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection","summary":"  The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.\n","authors":["Yupeng Li","Haorui He","Jin Bai","Dacheng Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09092v1.pdf","comment":"Accepted by the ACM Web Conference 2024 (WWW 2024) oral, dataset\n  available: https://github.com/TrustworthyComp"},{"id":"http://arxiv.org/abs/2310.01798v2","updated":"2024-03-14T04:27:52Z","published":"2023-10-03T04:56:12Z","title":"Large Language Models Cannot Self-Correct Reasoning Yet","summary":"  Large Language Models (LLMs) have emerged as a groundbreaking technology with\ntheir unparalleled text generation capabilities across various applications.\nNevertheless, concerns persist regarding the accuracy and appropriateness of\ntheir generated content. A contemporary methodology, self-correction, has been\nproposed as a remedy to these issues. Building upon this premise, this paper\ncritically examines the role and efficacy of self-correction within LLMs,\nshedding light on its true potential and limitations. Central to our\ninvestigation is the notion of intrinsic self-correction, whereby an LLM\nattempts to correct its initial responses based solely on its inherent\ncapabilities, without the crutch of external feedback. In the context of\nreasoning, our research indicates that LLMs struggle to self-correct their\nresponses without external feedback, and at times, their performance even\ndegrades after self-correction. Drawing from these insights, we offer\nsuggestions for future research and practical applications in this field.\n","authors":["Jie Huang","Xinyun Chen","Swaroop Mishra","Huaixiu Steven Zheng","Adams Wei Yu","Xinying Song","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.01798v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09085v1","updated":"2024-03-14T04:06:13Z","published":"2024-03-14T04:06:13Z","title":"Meaningful Learning: Advancing Abstract Reasoning in Large Language\n  Models via Generic Fact Guidance","summary":"  Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nsimple questions supported by a generic fact, LLMs often fail to provide\nconsistent and precise answers, indicating a deficiency in abstract reasoning\nabilities. This has sparked a vigorous debate about whether LLMs are genuinely\nreasoning or merely memorizing. In light of this, we design a preliminary study\nto quantify and delve into the abstract reasoning abilities of existing LLMs.\nOur findings reveal a substantial discrepancy between their general reasoning\nand abstract reasoning performances. To relieve this problem, we tailor an\nabstract reasoning dataset (AbsR) together with a meaningful learning paradigm\nto teach LLMs how to leverage generic facts for reasoning purposes. The results\nshow that our approach not only boosts the general reasoning performance of\nLLMs but also makes considerable strides towards their capacity for abstract\nreasoning, moving beyond simple memorization or imitation to a more nuanced\nunderstanding and application of generic facts.\n","authors":["Kai Xiong","Xiao Ding","Ting Liu","Bing Qin","Dongliang Xu","Qing Yang","Hongtao Liu","Yixin Cao"],"pdf_url":"https://arxiv.org/pdf/2403.09085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09077v1","updated":"2024-03-14T03:49:36Z","published":"2024-03-14T03:49:36Z","title":"Information Extraction: An application to the domain of hyper-local\n  financial data on developing countries","summary":"  Despite the need for financial data on company activities in developing\ncountries for development research and economic analysis, such data does not\nexist. In this project, we develop and evaluate two Natural Language Processing\n(NLP) based techniques to address this issue. First, we curate a custom dataset\nspecific to the domain of financial text data on developing countries and\nexplore multiple approaches for information extraction. We then explore a\ntext-to-text approach with the transformer-based T5 model with the goal of\nundertaking simultaneous NER and relation extraction. We find that this model\nis able to learn the custom text structure output data corresponding to the\nentities and their relations, resulting in an accuracy of 92.44\\%, a precision\nof 68.25\\% and a recall of 54.20\\% from our best T5 model on the combined task.\nSecondly, we explore an approach with sequential NER and relation extration.\nFor the NER, we run pre-trained and fine-tuned models using SpaCy, and we\ndevelop a custom relation extraction model using SpaCy's Dependency Parser\noutput and some heuristics to determine entity relationships \\cite{spacy}. We\nobtain an accuracy of 84.72\\%, a precision of 6.06\\% and a recall of 5.57\\% on\nthis sequential task.\n","authors":["Abuzar Royesh","Olamide Oladeji"],"pdf_url":"https://arxiv.org/pdf/2403.09077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04614v3","updated":"2024-03-14T03:48:08Z","published":"2024-02-07T06:32:50Z","title":"Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations\n  from Large Language Models","summary":"  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n","authors":["Chirag Agarwal","Sree Harsha Tanneru","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2402.04614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09073v1","updated":"2024-03-14T03:33:46Z","published":"2024-03-14T03:33:46Z","title":"Large Language Models are Parallel Multilingual Learners","summary":"  In this study, we reveal an in-context learning (ICL) capability of\nmultilingual large language models (LLMs): by translating the input to several\nlanguages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which\nsignificantly enhances their comprehension abilities. To test this capability,\nwe design extensive experiments encompassing 8 typical datasets, 7 languages\nand 8 state-of-the-art multilingual LLMs. Experimental results show that (1)\nincorporating more languages help PiM surpass the conventional ICL further; (2)\neven combining with the translations that are inferior to baseline performance\ncan also help. Moreover, by examining the activated neurons in LLMs, we\ndiscover a counterintuitive but interesting phenomenon. Contrary to the common\nthought that PiM would activate more neurons than monolingual input to leverage\nknowledge learned from diverse languages, PiM actually inhibits neurons and\npromotes more precise neuron activation especially when more languages are\nadded. This phenomenon aligns with the neuroscience insight about synaptic\npruning, which removes less used neural connections, strengthens remainders,\nand then enhances brain intelligence.\n","authors":["Yongyu Mu","Peinan Feng","Zhiquan Cao","Yuzhang Wu","Bei Li","Chenglong Wang","Tong Xiao","Kai Song","Tongran Liu","Chunliang Zhang","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.09073v1.pdf","comment":"Working in process"},{"id":"http://arxiv.org/abs/2403.09072v1","updated":"2024-03-14T03:29:58Z","published":"2024-03-14T03:29:58Z","title":"UniCode: Learning a Unified Codebook for Multimodal Large Language\n  Models","summary":"  In this paper, we propose \\textbf{UniCode}, a novel approach within the\ndomain of multimodal large language models (MLLMs) that learns a unified\ncodebook to efficiently tokenize visual, text, and potentially other types of\nsignals. This innovation addresses a critical limitation in existing MLLMs:\ntheir reliance on a text-only codebook, which restricts MLLM's ability to\ngenerate images and texts in a multimodal context. Towards this end, we propose\na language-driven iterative training paradigm, coupled with an in-context\npre-training task we term ``image decompression'', enabling our model to\ninterpret compressed visual data and generate high-quality images.The unified\ncodebook empowers our model to extend visual instruction tuning to\nnon-linguistic generation tasks. Moreover, UniCode is adaptable to diverse\nstacked quantization approaches in order to compress visual signals into a more\ncompact token representation. Despite using significantly fewer parameters and\nless data during training, Unicode demonstrates promising capabilities in\nvisual reconstruction and generation. It also achieves performances comparable\nto leading MLLMs across a spectrum of VQA benchmarks.\n","authors":["Sipeng Zheng","Bohan Zhou","Yicheng Feng","Ye Wang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09072v1.pdf","comment":"14 pages, 2 figures, 11 tables"},{"id":"http://arxiv.org/abs/2310.08992v3","updated":"2024-03-14T03:29:09Z","published":"2023-10-13T10:17:48Z","title":"CodeChain: Towards Modular Code Generation Through Chain of\n  Self-revisions with Representative Sub-modules","summary":"  Large Language Models (LLMs) have already become quite proficient at solving\nsimpler programming tasks like those in HumanEval or MBPP benchmarks. However,\nsolving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate\nsolutions as monolithic code blocks instead of decomposing them into logical\nsub-tasks and sub-modules. On the other hand, experienced programmers\ninstinctively write modularized code with abstraction for solving complex\ntasks, often reusing previously developed modules. To address this gap, we\npropose CodeChain, a novel framework for inference that elicits modularized\ncode generation through a chain of self-revisions, each being guided by some\nrepresentative sub-modules generated in previous iterations. Concretely,\nCodeChain first instructs the LLM to generate modularized codes through\nchain-of-thought prompting. Then it applies a chain of self-revisions by\niterating the two steps: 1) extracting and clustering the generated sub-modules\nand selecting the cluster representatives as the more generic and re-usable\nimplementations, and 2) augmenting the original chain-of-thought prompt with\nthese selected module-implementations and instructing the LLM to re-generate\nnew modularized solutions. We find that by naturally encouraging the LLM to\nreuse the previously developed and verified sub-modules, CodeChain can\nsignificantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as\nopen-sourced LLMs like WizardCoder. We also conduct comprehensive ablation\nstudies with different methods of prompting, number of clusters, model sizes,\nprogram qualities, etc., to provide useful insights that underpin CodeChain's\nsuccess.\n","authors":["Hung Le","Hailin Chen","Amrita Saha","Akash Gokul","Doyen Sahoo","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2310.08992v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08715v2","updated":"2024-03-14T03:13:20Z","published":"2024-03-13T17:17:48Z","title":"SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language\n  Agents","summary":"  Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.\n","authors":["Ruiyi Wang","Haofei Yu","Wenxin Zhang","Zhengyang Qi","Maarten Sap","Graham Neubig","Yonatan Bisk","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.08715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09059v1","updated":"2024-03-14T02:56:38Z","published":"2024-03-14T02:56:38Z","title":"LAMP: A Language Model on the Map","summary":"  Large Language Models (LLMs) are poised to play an increasingly important\nrole in our lives, providing assistance across a wide array of tasks. In the\ngeospatial domain, LLMs have demonstrated the ability to answer generic\nquestions, such as identifying a country's capital; nonetheless, their utility\nis hindered when it comes to answering fine-grained questions about specific\nplaces, such as grocery stores or restaurants, which constitute essential\naspects of people's everyday lives. This is mainly because the places in our\ncities haven't been systematically fed into LLMs, so as to understand and\nmemorize them. This study introduces a novel framework for fine-tuning a\npre-trained model on city-specific data, to enable it to provide accurate\nrecommendations, while minimizing hallucinations. We share our model, LAMP, and\nthe data used to train it. We conduct experiments to analyze its ability to\ncorrectly retrieving spatial objects, and compare it to well-known open- and\nclosed- source language models, such as GPT-4. Finally, we explore its emerging\ncapabilities through a case study on day planning.\n","authors":["Pasquale Balsebre","Weiming Huang","Gao Cong"],"pdf_url":"https://arxiv.org/pdf/2403.09059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07022v2","updated":"2024-03-14T02:56:32Z","published":"2023-04-14T09:31:17Z","title":"Label Dependencies-aware Set Prediction Networks for Multi-label Text\n  Classification","summary":"  Multi-label text classification involves extracting all relevant labels from\na sentence. Given the unordered nature of these labels, we propose approaching\nthe problem as a set prediction task. To address the correlation between\nlabels, we leverage Graph Convolutional Networks and construct an adjacency\nmatrix based on the statistical relations between labels. Additionally, we\nenhance recall ability by applying the Bhattacharyya distance to the output\ndistributions of the set prediction networks. We evaluate the effectiveness of\nour approach on two multi-label datasets and demonstrate its superiority over\nprevious baselines through experimental results.\n","authors":["Du Xinkai","Han Quanjie","Sun Yalin","Lv Chao","Sun Maosong"],"pdf_url":"https://arxiv.org/pdf/2304.07022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09057v1","updated":"2024-03-14T02:55:37Z","published":"2024-03-14T02:55:37Z","title":"A Continued Pretrained LLM Approach for Automatic Medical Note\n  Generation","summary":"  LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like\nGPT-4, is too costly for most domain-specific scenarios. We present the first\ncontinuously trained 13B Llama2-based LLM that is purpose-built for medical\nconversations and measured on automated scribing. Our results show that our\nmodel outperforms GPT-4 in PubMedQA with 76.6\\% accuracy and matches its\nperformance in summarizing medical conversations into SOAP notes. Notably, our\nmodel exceeds GPT-4 in capturing a higher number of correct medical concepts\nand outperforms human scribes with higher correctness and completeness.\n","authors":["Dong Yuan","Eti Rastogi","Gautam Naik","Jai Chintagunta","Sree Prasanna Rajagopal","Fen Zhao","Sagar Goyal","Jeff Ward"],"pdf_url":"https://arxiv.org/pdf/2403.09057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07556v2","updated":"2024-03-14T02:40:22Z","published":"2024-03-12T11:40:44Z","title":"Truth-Aware Context Selection: Mitigating the Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts","summary":"  Although large language models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by the untruthful context\nprovided by users or knowledge augmentation tools, thereby producing\nhallucinations. To alleviate the LLMs from being misled by untruthful\ninformation and take advantage of knowledge augmentation, we propose\nTruth-Aware Context Selection (TACS), a lightweight method to shield untruthful\ncontext from the inputs. TACS begins by performing truth detection on the input\ncontext, leveraging the parameterized knowledge within the LLM. Subsequently,\nit constructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results show that TACS can effectively\nfilter information in context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.\n","authors":["Tian Yu","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07556v2.pdf","comment":"Code is available at: https://github.com/ictnlp/TACS"},{"id":"http://arxiv.org/abs/2308.10792v5","updated":"2024-03-14T02:28:22Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v5.pdf","comment":"V2; Last update: March 12, 2024"},{"id":"http://arxiv.org/abs/2403.09040v1","updated":"2024-03-14T02:26:31Z","published":"2024-03-14T02:26:31Z","title":"RAGGED: Towards Informed Design of Retrieval Augmented Generation\n  Systems","summary":"  Retrieval-augmented generation (RAG) greatly benefits language models (LMs)\nby providing additional context for tasks such as document-based question\nanswering (DBQA). Despite its potential, the power of RAG is highly dependent\non its configuration, raising the question: What is the optimal RAG\nconfiguration? To answer this, we introduce the RAGGED framework to analyze and\noptimize RAG systems. On a set of representative DBQA tasks, we study two\nclassic sparse and dense retrievers, and four top-performing LMs in\nencoder-decoder and decoder-only architectures. Through RAGGED, we uncover that\ndifferent models suit substantially varied RAG setups. While encoder-decoder\nmodels monotonically improve with more documents, we find decoder-only models\ncan only effectively use < 5 documents, despite often having a longer context\nwindow. RAGGED offers further insights into LMs' context utilization habits,\nwhere we find that encoder-decoder models rely more on contexts and are thus\nmore sensitive to retrieval quality, while decoder-only models tend to rely on\nknowledge memorized during training.\n","authors":["Jennifer Hsia","Afreen Shaikh","Zhiruo Wang","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2403.09040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09037v1","updated":"2024-03-14T02:25:35Z","published":"2024-03-14T02:25:35Z","title":"The First to Know: How Token Distributions Reveal Hidden Knowledge in\n  Large Vision-Language Models?","summary":"  Large vision-language models (LVLMs), designed to interpret and respond to\nhuman instructions, occasionally generate hallucinated or harmful content due\nto inappropriate instructions. This study uses linear probing to shed light on\nthe hidden knowledge at the output layer of LVLMs. We demonstrate that the\nlogit distributions of the first tokens contain sufficient information to\ndetermine whether to respond to the instructions, including recognizing\nunanswerable visual questions, defending against multi-modal jailbreaking\nattack, and identifying deceptive questions. Such hidden knowledge is gradually\nlost in logits of subsequent tokens during response generation. Then, we\nillustrate a simple decoding strategy at the generation of the first token,\neffectively improving the generated content. In experiments, we find a few\ninteresting insights: First, the CLIP model already contains a strong signal\nfor solving these tasks, indicating potential bias in the existing datasets.\nSecond, we observe performance improvement by utilizing the first logit\ndistributions on three additional tasks, including indicting uncertainty in\nmath solving, mitigating hallucination, and image classification. Last, with\nthe same training data, simply finetuning LVLMs improve models' performance but\nis still inferior to linear probing on these tasks.\n","authors":["Qinyu Zhao","Ming Xu","Kartik Gupta","Akshay Asthana","Liang Zheng","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2403.09037v1.pdf","comment":"Under review. Project page:\n  https://github.com/Qinyu-Allen-Zhao/LVLM-LP"},{"id":"http://arxiv.org/abs/2308.07702v2","updated":"2024-03-14T02:07:11Z","published":"2023-08-15T11:08:30Z","title":"Better Zero-Shot Reasoning with Role-Play Prompting","summary":"  Modern large language models (LLMs) exhibit a remarkable capacity for\nrole-playing, enabling them to embody not only human characters but also\nnon-human entities. This versatility allows them to simulate complex human-like\ninteractions and behaviors within various contexts, as well as to emulate\nspecific objects or systems. While these capabilities have enhanced user\nengagement and introduced novel modes of interaction, the influence of\nrole-playing on LLMs' reasoning abilities remains underexplored. In this study,\nwe introduce a strategically designed role-play prompting methodology and\nassess its performance under the zero-shot setting across twelve diverse\nreasoning benchmarks. Our empirical results illustrate that role-play prompting\nconsistently surpasses the standard zero-shot approach across most datasets.\nNotably, in experiments conducted using ChatGPT, accuracy on AQuA rises from\n53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%.Upon further comparison\nwith the Zero-Shot-CoT technique, which prompts the model to \"think step by\nstep\", our study demonstrates that role-play prompting acts as a more effective\ntrigger for the CoT process. This highlights its potential to augment the\nreasoning capabilities of LLMs. We release our code at\nhttps://github.com/NKU-HLT/Role-Play-Prompting.\n","authors":["Aobo Kong","Shiwan Zhao","Hao Chen","Qicheng Li","Yong Qin","Ruiqi Sun","Xin Zhou","Enzhi Wang","Xiaohang Dong"],"pdf_url":"https://arxiv.org/pdf/2308.07702v2.pdf","comment":"NAACL 2024, Main Conference"},{"id":"http://arxiv.org/abs/2403.07708v2","updated":"2024-03-14T02:02:31Z","published":"2024-03-12T14:51:57Z","title":"Improving Reinforcement Learning from Human Feedback Using Contrastive\n  Rewards","summary":"  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm\nused to align large language models (LLMs) with human preferences. Yet existing\nRLHF heavily relies on accurate and informative reward models, which are\nvulnerable and sensitive to noise from various sources, e.g. human labeling\nerrors, making the pipeline fragile. In this work, we improve the effectiveness\nof the reward model by introducing a penalty term on the reward, named as\n\\textit{contrastive rewards}. %Contrastive rewards Our approach involves two\nsteps: (1) an offline sampling step to obtain responses to prompts that serve\nas baseline calculation and (2) a contrastive reward calculated using the\nbaseline responses and used in the Proximal Policy Optimization (PPO) step. We\nshow that contrastive rewards enable the LLM to penalize reward uncertainty,\nimprove robustness, encourage improvement over baselines, calibrate according\nto task difficulty, and reduce variance in PPO. We show empirically contrastive\nrewards can improve RLHF substantially, evaluated by both GPTs and humans, and\nour method consistently outperforms strong baselines.\n","authors":["Wei Shen","Xiaoying Zhang","Yuanshun Yao","Rui Zheng","Hongyi Guo","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07708v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09032v1","updated":"2024-03-14T01:51:35Z","published":"2024-03-14T01:51:35Z","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences","summary":"  Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires assessing intricate\ntextual LLMs' outputs. By relying on automated metrics and static analysis\ntools, existing benchmarks fail to assess nuances in user instructions and LLM\noutputs, highlighting the need for large-scale datasets and benchmarks for LLM\npreference alignment. In this paper, we introduce CodeUltraFeedback, a\npreference dataset of 10,000 complex instructions to tune and align LLMs to\ncoding preferences through AI feedback. We generate responses to the\ninstructions using a pool of 14 diverse LLMs, which we then annotate according\nto their alignment with five coding preferences using the LLM-as-a-Judge\napproach with GPT-3.5, producing both numerical and textual feedback. We also\npresent CODAL-Bench, a benchmark for assessing LLM alignment with these coding\npreferences. Our results show that CodeLlama-7B-Instruct, aligned through\nreinforcement learning from AI feedback (RLAIF) with direct preference\noptimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B\nLLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference\ntuning. Furthermore, we show our DPO-aligned CodeLlama model improves\nfunctional correctness on HumanEval+ compared to the unaligned base model.\nTherefore, our contributions bridge the gap in preference tuning of LLMs for\ncode and set the stage for further advancements in model alignment and RLAIF\nfor code intelligence. Our code and data are available at\nhttps://github.com/martin-wey/CodeUltraFeedback.\n","authors":["Martin Weyssow","Aton Kamanda","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2403.09032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09028v1","updated":"2024-03-14T01:40:23Z","published":"2024-03-14T01:40:23Z","title":"ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning","summary":"  Charts provide visual representations of data and are widely used for\nanalyzing information, addressing queries, and conveying insights to others.\nVarious chart-related downstream tasks have emerged recently, such as\nquestion-answering and summarization. A common strategy to solve these tasks is\nto fine-tune various models originally trained on vision tasks language.\nHowever, such task-specific models are not capable of solving a wide range of\nchart-related tasks, constraining their real-world applicability. To overcome\nthese challenges, we introduce ChartInstruct: a novel chart-specific\nvision-language Instruction-following dataset comprising 191K instructions\ngenerated with 71K charts. We then present two distinct systems for instruction\ntuning on such datasets: (1) an end-to-end model that connects a vision encoder\nfor chart understanding with a LLM; and (2) a pipeline model that employs a\ntwo-step approach to extract chart data tables and input them into the LLM. In\nexperiments on four downstream tasks, we first show the effectiveness of our\nmodel--achieving a new set of state-of-the-art results. Further evaluation\nshows that our instruction-tuning approach supports a wide array of real-world\nchart comprehension and reasoning scenarios, thereby expanding the scope and\napplicability of our models to new kinds of tasks.\n","authors":["Ahmed Masry","Mehrad Shahmohammadi","Md Rizwan Parvez","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2403.09028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01579v2","updated":"2024-03-14T01:39:58Z","published":"2023-05-02T16:28:10Z","title":"Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models\n  against Counterfactual Noise","summary":"  Most existing retrieval-augmented language models (LMs) assume a naive\ndichotomy within a retrieved document set: query-relevance and irrelevance. Our\nwork investigates a more challenging scenario in which even the \"relevant\"\ndocuments may contain misleading or incorrect information, causing conflict\namong the retrieved documents and thereby negatively influencing model\ndecisions as noise. We observe that existing LMs are highly brittle to the\npresence of conflicting information in both the fine-tuning and in-context\nfew-shot learning scenarios. We propose approaches for handling knowledge\nconflicts among retrieved documents by explicitly fine-tuning a discriminator\nor prompting GPT-3.5 to elicit its discriminative capability. Our empirical\nresults on open-domain QA show that these approaches significantly enhance\nmodel robustness. We also provide our findings on incorporating the fine-tuned\ndiscriminator's decision into the in-context learning process, proposing a way\nto exploit the benefits of two disparate learning schemes. Alongside our\nfindings, we provide MacNoise, a machine-generated, conflict-induced dataset to\nfurther encourage research in this direction.\n","authors":["Giwon Hong","Jeonghwan Kim","Junmo Kang","Sung-Hyon Myaeng","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2305.01579v2.pdf","comment":"NAACL 2024 (Findings; Long Paper)"},{"id":"http://arxiv.org/abs/2403.09024v1","updated":"2024-03-14T01:28:13Z","published":"2024-03-14T01:28:13Z","title":"Semiparametric Token-Sequence Co-Supervision","summary":"  In this work, we introduce a semiparametric token-sequence co-supervision\ntraining method. It trains a language model by simultaneously leveraging\nsupervision from the traditional next token prediction loss which is calculated\nover the parametric token embedding space and the next sequence prediction loss\nwhich is calculated over the nonparametric sequence embedding space. The\nnonparametric sequence embedding space is constructed by a separate language\nmodel tasked to condense an input text into a single representative embedding.\nOur experiments demonstrate that a model trained via both supervisions\nconsistently surpasses models trained via each supervision independently.\nAnalysis suggests that this co-supervision encourages a broader generalization\ncapability across the model. Especially, the robustness of parametric token\nspace which is established during the pretraining step tends to effectively\nenhance the stability of nonparametric sequence embedding space, a new space\nestablished by another language model.\n","authors":["Hyunji Lee","Doyoung Kim","Jihoon Jun","Sejune Joo","Joel Jang","Kyoung-Woon On","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2403.09024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09017v1","updated":"2024-03-14T00:45:24Z","published":"2024-03-14T00:45:24Z","title":"AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic","summary":"  The swift progress and widespread acceptance of artificial intelligence (AI)\nsystems highlight a pressing requirement to comprehend both the capabilities\nand potential risks associated with AI. Given the linguistic complexity,\ncultural richness, and underrepresented status of Arabic in AI research, there\nis a pressing need to focus on Large Language Models (LLMs) performance and\nsafety for Arabic related tasks. Despite some progress in their development,\nthere is a lack of comprehensive trustworthiness evaluation benchmarks which\npresents a major challenge in accurately assessing and improving the safety of\nLLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first\ncomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises\n516 human-written multiple-choice questions addressing diverse dimensions\nrelated to truthfulness, ethics, safety, physical health, mental health,\nunfairness, illegal activities, privacy, and offensive language. By introducing\nAraTrust, we aim to promote collaborative efforts to create safer and more\ntrustworthy LLMs for Arabic users. We evaluated a set of LLMs against our\nbenchmark to assess its trustworthiness. GPT-4 showed to be the most\ntrustworthy regarding Arabic language.\n","authors":["Emad A. Alghamdi","Reem I. Masoud","Deema Alnuhait","Afnan Y. Alomairi","Ahmed Ashraf","Mohamed Zaytoon"],"pdf_url":"https://arxiv.org/pdf/2403.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13897v2","updated":"2024-03-14T00:21:09Z","published":"2024-02-21T16:09:25Z","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and\n  Logical Reasoning","summary":"  Information retrieval is a rapidly evolving field. However it still faces\nsignificant limitations in the scientific and industrial vast amounts of\ninformation, such as semantic divergence and vocabulary gaps in sparse\nretrieval, low precision and lack of interpretability in semantic search, or\nhallucination and outdated information in generative models. In this paper, we\nintroduce a two-block approach to tackle these hurdles for long documents. The\nfirst block enhances language understanding in sparse retrieval by query\nexpansion to retrieve relevant documents. The second block deepens the result\nby providing comprehensive and informative answers to the complex question\nusing only the information spread in the long document, enabling bidirectional\nengagement. At various stages of the pipeline, intermediate results are\npresented to users to facilitate understanding of the system's reasoning. We\nbelieve this bidirectional approach brings significant advancements in terms of\ntransparency, logical thinking, and comprehensive understanding in the field of\nscientific information retrieval.\n","authors":["Loïc Rakotoson","Sylvain Massip","Fréjus A. A. Laleye"],"pdf_url":"https://arxiv.org/pdf/2402.13897v2.pdf","comment":"6 pages, 3 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.09639v1","updated":"2024-03-14T17:59:59Z","published":"2024-03-14T17:59:59Z","title":"GroupContrast: Semantic-aware Self-supervised Representation Learning\n  for 3D Understanding","summary":"  Self-supervised 3D representation learning aims to learn effective\nrepresentations from large-scale unlabeled point clouds. Most existing\napproaches adopt point discrimination as the pretext task, which assigns\nmatched points in two distinct views as positive pairs and unmatched points as\nnegative pairs. However, this approach often results in semantically identical\npoints having dissimilar representations, leading to a high number of false\nnegatives and introducing a \"semantic conflict\" problem. To address this issue,\nwe propose GroupContrast, a novel approach that combines segment grouping and\nsemantic-aware contrastive learning. Segment grouping partitions points into\nsemantically meaningful regions, which enhances semantic coherence and provides\nsemantic guidance for the subsequent contrastive representation learning.\nSemantic-aware contrastive learning augments the semantic information extracted\nfrom segment grouping and helps to alleviate the issue of \"semantic conflict\".\nWe conducted extensive experiments on multiple 3D scene understanding tasks.\nThe results demonstrate that GroupContrast learns semantically meaningful\nrepresentations and achieves promising transfer learning performance.\n","authors":["Chengyao Wang","Li Jiang","Xiaoyang Wu","Zhuotao Tian","Bohao Peng","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.09639v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09638v1","updated":"2024-03-14T17:59:55Z","published":"2024-03-14T17:59:55Z","title":"SCP-Diff: Photo-Realistic Semantic Image Synthesis with\n  Spatial-Categorical Joint Prior","summary":"  Semantic image synthesis (SIS) shows good promises for sensor simulation.\nHowever, current best practices in this field, based on GANs, have not yet\nreached the desired level of quality. As latent diffusion models make\nsignificant strides in image generation, we are prompted to evaluate\nControlNet, a notable method for its dense control capabilities. Our\ninvestigation uncovered two primary issues with its results: the presence of\nweird sub-structures within large semantic areas and the misalignment of\ncontent with the semantic mask. Through empirical study, we pinpointed the\ncause of these problems as a mismatch between the noised training data\ndistribution and the standard normal prior applied at the inference stage. To\naddress this challenge, we developed specific noise priors for SIS,\nencompassing spatial, categorical, and a novel spatial-categorical joint prior\nfor inference. This approach, which we have named SCP-Diff, has yielded\nexceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on\nADE20K.The code and models can be accessed via the project page.\n","authors":["Huan-ang Gao","Mingju Gao","Jiaju Li","Wenyi Li","Rong Zhi","Hao Tang","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.09638v1.pdf","comment":"Project Page: https://air-discover.github.io/SCP-Diff/"},{"id":"http://arxiv.org/abs/2403.09637v1","updated":"2024-03-14T17:59:46Z","published":"2024-03-14T17:59:46Z","title":"GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary\n  Robotic Grasping","summary":"  Constructing a 3D scene capable of accommodating open-ended language queries,\nis a pivotal pursuit, particularly within the domain of robotics. Such\ntechnology facilitates robots in executing object manipulations based on human\nlanguage directives. To tackle this challenge, some research efforts have been\ndedicated to the development of language-embedded implicit fields. However,\nimplicit fields (e.g. NeRF) encounter limitations due to the necessity of\nprocessing a large number of input views for reconstruction, coupled with their\ninherent inefficiencies in inference. Thus, we present the GaussianGrasper,\nwhich utilizes 3D Gaussian Splatting to explicitly represent the scene as a\ncollection of Gaussian primitives. Our approach takes a limited set of RGB-D\nviews and employs a tile-based splatting technique to create a feature field.\nIn particular, we propose an Efficient Feature Distillation (EFD) module that\nemploys contrastive learning to efficiently and accurately distill language\nembeddings derived from foundational models. With the reconstructed geometry of\nthe Gaussian field, our method enables the pre-trained grasping model to\ngenerate collision-free grasp pose candidates. Furthermore, we propose a\nnormal-guided grasp module to select the best grasp pose. Through comprehensive\nreal-world experiments, we demonstrate that GaussianGrasper enables robots to\naccurately query and grasp objects with language instructions, providing a new\nsolution for language-guided manipulation tasks. Data and codes can be\navailable at https://github.com/MrSecant/GaussianGrasper.\n","authors":["Yuhang Zheng","Xiangyu Chen","Yupeng Zheng","Songen Gu","Runyi Yang","Bu Jin","Pengfei Li","Chengliang Zhong","Zengmao Wang","Lina Liu","Chao Yang","Dawei Wang","Zhen Chen","Xiaoxiao Long","Meiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04315v3","updated":"2024-03-14T17:59:42Z","published":"2023-11-07T19:41:19Z","title":"A Data Perspective on Enhanced Identity Preservation for Diffusion\n  Personalization","summary":"  Large text-to-image models have revolutionized the ability to generate\nimagery using natural language. However, particularly unique or personal visual\nconcepts, such as pets and furniture, will not be captured by the original\nmodel. This has led to interest in how to personalize a text-to-image model.\nDespite significant progress, this task remains a formidable challenge,\nparticularly in preserving the subject's identity. Most researchers attempt to\naddress this issue by modifying model architectures. These methods are capable\nof keeping the subject structure and color but fail to preserve identity\ndetails. Towards this issue, our approach takes a data-centric perspective. We\nintroduce a novel regularization dataset generation strategy on both the text\nand image level. This strategy enables the model to preserve fine details of\nthe desired subjects, such as text and logos. Our method is\narchitecture-agnostic and can be flexibly applied on various text-to-image\nmodels. We show on established benchmarks that our data-centric approach forms\nthe new state of the art in terms of identity preservation and text alignment.\n","authors":["Xingzhe He","Zhiwen Cao","Nicholas Kolkin","Lantao Yu","Kun Wan","Helge Rhodin","Ratheesh Kalarot"],"pdf_url":"https://arxiv.org/pdf/2311.04315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09635v1","updated":"2024-03-14T17:59:14Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v1.pdf","comment":"Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.\n  Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable"},{"id":"http://arxiv.org/abs/2403.09634v1","updated":"2024-03-14T17:59:13Z","published":"2024-03-14T17:59:13Z","title":"OneTracker: Unifying Visual Object Tracking with Foundation Models and\n  Efficient Tuning","summary":"  Visual object tracking aims to localize the target object of each frame based\non its initial appearance in the first frame. Depending on the input modility,\ntracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and\nRGB+D) tracking. Despite the different input modalities, the core aspect of\ntracking is the temporal matching. Based on this common ground, we present a\ngeneral framework to unify various tracking tasks, termed as OneTracker.\nOneTracker first performs a large-scale pre-training on a RGB tracker called\nFoundation Tracker. This pretraining phase equips the Foundation Tracker with a\nstable ability to estimate the location of the target object. Then we regard\nother modality information as prompt and build Prompt Tracker upon Foundation\nTracker. Through freezing the Foundation Tracker and only adjusting some\nadditional trainable parameters, Prompt Tracker inhibits the strong\nlocalization ability from Foundation Tracker and achieves parameter-efficient\nfinetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of\nour general framework OneTracker, which is consisted of Foundation Tracker and\nPrompt Tracker, we conduct extensive experiments on 6 popular tracking tasks\nacross 11 benchmarks and our OneTracker outperforms other models and achieves\nstate-of-the-art performance.\n","authors":["Lingyi Hong","Shilin Yan","Renrui Zhang","Wanyun Li","Xinyu Zhou","Pinxue Guo","Kaixun Jiang","Yiting Chen","Jinglun Li","Zhaoyu Chen","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.09634v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09632v1","updated":"2024-03-14T17:58:56Z","published":"2024-03-14T17:58:56Z","title":"Holo-Relighting: Controllable Volumetric Portrait Relighting from a\n  Single Image","summary":"  At the core of portrait photography is the search for ideal lighting and\nviewpoint. The process often requires advanced knowledge in photography and an\nelaborate studio setup. In this work, we propose Holo-Relighting, a volumetric\nrelighting method that is capable of synthesizing novel viewpoints, and novel\nlighting from a single image. Holo-Relighting leverages the pretrained 3D GAN\n(EG3D) to reconstruct geometry and appearance from an input portrait as a set\nof 3D-aware features. We design a relighting module conditioned on a given\nlighting to process these features, and predict a relit 3D representation in\nthe form of a tri-plane, which can render to an arbitrary viewpoint through\nvolume rendering. Besides viewpoint and lighting control, Holo-Relighting also\ntakes the head pose as a condition to enable head-pose-dependent lighting\neffects. With these novel designs, Holo-Relighting can generate complex\nnon-Lambertian lighting effects (e.g., specular highlights and cast shadows)\nwithout using any explicit physical lighting priors. We train Holo-Relighting\nwith data captured with a light stage, and propose two data-rendering\ntechniques to improve the data quality for training the volumetric relighting\nsystem. Through quantitative and qualitative experiments, we demonstrate\nHolo-Relighting can achieve state-of-the-arts relighting quality with better\nphotorealism, 3D consistency and controllability.\n","authors":["Yiqun Mei","Yu Zeng","He Zhang","Zhixin Shu","Xuaner Zhang","Sai Bi","Jianming Zhang","HyunJoon Jung","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2403.09632v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.09631v1","updated":"2024-03-14T17:58:41Z","published":"2024-03-14T17:58:41Z","title":"3D-VLA: A 3D Vision-Language-Action Generative World Model","summary":"  Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.\n","authors":["Haoyu Zhen","Xiaowen Qiu","Peihao Chen","Jincheng Yang","Xin Yan","Yilun Du","Yining Hong","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09631v1.pdf","comment":"Project page: https://vis-www.cs.umass.edu/3dvla/"},{"id":"http://arxiv.org/abs/2403.09630v1","updated":"2024-03-14T17:58:33Z","published":"2024-03-14T17:58:33Z","title":"Generalized Predictive Model for Autonomous Driving","summary":"  In this paper, we introduce the first large-scale video prediction model in\nthe autonomous driving discipline. To eliminate the restriction of high-cost\ndata collection and empower the generalization ability of our model, we acquire\nmassive data from the web and pair it with diverse and high-quality text\ndescriptions. The resultant dataset accumulates over 2000 hours of driving\nvideos, spanning areas all over the world with diverse weather conditions and\ntraffic scenarios. Inheriting the merits from recent latent diffusion models,\nour model, dubbed GenAD, handles the challenging dynamics in driving scenes\nwith novel temporal reasoning blocks. We showcase that it can generalize to\nvarious unseen driving datasets in a zero-shot manner, surpassing general or\ndriving-specific video prediction counterparts. Furthermore, GenAD can be\nadapted into an action-conditioned prediction model or a motion planner,\nholding great potential for real-world driving applications.\n","authors":["Jiazhi Yang","Shenyuan Gao","Yihang Qiu","Li Chen","Tianyu Li","Bo Dai","Kashyap Chitta","Penghao Wu","Jia Zeng","Ping Luo","Jun Zhang","Andreas Geiger","Yu Qiao","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2403.09630v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2311.17061v2","updated":"2024-03-14T17:58:14Z","published":"2023-11-28T18:59:58Z","title":"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting","summary":"  Realistic 3D human generation from text prompts is a desirable yet\nchallenging task. Existing methods optimize 3D representations like mesh or\nneural fields via score distillation sampling (SDS), which suffers from\ninadequate fine details or excessive training time. In this paper, we propose\nan efficient yet effective framework, HumanGaussian, that generates\nhigh-quality 3D humans with fine-grained geometry and realistic appearance. Our\nkey insight is that 3D Gaussian Splatting is an efficient renderer with\nperiodic Gaussian shrinkage or growing, where such adaptive density control can\nbe naturally guided by intrinsic human structures. Specifically, 1) we first\npropose a Structure-Aware SDS that simultaneously optimizes human appearance\nand geometry. The multi-modal score function from both RGB and depth space is\nleveraged to distill the Gaussian densification and pruning process. 2)\nMoreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS\ninto a noisier generative score and a cleaner classifier score, which well\naddresses the over-saturation issue. The floating artifacts are further\neliminated based on Gaussian size in a prune-only phase to enhance generation\nsmoothness. Extensive experiments demonstrate the superior efficiency and\ncompetitive quality of our framework, rendering vivid 3D humans under diverse\nscenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian\n","authors":["Xian Liu","Xiaohang Zhan","Jiaxiang Tang","Ying Shan","Gang Zeng","Dahua Lin","Xihui Liu","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2311.17061v2.pdf","comment":"Accepted by CVPR 2024, camera-ready version. Project Page:\n  https://alvinliu0.github.io/projects/HumanGaussian"},{"id":"http://arxiv.org/abs/2403.09626v1","updated":"2024-03-14T17:57:07Z","published":"2024-03-14T17:57:07Z","title":"Video Mamba Suite: State Space Model as a Versatile Alternative for\n  Video Understanding","summary":"  Understanding videos is one of the fundamental directions in computer vision\nresearch, with extensive efforts dedicated to exploring various architectures\nsuch as RNN, 3D CNN, and Transformers. The newly proposed architecture of state\nspace model, e.g., Mamba, shows promising traits to extend its success in long\nsequence modeling to video modeling. To assess whether Mamba can be a viable\nalternative to Transformers in the video understanding domain, in this work, we\nconduct a comprehensive set of studies, probing different roles Mamba can play\nin modeling videos, while investigating diverse tasks where Mamba could exhibit\nsuperiority. We categorize Mamba into four roles for modeling videos, deriving\na Video Mamba Suite composed of 14 models/modules, and evaluating them on 12\nvideo understanding tasks. Our extensive experiments reveal the strong\npotential of Mamba on both video-only and video-language tasks while showing\npromising efficiency-performance trade-offs. We hope this work could provide\nvaluable data points and insights for future research on video understanding.\nCode is public: https://github.com/OpenGVLab/video-mamba-suite.\n","authors":["Guo Chen","Yifei Huang","Jilan Xu","Baoqi Pei","Zhe Chen","Zhiqi Li","Jiahao Wang","Kunchang Li","Tong Lu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09626v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2403.09625v1","updated":"2024-03-14T17:57:04Z","published":"2024-03-14T17:57:04Z","title":"Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation","summary":"  Recent years have witnessed the strong power of 3D generation models, which\noffer a new level of creative flexibility by allowing users to guide the 3D\ncontent generation process through a single image or natural language. However,\nit remains challenging for existing 3D generation methods to create\nsubject-driven 3D content across diverse prompts. In this paper, we introduce a\nnovel 3D customization method, dubbed Make-Your-3D that can personalize\nhigh-fidelity and consistent 3D content from only a single image of a subject\nwith text description within 5 minutes. Our key insight is to harmonize the\ndistributions of a multi-view diffusion model and an identity-specific 2D\ngenerative model, aligning them with the distribution of the desired 3D\nsubject. Specifically, we design a co-evolution framework to reduce the\nvariance of distributions, where each model undergoes a process of learning\nfrom the other through identity-aware optimization and subject-prior\noptimization, respectively. Extensive experiments demonstrate that our method\ncan produce high-quality, consistent, and subject-specific 3D content with\ntext-driven modifications that are unseen in subject image.\n","authors":["Fangfu Liu","Hanyang Wang","Weiliang Chen","Haowen Sun","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2403.09625v1.pdf","comment":"Project page: https://liuff19.github.io/Make-Your-3D"},{"id":"http://arxiv.org/abs/2403.09623v1","updated":"2024-03-14T17:56:14Z","published":"2024-03-14T17:56:14Z","title":"Score-Guided Diffusion for 3D Human Recovery","summary":"  We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for\nsolving inverse problems for 3D human pose and shape reconstruction. These\ninverse problems involve fitting a human body model to image observations,\ntraditionally solved through optimization techniques. ScoreHMR mimics model\nfitting approaches, but alignment with the image observation is achieved\nthrough score guidance in the latent space of a diffusion model. The diffusion\nmodel is trained to capture the conditional distribution of the human model\nparameters given an input image. By guiding its denoising process with a\ntask-specific score, ScoreHMR effectively solves inverse problems for various\napplications without the need for retraining the task-agnostic diffusion model.\nWe evaluate our approach on three settings/applications. These are: (i)\nsingle-frame model fitting; (ii) reconstruction from multiple uncalibrated\nviews; (iii) reconstructing humans in video sequences. ScoreHMR consistently\noutperforms all optimization baselines on popular benchmarks across all\nsettings. We make our code and models available at the\nhttps://statho.github.io/ScoreHMR.\n","authors":["Anastasis Stathopoulos","Ligong Han","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2403.09623v1.pdf","comment":"CVPR 2024 (project page: https://statho.github.io/ScoreHMR)"},{"id":"http://arxiv.org/abs/2403.09622v1","updated":"2024-03-14T17:55:33Z","published":"2024-03-14T17:55:33Z","title":"Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering","summary":"  Visual text rendering poses a fundamental challenge for contemporary\ntext-to-image generation models, with the core problem lying in text encoder\ndeficiencies. To achieve accurate text rendering, we identify two crucial\nrequirements for text encoders: character awareness and alignment with glyphs.\nOur solution involves crafting a series of customized text encoder, Glyph-ByT5,\nby fine-tuning the character-aware ByT5 encoder using a meticulously curated\npaired glyph-text dataset. We present an effective method for integrating\nGlyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for\ndesign image generation. This significantly enhances text rendering accuracy,\nimproving it from less than $20\\%$ to nearly $90\\%$ on our design image\nbenchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph\nrendering, achieving high spelling accuracy for tens to hundreds of characters\nwith automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with\na small set of high-quality, photorealistic images featuring visual text, we\nshowcase a substantial improvement in scene text rendering capabilities in\nopen-domain real images. These compelling outcomes aim to encourage further\nexploration in designing customized text encoders for diverse and challenging\ntasks.\n","authors":["Zeyu Liu","Weicong Liang","Zhanhao Liang","Chong Luo","Ji Li","Gao Huang","Yuhui Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.09622v1.pdf","comment":"technical report, 18 pages, 19 figures"},{"id":"http://arxiv.org/abs/2403.09620v1","updated":"2024-03-14T17:55:03Z","published":"2024-03-14T17:55:03Z","title":"PosSAM: Panoptic Open-vocabulary Segment Anything","summary":"  In this paper, we introduce an open-vocabulary panoptic segmentation model\nthat effectively unifies the strengths of the Segment Anything Model (SAM) with\nthe vision-language CLIP model in an end-to-end framework. While SAM excels in\ngenerating spatially-aware masks, it's decoder falls short in recognizing\nobject class information and tends to oversegment without additional guidance.\nExisting approaches address this limitation by using multi-stage techniques and\nemploying separate models to generate class-aware prompts, such as bounding\nboxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model\nwhich leverages SAM's spatially rich features to produce instance-aware masks\nand harnesses CLIP's semantically discriminative features for effective\ninstance classification. Specifically, we address the limitations of SAM and\npropose a novel Local Discriminative Pooling (LDP) module leveraging\nclass-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary\nclassification. Furthermore, we introduce a Mask-Aware Selective Ensembling\n(MASE) algorithm that adaptively enhances the quality of generated masks and\nboosts the performance of open-vocabulary classification during inference for\neach image. We conducted extensive experiments to demonstrate our methods\nstrong generalization properties across multiple datasets, achieving\nstate-of-the-art performance with substantial improvements over SOTA\nopen-vocabulary panoptic segmentation methods. In both COCO to ADE20K and\nADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art\nmethods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website:\nhttps://vibashan.github.io/possam-web/.\n","authors":["Vibashan VS","Shubhankar Borse","Hyojin Park","Debasmit Das","Vishal Patel","Munawar Hayat","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2403.09620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09616v1","updated":"2024-03-14T17:52:31Z","published":"2024-03-14T17:52:31Z","title":"Explore In-Context Segmentation via Latent Diffusion Models","summary":"  In-context segmentation has drawn more attention with the introduction of\nvision foundation models. Most existing approaches adopt metric learning or\nmasked image modeling to build the correlation between visual prompts and input\nimage queries. In this work, we explore this problem from a new perspective,\nusing one representative generation model, the latent diffusion model (LDM). We\nobserve a task gap between generation and segmentation in diffusion models, but\nLDM is still an effective minimalist for in-context segmentation. In\nparticular, we propose two meta-architectures and correspondingly design\nseveral output alignment and optimization strategies. We have conducted\ncomprehensive ablation studies and empirically found that the segmentation\nquality counts on output alignment and in-context instructions. Moreover, we\nbuild a new and fair in-context segmentation benchmark that includes both image\nand video datasets. Experiments validate the efficiency of our approach,\ndemonstrating comparable or even stronger results than previous specialist\nmodels or visual foundation models. Our study shows that LDMs can also achieve\ngood enough results for challenging in-context segmentation tasks.\n","authors":["Chaoyang Wang","Xiangtai Li","Henghui Ding","Lu Qi","Jiangning Zhang","Yunhai Tong","Chen Change Loy","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2403.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09611v1","updated":"2024-03-14T17:51:32Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, consisting of\nboth dense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09605v1","updated":"2024-03-14T17:47:01Z","published":"2024-03-14T17:47:01Z","title":"Counterfactual contrastive learning: robust representations via causal\n  image synthesis","summary":"  Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training.\n","authors":["Melanie Roschewitz","Fabio De Sousa Ribeiro","Tian Xia","Galvin Khara","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2403.09605v1.pdf","comment":"Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive"},{"id":"http://arxiv.org/abs/2403.09593v1","updated":"2024-03-14T17:35:32Z","published":"2024-03-14T17:35:32Z","title":"Renovating Names in Open-Vocabulary Segmentation Benchmarks","summary":"  Names are essential to both human cognition and vision-language models.\nOpen-vocabulary models utilize class names as text prompts to generalize to\ncategories unseen during training. However, name qualities are often overlooked\nand lack sufficient precision in existing datasets. In this paper, we address\nthis underexplored problem by presenting a framework for \"renovating\" names in\nopen-vocabulary segmentation benchmarks (RENOVATE). Through human study, we\ndemonstrate that the names generated by our model are more precise descriptions\nof the visual segments and hence enhance the quality of existing datasets by\nmeans of simple renaming. We further demonstrate that using our renovated names\nenables training of stronger open-vocabulary segmentation models. Using\nopen-vocabulary segmentation for name quality evaluation, we show that our\nrenovated names lead to up to 16% relative improvement from the original names\non various benchmarks across various state-of-the-art models. We provide our\ncode and relabelings for several popular segmentation datasets (ADE20K,\nCityscapes, PASCAL Context) to the research community.\n","authors":["Haiwen Huang","Songyou Peng","Dan Zhang","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.09593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09577v1","updated":"2024-03-14T17:11:49Z","published":"2024-03-14T17:11:49Z","title":"The NeRFect Match: Exploring NeRF Features for Visual Localization","summary":"  In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene\nrepresentation for visual localization. Recently, NeRF has been employed to\nenhance pose regression and scene coordinate regression models by augmenting\nthe training database, providing auxiliary supervision through rendered images,\nor serving as an iterative refinement module. We extend its recognized\nadvantages -- its ability to provide a compact scene representation with\nrealistic appearances and accurate geometry -- by exploring the potential of\nNeRF's internal features in establishing precise 2D-3D matches for\nlocalization. To this end, we conduct a comprehensive examination of NeRF's\nimplicit knowledge, acquired through view synthesis, for matching under various\nconditions. This includes exploring different matching network architectures,\nextracting encoder features at multiple layers, and varying training\nconfigurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D\nmatching function that capitalizes on the internal knowledge of NeRF learned\nvia view synthesis. Our evaluation of NeRFMatch on standard localization\nbenchmarks, within a structure-based pipeline, sets a new state-of-the-art for\nlocalization performance on Cambridge Landmarks.\n","authors":["Qunjie Zhou","Maxim Maximov","Or Litany","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2403.09577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01412v4","updated":"2024-03-14T17:05:43Z","published":"2023-10-02T17:59:52Z","title":"DriveGPT4: Interpretable End-to-end Autonomous Driving via Large\n  Language Model","summary":"  Multimodal large language models (MLLMs) have emerged as a prominent area of\ninterest within the research community, given their proficiency in handling and\nreasoning with non-textual data, including images and videos. This study seeks\nto extend the application of MLLMs to the realm of autonomous driving by\nintroducing DriveGPT4, a novel interpretable end-to-end autonomous driving\nsystem based on LLMs. Capable of processing multi-frame video inputs and\ntextual queries, DriveGPT4 facilitates the interpretation of vehicle actions,\noffers pertinent reasoning, and effectively addresses a diverse range of\nquestions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle\ncontrol signals in an end-to-end fashion. These advanced capabilities are\nachieved through the utilization of a bespoke visual instruction tuning\ndataset, specifically tailored for autonomous driving applications, in\nconjunction with a mix-finetuning training strategy. DriveGPT4 represents the\npioneering effort to leverage LLMs for the development of an interpretable\nend-to-end autonomous driving solution. Evaluations conducted on the BDD-X\ndataset showcase the superior qualitative and quantitative performance of\nDriveGPT4. Additionally, the fine-tuning of domain-specific data enables\nDriveGPT4 to yield close or even improved results in terms of autonomous\ndriving grounding when contrasted with GPT4-V. The code and dataset will be\npublicly available.\n","authors":["Zhenhua Xu","Yujia Zhang","Enze Xie","Zhen Zhao","Yong Guo","Kwan-Yee. K. Wong","Zhenguo Li","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.01412v4.pdf","comment":"The project page is available at\n  https://tonyxuqaq.github.io/projects/DriveGPT4/"},{"id":"http://arxiv.org/abs/2403.09572v1","updated":"2024-03-14T17:03:04Z","published":"2024-03-14T17:03:04Z","title":"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text\n  Transformation","summary":"  Multimodal large language models (MLLMs) have shown impressive reasoning\nabilities, which, however, are also more vulnerable to jailbreak attacks than\ntheir LLM predecessors. Although still capable of detecting unsafe responses,\nwe observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be\neasily bypassed due to the introduction of image features. To construct robust\nMLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free\nprotecting approach that exploits the inherent safety awareness of MLLMs, and\ngenerates safer responses via adaptively transforming unsafe images into texts\nto activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.\nExperiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO\nenhances model safety significantly (e.g., a 37.6% improvement on the\nMM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while\nconsistently maintaining utility results on common MLLM benchmarks.\nFurthermore, we show that ECSO can be used as a data engine to generate\nsupervised-finetuning (SFT) data for MLLM alignment without extra human\nintervention.\n","authors":["Yunhao Gou","Kai Chen","Zhili Liu","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.09572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09559v1","updated":"2024-03-14T16:47:25Z","published":"2024-03-14T16:47:25Z","title":"Less is More: Data Value Estimation for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16594v2","updated":"2024-03-14T16:45:04Z","published":"2024-02-26T14:18:12Z","title":"CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition","summary":"  To achieve greater accuracy, hypergraph matching algorithms require\nexponential increases in computational resources. Recent kd-tree-based\napproximate nearest neighbor (ANN) methods, despite the sparsity of their\ncompatibility tensor, still require exhaustive calculations for large-scale\ngraph matching. This work utilizes CUR tensor decomposition and introduces a\nnovel cascaded second and third-order hypergraph matching framework (CURSOR)\nfor efficient hypergraph matching. A CUR-based second-order graph matching\nalgorithm is used to provide a rough match, and then the core of CURSOR, a\nfiber-CUR-based tensor generation method, directly calculates entries of the\ncompatibility tensor by leveraging the initial second-order match result. This\nsignificantly decreases the time complexity and tensor density. A probability\nrelaxation labeling (PRL)-based matching algorithm, specifically suitable for\nsparse tensors, is developed. Experiment results on large-scale synthetic\ndatasets and widely-adopted benchmark sets demonstrate the superiority of\nCURSOR over existing methods. The tensor generation method in CURSOR can be\nintegrated seamlessly into existing hypergraph matching methods to improve\ntheir performance and lower their computational costs.\n","authors":["Qixuan Zheng","Ming Zhang","Hong Yan"],"pdf_url":"https://arxiv.org/pdf/2402.16594v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09554v1","updated":"2024-03-14T16:41:26Z","published":"2024-03-14T16:41:26Z","title":"Cloud gap-filling with deep learning for improved grassland monitoring","summary":"  Uninterrupted optical image time series are crucial for the timely monitoring\nof agricultural land changes. However, the continuity of such time series is\noften disrupted by clouds. In response to this challenge, we propose a deep\nlearning method that integrates cloud-free optical (Sentinel-2) observations\nand weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using\na combined Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN)\narchitecture to generate continuous Normalized Difference Vegetation Index\n(NDVI) time series. We emphasize the significance of observation continuity by\nassessing the impact of the generated time series on the detection of grassland\nmowing events. We focus on Lithuania, a country characterized by extensive\ncloud coverage, and compare our approach with alternative interpolation\ntechniques (i.e., linear, Akima, quadratic). Our method surpasses these\ntechniques, with an average MAE of 0.024 and R^2 of 0.92. It not only improves\nthe accuracy of event detection tasks by employing a continuous time series,\nbut also effectively filters out sudden shifts and noise originating from\ncloudy observations that cloud masks often fail to detect.\n","authors":["Iason Tsardanidis","Alkiviadis Koukos","Vasileios Sitokonstantinou","Thanassis Drivas","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2403.09554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09551v1","updated":"2024-03-14T16:39:11Z","published":"2024-03-14T16:39:11Z","title":"WeakSurg: Weakly supervised surgical instrument segmentation using\n  temporal equivariance and semantic continuity","summary":"  Weakly supervised surgical instrument segmentation with only instrument\npresence labels has been rarely explored in surgical domain. To mitigate the\nhighly under-constrained challenges, we extend a two-stage weakly supervised\nsegmentation paradigm with temporal attributes from two perspectives. From a\ntemporal equivariance perspective, we propose a prototype-based temporal\nequivariance regulation loss to enhance pixel-wise consistency between adjacent\nfeatures. From a semantic continuity perspective, we propose a class-aware\ntemporal semantic continuity loss to constrain the semantic consistency between\na global view of target frame and local non-discriminative regions of adjacent\nreference frame. To the best of our knowledge, WeakSurg is the first\ninstrument-presence-only weakly supervised segmentation architecture to take\ntemporal information into account for surgical scenarios. Extensive experiments\nare validated on Cholec80, an open benchmark for phase and instrument\nrecognition. We annotate instance-wise instrument labels with fixed time-steps\nwhich are double checked by a clinician with 3-years experience. Our results\nshow that WeakSurg compares favorably with state-of-the-art methods not only on\nsemantic segmentation metrics but also on instance segmentation metrics.\n","authors":["Qiyuan Wang","Yanzhe Liu","Shang Zhao","Rong Liu","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.09551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10424v8","updated":"2024-03-14T16:38:36Z","published":"2023-05-17T17:56:59Z","title":"ZeroFlow: Scalable Scene Flow via Distillation","summary":"  Scene flow estimation is the task of describing the 3D motion field between\ntemporally successive point clouds. State-of-the-art methods use strong priors\nand test-time optimization techniques, but require on the order of tens of\nseconds to process full-size point clouds, making them unusable as computer\nvision primitives for real-time applications such as open world object\ndetection. Feedforward methods are considerably faster, running on the order of\ntens to hundreds of milliseconds for full-size point clouds, but require\nexpensive human supervision. To address both limitations, we propose Scene Flow\nvia Distillation, a simple, scalable distillation framework that uses a\nlabel-free optimization method to produce pseudo-labels to supervise a\nfeedforward model. Our instantiation of this framework, ZeroFlow, achieves\nstate-of-the-art performance on the Argoverse 2 Self-Supervised Scene Flow\nChallenge while using zero human labels by simply training on large-scale,\ndiverse unlabeled data. At test-time, ZeroFlow is over 1000x faster than\nlabel-free state-of-the-art optimization-based methods on full-size point\nclouds (34 FPS vs 0.028 FPS) and over 1000x cheaper to train on unlabeled data\ncompared to the cost of human annotation (\\$394 vs ~\\$750,000). To facilitate\nfurther research, we release our code, trained model weights, and high quality\npseudo-labels for the Argoverse 2 and Waymo Open datasets at\nhttps://vedder.io/zeroflow.html\n","authors":["Kyle Vedder","Neehar Peri","Nathaniel Chodosh","Ishan Khatri","Eric Eaton","Dinesh Jayaraman","Yang Liu","Deva Ramanan","James Hays"],"pdf_url":"https://arxiv.org/pdf/2305.10424v8.pdf","comment":"Accepted to ICLR 2024. 9 pages, 4 pages of citations, 6 pages of\n  Supplemental. Project page with data releases is at\n  http://vedder.io/zeroflow.html"},{"id":"http://arxiv.org/abs/2403.06726v2","updated":"2024-03-14T16:35:41Z","published":"2024-03-11T13:44:49Z","title":"Probabilistic Contrastive Learning for Long-Tailed Visual Recognition","summary":"  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n","authors":["Chaoqun Du","Yulin Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.06726v2.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)"},{"id":"http://arxiv.org/abs/2403.09543v1","updated":"2024-03-14T16:30:52Z","published":"2024-03-14T16:30:52Z","title":"Explorations in Texture Learning","summary":"  In this work, we investigate \\textit{texture learning}: the identification of\ntextures learned by object classification models, and the extent to which they\nrely on these textures. We build texture-object associations that uncover new\ninsights about the relationships between texture and object classes in CNNs and\nfind three classes of results: associations that are strong and expected,\nstrong and not expected, and expected but not present. Our analysis\ndemonstrates that investigations in texture learning enable new methods for\ninterpretability and have the potential to uncover unexpected biases.\n","authors":["Blaine Hoak","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2403.09543v1.pdf","comment":"Accepted to ICLR 2024, Tiny Papers Track"},{"id":"http://arxiv.org/abs/2403.01977v2","updated":"2024-03-14T16:30:48Z","published":"2024-03-04T12:20:29Z","title":"TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation\n  under Visual Corruptions","summary":"  Robot navigation under visual corruption presents a formidable challenge. To\naddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\nfor point-goal navigation under visual corruptions. Our \"plug-and-play\" method\nincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\npre-trained navigation model gets a corrupted image and extracts features.\nSecondly, the top-down decoder produces the reconstruction given the high-level\nfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\nof a corrupted image back to the pre-trained model. Finally, the pre-trained\nmodel does forward pass again to output action. Despite being trained solely on\nclean images, the top-down decoder can reconstruct cleaner images from\ncorrupted ones without the need for gradient-based adaptation. The pre-trained\nnavigation model with our top-down decoder significantly enhances navigation\nperformance across almost all visual corruptions in our benchmarks. Our method\nimproves the success rate of point-goal navigation from the state-of-the-art\nresult of 46% to 94% on the most severe corruption. This suggests its potential\nfor broader application in robotic visual navigation. Project page:\nhttps://sites.google.com/view/tta-nav\n","authors":["Maytus Piriyajitakonkij","Mingfei Sun","Mengmi Zhang","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.01977v2.pdf","comment":"Submitted to IROS2024"},{"id":"http://arxiv.org/abs/2402.05892v3","updated":"2024-03-14T16:16:58Z","published":"2024-02-08T18:30:50Z","title":"Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data","summary":"  In recent years, Transformers have become the de-facto architecture for\nsequence modeling on text and a variety of multi-dimensional data, such as\nimages and video. However, the use of self-attention layers in a Transformer\nincurs prohibitive compute and memory complexity that scales quadratically\nw.r.t. the sequence length. A recent architecture, Mamba, based on state space\nmodels has been shown to achieve comparable performance for modeling text\nsequences, while scaling linearly with the sequence length. In this work, we\npresent Mamba-ND, a generalized design extending the Mamba architecture to\narbitrary multi-dimensional data. Our design alternatively unravels the input\ndata across different dimensions following row-major orderings. We provide a\nsystematic comparison of Mamba-ND with several other alternatives, based on\nprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.\nEmpirically, we show that Mamba-ND demonstrates performance competitive with\nthe state-of-the-art on a variety of multi-dimensional benchmarks, including\nImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather\nforecasting.\n","authors":["Shufan Li","Harkanwar Singh","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2402.05892v3.pdf","comment":"22 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.09530v1","updated":"2024-03-14T16:13:00Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v1.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2312.05525v2","updated":"2024-03-14T15:59:27Z","published":"2023-12-09T10:36:43Z","title":"You Only Learn One Query: Learning Unified Human Query for Single-Stage\n  Multi-Person Multi-Task Human-Centric Perception","summary":"  Human-centric perception (e.g. pedetrian detection, segmentation, pose\nestimation, and attribute analysis) is a long-standing problem for computer\nvision. This paper introduces a unified and versatile framework (HQNet) for\nsingle-stage multi-person multi-task human-centric perception (HCP). Our\napproach centers on learning a unified human query representation, denoted as\nHuman Query, which captures intricate instance-level features for individual\npersons and disentangles complex multi-person scenarios. Although different HCP\ntasks have been well-studied individually, single-stage multi-task learning of\nHCP tasks has not been fully exploited in the literature due to the absence of\na comprehensive benchmark dataset. To address this gap, we propose\nCOCO-UniHuman benchmark dataset to enable model development and comprehensive\nevaluation. Experimental results demonstrate the proposed method's\nstate-of-the-art performance among multi-task HCP models and its competitive\nperformance compared to task-specific HCP models. Moreover, our experiments\nunderscore Human Query's adaptability to new HCP tasks, thus demonstrating its\nrobust generalization capability. Codes and data will be publicly accessible.\n","authors":["Sheng Jin","Shuhuai Li","Tong Li","Wentao Liu","Chen Qian","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2312.05525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09508v1","updated":"2024-03-14T15:55:53Z","published":"2024-03-14T15:55:53Z","title":"SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition","summary":"  Skeleton-based action recognition, which classifies human actions based on\nthe coordinates of joints and their connectivity within skeleton data, is\nwidely utilized in various scenarios. While Graph Convolutional Networks (GCNs)\nhave been proposed for skeleton data represented as graphs, they suffer from\nlimited receptive fields constrained by joint connectivity. To address this\nlimitation, recent advancements have introduced transformer-based methods.\nHowever, capturing correlations between all joints in all frames requires\nsubstantial memory resources. To alleviate this, we propose a novel approach\ncalled Skeletal-Temporal Transformer (SkateFormer) that partitions joints and\nframes based on different types of skeletal-temporal relation (Skate-Type) and\nperforms skeletal-temporal self-attention (Skate-MSA) within each partition. We\ncategorize the key skeletal-temporal relations for action recognition into a\ntotal of four distinct types. These types combine (i) two skeletal relation\ntypes based on physically neighboring and distant joints, and (ii) two temporal\nrelation types based on neighboring and distant frames. Through this\npartition-specific attention strategy, our SkateFormer can selectively focus on\nkey joints and frames crucial for action recognition in an action-adaptive\nmanner with efficient computation. Extensive experiments on various benchmark\ndatasets validate that our SkateFormer outperforms recent state-of-the-art\nmethods.\n","authors":["Jeonghyeok Do","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2403.09508v1.pdf","comment":"Please visit our project page at\n  https://jeonghyeokdo.github.io/SkateFormer_site/"},{"id":"http://arxiv.org/abs/2403.09506v1","updated":"2024-03-14T15:53:04Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: A Motion Coherent Augmentation for Video\n  Recognition","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video recognition and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video recognition, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v1.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2312.10032v3","updated":"2024-03-14T15:50:17Z","published":"2023-12-15T18:58:11Z","title":"Osprey: Pixel Understanding with Visual Instruction Tuning","summary":"  Multimodal large language models (MLLMs) have recently achieved impressive\ngeneral-purpose vision-language capabilities through visual instruction tuning.\nHowever, current MLLMs primarily focus on image-level or box-level\nunderstanding, falling short in achieving fine-grained vision-language\nalignment at pixel level. Besides, the lack of mask-based instruction data\nlimits their advancements. In this paper, we propose Osprey, a mask-text\ninstruction tuning approach, to extend MLLMs by incorporating fine-grained mask\nregions into language instruction, aiming at achieving pixel-wise visual\nunderstanding. To achieve this goal, we first meticulously curate a mask-based\nregion-text dataset with 724K samples, and then design a vision-language model\nby injecting pixel-level representation into LLM. Specifically, Osprey adopts a\nconvolutional CLIP backbone as the vision encoder and employs a mask-aware\nvisual extractor to extract precise visual mask features from high resolution\ninput. Experimental results demonstrate Osprey's superiority in various region\nunderstanding tasks, showcasing its new capability for pixel-level instruction\ntuning. In particular, Osprey can be integrated with Segment Anything Model\n(SAM) seamlessly to obtain multi-granularity semantics. The source code,\ndataset and demo can be found at https://github.com/CircleRadon/Osprey.\n","authors":["Yuqian Yuan","Wentong Li","Jian Liu","Dongqi Tang","Xinjie Luo","Chi Qin","Lei Zhang","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.10032v3.pdf","comment":"CVPR2024, Code and Demo link:https://github.com/CircleRadon/Osprey"},{"id":"http://arxiv.org/abs/2403.05327v2","updated":"2024-03-14T15:48:43Z","published":"2024-03-08T14:06:15Z","title":"DiffSF: Diffusion Models for Scene Flow Estimation","summary":"  Scene flow estimation is an essential ingredient for a variety of real-world\napplications, especially for autonomous agents, such as self-driving cars and\nrobots. While recent scene flow estimation approaches achieve a reasonable\naccuracy, their applicability to real-world systems additionally benefits from\na reliability measure. Aiming at improving accuracy while additionally\nproviding an estimate for uncertainty, we propose DiffSF that combines\ntransformer-based scene flow estimation with denoising diffusion models. In the\ndiffusion process, the ground truth scene flow vector field is gradually\nperturbed by adding Gaussian noise. In the reverse process, starting from\nrandomly sampled Gaussian noise, the scene flow vector field prediction is\nrecovered by conditioning on a source and a target point cloud. We show that\nthe diffusion process greatly increases the robustness of predictions compared\nto prior approaches resulting in state-of-the-art performance on standard scene\nflow estimation benchmarks. Moreover, by sampling multiple times with different\ninitial states, the denoising process predicts multiple hypotheses, which\nenables measuring the output uncertainty, allowing our approach to detect a\nmajority of the inaccurate predictions. The code is available at\nhttps://github.com/ZhangYushan3/DiffSF.\n","authors":["Yushan Zhang","Bastian Wandt","Maria Magnusson","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2403.05327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09500v1","updated":"2024-03-14T15:42:31Z","published":"2024-03-14T15:42:31Z","title":"Faceptor: A Generalist Model for Face Perception","summary":"  With the comprehensive research conducted on various face analysis tasks,\nthere is a growing interest among researchers to develop a unified approach to\nface perception. Existing methods mainly discuss unified representation and\ntraining, which lack task extensibility and application efficiency. To tackle\nthis issue, we focus on the unified model structure, exploring a face\ngeneralist model. As an intuitive design, Naive Faceptor enables tasks with the\nsame output shape and granularity to share the structural design of the\nstandardized output head, achieving improved task extensibility. Furthermore,\nFaceptor is proposed to adopt a well-designed single-encoder dual-decoder\narchitecture, allowing task-specific queries to represent new-coming semantics.\nThis design enhances the unification of model structure while improving\napplication efficiency in terms of storage overhead. Additionally, we introduce\nLayer-Attention into Faceptor, enabling the model to adaptively select features\nfrom optimal layers to perform the desired tasks. Through joint training on 13\nface perception datasets, Faceptor achieves exceptional performance in facial\nlandmark localization, face parsing, age estimation, expression recognition,\nbinary attribute classification, and face recognition, achieving or surpassing\nspecialized methods in most tasks. Our training framework can also be applied\nto auxiliary supervised learning, significantly improving performance in\ndata-sparse tasks such as age estimation and expression recognition. The code\nand models will be made publicly available at\nhttps://github.com/lxq1000/Faceptor.\n","authors":["Lixiong Qin","Mei Wang","Xuannan Liu","Yuhang Zhang","Wei Deng","Xiaoshuai Song","Weiran Xu","Weihong Deng"],"pdf_url":"https://arxiv.org/pdf/2403.09500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05930v2","updated":"2024-03-14T15:39:55Z","published":"2023-12-10T16:33:41Z","title":"A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary\n  Analysis","summary":"  Nailfold capillaroscopy is widely used in assessing health conditions,\nhighlighting the pressing need for an automated nailfold capillary analysis\nsystem. In this study, we present a pioneering effort in constructing a\ncomprehensive nailfold capillary dataset-321 images, 219 videos from 68\nsubjects, with clinic reports and expert annotations-that serves as a crucial\nresource for training deep-learning models. Leveraging this dataset, we\nfinetuned three deep learning models with expert annotations as supervised\nlabels and integrated them into a novel end-to-end nailfold capillary analysis\npipeline. This pipeline excels in automatically detecting and measuring a wide\nrange of size factors, morphological features, and dynamic aspects of nailfold\ncapillaries. We compared our outcomes with clinical reports. Experiment results\nshowed that our automated pipeline achieves an average of sub-pixel level\nprecision in measurements and 89.9% accuracy in identifying morphological\nabnormalities. These results underscore its potential for advancing\nquantitative medical research and enabling pervasive computing in healthcare.\nOur data and code are available at\nhttps://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.\n","authors":["Linxi Zhao","Jiankai Tang","Dongyu Chen","Xiaohong Liu","Yong Zhou","Yuanchun Shi","Guangyu Wang","Yuntao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.05930v2.pdf","comment":"Dataset, code, pretrained models:\n  https://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary"},{"id":"http://arxiv.org/abs/2403.01818v3","updated":"2024-03-14T15:39:51Z","published":"2024-03-04T08:06:41Z","title":"AllSpark: Reborn Labeled Features from Unlabeled in Transformer for\n  Semi-Supervised Semantic Segmentation","summary":"  Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate\nthe burden of time-consuming pixel-level manual labeling, which leverages\nlimited labeled data along with larger amounts of unlabeled data. Current\nstate-of-the-art methods train the labeled data with ground truths and\nunlabeled data with pseudo labels. However, the two training flows are\nseparate, which allows labeled data to dominate the training process, resulting\nin low-quality pseudo labels and, consequently, sub-optimal results. To\nalleviate this issue, we present AllSpark, which reborns the labeled features\nfrom unlabeled ones with the channel-wise cross-attention mechanism. We further\nintroduce a Semantic Memory along with a Channel Semantic Grouping strategy to\nensure that unlabeled features adequately represent labeled features. The\nAllSpark shed new light on the architecture level designs of SSSS rather than\nframework level, which avoids increasingly complicated training pipeline\ndesigns. It can also be regarded as a flexible bottleneck module that can be\nseamlessly integrated into a general transformer-based segmentation model. The\nproposed AllSpark outperforms existing methods across all evaluation protocols\non Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and\nmodel weights are available at: https://github.com/xmed-lab/AllSpark.\n","authors":["Haonan Wang","Qixiang Zhang","Yi Li","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.01818v3.pdf","comment":"Accepted by CVPR 2024; correct typos; this is not the camera-ready\n  version"},{"id":"http://arxiv.org/abs/2211.11612v2","updated":"2024-03-14T15:35:59Z","published":"2022-11-21T16:13:23Z","title":"Plug and Play Active Learning for Object Detection","summary":"  Annotating datasets for object detection is an expensive and time-consuming\nendeavor. To minimize this burden, active learning (AL) techniques are employed\nto select the most informative samples for annotation within a constrained\n\"annotation budget\". Traditional AL strategies typically rely on model\nuncertainty or sample diversity for query sampling, while more advanced methods\nhave focused on developing AL-specific object detector architectures to enhance\nperformance. However, these specialized approaches are not readily adaptable to\ndifferent object detectors due to the significant engineering effort required\nfor integration. To overcome this challenge, we introduce Plug and Play Active\nLearning (PPAL), a simple and effective AL strategy for object detection. PPAL\nis a two-stage method comprising uncertainty-based and diversity-based sampling\nphases. In the first stage, our Difficulty Calibrated Uncertainty Sampling\nleverage a category-wise difficulty coefficient that combines both\nclassification and localisation difficulties to re-weight instance\nuncertainties, from which we sample a candidate pool for the subsequent\ndiversity-based sampling. In the second stage, we propose Category Conditioned\nMatching Similarity to better compute the similarities of multi-instance images\nas ensembles of their instance similarities, which is used by the k-Means++\nalgorithm to sample the final AL queries. PPAL makes no change to model\narchitectures or detector training pipelines; hence it can be easily\ngeneralized to different object detectors. We benchmark PPAL on the MS-COCO and\nPascal VOC datasets using different detector architectures and show that our\nmethod outperforms prior work by a large margin. Code is available at\nhttps://github.com/ChenhongyiYang/PPAL\n","authors":["Chenhongyi Yang","Lichao Huang","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2211.11612v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09493v1","updated":"2024-03-14T15:35:07Z","published":"2024-03-14T15:35:07Z","title":"Anomaly Detection by Adapting a pre-trained Vision Language Model","summary":"  Recently, large vision and language models have shown their success when\nadapting them to many downstream tasks. In this paper, we present a unified\nframework named CLIP-ADA for Anomaly Detection by Adapting a pre-trained CLIP\nmodel. To this end, we make two important improvements: 1) To acquire unified\nanomaly detection across industrial images of multiple categories, we introduce\nthe learnable prompt and propose to associate it with abnormal patterns through\nself-supervised learning. 2) To fully exploit the representation power of CLIP,\nwe introduce an anomaly region refinement strategy to refine the localization\nquality. During testing, the anomalies are localized by directly calculating\nthe similarity between the representation of the learnable prompt and the\nimage. Comprehensive experiments demonstrate the superiority of our framework,\ne.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and\nVisA for anomaly detection and localization. In addition, the proposed method\nalso achieves encouraging performance with marginal training data, which is\nmore challenging.\n","authors":["Yuxuan Cai","Xinwei He","Dingkang Liang","Ao Tong","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2403.09493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09486v1","updated":"2024-03-14T15:29:09Z","published":"2024-03-14T15:29:09Z","title":"SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with\n  Spike Streams","summary":"  Reconstructing a sequence of sharp images from the blurry input is crucial\nfor enhancing our insights into the captured scene and poses a significant\nchallenge due to the limited temporal features embedded in the image. Spike\ncameras, sampling at rates up to 40,000 Hz, have proven effective in capturing\nmotion features and beneficial for solving this ill-posed problem. Nonetheless,\nexisting methods fall into the supervised learning paradigm, which suffers from\nnotable performance degradation when applied to real-world scenarios that\ndiverge from the synthetic training data domain. Moreover, the quality of\nreconstructed images is capped by the generated images based on motion analysis\ninterpolation, which inherently differs from the actual scene, affecting the\ngeneralization ability of these methods in real high-speed scenarios. To\naddress these challenges, we propose the first self-supervised framework for\nthe task of spike-guided motion deblurring. Our approach begins with the\nformulation of a spike-guided deblurring model that explores the theoretical\nrelationships among spike streams, blurry images, and their corresponding sharp\nsequences. We subsequently develop a self-supervised cascaded framework to\nalleviate the issues of spike noise and spatial-resolution mismatching\nencountered in the deblurring model. With knowledge distillation and\nre-blurring loss, we further design a lightweight deblur network to generate\nhigh-quality sequences with brightness and texture consistency with the\noriginal input. Quantitative and qualitative experiments conducted on our\nreal-world and synthetic datasets with spikes validate the superior\ngeneralization of the proposed framework. Our code, data and trained models\nwill be available at \\url{https://github.com/chenkang455/S-SDM}.\n","authors":["Kang Chen","Shiyan Chen","Jiyuan Zhang","Baoyue Zhang","Yajing Zheng","Tiejun Huang","Zhaofei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09486v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.09480v1","updated":"2024-03-14T15:22:33Z","published":"2024-03-14T15:22:33Z","title":"What Sketch Explainability Really Means for Downstream Tasks","summary":"  In this paper, we explore the unique modality of sketch for explainability,\nemphasising the profound impact of human strokes compared to conventional\npixel-oriented studies. Beyond explanations of network behavior, we discern the\ngenuine implications of explainability across diverse downstream sketch-related\ntasks. We propose a lightweight and portable explainability solution -- a\nseamless plugin that integrates effortlessly with any pre-trained model,\neliminating the need for re-training. Demonstrating its adaptability, we\npresent four applications: highly studied retrieval and generation, and\ncompletely novel assisted drawing and sketch adversarial attacks. The\ncentrepiece to our solution is a stroke-level attribution map that takes\ndifferent forms when linked with downstream tasks. By addressing the inherent\nnon-differentiability of rasterisation, we enable explanations at both coarse\nstroke level (SLA) and partial stroke level (P-SLA), each with its advantages\nfor specific downstream tasks.\n","authors":["Hmrishav Bandyopadhyay","Pinaki Nath Chowdhury","Ayan Kumar Bhunia","Aneeshan Sain","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2403.09480v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09471v1","updated":"2024-03-14T15:10:54Z","published":"2024-03-14T15:10:54Z","title":"MambaTalk: Efficient Holistic Gesture Synthesis with Selective State\n  Space Models","summary":"  Gesture synthesis is a vital realm of human-computer interaction, with\nwide-ranging applications across various fields like film, robotics, and\nvirtual reality. Recent advancements have utilized the diffusion model and\nattention mechanisms to improve gesture synthesis. However, due to the high\ncomputational complexity of these techniques, generating long and diverse\nsequences with low latency remains a challenge. We explore the potential of\nstate space models (SSMs) to address the challenge, implementing a two-stage\nmodeling strategy with discrete motion priors to enhance the quality of\ngestures. Leveraging the foundational Mamba block, we introduce MambaTalk,\nenhancing gesture diversity and rhythm through multimodal integration.\nExtensive experiments demonstrate that our method matches or exceeds the\nperformance of state-of-the-art models.\n","authors":["Zunnan Xu","Yukang Lin","Haonan Han","Sicheng Yang","Ronghui Li","Yachao Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.09471v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2310.11482v2","updated":"2024-03-14T15:10:05Z","published":"2023-10-17T13:06:39Z","title":"Rethinking Class-incremental Learning in the Era of Large Pre-trained\n  Models via Test-Time Adaptation","summary":"  Class-incremental learning (CIL) is a challenging task that involves\nsequentially learning to categorize classes from new tasks without forgetting\npreviously learned information. The advent of large pre-trained models (PTMs)\nhas fast-tracked the progress in CIL due to the highly transferable PTM\nrepresentations, where tuning a small set of parameters leads to\nstate-of-the-art performance when compared with the traditional CIL methods\nthat are trained from scratch. However, repeated fine-tuning on each task\ndestroys the rich representations of the PTMs and further leads to forgetting\nprevious tasks. To strike a balance between the stability and plasticity of\nPTMs for CIL, we propose a novel perspective of eliminating training on every\nnew task and instead train PTM only on the first task, and then refine its\nrepresentation at inference time using test-time adaptation (TTA). Concretely,\nwe propose Test-Time Adaptation for Class-Incremental Learning (TTACIL) that\nfirst fine-tunes PTMs using Adapters on the first task, then adjusts Layer Norm\nparameters of the PTM on each test instance for learning task-specific\nfeatures, and finally resets them back to the adapted model to preserve\nstability. As a consequence, our TTACIL does not undergo any forgetting, while\nbenefiting each task with the rich PTM features. Additionally, by design, our\nTTACIL is robust to common data corruptions. Our method outperforms several\nstate-of-the-art CIL methods when evaluated on multiple CIL benchmarks under\nboth clean and corrupted data. Code is available at:\nhttps://github.com/IemProg/TTACIL.\n","authors":["Imad Eddine Marouf","Subhankar Roy","Enzo Tartaglione","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2310.11482v2.pdf","comment":"8 pages,5 figures"},{"id":"http://arxiv.org/abs/2311.10517v2","updated":"2024-03-14T15:09:08Z","published":"2023-11-17T13:40:10Z","title":"Mind the map! Accounting for existing map information when estimating\n  online HDMaps from sensor","summary":"  While HDMaps are a crucial component of autonomous driving, they are\nexpensive to acquire and maintain. Estimating these maps from sensors therefore\npromises to significantly lighten costs. These estimations however overlook\nexisting HDMaps, with current methods at most geolocalizing low quality maps or\nconsidering a general database of known maps. In this paper, we propose to\naccount for existing maps of the precise situation studied when estimating\nHDMaps. We identify 3 reasonable types of useful existing maps (minimalist,\nnoisy, and outdated). We also introduce MapEX, a novel online HDMap estimation\nframework that accounts for existing maps. MapEX achieves this by encoding map\nelements into query tokens and by refining the matching algorithm used to train\nclassic query based map estimation models. We demonstrate that MapEX brings\nsignificant improvements on the nuScenes dataset. For instance, MapEX - given\nnoisy maps - improves by 38% over the MapTRv2 detector it is based on and by 8%\nover the current SOTA.\n","authors":["Rémy Sun","Li Yang","Diane Lingrand","Frédéric Precioso"],"pdf_url":"https://arxiv.org/pdf/2311.10517v2.pdf","comment":"23 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.09468v1","updated":"2024-03-14T15:07:36Z","published":"2024-03-14T15:07:36Z","title":"Eta Inversion: Designing an Optimal Eta Function for Diffusion-based\n  Real Image Editing","summary":"  Diffusion models have achieved remarkable success in the domain of\ntext-guided image generation and, more recently, in text-guided image editing.\nA commonly adopted strategy for editing real images involves inverting the\ndiffusion process to obtain a noisy representation of the original image, which\nis then denoised to achieve the desired edits. However, current methods for\ndiffusion inversion often struggle to produce edits that are both faithful to\nthe specified text prompt and closely resemble the source image. To overcome\nthese limitations, we introduce a novel and adaptable diffusion inversion\ntechnique for real image editing, which is grounded in a theoretical analysis\nof the role of $\\eta$ in the DDIM sampling equation for enhanced editability.\nBy designing a universal diffusion inversion method with a time- and\nregion-dependent $\\eta$ function, we enable flexible control over the editing\nextent. Through a comprehensive series of quantitative and qualitative\nassessments, involving a comparison with a broad array of recent methods, we\ndemonstrate the superiority of our approach. Our method not only sets a new\nbenchmark in the field but also significantly outperforms existing strategies.\nOur code is available at https://github.com/furiosa-ai/eta-inversion\n","authors":["Wonjun Kang","Kevin Galim","Hyung Il Koo"],"pdf_url":"https://arxiv.org/pdf/2403.09468v1.pdf","comment":"https://github.com/furiosa-ai/eta-inversion"},{"id":"http://arxiv.org/abs/2403.09451v1","updated":"2024-03-14T14:49:40Z","published":"2024-03-14T14:49:40Z","title":"M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in\n  Cognitive Load Assessment","summary":"  This paper introduces the M&M model, a novel multimodal-multitask learning\nframework, applied to the AVCAffe dataset for cognitive load assessment (CLA).\nM&M uniquely integrates audiovisual cues through a dual-pathway architecture,\nfeaturing specialized streams for audio and video inputs. A key innovation lies\nin its cross-modality multihead attention mechanism, fusing the different\nmodalities for synchronized multitasking. Another notable feature is the\nmodel's three specialized branches, each tailored to a specific cognitive load\nlabel, enabling nuanced, task-specific analysis. While it shows modest\nperformance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a\npromising framework for integrated multimodal processing. This work paves the\nway for future enhancements in multimodal-multitask learning systems,\nemphasizing the fusion of diverse data types for complex task handling.\n","authors":["Long Nguyen-Phuoc","Renald Gaboriau","Dimitri Delacroix","Laurent Navarro"],"pdf_url":"https://arxiv.org/pdf/2403.09451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09883v2","updated":"2024-03-14T14:40:15Z","published":"2023-10-15T16:42:14Z","title":"Zero-Shot Object Goal Visual Navigation With Class-Independent\n  Relationship Network","summary":"  This paper investigates the zero-shot object goal visual navigation problem.\nIn the object goal visual navigation task, the agent needs to locate navigation\ntargets from its egocentric visual input. \"Zero-shot\" means that the target the\nagent needs to find is not trained during the training phase. To address the\nissue of coupling navigation ability with target features during training, we\npropose the Class-Independent Relationship Network (CIRN). This method combines\ntarget detection information with the relative semantic similarity between the\ntarget and the navigation target, and constructs a brand new state\nrepresentation based on similarity ranking, this state representation does not\ninclude target feature or environment feature, effectively decoupling the\nagent's navigation ability from target features. And a Graph Convolutional\nNetwork (GCN) is employed to learn the relationships between different objects\nbased on their similarities. During testing, our approach demonstrates strong\ngeneralization capabilities, including zero-shot navigation tasks with\ndifferent targets and environments. Through extensive experiments in the\nAI2-THOR virtual environment, our method outperforms the current\nstate-of-the-art approaches in the zero-shot object goal visual navigation\ntask. Furthermore, we conducted experiments in more challenging cross-target\nand cross-scene settings, which further validate the robustness and\ngeneralization ability of our method. Our code is available at:\nhttps://github.com/SmartAndCleverRobot/ICRA-CIRN.\n","authors":["Xinting Li","Shiguang Zhang","Yue LU","Kerry Dang","Lingyan Ran"],"pdf_url":"https://arxiv.org/pdf/2310.09883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11881v3","updated":"2024-03-14T14:39:07Z","published":"2023-10-18T11:06:41Z","title":"A Comparative Study of Image Restoration Networks for General Backbone\n  Network Design","summary":"  Despite the significant progress made by deep models in various image\nrestoration tasks, existing image restoration networks still face challenges in\nterms of task generality. An intuitive manifestation is that networks which\nexcel in certain tasks often fail to deliver satisfactory results in others. To\nillustrate this point, we select five representative networks and conduct a\ncomparative study on five classic image restoration tasks. First, we provide a\ndetailed explanation of the characteristics of different image restoration\ntasks and backbone networks. Following this, we present the benchmark results\nand analyze the reasons behind the performance disparity of different models\nacross various tasks. Drawing from this comparative study, we propose that a\ngeneral image restoration backbone network needs to meet the functional\nrequirements of diverse tasks. Based on this principle, we design a new general\nimage restoration backbone network, X-Restormer. Extensive experiments\ndemonstrate that X-Restormer possesses good task generality and achieves\nstate-of-the-art performance across a variety of tasks.\n","authors":["Xiangyu Chen","Zheyuan Li","Yuandong Pu","Yihao Liu","Jiantao Zhou","Yu Qiao","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2310.11881v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11317v4","updated":"2024-03-14T14:36:07Z","published":"2023-11-19T13:07:06Z","title":"Discrete approximations of Gaussian smoothing and Gaussian derivatives","summary":"  This paper develops an in-depth treatment concerning the problem of\napproximating the Gaussian smoothing and Gaussian derivative computations in\nscale-space theory for application on discrete data. With close connections to\nprevious axiomatic treatments of continuous and discrete scale-space theory, we\nconsider three main ways discretizing these scale-space operations in terms of\nexplicit discrete convolutions, based on either (i) sampling the Gaussian\nkernels and the Gaussian derivative kernels, (ii) locally integrating the\nGaussian kernels and the Gaussian derivative kernels over each pixel support\nregion and (iii) basing the scale-space analysis on the discrete analogue of\nthe Gaussian kernel, and then computing derivative approximations by applying\nsmall-support central difference operators to the spatially smoothed image\ndata.\n  We study the properties of these three main discretization methods both\ntheoretically and experimentally, and characterize their performance by\nquantitative measures, including the results they give rise to with respect to\nthe task of scale selection, investigated for four different use cases, and\nwith emphasis on the behaviour at fine scales. The results show that the\nsampled Gaussian kernels and derivatives as well as the integrated Gaussian\nkernels and derivatives perform very poorly at very fine scales. At very fine\nscales, the discrete analogue of the Gaussian kernel with its corresponding\ndiscrete derivative approximations performs substantially better. The sampled\nGaussian kernel and the sampled Gaussian derivatives do, on the other hand,\nlead to numerically very good approximations of the corresponding continuous\nresults, when the scale parameter is sufficiently large, in the experiments\npresented in the paper, when the scale parameter is greater than a value of\nabout 1, in units of the grid spacing.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2311.11317v4.pdf","comment":"40 pages, 21 figures"},{"id":"http://arxiv.org/abs/2306.10322v3","updated":"2024-03-14T14:33:51Z","published":"2023-06-17T11:44:04Z","title":"CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot\n  Vision-and-Language Navigation","summary":"  Understanding and following natural language instructions while navigating\nthrough complex, real-world environments poses a significant challenge for\ngeneral-purpose robots. These environments often include obstacles and\npedestrians, making it essential for autonomous agents to possess the\ncapability of self-corrected planning to adjust their actions based on feedback\nfrom the surroundings. However, the majority of existing vision-and-language\nnavigation (VLN) methods primarily operate in less realistic simulator settings\nand do not incorporate environmental feedback into their decision-making\nprocesses. To address this gap, we introduce a novel zero-shot framework called\nCorNav, utilizing a large language model for decision-making and comprising two\nkey components: 1) incorporating environmental feedback for refining future\nplans and adjusting its actions, and 2) multiple domain experts for parsing\ninstructions, scene understanding, and refining predicted actions. In addition\nto the framework, we develop a 3D simulator that renders realistic scenarios\nusing Unreal Engine 5. To evaluate the effectiveness and generalization of\nnavigation agents in a zero-shot multi-task setting, we create a benchmark\ncalled NavBench. Extensive experiments demonstrate that CorNav consistently\noutperforms all baselines by a significant margin across all tasks. On average,\nCorNav achieves a success rate of 28.1\\%, surpassing the best baseline's\nperformance of 20.5\\%.\n","authors":["Xiwen Liang","Liang Ma","Shanshan Guo","Jianhua Han","Hang Xu","Shikui Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.10322v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.09439v1","updated":"2024-03-14T14:31:22Z","published":"2024-03-14T14:31:22Z","title":"3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation","summary":"  Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.\n","authors":["Frank Zhang","Yibo Zhang","Quan Zheng","Rui Ma","Wei Hua","Hujun Bao","Weiwei Xu","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09439v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.09437v1","updated":"2024-03-14T14:30:31Z","published":"2024-03-14T14:30:31Z","title":"Improving Real-Time Omnidirectional 3D Multi-Person Human Pose\n  Estimation with People Matching and Unsupervised 2D-3D Lifting","summary":"  Current human pose estimation systems focus on retrieving an accurate 3D\nglobal estimate of a single person. Therefore, this paper presents one of the\nfirst 3D multi-person human pose estimation systems that is able to work in\nreal-time and is also able to handle basic forms of occlusion. First, we adjust\nan off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use\nwith a 360$^\\circ$ panoramic camera and mmWave radar sensors. We then introduce\nseveral contributions, including camera and radar calibrations, and the\nimproved matching of people within the image and radar space. The system\naddresses both the depth and scale ambiguity problems by employing a\nlightweight 2D-3D pose lifting algorithm that is able to work in real-time\nwhile exhibiting accurate performance in both indoor and outdoor environments\nwhich offers both an affordable and scalable solution. Notably, our system's\ntime complexity remains nearly constant irrespective of the number of detected\nindividuals, achieving a frame rate of approximately 7-8 fps on a laptop with a\ncommercial-grade GPU.\n","authors":["Pawel Knap","Peter Hardy","Alberto Tamajo","Hwasup Lim","Hansung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.09437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09433v1","updated":"2024-03-14T14:25:10Z","published":"2024-03-14T14:25:10Z","title":"Open-Vocabulary Object Detection with Meta Prompt Representation and\n  Instance Contrastive Optimization","summary":"  Classical object detectors are incapable of detecting novel class objects\nthat are not encountered before. Regarding this issue, Open-Vocabulary Object\nDetection (OVOD) is proposed, which aims to detect the objects in the candidate\nclass list. However, current OVOD models are suffering from overfitting on the\nbase classes, heavily relying on the large-scale extra data, and complex\ntraining process. To overcome these issues, we propose a novel framework with\nMeta prompt and Instance Contrastive learning (MIC) schemes. Firstly, we\nsimulate a novel-class-emerging scenario to help the prompt learner that learns\nclass and background prompts generalize to novel classes. Secondly, we design\nan instance-level contrastive strategy to promote intra-class compactness and\ninter-class separation, which benefits generalization of the detector to novel\nclass objects. Without using knowledge distillation, ensemble model or extra\ntraining data during detector training, our proposed MIC outperforms previous\nSOTA methods trained with these complex techniques on LVIS. Most importantly,\nMIC shows great generalization ability on novel classes, e.g., with $+4.3\\%$\nand $+1.9\\% \\ \\mathrm{AP}$ improvement compared with previous SOTA on COCO and\nObjects365, respectively.\n","authors":["Zhao Wang","Aoxue Li","Fengwei Zhou","Zhenguo Li","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2403.09433v1.pdf","comment":"BMVC 2023"},{"id":"http://arxiv.org/abs/2403.09434v1","updated":"2024-03-14T14:25:10Z","published":"2024-03-14T14:25:10Z","title":"Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D\n  Gaussians","summary":"  Reconstructing and simulating elastic objects from visual observations is\ncrucial for applications in computer vision and robotics. Existing methods,\nsuch as 3D Gaussians, provide modeling for 3D appearance and geometry but lack\nthe ability to simulate physical properties or optimize parameters for\nheterogeneous objects. We propose Spring-Gaus, a novel framework that\nintegrates 3D Gaussians with physics-based simulation for reconstructing and\nsimulating elastic objects from multi-view videos. Our method utilizes a 3D\nSpring-Mass model, enabling the optimization of physical parameters at the\nindividual point level while decoupling the learning of physics and appearance.\nThis approach achieves great sample efficiency, enhances generalization, and\nreduces sensitivity to the distribution of simulation particles. We evaluate\nSpring-Gaus on both synthetic and real-world datasets, demonstrating accurate\nreconstruction and simulation of elastic objects. This includes future\nprediction and simulation under varying initial states and environmental\nparameters. Project page: https://zlicheng.com/spring_gaus.\n","authors":["Licheng Zhong","Hong-Xing Yu","Jiajun Wu","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2403.09434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09432v1","updated":"2024-03-14T14:23:23Z","published":"2024-03-14T14:23:23Z","title":"Efficient Transferability Assessment for Selection of Pre-trained\n  Detectors","summary":"  Large-scale pre-training followed by downstream fine-tuning is an effective\nsolution for transferring deep-learning-based models. Since finetuning all\npossible pre-trained models is computational costly, we aim to predict the\ntransferability performance of these pre-trained models in a computational\nefficient manner. Different from previous work that seek out suitable models\nfor downstream classification and segmentation tasks, this paper studies the\nefficient transferability assessment of pre-trained object detectors. To this\nend, we build up a detector transferability benchmark which contains a large\nand diverse zoo of pre-trained detectors with various architectures, source\ndatasets and training schemes. Given this zoo, we adopt 7 target datasets from\n5 diverse domains as the downstream target tasks for evaluation. Further, we\npropose to assess classification and regression sub-tasks simultaneously in a\nunified framework. Additionally, we design a complementary metric for\nevaluating tasks with varying objects. Experimental results demonstrate that\nour method outperforms other state-of-the-art approaches in assessing\ntransferability under different target domains while efficiently reducing\nwall-clock time 32$\\times$ and requires a mere 5.2\\% memory footprint compared\nto brute-force fine-tuning of all pre-trained detectors.\n","authors":["Zhao Wang","Aoxue Li","Zhenguo Li","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2403.09432v1.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2305.15583v6","updated":"2024-03-14T14:15:48Z","published":"2023-05-24T21:39:27Z","title":"Alleviating Exposure Bias in Diffusion Models through Sampling with\n  Shifted Time Steps","summary":"  Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the\nsynthesis of high-quality images. However, their inference process\ncharacteristically requires numerous, potentially hundreds, of iterative steps,\nwhich could exaggerate the problem of exposure bias due to the training and\ninference discrepancy. Previous work has attempted to mitigate this issue by\nperturbing inputs during training, which consequently mandates the retraining\nof the DPM. In this work, we conduct a systematic study of exposure bias in DPM\nand, intriguingly, we find that the exposure bias could be alleviated with a\nnovel sampling method that we propose, without retraining the model. We\nempirically and theoretically show that, during inference, for each backward\ntime step $t$ and corresponding state $\\hat{x}_t$, there might exist another\ntime step $t_s$ which exhibits superior coupling with $\\hat{x}_t$. Based on\nthis finding, we introduce a sampling method named Time-Shift Sampler. Our\nframework can be seamlessly integrated to existing sampling algorithms, such as\nDDPM, DDIM and other high-order solvers, inducing merely minimal additional\ncomputations. Experimental results show our method brings significant and\nconsistent improvements in FID scores on different datasets and sampling\nmethods. For example, integrating Time-Shift Sampler to F-PNDM yields a\nFID=3.88, achieving 44.49\\% improvements as compared to F-PNDM, on CIFAR-10\nwith 10 sampling steps, which is more performant than the vanilla DDIM with 100\nsampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.\n","authors":["Mingxiao Li","Tingyu Qu","Ruicong Yao","Wei Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2305.15583v6.pdf","comment":"Accepted at International Conference on Learning Representations\n  (ICLR2024)"},{"id":"http://arxiv.org/abs/2403.09422v1","updated":"2024-03-14T14:14:47Z","published":"2024-03-14T14:14:47Z","title":"Mitigating attribute amplification in counterfactual image generation","summary":"  Causal generative modelling is gaining interest in medical imaging due to its\nability to answer interventional and counterfactual queries. Most work focuses\non generating counterfactual images that look plausible, using auxiliary\nclassifiers to enforce effectiveness of simulated interventions. We investigate\npitfalls in this approach, discovering the issue of attribute amplification,\nwhere unrelated attributes are spuriously affected during interventions,\nleading to biases across protected characteristics and disease status. We show\nthat attribute amplification is caused by the use of hard labels in the\ncounterfactual training process and propose soft counterfactual fine-tuning to\nmitigate this issue. Our method substantially reduces the amplification effect\nwhile maintaining effectiveness of generated images, demonstrated on a large\nchest X-ray dataset. Our work makes an important advancement towards more\nfaithful and unbiased causal modelling in medical imaging.\n","authors":["Tian Xia","Mélanie Roschewitz","Fabio De Sousa Ribeiro","Charles Jones","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2403.09422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09419v1","updated":"2024-03-14T14:08:59Z","published":"2024-03-14T14:08:59Z","title":"RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban\n  Scenes","summary":"  The task of separating dynamic objects from static environments using NeRFs\nhas been widely studied in recent years. However, capturing large-scale scenes\nstill poses a challenge due to their complex geometric structures and\nunconstrained dynamics. Without the help of 3D motion cues, previous methods\noften require simplified setups with slow camera motion and only a few/single\ndynamic actors, leading to suboptimal solutions in most urban setups. To\novercome such limitations, we present RoDUS, a pipeline for decomposing static\nand dynamic elements in urban scenes, with thoughtfully separated NeRF models\nfor moving and non-moving components. Our approach utilizes a robust\nkernel-based initialization coupled with 4D semantic information to selectively\nguide the learning process. This strategy enables accurate capturing of the\ndynamics in the scene, resulting in reduced artifacts caused by NeRF on\nbackground reconstruction, all by using self-supervision. Notably, experimental\nevaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of\nour method in decomposing challenging urban scenes into precise static and\ndynamic components.\n","authors":["Thang-Anh-Quan Nguyen","Luis Roldão","Nathan Piasco","Moussab Bennehar","Dzmitry Tsishkou"],"pdf_url":"https://arxiv.org/pdf/2403.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18961v7","updated":"2024-03-14T14:08:05Z","published":"2023-10-29T10:03:49Z","title":"AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly\n  Detection","summary":"  Zero-shot anomaly detection (ZSAD) requires detection models trained using\nauxiliary data to detect anomalies without any training sample in a target\ndataset. It is a crucial task when training data is not accessible due to\nvarious concerns, eg, data privacy, yet it is challenging since the models need\nto generalize to anomalies across different domains where the appearance of\nforeground objects, abnormal regions, and background features, such as\ndefects/tumors on different products/organs, can vary significantly. Recently\nlarge pre-trained vision-language models (VLMs), such as CLIP, have\ndemonstrated strong zero-shot recognition ability in various vision tasks,\nincluding anomaly detection. However, their ZSAD performance is weak since the\nVLMs focus more on modeling the class semantics of the foreground objects\nrather than the abnormality/normality in the images. In this paper we introduce\na novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across\ndifferent domains. The key insight of AnomalyCLIP is to learn object-agnostic\ntext prompts that capture generic normality and abnormality in an image\nregardless of its foreground objects. This allows our model to focus on the\nabnormal image regions rather than the object semantics, enabling generalized\nnormality and abnormality recognition on diverse types of objects. Large-scale\nexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIP\nachieves superior zero-shot performance of detecting and segmenting anomalies\nin datasets of highly diverse class semantics from various defect inspection\nand medical imaging domains. Code will be made available at\nhttps://github.com/zqhang/AnomalyCLIP.\n","authors":["Qihang Zhou","Guansong Pang","Yu Tian","Shibo He","Jiming Chen"],"pdf_url":"https://arxiv.org/pdf/2310.18961v7.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2308.14469v3","updated":"2024-03-14T14:06:44Z","published":"2023-08-28T10:15:57Z","title":"Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and\n  Personalized Stylization","summary":"  Diffusion models have demonstrated impressive performance in various image\ngeneration, editing, enhancement and translation tasks. In particular, the\npre-trained text-to-image stable diffusion models provide a potential solution\nto the challenging realistic image super-resolution (Real-ISR) and image\nstylization problems with their strong generative priors. However, the existing\nmethods along this line often fail to keep faithful pixel-wise image\nstructures. If extra skip connections are used to reproduce details, additional\ntraining in image space will be required, limiting the application to tasks in\nlatent space such as image stylization. In this work, we propose a pixel-aware\nstable diffusion (PASD) network to achieve robust Real-ISR and personalized\nimage stylization. Specifically, a pixel-aware cross attention module is\nintroduced to enable diffusion models perceiving image local structures in\npixel-wise level, while a degradation removal module is used to extract\ndegradation insensitive features to guide the diffusion process together with\nimage high level information. An adjustable noise schedule is introduced to\nfurther improve the image restoration results. By simply replacing the base\ndiffusion model with a stylized one, PASD can generate diverse stylized images\nwithout collecting pairwise training data, and by shifting the base model with\nan aesthetic one, PASD can bring old photos back to life. Extensive experiments\nin a variety of image enhancement and stylization tasks demonstrate the\neffectiveness of our proposed PASD approach. Our source codes are available at\n\\url{https://github.com/yangxy/PASD/}.\n","authors":["Tao Yang","Rongyuan Wu","Peiran Ren","Xuansong Xie","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.14469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09414v1","updated":"2024-03-14T14:04:29Z","published":"2024-03-14T14:04:29Z","title":"Region-based U-net for accelerated training and enhanced precision in\n  deep brain segmentation","summary":"  Segmentation of brain structures on MRI is the primary step for further\nquantitative analysis of brain diseases. Manual segmentation is still\nconsidered the gold standard in terms of accuracy; however, such data is\nextremely time-consuming to generate. This paper presents a deep learning-based\nsegmentation approach for 12 deep-brain structures, utilizing multiple\nregion-based U-Nets. The brain is divided into three focal regions of interest\nthat encompass the brainstem, the ventricular system, and the striatum. Next,\nthree region-based U-nets are run in parallel to parcellate these larger\nstructures into their respective four substructures. This approach not only\ngreatly reduces the training and processing times but also significantly\nenhances the segmentation accuracy, compared to segmenting the entire MRI image\nat once. Our approach achieves remarkable accuracy with an average Dice\nSimilarity Coefficient (DSC) of 0.901 and 95% Hausdorff Distance (HD95) of\n1.155 mm. The method was compared with state-of-the-art segmentation\napproaches, demonstrating a high level of accuracy and robustness of the\nproposed method.\n","authors":["Mengyu Li","Magnus Magnusson","Thilo van Eimeren","Lotta M. Ellingsen"],"pdf_url":"https://arxiv.org/pdf/2403.09414v1.pdf","comment":"5 pages, 2 figures, 21st IEEE International Symposium on Biomedical\n  Imaging"},{"id":"http://arxiv.org/abs/2403.09413v1","updated":"2024-03-14T14:04:21Z","published":"2024-03-14T14:04:21Z","title":"Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting","summary":"  3D Gaussian splatting (3DGS) has recently demonstrated impressive\ncapabilities in real-time novel view synthesis and 3D reconstruction. However,\n3DGS heavily depends on the accurate initialization derived from\nStructure-from-Motion (SfM) methods. When trained with randomly initialized\npoint clouds, 3DGS fails to maintain its ability to produce high-quality\nimages, undergoing large performance drops of 4-5 dB in PSNR. Through extensive\nanalysis of SfM initialization in the frequency domain and analysis of a 1D\nregression task with multiple 1D Gaussians, we propose a novel optimization\nstrategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D\nGaussian Splatting), that successfully trains 3D Gaussians from random point\nclouds. We show the effectiveness of our strategy through quantitative and\nqualitative comparisons on multiple datasets, largely improving the performance\nin all settings. Our project page and code can be found at\nhttps://ku-cvlab.github.io/RAIN-GS.\n","authors":["Jaewoo Jung","Jisang Han","Honggyu An","Jiwon Kang","Seonghoon Park","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.09413v1.pdf","comment":"Project Page: https://ku-cvlab.github.io/RAIN-GS"},{"id":"http://arxiv.org/abs/2403.09412v1","updated":"2024-03-14T14:03:29Z","published":"2024-03-14T14:03:29Z","title":"OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in\n  Large-Scale Outdoor Environments","summary":"  Environment maps endowed with sophisticated semantics are pivotal for\nfacilitating seamless interaction between robots and humans, enabling them to\neffectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including\nmultimodal retrieval and open-set classes. However, existing open-vocabulary\nmaps are constrained to closed indoor scenarios and VLM features, thereby\ndiminishing their usability and inference capabilities. Moreover, the absence\nof topological relationships further complicates the accurate querying of\nspecific instances. In this work, we propose OpenGraph, a representation of\nopen-vocabulary hierarchical graph structure designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images using 2D foundation models, encoding the captions with features\nto enhance textual reasoning. Subsequently, 3D incremental panoramic mapping\nwith feature embedding is achieved by projecting images onto LiDAR point\nclouds. Finally, the environment is segmented based on lane graph connectivity\nto construct a hierarchical graph. Validation results from real public dataset\nSemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph\nexhibits the ability to generalize to novel semantic classes and achieve the\nhighest segmentation and query accuracy. The source code of OpenGraph is\npublicly available at https://github.com/BIT-DYN/OpenGraph.\n","authors":["Yinan Deng","Jiahui Wang","Jingyu Zhao","Xinyu Tian","Guangyan Chen","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2403.09412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09410v1","updated":"2024-03-14T14:02:01Z","published":"2024-03-14T14:02:01Z","title":"XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via\n  Concept-guided Context Optimization","summary":"  Utilizing potent representations of the large vision-language models (VLMs)\nto accomplish various downstream tasks has attracted increasing attention.\nWithin this research field, soft prompt learning has become a representative\napproach for efficiently adapting VLMs such as CLIP, to tasks like image\nclassification. However, most existing prompt learning methods learn text\ntokens that are unexplainable, which cannot satisfy the stringent\ninterpretability requirements of Explainable Artificial Intelligence (XAI) in\nhigh-stakes scenarios like healthcare. To address this issue, we propose a\nnovel explainable prompt learning framework that leverages medical knowledge by\naligning the semantics of images, learnable prompts, and clinical\nconcept-driven prompts at multiple granularities. Moreover, our framework\naddresses the lack of valuable concept annotations by eliciting knowledge from\nlarge language models and offers both visual and textual explanations for the\nprompts. Extensive experiments and explainability analyses conducted on various\ndatasets, with and without concept labels, demonstrate that our method\nsimultaneously achieves superior diagnostic performance, flexibility, and\ninterpretability, shedding light on the effectiveness of foundation models in\nfacilitating XAI. The code will be made publically available.\n","authors":["Yequan Bie","Luyang Luo","Zhixuan Chen","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.09410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03357v2","updated":"2024-03-14T13:58:06Z","published":"2023-12-06T08:54:04Z","title":"RING-NeRF : Rethinking Inductive Biases for Versatile and Efficient\n  Neural Fields","summary":"  Recent advances in Neural Fields mostly rely on developing task-specific\nsupervision which often complicates the models. Rather than developing\nhard-to-combine and specific modules, another approach generally overlooked is\nto directly inject generic priors on the scene representation (also called\ninductive biases) into the NeRF architecture. Based on this idea, we propose\nthe RING-NeRF architecture which includes two inductive biases : a continuous\nmulti-scale representation of the scene and an invariance of the decoder's\nlatent space over spatial and scale domains. We also design a single\nreconstruction process that takes advantage of those inductive biases and\nexperimentally demonstrates on-par performances in terms of quality with\ndedicated architecture on multiple tasks (anti-aliasing, few view\nreconstruction, SDF reconstruction without scene-specific initialization) while\nbeing more efficient. Moreover, RING-NeRF has the distinctive ability to\ndynamically increase the resolution of the model, opening the way to adaptive\nreconstruction.\n","authors":["Doriand Petit","Steve Bourgeois","Dumitru Pavel","Vincent Gay-Bellile","Florian Chabot","Loic Barthe"],"pdf_url":"https://arxiv.org/pdf/2312.03357v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09401v1","updated":"2024-03-14T13:52:03Z","published":"2024-03-14T13:52:03Z","title":"Unsupervised Modality-Transferable Video Highlight Detection with\n  Representation Activation Sequence Learning","summary":"  Identifying highlight moments of raw video materials is crucial for improving\nthe efficiency of editing videos that are pervasive on internet platforms.\nHowever, the extensive work of manually labeling footage has created obstacles\nto applying supervised methods to videos of unseen categories. The absence of\nan audio modality that contains valuable cues for highlight detection in many\nvideos also makes it difficult to use multimodal strategies. In this paper, we\npropose a novel model with cross-modal perception for unsupervised highlight\ndetection. The proposed model learns representations with visual-audio level\nsemantics from image-audio pair data via a self-reconstruction task. To achieve\nunsupervised highlight detection, we investigate the latent representations of\nthe network and propose the representation activation sequence learning (RASL)\nmodule with k-point contrastive learning to learn significant representation\nactivations. To connect the visual modality with the audio modality, we use the\nsymmetric contrastive learning (SCL) module to learn the paired visual and\naudio representations. Furthermore, an auxiliary task of masked feature vector\nsequence (FVS) reconstruction is simultaneously conducted during pretraining\nfor representation enhancement. During inference, the cross-modal pretrained\nmodel can generate representations with paired visual-audio semantics given\nonly the visual modality. The RASL module is used to output the highlight\nscores. The experimental results show that the proposed framework achieves\nsuperior performance compared to other state-of-the-art approaches.\n","authors":["Tingtian Li","Zixun Sun","Xinyu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.09401v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing, 2024"},{"id":"http://arxiv.org/abs/2403.09400v1","updated":"2024-03-14T13:50:44Z","published":"2024-03-14T13:50:44Z","title":"ConDiSR: Contrastive Disentanglement and Style Regularization for Single\n  Domain Generalization","summary":"  Medical data often exhibits distribution shifts, which cause test-time\nperformance degradation for deep learning models trained using standard\nsupervised learning pipelines. This challenge is addressed in the field of\nDomain Generalization (DG) with the sub-field of Single Domain Generalization\n(SDG) being specifically interesting due to the privacy- or logistics-related\nissues often associated with medical data. Existing disentanglement-based SDG\nmethods heavily rely on structural information embedded in segmentation masks,\nhowever classification labels do not provide such dense information. This work\nintroduces a novel SDG method aimed at medical image classification that\nleverages channel-wise contrastive disentanglement. It is further enhanced with\nreconstruction-based style regularization to ensure extraction of distinct\nstyle and structure feature representations. We evaluate our method on the\ncomplex task of multicenter histopathology image classification, comparing it\nagainst state-of-the-art (SOTA) SDG baselines. Results demonstrate that our\nmethod surpasses the SOTA by a margin of 1% in average accuracy while also\nshowing more stable performance. This study highlights the importance and\nchallenges of exploring SDG frameworks in the context of the classification\ntask. The code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/ConDiSR\n","authors":["Aleksandr Matsun","Numan Saeed","Fadillah Adamsyah Maani","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.09400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09394v1","updated":"2024-03-14T13:47:41Z","published":"2024-03-14T13:47:41Z","title":"GiT: Towards Generalist Vision Transformer through Universal Language\n  Interface","summary":"  This paper proposes a simple, yet effective framework, called GiT,\nsimultaneously applicable for various vision tasks only with a vanilla ViT.\nMotivated by the universality of the Multi-layer Transformer architecture (e.g,\nGPT) widely used in large language models (LLMs), we seek to broaden its scope\nto serve as a powerful vision foundation model (VFM). However, unlike language\nmodeling, visual tasks typically require specific modules, such as bounding box\nheads for detection and pixel decoders for segmentation, greatly hindering the\napplication of powerful multi-layer transformers in the vision domain. To solve\nthis, we design a universal language interface that empowers the successful\nauto-regressive decoding to adeptly unify various visual tasks, from\nimage-level understanding (e.g., captioning), over sparse perception (e.g.,\ndetection), to dense prediction (e.g., segmentation). Based on the above\ndesigns, the entire model is composed solely of a ViT, without any specific\nadditions, offering a remarkable architectural simplification. GiT is a\nmulti-task visual model, jointly trained across five representative benchmarks\nwithout task-specific fine-tuning. Interestingly, our GiT builds a new\nbenchmark in generalist performance, and fosters mutual enhancement across\ntasks, leading to significant improvements compared to isolated training. This\nreflects a similar impact observed in LLMs. Further enriching training with 27\ndatasets, GiT achieves strong zero-shot results over various tasks. Due to its\nsimple design, this paradigm holds promise for narrowing the architectural gap\nbetween vision and language. Code and models will be available at\n\\url{https://github.com/Haiyang-W/GiT}.\n","authors":["Haiyang Wang","Hao Tang","Li Jiang","Shaoshuai Shi","Muhammad Ferjad Naeem","Hongsheng Li","Bernt Schiele","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09392v1","updated":"2024-03-14T13:45:09Z","published":"2024-03-14T13:45:09Z","title":"Event-based Asynchronous HDR Imaging by Temporal Incident Light\n  Modulation","summary":"  Dynamic Range (DR) is a pivotal characteristic of imaging systems. Current\nframe-based cameras struggle to achieve high dynamic range imaging due to the\nconflict between globally uniform exposure and spatially variant scene\nillumination. In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR\nimaging system, based on key insights into the challenges in HDR imaging and\nthe unique event-generating mechanism of Dynamic Vision Sensors (DVS). Our\nproposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD\npanels modulate the irradiance incident upon the DVS by altering their\ntransparency, thereby triggering the pixel-independent event streams. The HDR\nimage is subsequently decoded from the event streams through our\ntemporal-weighted algorithm. Experiments under standard test platform and\nseveral challenging scenes have verified the feasibility of the system in HDR\nimaging task.\n","authors":["Yuliang Wu","Ganchao Tan","Jinze Chen","Wei Zhai","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2403.09392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07190v2","updated":"2024-03-14T13:40:56Z","published":"2023-12-12T11:53:23Z","title":"Shifted Autoencoders for Point Annotation Restoration in Object Counting","summary":"  Object counting typically uses 2D point annotations. The complexity of object\nshapes and the subjectivity of annotators may lead to annotation inconsistency,\npotentially confusing counting model training. Some sophisticated\nnoise-resistance counting methods have been proposed to alleviate this issue.\nDifferently, we aim to directly refine the initial point annotations before\ntraining counting models. For that, we propose the Shifted Autoencoders (SAE),\nwhich enhances annotation consistency. Specifically, SAE applies random shifts\nto initial point annotations and employs a UNet to restore them to their\noriginal positions. Similar to MAE reconstruction, the trained SAE captures\ngeneral position knowledge and ignores specific manual offset noise. This\nallows to restore the initial point annotations to more general and thus\nconsistent positions. Extensive experiments show that using such refined\nconsistent annotations to train some advanced (including noise-resistance)\nobject counting models steadily/significantly boosts their performances.\nRemarkably, the proposed SAE helps to set new records on nine datasets. We will\nmake codes and refined point annotations available.\n","authors":["Yuda Zou","Xin Xiao","Peilin Zhou","Zhichao Sun","Bo Du","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12751v2","updated":"2024-03-14T13:38:53Z","published":"2023-11-21T17:52:30Z","title":"Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with\n  Spatial Relation Matching","summary":"  Navigating drones through natural language commands remains challenging due\nto the dearth of accessible multi-modal datasets and the stringent precision\nrequirements for aligning visual and textual data. To address this pressing\nneed, we introduce GeoText-1652, a new natural language-guided geo-localization\nbenchmark. This dataset is systematically constructed through an interactive\nhuman-computer process leveraging Large Language Model (LLM) driven annotation\ntechniques in conjunction with pre-trained vision models. GeoText-1652 extends\nthe established University-1652 image dataset with spatial-aware text\nannotations, thereby establishing one-to-one correspondences between image,\ntext, and bounding box elements. We further introduce a new optimization\nobjective to leverage fine-grained spatial associations, called blending\nspatial matching, for region-level spatial relation matching. Extensive\nexperiments reveal that our approach maintains a competitive recall rate\ncomparing other prevailing cross-modality methods. This underscores the\npromising potential of our approach in elevating drone control and navigation\nthrough the seamless integration of natural language commands in real-world\nscenarios.\n","authors":["Meng Chu","Zhedong Zheng","Wei Ji","Tingyu Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.12751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09380v1","updated":"2024-03-14T13:31:56Z","published":"2024-03-14T13:31:56Z","title":"Impact of Synthetic Images on Morphing Attack Detection Using a Siamese\n  Network","summary":"  This paper evaluated the impact of synthetic images on Morphing Attack\nDetection (MAD) using a Siamese network with a semi-hard-loss function. Intra\nand cross-dataset evaluations were performed to measure synthetic image\ngeneralisation capabilities using a cross-dataset for evaluation. Three\ndifferent pre-trained networks were used as feature extractors from traditional\nMobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained\non EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in\ncomparison with SOTA. Conversely, worse performances were reached when the\nsystem was trained only with synthetic images. A mixed approach (synthetic +\ndigital) database may help to improve MAD and reduce the error rate. This fact\nshows that we still need to keep going with our efforts to include synthetic\nimages in the training process.\n","authors":["Juan Tapia","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2403.09380v1.pdf","comment":"Arxiv version of CIARP2023"},{"id":"http://arxiv.org/abs/2403.09377v1","updated":"2024-03-14T13:27:42Z","published":"2024-03-14T13:27:42Z","title":"Introducing Routing Functions to Vision-Language Parameter-Efficient\n  Fine-Tuning with Low-Rank Bottlenecks","summary":"  Mainstream parameter-efficient fine-tuning (PEFT) methods, such as LoRA or\nAdapter, project a model's hidden states to a lower dimension, allowing\npre-trained models to adapt to new data through this low-rank bottleneck.\nHowever, PEFT tasks involving multiple modalities, like vision-language (VL)\ntasks, require not only adaptation to new data but also learning the\nrelationship between different modalities. Targeting at VL PEFT tasks, we\npropose a family of operations, called routing functions, to enhance VL\nalignment in the low-rank bottlenecks. The routing functions adopt linear\noperations and do not introduce new trainable parameters. In-depth analyses are\nconducted to study their behavior. In various VL PEFT settings, the routing\nfunctions significantly improve performance of the original PEFT methods,\nachieving over 20% improvement on VQAv2\n($\\text{RoBERTa}_{\\text{large}}$+ViT-L/16) and 30% on COCO Captioning\n(GPT2-medium+ViT-L/16). Also when fine-tuning a pre-trained multimodal model\nsuch as CLIP-BART, we observe smaller but consistent improvements across a\nrange of VL PEFT tasks.\n","authors":["Tingyu Qu","Tinne Tuytelaars","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2403.09377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17098v3","updated":"2024-03-14T13:22:51Z","published":"2023-11-28T07:52:41Z","title":"DyRA: Portable Dynamic Resolution Adjustment Network for Existing\n  Detectors","summary":"  Achieving constant accuracy in object detection is challenging due to the\ninherent variability of object sizes. One effective approach to this problem\ninvolves optimizing input resolution, referred to as a multi-resolution\nstrategy. Previous approaches to resolution optimization have often been based\non pre-defined resolutions with manual selection. However, there is a lack of\nstudy on run-time resolution optimization for existing architectures. This\npaper introduces DyRA, a dynamic resolution adjustment network providing an\nimage-specific scale factor for existing detectors. This network is co-trained\nwith detectors utilizing specially designed loss functions, namely\nParetoScaleLoss and BalanceLoss. ParetoScaleLoss determines an adaptive scale\nfactor for robustness, while BalanceLoss optimizes overall scale factors\naccording to the localization performance of the detector. The loss function is\ndevised to minimize the accuracy drop across contrasting objectives of\ndifferent-sized objects for scaling. Our proposed network can improve accuracy\nacross various models, including RetinaNet, Faster-RCNN, FCOS, DINO, and\nH-Deformable-DETR. The code is available at\nhttps://github.com/DaEunFullGrace/DyRA.git.\n","authors":["Daeun Seo","Hoeseok Yang","Hyungshin Kim"],"pdf_url":"https://arxiv.org/pdf/2311.17098v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09367v1","updated":"2024-03-14T13:15:46Z","published":"2024-03-14T13:15:46Z","title":"DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local\n  Climate Zone Classification","summary":"  Recent advancements in remote sensing (RS) technologies have shown their\npotential in accurately classifying local climate zones (LCZs). However,\ntraditional scene-level methods using convolutional neural networks (CNNs)\noften struggle to integrate prior knowledge of ground objects effectively.\nMoreover, commonly utilized data sources like Sentinel-2 encounter difficulties\nin capturing detailed ground object information. To tackle these challenges, we\npropose a data fusion method that integrates ground object priors extracted\nfrom high-resolution Google imagery with Sentinel-2 multispectral imagery. The\nproposed method introduces a novel Dual-stream Fusion framework for LCZ\nclassification (DF4LCZ), integrating instance-based location features from\nGoogle imagery with the scene-level spatial-spectral features extracted from\nSentinel-2 imagery. The framework incorporates a Graph Convolutional Network\n(GCN) module empowered by the Segment Anything Model (SAM) to enhance feature\nextraction from Google imagery. Simultaneously, the framework employs a 3D-CNN\narchitecture to learn the spectral-spatial features of Sentinel-2 imagery.\nExperiments are conducted on a multi-source remote sensing image dataset\nspecifically designed for LCZ classification, validating the effectiveness of\nthe proposed DF4LCZ. The related code and dataset are available at\nhttps://github.com/ctrlovefly/DF4LCZ.\n","authors":["Qianqian Wu","Xianping Ma","Jialu Sui","Man-On Pun"],"pdf_url":"https://arxiv.org/pdf/2403.09367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07560v2","updated":"2024-03-14T13:13:21Z","published":"2024-03-12T11:48:49Z","title":"Unleashing Network Potentials for Semantic Scene Completion","summary":"  Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy\nand semantics from a single-view RGB-D image, and recent SSC methods commonly\nadopt multi-modal inputs. However, our investigation reveals two limitations:\nineffective feature learning from single modalities and overfitting to limited\ndatasets. To address these issues, this paper proposes a novel SSC framework -\nAdversarial Modality Modulation Network (AMMNet) - with a fresh perspective of\noptimizing gradient updates. The proposed AMMNet introduces two core modules: a\ncross-modal modulation enabling the interdependence of gradient flows between\nmodalities, and a customized adversarial training scheme leveraging dynamic\ngradient competition. Specifically, the cross-modal modulation adaptively\nre-calibrates the features to better excite representation potentials from each\nsingle modality. The adversarial training employs a minimax game of evolving\ngradients, with customized guidance to strengthen the generator's perception of\nvisual fidelity from both geometric completeness and semantic correctness.\nExtensive experimental results demonstrate that AMMNet outperforms\nstate-of-the-art SSC methods by a large margin, providing a promising direction\nfor improving the effectiveness and generalization of SSC methods.\n","authors":["Fengyun Wang","Qianru Sun","Dong Zhang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07560v2.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.09363v1","updated":"2024-03-14T13:12:49Z","published":"2024-03-14T13:12:49Z","title":"Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without\n  Real Data Exposure","summary":"  With increasing concerns over data privacy and model copyrights, especially\nin the context of collaborations between AI service providers and data owners,\nan innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to\nfoster efficient collaboration without the need to exchange models or sensitive\ndata. It consists of a teacher model, a student model and a generator that\nlinks both model entities. The teacher model serves as a sentinel on behalf of\nthe data owner, replacing real data, to guide the student model at the AI\nservice provider's end during training. Considering the disparity of knowledge\nspace between the teacher and student, we introduce two variants of the teacher\nmodel: the omniscient and the quasi-omniscient teachers. Under these teachers'\nguidance, the student model seeks to match the teacher model's performance and\nexplores domains that the teacher has not covered. To trade off between privacy\nand performance, we further introduce two distinct security-level training\nprotocols: white-box and black-box, enhancing the paradigm's adaptability.\nDespite the inherent challenges of real data absence in the SG-ZSL paradigm, it\nconsistently outperforms in ZSL and GZSL tasks, notably in the white-box\nprotocol. Our comprehensive evaluation further attests to its robustness and\nefficiency across various setups, including stringent black-box training\nprotocol.\n","authors":["Fan Wan","Xingyu Miao","Haoran Duan","Jingjing Deng","Rui Gao","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2403.09363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03244v2","updated":"2024-03-14T13:08:20Z","published":"2023-09-06T08:50:04Z","title":"EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by\n  Semantic Segmentation","summary":"  We introduce EGIC, an enhanced generative image compression method that\nallows traversing the distortion-perception curve efficiently from a single\nmodel. EGIC is based on two novel building blocks: i) OASIS-C, a conditional\npre-trained semantic segmentation-guided discriminator, which provides both\nspatially and semantically-aware gradient feedback to the generator,\nconditioned on the latent image distribution, and ii) Output Residual\nPrediction (ORP), a retrofit solution for multi-realism image compression that\nallows control over the synthesis process by adjusting the impact of the\nresidual between an MSE-optimized and GAN-optimized decoder output on the\nGAN-based reconstruction. Together, EGIC forms a powerful codec, outperforming\nstate-of-the-art diffusion and GAN-based methods (e.g., HiFiC, MS-ILLM, and\nDIRAC-100), while performing almost on par with VTM-20.0 on the distortion end.\nEGIC is simple to implement, very lightweight, and provides excellent\ninterpolation characteristics, which makes it a promising candidate for\npractical applications targeting the low bit range.\n","authors":["Nikolai Körber","Eduard Kromer","Andreas Siebert","Sascha Hauke","Daniel Mueller-Gritschneder","Björn Schuller"],"pdf_url":"https://arxiv.org/pdf/2309.03244v2.pdf","comment":"revised version"},{"id":"http://arxiv.org/abs/2403.09359v1","updated":"2024-03-14T13:05:43Z","published":"2024-03-14T13:05:43Z","title":"D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap\n  for Domain-Adaptive Object Detection","summary":"  Domain adaptation for object detection typically entails transferring\nknowledge from one visible domain to another visible domain. However, there are\nlimited studies on adapting from the visible to the thermal domain, because the\ndomain gap between the visible and thermal domains is much larger than\nexpected, and traditional domain adaptation can not successfully facilitate\nlearning in this situation. To overcome this challenge, we propose a\nDistinctive Dual-Domain Teacher (D3T) framework that employs distinct training\nparadigms for each domain. Specifically, we segregate the source and target\ntraining sets for building dual-teachers and successively deploy exponential\nmoving average to the student model to individual teachers of each domain. The\nframework further incorporates a zigzag learning method between dual teachers,\nfacilitating a gradual transition from the visible to thermal domains during\ntraining. We validate the superiority of our method through newly designed\nexperimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.\nSource code is available at https://github.com/EdwardDo69/D3T .\n","authors":["Dinh Phat Do","Taehoon Kim","Jaemin Na","Jiwon Kim","Keonho Lee","Kyunghwan Cho","Wonjun Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.09359v1.pdf","comment":"Accepted by CVPR 2024. Link: https://github.com/EdwardDo69/D3T"},{"id":"http://arxiv.org/abs/2311.14064v2","updated":"2024-03-14T13:03:53Z","published":"2023-11-23T15:42:42Z","title":"HGCLIP: Exploring Vision-Language Models with Graph Representations for\n  Hierarchical Understanding","summary":"  Object categories are typically organized into a multi-granularity taxonomic\nhierarchy. When classifying categories at different hierarchy levels,\ntraditional uni-modal approaches focus primarily on image features, revealing\nlimitations in complex scenarios. Recent studies integrating Vision-Language\nModels (VLMs) with class hierarchies have shown promise, yet they fall short of\nfully exploiting the hierarchical relationships. These efforts are constrained\nby their inability to perform effectively across varied granularity of\ncategories. To tackle this issue, we propose a novel framework (HGCLIP) that\neffectively combines CLIP with a deeper exploitation of the Hierarchical class\nstructure via Graph representation learning. We explore constructing the class\nhierarchy into a graph, with its nodes representing the textual or image\nfeatures of each category. After passing through a graph encoder, the textual\nfeatures incorporate hierarchical structure information, while the image\nfeatures emphasize class-aware features derived from prototypes through the\nattention mechanism. Our approach demonstrates significant improvements on 11\ndiverse visual recognition benchmarks. Our codes are fully available at\nhttps://github.com/richard-peng-xia/HGCLIP.\n","authors":["Peng Xia","Xingtong Yu","Ming Hu","Lie Ju","Zhiyong Wang","Peibo Duan","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2311.14064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09355v1","updated":"2024-03-14T12:58:28Z","published":"2024-03-14T12:58:28Z","title":"Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion\n  Models for Sparse-view CT Reconstruction","summary":"  Sparse-view Computed Tomography (CT) image reconstruction is a promising\napproach to reduce radiation exposure, but it inevitably leads to image\ndegradation. Although diffusion model-based approaches are computationally\nexpensive and suffer from the training-sampling discrepancy, they provide a\npotential solution to the problem. This study introduces a novel Cascaded\nDiffusion with Discrepancy Mitigation (CDDM) framework, including the\nlow-quality image generation in latent space and the high-quality image\ngeneration in pixel space which contains data consistency and discrepancy\nmitigation in a one-step reconstruction process. The cascaded framework\nminimizes computational costs by moving some inference steps from pixel space\nto latent space. The discrepancy mitigation technique addresses the\ntraining-sampling gap induced by data consistency, ensuring the data\ndistribution is close to the original manifold. A specialized Alternating\nDirection Method of Multipliers (ADMM) is employed to process image gradients\nin separate directions, offering a more targeted approach to regularization.\nExperimental results across two datasets demonstrate CDDM's superior\nperformance in high-quality image generation with clearer boundaries compared\nto existing methods, highlighting the framework's computational efficiency.\n","authors":["Hanyu Chen","Zhixiu Hao","Lin Guo","Liying Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.09355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09346v1","updated":"2024-03-14T12:51:07Z","published":"2024-03-14T12:51:07Z","title":"AVIBench: Towards Evaluating the Robustness of Large Vision-Language\n  Model on Adversarial Visual-Instructions","summary":"  Large Vision-Language Models (LVLMs) have shown significant progress in well\nresponding to visual-instructions from users. However, these instructions,\nencompassing images and text, are susceptible to both intentional and\ninadvertent attacks. Despite the critical importance of LVLMs' robustness\nagainst such threats, current research in this area remains limited. To bridge\nthis gap, we introduce AVIBench, a framework designed to analyze the robustness\nof LVLMs when facing various adversarial visual-instructions (AVIs), including\nfour types of image-based AVIs, ten types of text-based AVIs, and nine types of\ncontent bias AVIs (such as gender, violence, cultural, and racial biases, among\nothers). We generate 260K AVIs encompassing five categories of multimodal\ncapabilities (nine tasks) and content bias. We then conduct a comprehensive\nevaluation involving 14 open-source LVLMs to assess their performance. AVIBench\nalso serves as a convenient tool for practitioners to evaluate the robustness\nof LVLMs against AVIs. Our findings and extensive experimental results shed\nlight on the vulnerabilities of LVLMs, and highlight that inherent biases exist\neven in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This\nunderscores the importance of enhancing the robustness, security, and fairness\nof LVLMs. The source code and benchmark will be made publicly available.\n","authors":["Hao Zhang","Wenqi Shao","Hong Liu","Yongqiang Ma","Ping Luo","Yu Qiao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.09346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09344v1","updated":"2024-03-14T12:49:29Z","published":"2024-03-14T12:49:29Z","title":"SketchINR: A First Look into Sketches as Implicit Neural Representations","summary":"  We propose SketchINR, to advance the representation of vector sketches with\nimplicit neural models. A variable length vector sketch is compressed into a\nlatent space of fixed dimension that implicitly encodes the underlying shape as\na function of time and strokes. The learned function predicts the $xy$ point\ncoordinates in a sketch at each time and stroke. Despite its simplicity,\nSketchINR outperforms existing representations at multiple tasks: (i) Encoding\nan entire sketch dataset into a fixed size latent vector, SketchINR gives\n$60\\times$ and $10\\times$ data compression over raster and vector sketches,\nrespectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity\nrepresentation than other learned vector sketch representations, and is\nuniquely able to scale to complex vector sketches such as FS-COCO. (iii)\nSketchINR supports parallelisation that can decode/render $\\sim$$100\\times$\nfaster than other learned vector representations such as SketchRNN. (iv)\nSketchINR, for the first time, emulates the human ability to reproduce a sketch\nwith varying abstraction in terms of number and complexity of strokes. As a\nfirst look at implicit sketches, SketchINR's compact high-fidelity\nrepresentation will support future work in modelling long and complex sketches.\n","authors":["Hmrishav Bandyopadhyay","Ayan Kumar Bhunia","Pinaki Nath Chowdhury","Aneeshan Sain","Tao Xiang","Timothy Hospedales","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2403.09344v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09338v1","updated":"2024-03-14T12:32:40Z","published":"2024-03-14T12:32:40Z","title":"LocalMamba: Visual State Space Model with Windowed Selective Scan","summary":"  Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba.\n","authors":["Tao Huang","Xiaohuan Pei","Shan You","Fei Wang","Chen Qian","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.09338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00377v4","updated":"2024-03-14T12:29:29Z","published":"2023-12-01T06:48:03Z","title":"SynFundus-1M: A High-quality Million-scale Synthetic fundus images\n  Dataset with Fifteen Types of Annotation","summary":"  Large-scale public datasets with high-quality annotations are rarely\navailable for intelligent medical imaging research, due to data privacy\nconcerns and the cost of annotations. In this paper, we release SynFundus-1M, a\nhigh-quality synthetic dataset containing over one million fundus images in\nterms of \\textbf{eleven disease types}. Furthermore, we deliberately assign\nfour readability labels to the key regions of the fundus images. To the best of\nour knowledge, SynFundus-1M is currently the largest fundus dataset with the\nmost sophisticated annotations. Leveraging over 1.3 million private authentic\nfundus images from various scenarios, we trained a powerful Denoising Diffusion\nProbabilistic Model, named SynFundus-Generator. The released SynFundus-1M are\ngenerated by SynFundus-Generator under predefined conditions. To demonstrate\nthe value of SynFundus-1M, extensive experiments are designed in terms of the\nfollowing aspect: 1) Authenticity of the images: we randomly blend the\nsynthetic images with authentic fundus images, and find that experienced\nannotators can hardly distinguish the synthetic images from authentic ones.\nMoreover, we show that the disease-related vision features (e.g. lesions) are\nwell simulated in the synthetic images. 2) Effectiveness for down-stream\nfine-tuning and pretraining: we demonstrate that retinal disease diagnosis\nmodels of either convolutional neural networks (CNN) or Vision Transformer\n(ViT) architectures can benefit from SynFundus-1M, and compared to the datasets\ncommonly used for pretraining, models trained on SynFundus-1M not only achieve\nsuperior performance but also demonstrate faster convergence on various\ndownstream tasks. SynFundus-1M is already public available for the open-source\ncommunity.\n","authors":["Fangxin Shang","Jie Fu","Yehui Yang","Haifeng Huang","Junwei Liu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2312.00377v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08568v2","updated":"2024-03-14T12:26:17Z","published":"2024-03-13T14:24:09Z","title":"Consistent Prompting for Rehearsal-Free Continual Learning","summary":"  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n","authors":["Zhanxin Gao","Jun Cen","Xiaobin Chang"],"pdf_url":"https://arxiv.org/pdf/2403.08568v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.09334v1","updated":"2024-03-14T12:22:54Z","published":"2024-03-14T12:22:54Z","title":"Video Editing via Factorized Diffusion Distillation","summary":"  We introduce Emu Video Edit (EVE), a model that establishes a new\nstate-of-the art in video editing without relying on any supervised video\nediting data. To develop EVE we separately train an image editing adapter and a\nvideo generation adapter, and attach both to the same text-to-image model.\nThen, to align the adapters towards video editing we introduce a new\nunsupervised distillation procedure, Factorized Diffusion Distillation. This\nprocedure distills knowledge from one or more teachers simultaneously, without\nany supervised data. We utilize this procedure to teach EVE to edit videos by\njointly distilling knowledge to (i) precisely edit each individual frame from\nthe image editing adapter, and (ii) ensure temporal consistency among the\nedited frames using the video generation adapter. Finally, to demonstrate the\npotential of our approach in unlocking other capabilities, we align additional\ncombinations of adapters\n","authors":["Uriel Singer","Amit Zohar","Yuval Kirstain","Shelly Sheynin","Adam Polyak","Devi Parikh","Yaniv Taigman"],"pdf_url":"https://arxiv.org/pdf/2403.09334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09333v1","updated":"2024-03-14T12:21:37Z","published":"2024-03-14T12:21:37Z","title":"Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling\n  and Visual-Language Co-Referring","summary":"  Large Vision Language Models have achieved fine-grained object perception,\nbut the limitation of image resolution remains a significant obstacle to\nsurpass the performance of task-specific experts in complex and dense\nscenarios. Such limitation further restricts the model's potential to achieve\nnuanced visual and language referring in domains such as GUI Agents, Counting\nand \\etc. To address this issue, we introduce a unified high-resolution\ngeneralist model, Griffon v2, enabling flexible object referring with visual\nand textual prompts. To efficiently scaling up image resolution, we design a\nsimple and lightweight down-sampling projector to overcome the input tokens\nconstraint in Large Language Models. This design inherently preserves the\ncomplete contexts and fine details, and significantly improves multimodal\nperception ability especially for small objects. Building upon this, we further\nequip the model with visual-language co-referring capabilities through a\nplug-and-play visual tokenizer. It enables user-friendly interaction with\nflexible target images, free-form texts and even coordinates. Experiments\ndemonstrate that Griffon v2 can localize any objects of interest with visual\nand textual referring, achieve state-of-the-art performance on REC, phrase\ngrounding, and REG tasks, and outperform expert models in object detection and\nobject counting. Data, codes and models will be released at\nhttps://github.com/jefferyZhan/Griffon.\n","authors":["Yufei Zhan","Yousong Zhu","Hongyin Zhao","Fan Yang","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09333v1.pdf","comment":"Tech report working in progress. Codes, models and datasets will be\n  released at https://github.com/jefferyZhan/Griffon"},{"id":"http://arxiv.org/abs/2403.09327v1","updated":"2024-03-14T12:17:07Z","published":"2024-03-14T12:17:07Z","title":"Perspective-Equivariant Imaging: an Unsupervised Framework for\n  Multispectral Pansharpening","summary":"  Ill-posed image reconstruction problems appear in many scenarios such as\nremote sensing, where obtaining high quality images is crucial for\nenvironmental monitoring, disaster management and urban planning. Deep learning\nhas seen great success in overcoming the limitations of traditional methods.\nHowever, these inverse problems rarely come with ground truth data,\nhighlighting the importance of unsupervised learning from partial and noisy\nmeasurements alone. We propose perspective-equivariant imaging (EI), a\nframework that leverages perspective variability in optical camera-based\nimaging systems, such as satellites or handheld cameras, to recover information\nlost in ill-posed optical camera imaging problems. This extends previous EI\nwork to include a much richer non-linear class of group transforms and is shown\nto be an excellent prior for satellite and urban image data, where\nperspective-EI achieves state-of-the-art results in multispectral\npansharpening, outperforming other unsupervised methods in the literature. Code\nat https://andrewwango.github.io/perspective-equivariant-imaging\n","authors":["Andrew Wang","Mike Davies"],"pdf_url":"https://arxiv.org/pdf/2403.09327v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2403.08733v2","updated":"2024-03-14T12:16:05Z","published":"2024-03-13T17:35:28Z","title":"GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting\n  Editing","summary":"  We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed\nby the 3D Gaussian Splatting (3DGS).\n  Our method first renders a collection of images by using the 3DGS and edits\nthem by using a pre-trained 2D diffusion model (ControlNet) based on the input\nprompt, which is then used to optimise the 3D model.\n  Our key contribution is multi-view consistent editing, which enables editing\nall images together instead of iteratively editing one image while updating the\n3D model as in previous works.\n  It leads to faster editing as well as higher visual quality.\n  This is achieved by the two terms:\n  (a) depth-conditioned editing that enforces geometric consistency across\nmulti-view images by leveraging naturally consistent depth maps.\n  (b) attention-based latent code alignment that unifies the appearance of\nedited images by conditioning their editing to several reference views through\nself and cross-view attention between images' latent representations.\n  Experiments demonstrate that our method achieves faster editing and better\nvisual results than previous state-of-the-art methods.\n","authors":["Jing Wu","Jia-Wang Bian","Xinghui Li","Guangrun Wang","Ian Reid","Philip Torr","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2403.08733v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.09323v1","updated":"2024-03-14T12:12:17Z","published":"2024-03-14T12:12:17Z","title":"EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion\n  Detection","summary":"  Multimodal image fusion and object detection play a vital role in autonomous\ndriving. Current joint learning methods have made significant progress in the\nmultimodal fusion detection task combining the texture detail and objective\nsemantic information. However, the tedious training steps have limited its\napplications to wider real-world industrial deployment. To address this\nlimitation, we propose a novel end-to-end multimodal fusion detection\nalgorithm, named EfficientMFD, to simplify models that exhibit decent\nperformance with only one training step. Synchronous joint optimization is\nutilized in an end-to-end manner between two components, thus not being\naffected by the local optimal solution of the individual task. Besides, a\ncomprehensive optimization is established in the gradient matrix between the\nshared parameters for both tasks. It can converge to an optimal point with\nfusion detection weights. We extensively test it on several public datasets,\ndemonstrating superior performance on not only visually appealing fusion but\nalso favorable detection performance (e.g., 6.6% mAP50:95) over other\nstate-of-the-art approaches.\n","authors":["Jiaqing Zhang","Mingxiang Cao","Xue Yang","Weiying Xie","Jie Lei","Daixun Li","Geng Yang","Wenbo Huang","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2403.09323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09317v1","updated":"2024-03-14T12:08:44Z","published":"2024-03-14T12:08:44Z","title":"SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D\n  Pose Estimation In Bin-picking Scenarios","summary":"  Despite the success in 6D pose estimation in bin-picking scenarios, existing\nmethods still struggle to produce accurate prediction results for symmetry\nobjects and real world scenarios. The primary bottlenecks include 1) the\nambiguity keypoints caused by object symmetries; 2) the domain gap between real\nand synthetic data. To circumvent these problem, we propose a new 6D pose\nestimation network with symmetric-aware keypoint prediction and self-training\ndomain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and\ndeep hough voting to perform reliable detection keypoint under clutter and\nocclusion. Specifically, at the keypoint prediction stage, we designe a robust\n3D keypoints selection strategy considering the symmetry class of objects and\nequivalent keypoints, which facilitate locating 3D keypoints even in highly\noccluded scenes. Additionally, we build an effective filtering algorithm on\npredicted keypoint to dynamically eliminate multiple ambiguity and outlier\nkeypoint candidates. At the domain adaptation stage, we propose the\nself-training framework using a student-teacher training scheme. To carefully\ndistinguish reliable predictions, we harnesses a tailored heuristics for 3D\ngeometry pseudo labelling based on semi-chamfer distance. On public Sil'eane\ndataset, SD-Net achieves state-of-the-art results, obtaining an average\nprecision of 96%. Testing learning and generalization abilities on public\nParametric datasets, SD-Net is 8% higher than the state-of-the-art method. The\ncode is available at https://github.com/dingthuang/SD-Net.\n","authors":["Ding-Tao Huang","En-Te Lin","Lipeng Chen","Li-Fu Liu","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.09317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10491v4","updated":"2024-03-14T12:06:10Z","published":"2023-09-19T09:59:08Z","title":"DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs","summary":"  Existing nighttime unmanned aerial vehicle (UAV) trackers follow an\n\"Enhance-then-Track\" architecture - first using a light enhancer to brighten\nthe nighttime video, then employing a daytime tracker to locate the object.\nThis separate enhancement and tracking fails to build an end-to-end trainable\nvision system. To address this, we propose a novel architecture called Darkness\nClue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by\nefficiently learning to generate darkness clue prompts. Without a separate\nenhancer, DCPT directly encodes anti-dark capabilities into prompts using a\ndarkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing\nand undermining projections for darkness clues. It then injects these learned\nvisual prompts into a daytime tracker with fixed parameters across transformer\nlayers. Moreover, a gated feature aggregation mechanism enables adaptive fusion\nbetween prompts and between prompts and the base model. Extensive experiments\nshow state-of-the-art performance for DCPT on multiple dark scenario\nbenchmarks. The unified end-to-end learning of enhancement and tracking in DCPT\nenables a more trainable system. The darkness clue prompting efficiently\ninjects anti-dark knowledge without extra modules. Code is available at\nhttps://github.com/bearyi26/DCPT.\n","authors":["Jiawen Zhu","Huayi Tang","Zhi-Qi Cheng","Jun-Yan He","Bin Luo","Shihao Qiu","Shengming Li","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2309.10491v4.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.09315v1","updated":"2024-03-14T12:05:25Z","published":"2024-03-14T12:05:25Z","title":"Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation\n  with Limited Annotations","summary":"  Accurate identification of breast masses is crucial in diagnosing breast\ncancer; however, it can be challenging due to their small size and being\ncamouflaged in surrounding normal glands. Worse still, it is also expensive in\nclinical practice to obtain adequate pixel-wise annotations for training deep\nneural networks. To overcome these two difficulties with one stone, we propose\na semi- and weakly-supervised learning framework for mass segmentation that\nutilizes limited strongly-labeled samples and sufficient weakly-labeled samples\nto achieve satisfactory performance. The framework consists of an auxiliary\nbranch to exclude lesion-irrelevant background areas, a segmentation branch for\nfinal prediction, and a spatial prompting module to integrate the complementary\ninformation of the two branches. We further disentangle encoded obscure\nfeatures into lesion-related and others to boost performance. Experiments on\nCBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.\n","authors":["Xinyu Xiong","Churan Wang","Wenxue Li","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.09315v1.pdf","comment":"Accepted to IEEE ISBI 2024"},{"id":"http://arxiv.org/abs/2403.09313v1","updated":"2024-03-14T12:03:28Z","published":"2024-03-14T12:03:28Z","title":"Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection","summary":"  In this paper we present YOLOX-ViT, a novel object detection model, and\ninvestigate the efficacy of knowledge distillation for model size reduction\nwithout sacrificing performance. Focused on underwater robotics, our research\naddresses key questions about the viability of smaller models and the impact of\nthe visual transformer layer in YOLOX. Furthermore, we introduce a new\nside-scan sonar image dataset, and use it to evaluate our object detector's\nperformance. Results show that knowledge distillation effectively reduces false\npositives in wall detection. Additionally, the introduced visual transformer\nlayer significantly improves object detection accuracy in the underwater\nenvironment. The source code of the knowledge distillation in the YOLOX-ViT is\nat https://github.com/remaro-network/KD-YOLOX-ViT.\n","authors":["Martin Aubard","László Antal","Ana Madureira","Erika Ábrahám"],"pdf_url":"https://arxiv.org/pdf/2403.09313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13835v2","updated":"2024-03-14T12:01:11Z","published":"2023-09-25T02:45:51Z","title":"IBVC: Interpolation-driven B-frame Video Compression","summary":"  Learned B-frame video compression aims to adopt bi-directional motion\nestimation and motion compensation (MEMC) coding for middle frame\nreconstruction. However, previous learned approaches often directly extend\nneural P-frame codecs to B-frame relying on bi-directional optical-flow\nestimation or video frame interpolation. They suffer from inaccurate quantized\nmotions and inefficient motion compensation. To address these issues, we\npropose a simple yet effective structure called Interpolation-driven B-frame\nVideo Compression (IBVC). Our approach only involves two major operations:\nvideo frame interpolation and artifact reduction compression. IBVC introduces a\nbit-rate free MEMC based on interpolation, which avoids optical-flow\nquantization and additional compression distortions. Later, to reduce duplicate\nbit-rate consumption and focus on unaligned artifacts, a residual guided\nmasking encoder is deployed to adaptively select the meaningful contexts with\ninterpolated multi-scale dependencies. In addition, a conditional\nspatio-temporal decoder is proposed to eliminate location errors and artifacts\ninstead of using MEMC coding in other methods. The experimental results on\nB-frame coding demonstrate that IBVC has significant improvements compared to\nthe relevant state-of-the-art methods. Meanwhile, our approach can save bit\nrates compared with the random access (RA) configuration of H.266 (VTM). The\ncode will be available at https://github.com/ruhig6/IBVC.\n","authors":["Chenming Xu","Meiqin Liu","Chao Yao","Weisi Lin","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2309.13835v2.pdf","comment":"Submitted to Pattern Recognition"},{"id":"http://arxiv.org/abs/2403.09307v1","updated":"2024-03-14T11:57:58Z","published":"2024-03-14T11:57:58Z","title":"Annotation Free Semantic Segmentation with Vision Foundation Models","summary":"  Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel-level\nannotations. With the success of foundation models and especially\nvision-language models, recent works attempt to achieve zero-shot semantic\nsegmentation while requiring either large scale training or additional\nimage/pixel-level annotations. In this work, we build a lightweight module on\ntop of a self-supervised pretrained vision encoder to align patch features with\na pre-trained text encoder. Importantly, we generate free annotations for any\nsemantic segmentation dataset using existing foundation models and train our\nalignment module cost free. We use CLIP to detect objects and SAM to generate\nhigh quality object masks. Our approach can bring language-based semantics to\nany pre-trained vision encoder with minimal training. Our module is\nlightweight, uses foundation models as a sole source of supervision and shows\nimpressive generalization capability from little training data with no\nannotation.\n","authors":["Soroush Seifi","Daniel Olmeda Reino","Fabien Despinoy","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2403.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09303v1","updated":"2024-03-14T11:51:01Z","published":"2024-03-14T11:51:01Z","title":"Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical\n  Perspective","summary":"  Medical anomaly detection aims to identify abnormal findings using only\nnormal training data, playing a crucial role in health screening and\nrecognizing rare diseases. Reconstruction-based methods, particularly those\nutilizing autoencoders (AEs), are dominant in this field. They work under the\nassumption that AEs trained on only normal data cannot reconstruct unseen\nabnormal regions well, thereby enabling the anomaly detection based on\nreconstruction errors. However, this assumption does not always hold due to the\nmismatch between the reconstruction training objective and the anomaly\ndetection task objective, rendering these methods theoretically unsound. This\nstudy focuses on providing a theoretical foundation for AE-based reconstruction\nmethods in anomaly detection. By leveraging information theory, we elucidate\nthe principles of these methods and reveal that the key to improving AE in\nanomaly detection lies in minimizing the information entropy of latent vectors.\nExperiments on four datasets with two image modalities validate the\neffectiveness of our theory. To the best of our knowledge, this is the first\neffort to theoretically clarify the principles and design philosophy of AE for\nanomaly detection. Code will be available upon acceptance.\n","authors":["Yu Cai","Hao Chen","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09302v1","updated":"2024-03-14T11:49:43Z","published":"2024-03-14T11:49:43Z","title":"StainFuser: Controlling Diffusion for Faster Neural Style Transfer in\n  Multi-Gigapixel Histology Images","summary":"  Stain normalization algorithms aim to transform the color and intensity\ncharacteristics of a source multi-gigapixel histology image to match those of a\ntarget image, mitigating inconsistencies in the appearance of stains used to\nhighlight cellular components in the images. We propose a new approach,\nStainFuser, which treats this problem as a style transfer task using a novel\nConditional Latent Diffusion architecture, eliminating the need for handcrafted\ncolor components. With this method, we curate SPI-2M the largest stain\nnormalization dataset to date of over 2 million histology images with neural\nstyle transfer for high-quality transformations. Trained on this data,\nStainFuser outperforms current state-of-the-art GAN and handcrafted methods in\nterms of the quality of normalized images. Additionally, compared to existing\napproaches, it improves the performance of nuclei instance segmentation and\nclassification models when used as a test time augmentation method on the\nchallenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel\nWhole Slide Images (WSIs) and demonstrate improved performance in terms of\ncomputational efficiency, image quality and consistency across tiles over\ncurrent methods.\n","authors":["Robert Jewsbury","Ruoyu Wang","Abhir Bhalerao","Nasir Rajpoot","Quoc Dang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.09302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15230v2","updated":"2024-03-14T11:49:40Z","published":"2023-11-26T08:04:43Z","title":"GAIA: Zero-shot Talking Avatar Generation","summary":"  Zero-shot talking avatar generation aims at synthesizing natural talking\nvideos from speech and a single portrait image. Previous methods have relied on\ndomain-specific heuristics such as warping-based motion representation and 3D\nMorphable Models, which limit the naturalness and diversity of the generated\navatars. In this work, we introduce GAIA (Generative AI for Avatar), which\neliminates the domain priors in talking avatar generation. In light of the\nobservation that the speech only drives the motion of the avatar while the\nappearance of the avatar and the background typically remain the same\nthroughout the entire video, we divide our approach into two stages: 1)\ndisentangling each frame into motion and appearance representations; 2)\ngenerating motion sequences conditioned on the speech and reference portrait\nimage. We collect a large-scale high-quality talking avatar dataset and train\nthe model on it with different scales (up to 2B parameters). Experimental\nresults verify the superiority, scalability, and flexibility of GAIA as 1) the\nresulting model beats previous baseline models in terms of naturalness,\ndiversity, lip-sync quality, and visual quality; 2) the framework is scalable\nsince larger models yield better results; 3) it is general and enables\ndifferent applications like controllable talking avatar generation and\ntext-instructed avatar generation.\n","authors":["Tianyu He","Junliang Guo","Runyi Yu","Yuchi Wang","Jialiang Zhu","Kaikai An","Leyi Li","Xu Tan","Chunyu Wang","Han Hu","HsiangTao Wu","Sheng Zhao","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2311.15230v2.pdf","comment":"ICLR 2024. Project page: https://microsoft.github.io/GAIA/"},{"id":"http://arxiv.org/abs/2403.09296v1","updated":"2024-03-14T11:36:36Z","published":"2024-03-14T11:36:36Z","title":"Select and Distill: Selective Dual-Teacher Knowledge Transfer for\n  Continual Learning on Vision-Language Models","summary":"  Large-scale vision-language models (VLMs) have shown a strong zero-shot\ngeneralization capability on unseen-domain data. However, when adapting\npre-trained VLMs to a sequence of downstream tasks, they are prone to\nforgetting previously learned knowledge and degrade their zero-shot\nclassification capability. To tackle this problem, we propose a unique\nSelective Dual-Teacher Knowledge Transfer framework that leverages the most\nrecent fine-tuned and the original pre-trained VLMs as dual teachers to\npreserve the previously learned knowledge and zero-shot capabilities,\nrespectively. With only access to an unlabeled reference dataset, our proposed\nframework performs a selective knowledge distillation mechanism by measuring\nthe feature discrepancy from the dual teacher VLMs. Consequently, our selective\ndual-teacher knowledge distillation would mitigate catastrophic forgetting of\npreviously learned knowledge while preserving the zero-shot capabilities from\npre-trained VLMs. Through extensive experiments on benchmark datasets, we show\nthat our proposed framework is favorable against state-of-the-art continual\nlearning approaches for preventing catastrophic forgetting and zero-shot\ndegradation.\n","authors":["Yu-Chu Yu","Chi-Pin Huang","Jr-Jen Chen","Kai-Po Chang","Yung-Hsuan Lai","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02638v2","updated":"2024-03-14T11:34:45Z","published":"2023-12-05T10:24:43Z","title":"Synchronization is All You Need: Exocentric-to-Egocentric Transfer for\n  Temporal Action Segmentation with Unlabeled Synchronized Video Pairs","summary":"  We consider the problem of transferring a temporal action segmentation system\ninitially designed for exocentric (fixed) cameras to an egocentric scenario,\nwhere wearable cameras capture video data. The conventional supervised approach\nrequires the collection and labeling of a new set of egocentric videos to adapt\nthe model, which is costly and time-consuming. Instead, we propose a novel\nmethodology which performs the adaptation leveraging existing labeled\nexocentric videos and a new set of unlabeled, synchronized\nexocentric-egocentric video pairs, for which temporal action segmentation\nannotations do not need to be collected. We implement the proposed methodology\nwith an approach based on knowledge distillation, which we investigate both at\nthe feature and Temporal Action Segmentation model level. Experiments on\nAssembly101 and EgoExo4D demonstrate the effectiveness of the proposed method\nagainst classic unsupervised domain adaptation and temporal alignment\napproaches. Without bells and whistles, our best model performs on par with\nsupervised approaches trained on labeled egocentric data, without ever seeing a\nsingle egocentric label, achieving a +15.99 improvement in the edit score\n(28.59 vs 12.60) on the Assembly101 dataset compared to a baseline model\ntrained solely on exocentric data. In similar settings, our method also\nimproves edit score by +3.32 on the challenging EgoExo4D benchmark.\n","authors":["Camillo Quattrocchi","Antonino Furnari","Daniele Di Mauro","Mario Valerio Giuffrida","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2312.02638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09294v1","updated":"2024-03-14T11:29:47Z","published":"2024-03-14T11:29:47Z","title":"Anatomical Structure-Guided Medical Vision-Language Pre-training","summary":"  Learning medical visual representations through vision-language pre-training\nhas reached remarkable progress. Despite the promising performance, it still\nfaces challenges, i.e., local alignment lacks interpretability and clinical\nrelevance, and the insufficient internal and external representation learning\nof image-report pairs. To address these issues, we propose an Anatomical\nStructure-Guided (ASG) framework. Specifically, we parse raw reports into\ntriplets <anatomical region, finding, existence>, and fully utilize each\nelement as supervision to enhance representation learning. For anatomical\nregion, we design an automatic anatomical region-sentence alignment paradigm in\ncollaboration with radiologists, considering them as the minimum semantic units\nto explore fine-grained local alignment. For finding and existence, we regard\nthem as image tags, applying an image-tag recognition decoder to associate\nimage features with their respective tags within each sample and constructing\nsoft labels for contrastive learning to improve the semantic association of\ndifferent image-report pairs. We evaluate the proposed ASG framework on two\ndownstream tasks, including five public benchmarks. Experimental results\ndemonstrate that our method outperforms the state-of-the-art methods.\n","authors":["Qingqiu Li","Xiaohan Yan","Jilan Xu","Runtian Yuan","Yuejie Zhang","Rui Feng","Quanli Shen","Xiaobo Zhang","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09290v1","updated":"2024-03-14T11:23:39Z","published":"2024-03-14T11:23:39Z","title":"SELECTOR: Heterogeneous graph network with convolutional masked\n  autoencoder for multimodal robust prediction of cancer survival","summary":"  Accurately predicting the survival rate of cancer patients is crucial for\naiding clinicians in planning appropriate treatment, reducing cancer-related\nmedical expenses, and significantly enhancing patients' quality of life.\nMultimodal prediction of cancer patient survival offers a more comprehensive\nand precise approach. However, existing methods still grapple with challenges\nrelated to missing multimodal data and information interaction within\nmodalities. This paper introduces SELECTOR, a heterogeneous graph-aware network\nbased on convolutional mask encoders for robust multimodal prediction of cancer\npatient survival. SELECTOR comprises feature edge reconstruction, convolutional\nmask encoder, feature cross-fusion, and multimodal survival prediction modules.\nInitially, we construct a multimodal heterogeneous graph and employ the\nmeta-path method for feature edge reconstruction, ensuring comprehensive\nincorporation of feature information from graph edges and effective embedding\nof nodes. To mitigate the impact of missing features within the modality on\nprediction accuracy, we devised a convolutional masked autoencoder (CMAE) to\nprocess the heterogeneous graph post-feature reconstruction. Subsequently, the\nfeature cross-fusion module facilitates communication between modalities,\nensuring that output features encompass all features of the modality and\nrelevant information from other modalities. Extensive experiments and analysis\non six cancer datasets from TCGA demonstrate that our method significantly\noutperforms state-of-the-art methods in both modality-missing and\nintra-modality information-confirmed cases. Our codes are made available at\nhttps://github.com/panliangrui/Selector.\n","authors":["Liangrui Pan","Yijun Peng","Yan Li","Xiang Wang","Wenjuan Liu","Liwen Xu","Qingchun Liang","Shaoliang Peng"],"pdf_url":"https://arxiv.org/pdf/2403.09290v1.pdf","comment":"Accepted on Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2403.09288v1","updated":"2024-03-14T11:22:06Z","published":"2024-03-14T11:22:06Z","title":"Adversarial Training with OCR Modality Perturbation for Scene-Text\n  Visual Question Answering","summary":"  Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text\nin images and answer questions related to the text content. Most existing\nmethods heavily rely on the accuracy of Optical Character Recognition (OCR)\nsystems, and aggressive fine-tuning based on limited spatial location\ninformation and erroneous OCR text information often leads to inevitable\noverfitting. In this paper, we propose a multimodal adversarial training\narchitecture with spatial awareness capabilities. Specifically, we introduce an\nAdversarial OCR Enhancement (AOE) module, which leverages adversarial training\nin the embedding space of OCR modality to enhance fault-tolerant representation\nof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We\nadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model better\ncapture the spatial relationships among OCR tokens. Various experiments\ndemonstrate that our method achieves significant performance improvements on\nboth the ST-VQA and TextVQA datasets and provides a novel paradigm for\nmultimodal adversarial training.\n","authors":["Zhixuan Shen","Haonan Luo","Sijia Li","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2403.09288v1.pdf","comment":"6 pages, 3 figures, accepted by 2024 IEEE International Conference on\n  Multimedia and Expo"},{"id":"http://arxiv.org/abs/2401.08328v2","updated":"2024-03-14T11:20:21Z","published":"2024-01-16T12:48:52Z","title":"Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal\n  Correlation","summary":"  Recent test-time adaptation methods heavily rely on nuanced adjustments of\nbatch normalization (BN) parameters. However, one critical assumption often\ngoes overlooked: that of independently and identically distributed (i.i.d.)\ntest batches with respect to unknown labels. This oversight leads to skewed BN\nstatistics and undermines the reliability of the model under non-i.i.d.\nscenarios. To tackle this challenge, this paper presents a novel method termed\n'Un-Mixing Test-Time Normalization Statistics' (UnMix-TNS). Our method\nre-calibrates the statistics for each instance within a test batch by mixing it\nwith multiple distinct statistics components, thus inherently simulating the\ni.i.d. scenario. The core of this method hinges on a distinctive online\nunmixing procedure that continuously updates these statistics components by\nincorporating the most similar instances from new test batches. Remarkably\ngeneric in its design, UnMix-TNS seamlessly integrates with a wide range of\nleading test-time adaptation methods and pre-trained architectures equipped\nwith BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS\nunder varied scenarios-ranging from single to continual and mixed domain\nshifts, particularly excelling with temporally correlated test data and\ncorrupted non-i.i.d. real-world streams. This adaptability is maintained even\nwith very small batch sizes or single instances. Our results highlight\nUnMix-TNS's capacity to markedly enhance stability and performance across\nvarious benchmarks. Our code is publicly available at\nhttps://github.com/devavratTomar/unmixtns.\n","authors":["Devavrat Tomar","Guillaume Vray","Jean-Philippe Thiran","Behzad Bozorgtabar"],"pdf_url":"https://arxiv.org/pdf/2401.08328v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09281v1","updated":"2024-03-14T11:08:33Z","published":"2024-03-14T11:08:33Z","title":"CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise\n  Classification","summary":"  The CLIP (Contrastive Language-Image Pretraining) model has exhibited\noutstanding performance in recognition problems, such as zero-shot image\nclassification and object detection. However, its ability to count remains\nunderstudied due to the inherent challenges of transforming counting--a\nregression task--into a recognition task. In this paper, we investigate CLIP's\npotential in counting, focusing specifically on estimating crowd sizes.\nExisting classification-based crowd-counting methods have encountered issues,\nincluding inappropriate discretization strategies, which impede the application\nof CLIP and result in suboptimal performance. To address these challenges, we\npropose the Enhanced Blockwise Classification (EBC) framework. In contrast to\nprevious methods, EBC relies on integer-valued bins that facilitate the\nlearning of robust decision boundaries. Within our model-agnostic EBC\nframework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting\nmodel capable of generating density maps. Comprehensive evaluations across\ndiverse crowd-counting datasets demonstrate the state-of-the-art performance of\nour methods. Particularly, EBC can improve existing models by up to 76.9%.\nMoreover, our CLIP-EBC model surpasses current crowd-counting methods,\nachieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part\nB datasets, respectively. The code will be made publicly available.\n","authors":["Yiming Ma","Victor Sanchez","Tanaya Guha"],"pdf_url":"https://arxiv.org/pdf/2403.09281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02672v2","updated":"2024-03-14T10:59:14Z","published":"2023-12-05T11:29:00Z","title":"Are Synthetic Data Useful for Egocentric Hand-Object Interaction\n  Detection?","summary":"  In this study, we investigate the effectiveness of synthetic data in\nenhancing egocentric hand-object interaction detection. Via extensive\nexperiments and comparative analyses on three egocentric datasets, VISOR,\nEgoHOS, and ENIGMA-51, our findings reveal how to exploit synthetic data for\nthe HOI detection task when real labeled data are scarce or unavailable.\nSpecifically, by leveraging only 10% of real labeled data, we achieve\nimprovements in Overall AP compared to baselines trained exclusively on real\ndata of: +5.67% on EPIC-KITCHENS VISOR, +8.24% on EgoHOS, and +11.69% on\nENIGMA-51. Our analysis is supported by a novel data generation pipeline and\nthe newly introduced HOI-Synth benchmark which augments existing datasets with\nsynthetic images of hand-object interactions automatically labeled with\nhand-object contact states, bounding boxes, and pixel-wise segmentation masks.\nWe publicly release the generated data, code, and data generation tools to\nsupport future research at the following link:\nhttps://iplab.dmi.unict.it/HOI-Synth/.\n","authors":["Rosario Leonardi","Antonino Furnari","Francesco Ragusa","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2312.02672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09274v1","updated":"2024-03-14T10:52:45Z","published":"2024-03-14T10:52:45Z","title":"EventRPG: Event Data Augmentation with Relevance Propagation Guidance","summary":"  Event camera, a novel bio-inspired vision sensor, has drawn a lot of\nattention for its low latency, low power consumption, and high dynamic range.\nCurrently, overfitting remains a critical problem in event-based classification\ntasks for Spiking Neural Network (SNN) due to its relatively weak spatial\nrepresentation capability. Data augmentation is a simple but efficient method\nto alleviate overfitting and improve the generalization ability of neural\nnetworks, and saliency-based augmentation methods are proven to be effective in\nthe image processing field. However, there is no approach available for\nextracting saliency maps from SNNs. Therefore, for the first time, we present\nSpiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking\nLayer-wise Relevance Propagation rule (SLRP) in order for SNN to generate\nstable and accurate CAMs and saliency maps. Based on this, we propose EventRPG,\nwhich leverages relevance propagation on the spiking neural network for more\nefficient augmentation. Our proposed method has been evaluated on several SNN\nstructures, achieving state-of-the-art performance in object recognition tasks\nincluding N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as\nwell as action recognition task SL-Animals with an accuracy of 91.59%. Our code\nis available at https://github.com/myuansun/EventRPG.\n","authors":["Mingyuan Sun","Donghao Zhang","Zongyuan Ge","Jiaxu Wang","Jia Li","Zheng Fang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2403.09274v1.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2312.16272v2","updated":"2024-03-14T10:44:49Z","published":"2023-12-26T14:39:11Z","title":"SSR-Encoder: Encoding Selective Subject Representation for\n  Subject-Driven Generation","summary":"  Recent advancements in subject-driven image generation have led to zero-shot\ngeneration, yet precise selection and focus on crucial subject representations\nremain challenging. Addressing this, we introduce the SSR-Encoder, a novel\narchitecture designed for selectively capturing any subject from single or\nmultiple reference images. It responds to various query modalities including\ntext and masks, without necessitating test-time fine-tuning. The SSR-Encoder\ncombines a Token-to-Patch Aligner that aligns query inputs with image patches\nand a Detail-Preserving Subject Encoder for extracting and preserving fine\nfeatures of the subjects, thereby generating subject embeddings. These\nembeddings, used in conjunction with original text embeddings, condition the\ngeneration process. Characterized by its model generalizability and efficiency,\nthe SSR-Encoder adapts to a range of custom models and control modules.\nEnhanced by the Embedding Consistency Regularization Loss for improved\ntraining, our extensive experiments demonstrate its effectiveness in versatile\nand high-quality image generation, indicating its broad applicability. Project\npage: https://ssr-encoder.github.io\n","authors":["Yuxuan Zhang","Yiren Song","Jiaming Liu","Rui Wang","Jinpeng Yu","Hao Tang","Huaxia Li","Xu Tang","Yao Hu","Han Pan","Zhongliang Jing"],"pdf_url":"https://arxiv.org/pdf/2312.16272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09262v1","updated":"2024-03-14T10:37:41Z","published":"2024-03-14T10:37:41Z","title":"Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for\n  BraTS 2023 Adult Glioma and Pediatric Tumor Tasks","summary":"  Automated segmentation proves to be a valuable tool in precisely detecting\ntumors within medical images. The accurate identification and segmentation of\ntumor types hold paramount importance in diagnosing, monitoring, and treating\nhighly fatal brain tumors. The BraTS challenge serves as a platform for\nresearchers to tackle this issue by participating in open challenges focused on\ntumor segmentation. This study outlines our methodology for segmenting tumors\nin the context of two distinct tasks from the BraTS 2023 challenge: Adult\nGlioma and Pediatric Tumors. Our approach leverages two encoder-decoder-based\nCNN models, namely SegResNet and MedNeXt, for segmenting three distinct\nsubregions of tumors. We further introduce a set of robust postprocessing to\nimprove the segmentation, especially for the newly introduced BraTS 2023\nmetrics. The specifics of our approach and comprehensive performance analyses\nare expounded upon in this work. Our proposed approach achieves third place in\nthe BraTS 2023 Adult Glioma Segmentation Challenges with an average of 0.8313\nand 36.38 Dice and HD95 scores on the test set, respectively.\n","authors":["Fadillah Maani","Anees Ur Rehman Hashmi","Mariam Aljuboory","Numan Saeed","Ikboljon Sobirov","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.09262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09257v1","updated":"2024-03-14T10:30:43Z","published":"2024-03-14T10:30:43Z","title":"WSI-SAM: Multi-resolution Segment Anything Model (SAM) for\n  histopathology whole-slide images","summary":"  The Segment Anything Model (SAM) marks a significant advancement in\nsegmentation models, offering powerful zero-shot capabilities and dynamic\nprompting. However, existing medical SAMs are not suitable for the multi-scale\nnature of whole-slide images (WSIs), restricting their effectiveness. To\nresolve this drawback, we present WSI-SAM, enhancing SAM with precise object\nsegmentation capabilities for histopathology images using multi-resolution\npatches, while preserving its original prompt-driven design, efficiency, and\nzero-shot adaptability. To fully exploit pretrained knowledge while minimizing\ntraining overhead, we keep SAM frozen, only introducing minimal additional\nparameters and computation. In particular, we introduce High-Resolution (HR)\ntoken, Low-Resolution (LR) token and dual mask decoder. This decoder integrates\nthe original SAM mask decoder with a lightweight fusion module that integrates\nfeatures at multiple scales. Instead of predicting a mask independently, we\nintegrate HR and LR token at intermediate layer to jointly learn features of\nthe same object across multiple resolutions. Experiments show that our WSI-SAM\noutperforms state-of-the-art SAM and its variants. In particular, our model\noutperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ\n(DCIS) segmentation tasks and breast cancer metastasis segmentation task\n(CAMELYON16 dataset). The code will be available at\nhttps://github.com/HongLiuuuuu/WSI-SAM.\n","authors":["Hong Liu","Haosen Yang","Paul J. van Diest","Josien P. W. Pluim","Mitko Veta"],"pdf_url":"https://arxiv.org/pdf/2403.09257v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.01567 by other authors"},{"id":"http://arxiv.org/abs/2403.09240v1","updated":"2024-03-14T10:03:58Z","published":"2024-03-14T10:03:58Z","title":"XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via\n  Controllable Diffusion Model","summary":"  Large-scale generative models have demonstrated impressive capacity in\nproducing visually compelling images, with increasing applications in medical\nimaging. However, they continue to grapple with the challenge of image\nhallucination and the generation of anatomically inaccurate outputs. These\nlimitations are mainly due to the sole reliance on textual inputs and lack of\nspatial control over the generated images, hindering the potential usefulness\nof such models in real-life settings. We present XReal, a novel controllable\ndiffusion model for generating realistic chest X-ray images through precise\nanatomy and pathology location control. Our lightweight method can seamlessly\nintegrate spatial control in a pre-trained text-to-image diffusion model\nwithout fine-tuning, retaining its existing knowledge while enhancing its\ngeneration capabilities. XReal outperforms state-of-the-art x-ray diffusion\nmodels in quantitative and qualitative metrics while showing 13% and 10%\nanatomy and pathology realism gain, respectively, based on the expert\nradiologist evaluation. Our model holds promise for advancing generative models\nin medical imaging, offering greater precision and adaptability while inviting\nfurther exploration in this evolving field. A large synthetically generated\ndata with annotations and code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/XReal.\n","authors":["Anees Ur Rehman Hashmi","Ibrahim Almakky","Mohammad Areeb Qazi","Santosh Sanjeev","Vijay Ram Papineni","Dwarikanath Mahapatra","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.09240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00390v2","updated":"2024-03-14T10:03:49Z","published":"2023-09-30T14:26:43Z","title":"InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision\n  Generalists","summary":"  Recent advances in generative diffusion models have enabled text-controlled\nsynthesis of realistic and diverse images with impressive quality. Despite\nthese remarkable advances, the application of text-to-image generative models\nin computer vision for standard visual recognition tasks remains limited. The\ncurrent de facto approach for these tasks is to design model architectures and\nloss functions that are tailored to the task at hand. In this paper, we develop\na unified language interface for computer vision tasks that abstracts away\ntask-specific design choices and enables task execution by following natural\nlanguage instructions. Our approach involves casting multiple computer vision\ntasks as text-to-image generation problems. Here, the text represents an\ninstruction describing the task, and the resulting image is a visually-encoded\ntask output. To train our model, we pool commonly-used computer vision datasets\ncovering a range of tasks, including segmentation, object detection, depth\nestimation, and classification. We then use a large language model to\nparaphrase prompt templates that convey the specific tasks to be conducted on\neach image, and through this process, we create a multi-modal and multi-task\ntraining dataset comprising input and output images along with annotated\ninstructions. Following the InstructPix2Pix architecture, we apply\ninstruction-tuning to a text-to-image diffusion model using our constructed\ndataset, steering its functionality from a generative model to an\ninstruction-guided multi-task vision learner. Experiments demonstrate that our\nmodel, dubbed InstructCV, performs competitively compared to other generalist\nand task-specific vision models. Moreover, it exhibits compelling\ngeneralization capabilities to unseen data, categories, and user instructions.\n","authors":["Yulu Gan","Sungwoo Park","Alexander Schubert","Anthony Philippakis","Ahmed M. Alaa"],"pdf_url":"https://arxiv.org/pdf/2310.00390v2.pdf","comment":"ICLR 2024; Code is available at https://github.com/AlaaLab/InstructCV"},{"id":"http://arxiv.org/abs/2403.09236v1","updated":"2024-03-14T09:59:55Z","published":"2024-03-14T09:59:55Z","title":"Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph","summary":"  Text-to-3D generation represents an exciting field that has seen rapid\nadvancements, facilitating the transformation of textual descriptions into\ndetailed 3D models. However, current progress often neglects the intricate\nhigh-order correlation of geometry and texture within 3D objects, leading to\nchallenges such as over-smoothness, over-saturation and the Janus problem. In\nthis work, we propose a method named ``3D Gaussian Generation via Hypergraph\n(Hyper-3DG)'', designed to capture the sophisticated high-order correlations\npresent within 3D objects. Our framework is anchored by a well-established\nmainflow and an essential module, named ``Geometry and Texture Hypergraph\nRefiner (HGRefiner)''. This module not only refines the representation of 3D\nGaussians but also accelerates the update process of these 3D Gaussians by\nconducting the Patch-3DGS Hypergraph Learning on both explicit attributes and\nlatent visual features. Our framework allows for the production of finely\ngenerated 3D objects within a cohesive optimization, effectively circumventing\ndegradation. Extensive experimentation has shown that our proposed method\nsignificantly enhances the quality of 3D generation while incurring no\nadditional computational overhead for the underlying framework. (Project code:\nhttps://github.com/yjhboy/Hyper3DG)\n","authors":["Donglin Di","Jiahui Yang","Chaofan Luo","Zhou Xue","Wei Chen","Xun Yang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2403.09236v1.pdf","comment":"27 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.09233v1","updated":"2024-03-14T09:57:15Z","published":"2024-03-14T09:57:15Z","title":"D-YOLO a robust framework for object detection in adverse weather\n  conditions","summary":"  Adverse weather conditions including haze, snow and rain lead to decline in\nimage qualities, which often causes a decline in performance for deep-learning\nbased detection networks. Most existing approaches attempts to rectify hazy\nimages before performing object detection, which increases the complexity of\nthe network and may result in the loss in latent information. To better\nintegrate image restoration and object detection tasks, we designed a\ndouble-route network with an attention feature fusion module, taking both hazy\nand dehazed features into consideration. We also proposed a subnetwork to\nprovide haze-free features to the detection network. Specifically, our D-YOLO\nimproves the performance of the detection network by minimizing the distance\nbetween the clear feature extraction subnetwork and detection network.\nExperiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates\nbetter performance compared to the state-of-the-art methods. It is a robust\ndetection framework for bridging the gap between low-level dehazing and\nhigh-level detection.\n","authors":["Zihan Chu"],"pdf_url":"https://arxiv.org/pdf/2403.09233v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.01373 by other authors"},{"id":"http://arxiv.org/abs/2312.10112v2","updated":"2024-03-14T09:56:35Z","published":"2023-12-15T09:09:25Z","title":"NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on\n  Normalizing Flows and Generative Adversarial Networks","summary":"  Modeling and synthesizing real sRGB noise is crucial for various low-level\nvision tasks, such as building datasets for training image denoising systems.\nThe distribution of real sRGB noise is highly complex and affected by a\nmultitude of factors, making its accurate modeling extremely challenging.\nTherefore, recent studies have proposed methods that employ data-driven\ngenerative models, such as generative adversarial networks (GAN) and\nNormalizing Flows. These studies achieve more accurate modeling of sRGB noise\ncompared to traditional noise modeling methods. However, there are performance\nlimitations due to the inherent characteristics of each generative model. To\naddress this issue, we propose NM-FlowGAN, a hybrid approach that exploits the\nstrengths of both GAN and Normalizing Flows. We simultaneously employ a\npixel-wise noise modeling network based on Normalizing Flows, and spatial\ncorrelation modeling networks based on GAN. In our experiments, our NM-FlowGAN\noutperforms other baselines on the sRGB noise synthesis task. Moreover, the\ndenoising neural network, trained with synthesized image pairs from our model,\nalso shows superior performance compared to other baselines. Our code is\navailable at: \\url{https://github.com/YoungJooHan/NM-FlowGAN}.\n","authors":["Young Joo Han","Ha-Jin Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10112v2.pdf","comment":"25 pages, 11 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.09230v1","updated":"2024-03-14T09:54:31Z","published":"2024-03-14T09:54:31Z","title":"Improving Distant 3D Object Detection Using 2D Box Supervision","summary":"  Improving the detection of distant 3d objects is an important yet challenging\ntask. For camera-based 3D perception, the annotation of 3d bounding relies\nheavily on LiDAR for accurate depth information. As such, the distance of\nannotation is often limited due to the sparsity of LiDAR points on distant\nobjects, which hampers the capability of existing detectors for long-range\nscenarios. We address this challenge by considering only 2D box supervision for\ndistant objects since they are easy to annotate. We propose LR3D, a framework\nthat learns to recover the missing depth of distant objects. LR3D adopts an\nimplicit projection head to learn the generation of mapping between 2D boxes\nand depth using the 3D supervision on close objects. This mapping allows the\ndepth estimation of distant objects conditioned on their 2D boxes, making\nlong-range 3D detection with 2D supervision feasible. Experiments show that\nwithout distant 3D annotations, LR3D allows camera-based methods to detect\ndistant objects (over 200m) with comparable accuracy to full 3D supervision.\nOur framework is general, and could widely benefit 3D detection methods to a\nlarge extent.\n","authors":["Zetong Yang","Zhiding Yu","Chris Choy","Renhao Wang","Anima Anandkumar","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2403.09230v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2310.15599v2","updated":"2024-03-14T09:53:05Z","published":"2023-10-24T08:01:12Z","title":"Grasp Multiple Objects with One Hand","summary":"  The intricate kinematics of the human hand enable simultaneous grasping and\nmanipulation of multiple objects, essential for tasks such as object transfer\nand in-hand manipulation. Despite its significance, the domain of robotic\nmulti-object grasping is relatively unexplored and presents notable challenges\nin kinematics, dynamics, and object configurations. This paper introduces\nMultiGrasp, a novel two-stage approach for multi-object grasping using a\ndexterous multi-fingered robotic hand on a tabletop. The process consists of\n(i) generating pre-grasp proposals and (ii) executing the grasp and lifting the\nobjects. Our experimental focus is primarily on dual-object grasping, achieving\na success rate of 44.13%, highlighting adaptability to new object\nconfigurations and tolerance for imprecise grasps. Additionally, the framework\ndemonstrates the potential for grasping more than two objects at the cost of\ninference speed.\n","authors":["Yuyang Li","Bo Liu","Yiran Geng","Puhao Li","Yaodong Yang","Yixin Zhu","Tengyu Liu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2310.15599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09212v1","updated":"2024-03-14T09:28:12Z","published":"2024-03-14T09:28:12Z","title":"PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of\n  Interest","summary":"  In this work, we present PoIFusion, a simple yet effective multi-modal 3D\nobject detection framework to fuse the information of RGB images and LiDAR\npoint clouds at the point of interest (abbreviated as PoI). Technically, our\nPoIFusion follows the paradigm of query-based object detection, formulating\nobject queries as dynamic 3D boxes. The PoIs are adaptively generated from each\nquery box on the fly, serving as the keypoints to represent a 3D object and\nplay the role of basic units in multi-modal fusion. Specifically, we project\nPoIs into the view of each modality to sample the corresponding feature and\nintegrate the multi-modal features at each PoI through a dynamic fusion block.\nFurthermore, the features of PoIs derived from the same query box are\naggregated together to update the query feature. Our approach prevents\ninformation loss caused by view transformation and eliminates the\ncomputation-intensive global attention, making the multi-modal 3D object\ndetector more applicable. We conducted extensive experiments on the nuScenes\ndataset to evaluate our approach. Remarkably, our PoIFusion achieves 74.9\\% NDS\nand 73.4\\% mAP, setting a state-of-the-art record on the multi-modal 3D object\ndetection benchmark. Codes will be made available via\n\\url{https://djiajunustc.github.io/projects/poifusion}.\n","authors":["Jiajun Deng","Sha Zhang","Feras Dayoub","Wanli Ouyang","Yanyong Zhang","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2403.09212v1.pdf","comment":"NIL"},{"id":"http://arxiv.org/abs/2311.11642v3","updated":"2024-03-14T09:13:55Z","published":"2023-11-20T10:01:13Z","title":"Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging","summary":"  Video face re-aging deals with altering the apparent age of a person to the\ntarget age in videos. This problem is challenging due to the lack of paired\nvideo datasets maintaining temporal consistency in identity and age. Most\nre-aging methods process each image individually without considering the\ntemporal consistency of videos. While some existing works address the issue of\ntemporal coherence through video facial attribute manipulation in latent space,\nthey often fail to deliver satisfactory performance in age transformation. To\ntackle the issues, we propose (1) a novel synthetic video dataset that features\nsubjects across a diverse range of age groups; (2) a baseline architecture\ndesigned to validate the effectiveness of our proposed dataset, and (3) the\ndevelopment of novel metrics tailored explicitly for evaluating the temporal\nconsistency of video re-aging techniques. Our comprehensive experiments on\npublic datasets, including VFHQ and CelebA-HQ, show that our method outperforms\nexisting approaches in age transformation accuracy and temporal consistency.\nNotably, in user studies, our method was preferred for temporal consistency by\n48.1\\% of participants for the older direction and by 39.3\\% for the younger\ndirection.\n","authors":["Abdul Muqeet","Kyuchul Lee","Bumsoo Kim","Yohan Hong","Hyungrae Lee","Woonggon Kim","KwangHee Lee"],"pdf_url":"https://arxiv.org/pdf/2311.11642v3.pdf","comment":"28 pages, 11 figures, 11 tables, Project page:\n  https://video-reaging.github.io/"},{"id":"http://arxiv.org/abs/2403.09199v1","updated":"2024-03-14T09:13:51Z","published":"2024-03-14T09:13:51Z","title":"Customizing Segmentation Foundation Model via Prompt Learning for\n  Instance Segmentation","summary":"  Recently, foundation models trained on massive datasets to adapt to a wide\nrange of domains have attracted considerable attention and are actively being\nexplored within the computer vision community. Among these, the Segment\nAnything Model (SAM) stands out for its remarkable progress in generalizability\nand flexibility for image segmentation tasks, achieved through prompt-based\nobject mask generation. However, despite its strength, SAM faces two key\nlimitations when applied to customized instance segmentation that segments\nspecific objects or those in unique environments not typically present in the\ntraining data: 1) the ambiguity inherent in input prompts and 2) the necessity\nfor extensive additional training to achieve optimal segmentation. To address\nthese challenges, we propose a novel method, customized instance segmentation\nvia prompt learning tailored to SAM. Our method involves a prompt learning\nmodule (PLM), which adjusts input prompts into the embedding space to better\nalign with user intentions, thereby enabling more efficient training.\nFurthermore, we introduce a point matching module (PMM) to enhance the feature\nrepresentation for finer segmentation by ensuring detailed alignment with\nground truth boundaries. Experimental results on various customized instance\nsegmentation scenarios demonstrate the effectiveness of the proposed method.\n","authors":["Hyung-Il Kim","Kimin Yun","Jun-Seok Yun","Yuseok Bae"],"pdf_url":"https://arxiv.org/pdf/2403.09199v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.09196v1","updated":"2024-03-14T09:09:06Z","published":"2024-03-14T09:09:06Z","title":"Noise Dimension of GAN: An Image Compression Perspective","summary":"  Generative adversial network (GAN) is a type of generative model that maps a\nhigh-dimensional noise to samples in target distribution. However, the\ndimension of noise required in GAN is not well understood. Previous approaches\nview GAN as a mapping from a continuous distribution to another continous\ndistribution. In this paper, we propose to view GAN as a discrete sampler\ninstead. From this perspective, we build a connection between the minimum noise\nrequired and the bits to losslessly compress the images. Furthermore, to\nunderstand the behaviour of GAN when noise dimension is limited, we propose\ndivergence-entropy trade-off. This trade-off depicts the best divergence we can\nachieve when noise is limited. And as rate distortion trade-off, it can be\nnumerically solved when source distribution is known. Finally, we verifies our\ntheory with experiments on image generation.\n","authors":["Ziran Zhu","Tongda Xu","Ling Li","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09196v1.pdf","comment":"ICME24"},{"id":"http://arxiv.org/abs/2403.09195v1","updated":"2024-03-14T09:07:34Z","published":"2024-03-14T09:07:34Z","title":"SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash\n  Attention to Achieve 30 times Acceleration","summary":"  Segment Anything Model (SAM) has garnered significant attention in\nsegmentation tasks due to their zero-shot generalization ability. However, a\nbroader application of SAMs to real-world practice has been restricted by their\nlow inference speed and high computational memory demands, which mainly stem\nfrom the attention mechanism. Existing work concentrated on optimizing the\nencoder, yet has not adequately addressed the inefficiency of the attention\nmechanism itself, even when distilled to a smaller model, which thus leaves\nspace for further improvement. In response, we introduce SAM-Lightening, a\nvariant of SAM, that features a re-engineered attention mechanism, termed\nDilated Flash Attention. It not only facilitates higher parallelism, enhancing\nprocessing efficiency but also retains compatibility with the existing\nFlashAttention. Correspondingly, we propose a progressive distillation to\nenable an efficient knowledge transfer from the vanilla SAM without costly\ntraining from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening\nsignificantly outperforms the state-of-the-art methods in both run-time\nefficiency and segmentation accuracy. Specifically, it can achieve an inference\nspeed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels,\nwhich is 30.1 times faster than the vanilla SAM and 2.1 times than the\nstate-of-the-art. Moreover, it takes only 244MB memory, which is 3.5\\% of the\nvanilla SAM. The code and weights are available at\nhttps://anonymous.4open.science/r/SAM-LIGHTENING-BC25/.\n","authors":["Yanfei Songa","Bangzheng Pua","Peng Wanga","Hongxu Jiang","Dong Donga","Yiqing Shen"],"pdf_url":"https://arxiv.org/pdf/2403.09195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09194v1","updated":"2024-03-14T09:07:31Z","published":"2024-03-14T09:07:31Z","title":"Intention-driven Ego-to-Exo Video Generation","summary":"  Ego-to-exo video generation refers to generating the corresponding exocentric\nvideo according to the egocentric video, providing valuable applications in\nAR/VR and embodied AI. Benefiting from advancements in diffusion model\ntechniques, notable progress has been achieved in video generation. However,\nexisting methods build upon the spatiotemporal consistency assumptions between\nadjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to\ndrastic changes in views. To this end, this paper proposes an Intention-Driven\nEgo-to-exo video generation framework (IDE) that leverages action intention\nconsisting of human movement and action description as view-independent\nrepresentation to guide video generation, preserving the consistency of content\nand motion. Specifically, the egocentric head trajectory is first estimated\nthrough multi-view stereo matching. Then, cross-view feature perception module\nis introduced to establish correspondences between exo- and ego- views, guiding\nthe trajectory transformation module to infer human full-body movement from the\nhead trajectory. Meanwhile, we present an action description unit that maps the\naction semantics into the feature space consistent with the exocentric image.\nFinally, the inferred human movement and high-level action descriptions jointly\nguide the generation of exocentric motion and interaction content (i.e.,\ncorresponding optical flow and occlusion maps) in the backward process of the\ndiffusion model, ultimately warping them into the corresponding exocentric\nvideo. We conduct extensive experiments on the relevant dataset with diverse\nexo-ego video pairs, and our IDE outperforms state-of-the-art models in both\nsubjective and objective assessments, demonstrating its efficacy in ego-to-exo\nvideo generation.\n","authors":["Hongchen Luo","Kai Zhu","Wei Zhai","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.09194v1.pdf","comment":"submitted to ICML 2024"},{"id":"http://arxiv.org/abs/2403.09193v1","updated":"2024-03-14T09:07:14Z","published":"2024-03-14T09:07:14Z","title":"Are Vision Language Models Texture or Shape Biased and Can We Steer\n  Them?","summary":"  Vision language models (VLMs) have drastically changed the computer vision\nmodel landscape in only a few years, opening an exciting array of new\napplications from zero-shot image classification, over to image captioning, and\nvisual question answering. Unlike pure vision models, they offer an intuitive\nway to access visual content through language prompting. The wide applicability\nof such models encourages us to ask whether they also align with human vision -\nspecifically, how far they adopt human-induced visual biases through multimodal\nfusion, or whether they simply inherit biases from pure vision models. One\nimportant visual bias is the texture vs. shape bias, or the dominance of local\nover global information. In this paper, we study this bias in a wide range of\npopular VLMs. Interestingly, we find that VLMs are often more shape-biased than\ntheir vision encoders, indicating that visual biases are modulated to some\nextent through text in multimodal models. If text does indeed influence visual\nbiases, this suggests that we may be able to steer visual biases not just\nthrough visual input but also through language: a hypothesis that we confirm\nthrough extensive experiments. For instance, we are able to steer shape bias\nfrom as low as 49% to as high as 72% through prompting alone. For now, the\nstrong human bias towards shape (96%) remains out of reach for all tested VLMs.\n","authors":["Paul Gavrikov","Jovita Lukasik","Steffen Jung","Robert Geirhos","Bianca Lamm","Muhammad Jehanzeb Mirza","Margret Keuper","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.09193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09192v1","updated":"2024-03-14T09:06:49Z","published":"2024-03-14T09:06:49Z","title":"PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient\n  Task Adaptation","summary":"  Recently, the scale of transformers has grown rapidly, which introduces\nconsiderable challenges in terms of training overhead and inference efficiency\nin the scope of task adaptation. Existing works, namely Parameter-Efficient\nFine-Tuning (PEFT) and model compression, have separately investigated the\nchallenges. However, PEFT cannot guarantee the inference efficiency of the\noriginal backbone, especially for large-scale models. Model compression\nrequires significant training costs for structure searching and re-training.\nConsequently, a simple combination of them cannot guarantee accomplishing both\ntraining efficiency and inference efficiency with minimal costs. In this paper,\nwe propose a novel Parallel Yielding Re-Activation (PYRA) method for such a\nchallenge of training-inference efficient task adaptation. PYRA first utilizes\nparallel yielding adaptive weights to comprehensively perceive the data\ndistribution in downstream tasks. A re-activation strategy for token modulation\nis then applied for tokens to be merged, leading to calibrated token features.\nExtensive experiments demonstrate that PYRA outperforms all competing methods\nunder both low compression rate and high compression rate, demonstrating its\neffectiveness and superiority in maintaining both training efficiency and\ninference efficiency for large-scale foundation models. Our code will be\nreleased to the public.\n","authors":["Yizhe Xiong","Hui Chen","Tianxiang Hao","Zijia Lin","Jungong Han","Yuesong Zhang","Guoxin Wang","Yongjun Bao","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2403.09192v1.pdf","comment":"15 pages, 5 figures, Under review"},{"id":"http://arxiv.org/abs/2403.09190v1","updated":"2024-03-14T09:05:25Z","published":"2024-03-14T09:05:25Z","title":"Intention-aware Denoising Diffusion Model for Trajectory Prediction","summary":"  Trajectory prediction is an essential component in autonomous driving,\nparticularly for collision avoidance systems. Considering the inherent\nuncertainty of the task, numerous studies have utilized generative models to\nproduce multiple plausible future trajectories for each agent. However, most of\nthem suffer from restricted representation ability or unstable training issues.\nTo overcome these limitations, we propose utilizing the diffusion model to\ngenerate the distribution of future trajectories. Two cruxes are to be settled\nto realize such an idea. First, the diversity of intention is intertwined with\nthe uncertain surroundings, making the true distribution hard to parameterize.\nSecond, the diffusion process is time-consuming during the inference phase,\nrendering it unrealistic to implement in a real-time driving system. We propose\nan Intention-aware denoising Diffusion Model (IDM), which tackles the above two\nproblems. We decouple the original uncertainty into intention uncertainty and\naction uncertainty and model them with two dependent diffusion processes. To\ndecrease the inference time, we reduce the variable dimensions in the\nintention-aware diffusion process and restrict the initial distribution of the\naction-aware diffusion process, which leads to fewer diffusion steps. To\nvalidate our approach, we conduct experiments on the Stanford Drone Dataset\n(SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with\nan FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY\ndataset. Compared with the original diffusion model, IDM reduces inference time\nby two-thirds. Interestingly, our experiments further reveal that introducing\nintention information is beneficial in modeling the diffusion process of fewer\nsteps.\n","authors":["Chen Liu","Shibo He","Haoyu Liu","Jiming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.09190v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.04747v2","updated":"2024-03-14T08:59:58Z","published":"2023-10-07T09:05:50Z","title":"Towards Dynamic and Small Objects Refinement for Unsupervised Domain\n  Adaptative Nighttime Semantic Segmentation","summary":"  Nighttime semantic segmentation plays a crucial role in practical\napplications, such as autonomous driving, where it frequently encounters\ndifficulties caused by inadequate illumination conditions and the absence of\nwell-annotated datasets. Moreover, semantic segmentation models trained on\ndaytime datasets often face difficulties in generalizing effectively to\nnighttime conditions. Unsupervised domain adaptation (UDA) has shown the\npotential to address the challenges and achieved remarkable results for\nnighttime semantic segmentation. However, existing methods still face\nlimitations in 1) their reliance on style transfer or relighting models, which\nstruggle to generalize to complex nighttime environments, and 2) their\nignorance of dynamic and small objects like vehicles and poles, which are\ndifficult to be directly learned from other domains. This paper proposes a\nnovel UDA method that refines both label and feature levels for dynamic and\nsmall objects for nighttime semantic segmentation. First, we propose a dynamic\nand small object refinement module to complement the knowledge of dynamic and\nsmall objects from the source domain to target the nighttime domain. These\ndynamic and small objects are normally context-inconsistent in under-exposed\nconditions. Then, we design a feature prototype alignment module to reduce the\ndomain gap by deploying contrastive learning between features and prototypes of\nthe same class from different domains, while re-weighting the categories of\ndynamic and small objects. Extensive experiments on three benchmark datasets\ndemonstrate that our method outperforms prior arts by a large margin for\nnighttime segmentation. Project page: https://rorisis.github.io/DSRNSS/.\n","authors":["Jingyi Pan","Sihang Li","Yucheng Chen","Jinjing Zhu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09183v1","updated":"2024-03-14T08:53:01Z","published":"2024-03-14T08:53:01Z","title":"Generalized Relevance Learning Grassmann Quantization","summary":"  Due to advancements in digital cameras, it is easy to gather multiple images\n(or videos) from an object under different conditions. Therefore, image-set\nclassification has attracted more attention, and different solutions were\nproposed to model them. A popular way to model image sets is subspaces, which\nform a manifold called the Grassmann manifold. In this contribution, we extend\nthe application of Generalized Relevance Learning Vector Quantization to deal\nwith Grassmann manifold. The proposed model returns a set of prototype\nsubspaces and a relevance vector. While prototypes model typical behaviours\nwithin classes, the relevance factors specify the most discriminative principal\nvectors (or images) for the classification task. They both provide insights\ninto the model's decisions by highlighting influential images and pixels for\npredictions. Moreover, due to learning prototypes, the model complexity of the\nnew method during inference is independent of dataset size, unlike previous\nworks. We applied it to several recognition tasks including handwritten digit\nrecognition, face recognition, activity recognition, and object recognition.\nExperiments demonstrate that it outperforms previous works with lower\ncomplexity and can successfully model the variation, such as handwritten style\nor lighting conditions. Moreover, the presence of relevances makes the model\nrobust to the selection of subspaces' dimensionality.\n","authors":["M. Mohammadi","M. Babai","M. H. F. Wilkinson"],"pdf_url":"https://arxiv.org/pdf/2403.09183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09176v1","updated":"2024-03-14T08:43:43Z","published":"2024-03-14T08:43:43Z","title":"Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse\n  Mixture-of-Experts","summary":"  Diffusion models have achieved remarkable success across a range of\ngenerative tasks. Recent efforts to enhance diffusion model architectures have\nreimagined them as a form of multi-task learning, where each task corresponds\nto a denoising task at a specific noise level. While these efforts have focused\non parameter isolation and task routing, they fall short of capturing detailed\ninter-task relationships and risk losing semantic information, respectively. In\nresponse, we introduce Switch Diffusion Transformer (Switch-DiT), which\nestablishes inter-task relationships between conflicting tasks without\ncompromising semantic information. To achieve this, we employ a sparse\nmixture-of-experts within each transformer block to utilize semantic\ninformation and facilitate handling conflicts in tasks through parameter\nisolation. Additionally, we propose a diffusion prior loss, encouraging similar\ntasks to share their denoising paths while isolating conflicting ones. Through\nthese, each transformer block contains a shared expert across all tasks, where\nthe common and task-specific denoising paths enable the diffusion model to\nconstruct its beneficial way of synergizing denoising tasks. Extensive\nexperiments validate the effectiveness of our approach in improving both image\nquality and convergence rate, and further analysis demonstrates that Switch-DiT\nconstructs tailored denoising paths across various generation scenarios.\n","authors":["Byeongjun Park","Hyojun Go","Jin-Young Kim","Sangmin Woo","Seokil Ham","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2403.09176v1.pdf","comment":"Project Page: https://byeongjun-park.github.io/Switch-DiT/"},{"id":"http://arxiv.org/abs/2311.17717v2","updated":"2024-03-14T08:35:10Z","published":"2023-11-29T15:19:49Z","title":"Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via\n  Lightweight Erasers","summary":"  Concept erasure in text-to-image diffusion models aims to disable pre-trained\ndiffusion models from generating images related to a target concept. To perform\nreliable concept erasure, the properties of robustness and locality are\ndesirable. The former refrains the model from producing images associated with\nthe target concept for any paraphrased or learned prompts, while the latter\npreserves its ability in generating images with non-target concepts. In this\npaper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler).\nIt learns a lightweight Eraser to perform concept erasing while satisfying the\nabove desirable properties by proposed concept-localized regularization and\nadversarial prompt learning schemes. Comprehensive experiments with various\nconcepts verify the superiority of Receler over previous methods. Our code will\nbe available upon acceptance.\n","authors":["Chi-Pin Huang","Kai-Po Chang","Chung-Ting Tsai","Yung-Hsuan Lai","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09172v1","updated":"2024-03-14T08:32:14Z","published":"2024-03-14T08:32:14Z","title":"SHAN: Object-Level Privacy Detection via Inference on Scene\n  Heterogeneous Graph","summary":"  With the rise of social platforms, protecting privacy has become an important\nissue. Privacy object detection aims to accurately locate private objects in\nimages. It is the foundation of safeguarding individuals' privacy rights and\nensuring responsible data handling practices in the digital age. Since privacy\nof object is not shift-invariant, the essence of the privacy object detection\ntask is inferring object privacy based on scene information. However, privacy\nobject detection has long been studied as a subproblem of common object\ndetection tasks. Therefore, existing methods suffer from serious deficiencies\nin accuracy, generalization, and interpretability. Moreover, creating\nlarge-scale privacy datasets is difficult due to legal constraints and existing\nprivacy datasets lack label granularity. The granularity of existing privacy\ndetection methods remains limited to the image level. To address the above two\nissues, we introduce two benchmark datasets for object-level privacy detection\nand propose SHAN, Scene Heterogeneous graph Attention Network, a model\nconstructs a scene heterogeneous graph from an image and utilizes\nself-attention mechanisms for scene inference to obtain object privacy. Through\nexperiments, we demonstrated that SHAN performs excellently in privacy object\ndetection tasks, with all metrics surpassing those of the baseline model.\n","authors":["Zhuohang Jiang","Bingkui Tong","Xia Du","Ahmed Alhammadi","Jizhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.09172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09283v8","updated":"2024-03-14T08:28:13Z","published":"2023-07-18T14:24:33Z","title":"RepViT: Revisiting Mobile CNN From ViT Perspective","summary":"  Recently, lightweight Vision Transformers (ViTs) demonstrate superior\nperformance and lower latency, compared with lightweight Convolutional Neural\nNetworks (CNNs), on resource-constrained mobile devices. Researchers have\ndiscovered many structural connections between lightweight ViTs and lightweight\nCNNs. However, the notable architectural disparities in the block structure,\nmacro, and micro designs between them have not been adequately examined. In\nthis study, we revisit the efficient design of lightweight CNNs from ViT\nperspective and emphasize their promising prospect for mobile devices.\nSpecifically, we incrementally enhance the mobile-friendliness of a standard\nlightweight CNN, \\ie, MobileNetV3, by integrating the efficient architectural\ndesigns of lightweight ViTs. This ends up with a new family of pure lightweight\nCNNs, namely RepViT. Extensive experiments show that RepViT outperforms\nexisting state-of-the-art lightweight ViTs and exhibits favorable latency in\nvarious vision tasks. Notably, on ImageNet, RepViT achieves over 80\\% top-1\naccuracy with 1.0 ms latency on an iPhone 12, which is the first time for a\nlightweight model, to the best of our knowledge. Besides, when RepViT meets\nSAM, our RepViT-SAM can achieve nearly 10$\\times$ faster inference than the\nadvanced MobileSAM. Codes and models are available at\n\\url{https://github.com/THU-MIG/RepViT}.\n","authors":["Ao Wang","Hui Chen","Zijia Lin","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2307.09283v8.pdf","comment":"CVPR 2024 Camera-ready Version"},{"id":"http://arxiv.org/abs/2403.06462v2","updated":"2024-03-14T08:25:49Z","published":"2024-03-11T06:59:05Z","title":"Towards the Uncharted: Density-Descending Feature Perturbation for\n  Semi-supervised Semantic Segmentation","summary":"  Semi-supervised semantic segmentation allows model to mine effective\nsupervision from unlabeled data to complement label-guided training. Recent\nresearch has primarily focused on consistency regularization techniques,\nexploring perturbation-invariant training at both the image and feature levels.\nIn this work, we proposed a novel feature-level consistency learning framework\nnamed Density-Descending Feature Perturbation (DDFP). Inspired by the\nlow-density separation assumption in semi-supervised learning, our key insight\nis that feature density can shed a light on the most promising direction for\nthe segmentation classifier to explore, which is the regions with lower\ndensity. We propose to shift features with confident predictions towards\nlower-density regions by perturbation injection. The perturbed features are\nthen supervised by the predictions on the original features, thereby compelling\nthe classifier to explore less dense regions to effectively regularize the\ndecision boundary. Central to our method is the estimation of feature density.\nTo this end, we introduce a lightweight density estimator based on normalizing\nflow, allowing for efficient capture of the feature density distribution in an\nonline manner. By extracting gradients from the density estimator, we can\ndetermine the direction towards less dense regions for each feature. The\nproposed DDFP outperforms other designs on feature-level perturbations and\nshows state of the art performances on both Pascal VOC and Cityscapes dataset\nunder various partition protocols. The project is available at\nhttps://github.com/Gavinwxy/DDFP.\n","authors":["Xiaoyang Wang","Huihui Bai","Limin Yu","Yao Zhao","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.06462v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.03477v2","updated":"2024-03-14T08:17:52Z","published":"2024-03-06T05:33:50Z","title":"Continual Segmentation with Disentangled Objectness Learning and Class\n  Recognition","summary":"  Most continual segmentation methods tackle the problem as a per-pixel\nclassification task. However, such a paradigm is very challenging, and we find\nquery-based segmenters with built-in objectness have inherent advantages\ncompared with per-pixel ones, as objectness has strong transfer ability and\nforgetting resistance. Based on these findings, we propose CoMasTRe by\ndisentangling continual segmentation into two stages: forgetting-resistant\ncontinual objectness learning and well-researched continual classification.\nCoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at\nthe first stage and leaving recognition to the second stage. During continual\nlearning, a simple but effective distillation is adopted to strengthen\nobjectness. To further mitigate the forgetting of old classes, we design a\nmulti-label class distillation strategy suited for segmentation. We assess the\neffectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show\nthat our method outperforms per-pixel and query-based methods on both datasets.\nCode will be available at https://github.com/jordangong/CoMasTRe.\n","authors":["Yizheng Gong","Siyue Yu","Xiaoyang Wang","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.03477v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2401.12175v2","updated":"2024-03-14T08:12:46Z","published":"2024-01-22T18:08:22Z","title":"Template-Free Single-View 3D Human Digitalization with Diffusion-Guided\n  LRM","summary":"  Reconstructing 3D humans from a single image has been extensively\ninvestigated. However, existing approaches often fall short on capturing fine\ngeometry and appearance details, hallucinating occluded parts with plausible\ndetails, and achieving generalization across unseen and in-the-wild datasets.\nWe present Human-LRM, a diffusion-guided feed-forward model that predicts the\nimplicit field of a human from a single image. Leveraging the power of the\nstate-of-the-art reconstruction model (i.e., LRM) and generative model (i.e\nStable Diffusion), our method is able to capture human without any template\nprior, e.g., SMPL, and effectively enhance occluded parts with rich and\nrealistic details. Our approach first uses a single-view LRM model with an\nenhanced geometry decoder to get the triplane NeRF representation. The novel\nview renderings from the triplane NeRF provide strong geometry and color prior,\nfrom which we generate photo-realistic details for the occluded parts using a\ndiffusion model. The generated multiple views then enable reconstruction with\nhigh-quality geometry and appearance, leading to superior overall performance\ncomparing to all existing human reconstruction methods.\n","authors":["Zhenzhen Weng","Jingyuan Liu","Hao Tan","Zhan Xu","Yang Zhou","Serena Yeung-Levy","Jimei Yang"],"pdf_url":"https://arxiv.org/pdf/2401.12175v2.pdf","comment":"Project Page: https://zzweng.github.io/humanlrm"},{"id":"http://arxiv.org/abs/2403.09157v1","updated":"2024-03-14T08:12:39Z","published":"2024-03-14T08:12:39Z","title":"VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation","summary":"  In the field of medical image segmentation, models based on both CNN and\nTransformer have been thoroughly investigated. However, CNNs have limited\nmodeling capabilities for long-range dependencies, making it challenging to\nexploit the semantic information within images fully. On the other hand, the\nquadratic computational complexity poses a challenge for Transformers.\nRecently, State Space Models (SSMs), such as Mamba, have been recognized as a\npromising method. They not only demonstrate superior performance in modeling\nlong-range interactions, but also preserve a linear computational complexity.\nInspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual\nState Space (VSS) Block is introduced to capture extensive contextual\ninformation, the Semantics and Detail Infusion (SDI) is introduced to augment\nthe infusion of low-level and high-level features. We conduct comprehensive\nexperiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB\nand ETIS-LaribPolypDB public datasets. The results indicate that VM-UNetV2\nexhibits competitive performance in medical image segmentation tasks. Our code\nis available at https://github.com/nobodyplayer1/VM-UNetV2.\n","authors":["Mingya Zhang","Yue Yu","Limei Gu","Tingsheng Lin","Xianping Tao"],"pdf_url":"https://arxiv.org/pdf/2403.09157v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2302.06072v2","updated":"2024-03-14T08:09:11Z","published":"2023-02-13T03:08:05Z","title":"Actional Atomic-Concept Learning for Demystifying Vision-Language\n  Navigation","summary":"  Vision-Language Navigation (VLN) is a challenging task which requires an\nagent to align complex visual observations to language instructions to reach\nthe goal position. Most existing VLN agents directly learn to align the raw\ndirectional features and visual features trained using one-hot labels to\nlinguistic instruction features. However, the big semantic gap among these\nmulti-modal inputs makes the alignment difficult and therefore limits the\nnavigation performance. In this paper, we propose Actional Atomic-Concept\nLearning (AACL), which maps visual observations to actional atomic concepts for\nfacilitating the alignment. Specifically, an actional atomic concept is a\nnatural language phrase containing an atomic action and an object, e.g., ``go\nup stairs''. These actional atomic concepts, which serve as the bridge between\nobservations and instructions, can effectively mitigate the semantic gap and\nsimplify the alignment. AACL contains three core components: 1) a concept\nmapping module to map the observations to the actional atomic concept\nrepresentations through the VLN environment and the recently proposed\nContrastive Language-Image Pretraining (CLIP) model, 2) a concept refining\nadapter to encourage more instruction-oriented object concept extraction by\nre-ranking the predicted object concepts by CLIP, and 3) an observation\nco-embedding module which utilizes concept representations to regularize the\nobservation representations. Our AACL establishes new state-of-the-art results\non both fine-grained (R2R) and high-level (REVERIE and R2R-Last) VLN\nbenchmarks. Moreover, the visualization shows that AACL significantly improves\nthe interpretability in action decision.\n","authors":["Bingqian Lin","Yi Zhu","Xiaodan Liang","Liang Lin","Jianzhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2302.06072v2.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2403.04321v2","updated":"2024-03-14T08:02:29Z","published":"2024-03-07T08:37:33Z","title":"Discriminative Probing and Tuning for Text-to-Image Generation","summary":"  Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.\n","authors":["Leigang Qu","Wenjie Wang","Yongqi Li","Hanwang Zhang","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2403.04321v2.pdf","comment":"CVPR 2024; project page: https://dpt-t2i.github.io/"},{"id":"http://arxiv.org/abs/2305.04276v2","updated":"2024-03-14T07:40:20Z","published":"2023-05-07T13:47:35Z","title":"AdaptiveClick: Clicks-aware Transformer with Adaptive Focal Loss for\n  Interactive Image Segmentation","summary":"  Interactive Image Segmentation (IIS) has emerged as a promising technique for\ndecreasing annotation time. Substantial progress has been made in pre- and\npost-processing for IIS, but the critical issue of interaction ambiguity,\nnotably hindering segmentation quality, has been under-researched. To address\nthis, we introduce AdaptiveClick -- a click-aware transformer incorporating an\nadaptive focal loss that tackles annotation inconsistencies with tools for\nmask- and pixel-level ambiguity resolution. To the best of our knowledge,\nAdaptiveClick is the first transformer-based, mask-adaptive segmentation\nframework for IIS. The key ingredient of our method is the Click-Aware\nMask-adaptive transformer Decoder (CAMD), which enhances the interaction\nbetween click and image features. Additionally, AdaptiveClick enables\npixel-adaptive differentiation of hard and easy samples in the decision space,\nindependent of their varying distributions. This is primarily achieved by\noptimizing a generalized Adaptive Focal Loss (AFL) with a theoretical\nguarantee, where two adaptive coefficients control the ratio of gradient values\nfor hard and easy pixels. Our analysis reveals that the commonly used Focal and\nBCE losses can be considered special cases of the proposed AFL. With a plain\nViT backbone, extensive experimental results on nine datasets demonstrate the\nsuperiority of AdaptiveClick compared to state-of-the-art methods. The source\ncode is publicly available at https://github.com/lab206/AdaptiveClick.\n","authors":["Jiacheng Lin","Jiajun Chen","Kailun Yang","Alina Roitberg","Siyu Li","Zhiyong Li","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2305.04276v2.pdf","comment":"Accepted to IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS). The source code is publicly available at\n  https://github.com/lab206/AdaptiveClick"},{"id":"http://arxiv.org/abs/2403.09140v1","updated":"2024-03-14T07:39:59Z","published":"2024-03-14T07:39:59Z","title":"Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D\n  Prior","summary":"  Recent works on text-to-3d generation show that using only 2D diffusion\nsupervision for 3D generation tends to produce results with inconsistent\nappearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals\nwith extra legs). Existing methods mainly address this issue by retraining\ndiffusion models with images rendered from 3D data to ensure multi-view\nconsistency while struggling to balance 2D generation quality with 3D\nconsistency. In this paper, we present a new framework Sculpt3D that equips the\ncurrent pipeline with explicit injection of 3D priors from retrieved reference\nobjects without re-training the 2D diffusion model. Specifically, we\ndemonstrate that high-quality and diverse 3D geometry can be guaranteed by\nkeypoints supervision through a sparse ray sampling approach. Moreover, to\nensure accurate appearances of different views, we further modulate the output\nof the 2D diffusion model to the correct patterns of the template views without\naltering the generated object's style. These two decoupled designs effectively\nharness 3D information from reference objects to generate 3D objects while\npreserving the generation quality of the 2D diffusion model. Extensive\nexperiments show our method can largely improve the multi-view consistency\nwhile retaining fidelity and diversity. Our project page is available at:\nhttps://stellarcheng.github.io/Sculpt3D/.\n","authors":["Cheng Chen","Xiaofeng Yang","Fan Yang","Chengzeng Feng","Zhoujie Fu","Chuan-Sheng Foo","Guosheng Lin","Fayao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.09140v1.pdf","comment":"Accepted by CVPR 2024. Project Page:\n  https://stellarcheng.github.io/Sculpt3D/"},{"id":"http://arxiv.org/abs/2403.09139v1","updated":"2024-03-14T07:38:22Z","published":"2024-03-14T07:38:22Z","title":"Metadata-Driven Federated Learning of Connectional Brain Templates in\n  Non-IID Multi-Domain Scenarios","summary":"  A connectional brain template (CBT) is a holistic representation of a\npopulation of multi-view brain connectivity graphs, encoding shared patterns\nand normalizing typical variations across individuals. The federation of CBT\nlearning allows for an inclusive estimation of the representative center of\nmulti-domain brain connectivity datasets in a fully data-preserving manner.\nHowever, existing methods overlook the non-independent and identically\ndistributed (non-IDD) issue stemming from multidomain brain connectivity\nheterogeneity, in which data domains are drawn from different hospitals and\nimaging modalities. To overcome this limitation, we unprecedentedly propose a\nmetadata-driven federated learning framework, called MetaFedCBT, for\ncross-domain CBT learning. Given the data drawn from a specific domain (i.e.,\nhospital), our model aims to learn metadata in a fully supervised manner by\nintroducing a local client-based regressor network. The generated meta-data is\nforced to meet the statistical attributes (e.g., mean) of other domains, while\npreserving their privacy. Our supervised meta-data generation approach boosts\nthe unsupervised learning of a more centered, representative, and holistic CBT\nof a particular brain state across diverse domains. As the federated learning\nprogresses over multiple rounds, the learned metadata and associated generated\nconnectivities are continuously updated to better approximate the target domain\ninformation. MetaFedCBT overcomes the non-IID issue of existing methods by\ngenerating informative brain connectivities for privacy-preserving holistic CBT\nlearning with guidance using metadata. Extensive experiments on multi-view\nmorphological brain networks of normal and patient subjects demonstrate that\nour MetaFedCBT is a superior federated CBT learning model and significantly\nadvances the state-of-the-art performance.\n","authors":["Geng Chen","Qingyue Wang","Islem Rekik"],"pdf_url":"https://arxiv.org/pdf/2403.09139v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2309.17175v2","updated":"2024-03-14T07:36:29Z","published":"2023-09-29T12:14:41Z","title":"TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy\n  Text Fields","summary":"  Recent works learn 3D representation explicitly under text-3D guidance.\nHowever, limited text-3D data restricts the vocabulary scale and text control\nof generations. Generators may easily fall into a stereotype concept for\ncertain text prompts, thus losing open-vocabulary generation ability. To tackle\nthis issue, we introduce a conditional 3D generative model, namely TextField3D.\nSpecifically, rather than using the text prompts as input directly, we suggest\nto inject dynamic noise into the latent space of given text prompts, i.e.,\nNoisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the\nappropriate range of textual latent space that is expanded by NTFs. To this\nend, an NTFGen module is proposed to model general text latent code in noisy\nfields. Meanwhile, an NTFBind module is proposed to align view-invariant image\nlatent code to noisy fields, further supporting image-conditional 3D\ngeneration. To guide the conditional generation in both geometry and texture,\nmulti-modal discrimination is constructed with a text-3D discriminator and a\ntext-2.5D discriminator. Compared to previous methods, TextField3D includes\nthree merits: 1) large vocabulary, 2) text consistency, and 3) low latency.\nExtensive experiments demonstrate that our method achieves a potential\nopen-vocabulary 3D generation capability.\n","authors":["Tianyu Huang","Yihan Zeng","Bowen Dong","Hang Xu","Songcen Xu","Rynson W. H. Lau","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2309.17175v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09136v1","updated":"2024-03-14T07:21:46Z","published":"2024-03-14T07:21:46Z","title":"Biophysics Informed Pathological Regularisation for Brain Tumour\n  Segmentation","summary":"  Recent advancements in deep learning have significantly improved brain tumour\nsegmentation techniques; however, the results still lack confidence and\nrobustness as they solely consider image data without biophysical priors or\npathological information. Integrating biophysics-informed regularisation is one\neffective way to change this situation, as it provides an prior regularisation\nfor automated end-to-end learning. In this paper, we propose a novel approach\nthat designs brain tumour growth Partial Differential Equation (PDE) models as\na regularisation with deep learning, operational with any network model. Our\nmethod introduces tumour growth PDE models directly into the segmentation\nprocess, improving accuracy and robustness, especially in data-scarce\nscenarios. This system estimates tumour cell density using a periodic\nactivation function. By effectively integrating this estimation with\nbiophysical models, we achieve a better capture of tumour characteristics. This\napproach not only aligns the segmentation closer to actual biological behaviour\nbut also strengthens the model's performance under limited data conditions. We\ndemonstrate the effectiveness of our framework through extensive experiments on\nthe BraTS 2023 dataset, showcasing significant improvements in both precision\nand reliability of tumour segmentation.\n","authors":["Lipei Zhang","Yanqi Cheng","Lihao Liu","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2403.09136v1.pdf","comment":"11 pages, 4 figures and 1 table"},{"id":"http://arxiv.org/abs/2403.08551v2","updated":"2024-03-14T06:32:00Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 1000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09128v1","updated":"2024-03-14T06:26:34Z","published":"2024-03-14T06:26:34Z","title":"Rethinking Referring Object Removal","summary":"  Referring object removal refers to removing the specific object in an image\nreferred by natural language expressions and filling the missing region with\nreasonable semantics. To address this task, we construct the ComCOCO, a\nsynthetic dataset consisting of 136,495 referring expressions for 34,615\nobjects in 23,951 image pairs. Each pair contains an image with referring\nexpressions and the ground truth after elimination. We further propose an\nend-to-end syntax-aware hybrid mapping network with an encoding-decoding\nstructure. Linguistic features are hierarchically extracted at the syntactic\nlevel and fused in the downsampling process of visual features with multi-head\nattention. The feature-aligned pyramid network is leveraged to generate\nsegmentation masks and replace internal pixels with region affinity learned\nfrom external semantics in high-level feature maps. Extensive experiments\ndemonstrate that our model outperforms diffusion models and two-stage methods\nwhich process the segmentation and inpainting task separately by a significant\nmargin.\n","authors":["Xiangtian Xue","Jiasong Wu","Youyong Kong","Lotfi Senhadji","Huazhong Shu"],"pdf_url":"https://arxiv.org/pdf/2403.09128v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.09599v1","updated":"2024-03-14T17:40:20Z","published":"2024-03-14T17:40:20Z","title":"Logical Discrete Graphical Models Must Supplement Large Language Models\n  for Information Synthesis","summary":"  Given the emergent reasoning abilities of large language models, information\nretrieval is becoming more complex. Rather than just retrieve a document,\nmodern information retrieval systems advertise that they can synthesize an\nanswer based on potentially many different documents, conflicting data sources,\nand using reasoning. We review recent literature and argue that the large\nlanguage model has crucial flaws that prevent it from on its own ever\nconstituting general intelligence, or answering general information synthesis\nrequests. This review shows that the following are problems for large language\nmodels: hallucinations, complex reasoning, planning under uncertainty, and\ncomplex calculations. We outline how logical discrete graphical models can\nsolve all of these problems, and outline a method of training a logical\ndiscrete model from unlabeled text.\n","authors":["Gregory Coppola"],"pdf_url":"https://arxiv.org/pdf/2403.09599v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2402.06557"},{"id":"http://arxiv.org/abs/2310.05116v3","updated":"2024-03-14T12:39:14Z","published":"2023-10-08T11:09:16Z","title":"Utilizing Contextual Clues and Role Correlations for Enhancing\n  Document-level Event Argument Extraction","summary":"  Document-level event argument extraction is a crucial yet challenging task\nwithin the field of information extraction. Current mainstream approaches\nprimarily focus on the information interaction between event triggers and their\narguments, facing two limitations: insufficient context interaction and the\nignorance of event correlations. Here, we introduce a novel framework named\nCARLG (Contextual Aggregation of clues and Role-based Latent Guidance),\ncomprising two innovative components: the Contextual Clues Aggregation (CCA)\nand the Role-based Latent Information Guidance (RLIG). The CCA module leverages\nthe attention weights derived from a pre-trained encoder to adaptively\nassimilates broader contextual information, while the RLIG module aims to\ncapture the semantic correlations among event roles. We then instantiate the\nCARLG framework into two variants based on two types of current mainstream EAE\napproaches. Notably, our CARLG framework introduces less than 1% new parameters\nyet significantly improving the performance. Comprehensive experiments across\nthe RAMS, WikiEvents, and MLEE datasets confirm the superiority of CARLG,\nshowing significant superiority in terms of both performance and inference\nspeed compared to major benchmarks. Further analyses demonstrate the\neffectiveness of the proposed modules.\n","authors":["Wanlong Liu","Dingyi Zeng","Li Zhou","Yichen Xiao","Weishan Kong","Malu Zhang","Shaohuan Cheng","Hongyang Zhao","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05116v3.pdf","comment":"pre-submission"},{"id":"http://arxiv.org/abs/2403.09298v1","updated":"2024-03-14T11:37:02Z","published":"2024-03-14T11:37:02Z","title":"More than words: Advancements and challenges in speech recognition for\n  singing","summary":"  This paper addresses the challenges and advancements in speech recognition\nfor singing, a domain distinctly different from standard speech recognition.\nSinging encompasses unique challenges, including extensive pitch variations,\ndiverse vocal styles, and background music interference. We explore key areas\nsuch as phoneme recognition, language identification in songs, keyword\nspotting, and full lyrics transcription. I will describe some of my own\nexperiences when performing research on these tasks just as they were starting\nto gain traction, but will also show how recent developments in deep learning\nand large-scale datasets have propelled progress in this field. My goal is to\nilluminate the complexities of applying speech recognition to singing, evaluate\ncurrent capabilities, and outline future research directions.\n","authors":["Anna Kruspe"],"pdf_url":"https://arxiv.org/pdf/2403.09298v1.pdf","comment":"Conference on Electronic Speech Signal Processing (ESSV) 2024,\n  Keynote"},{"id":"http://arxiv.org/abs/2403.09295v1","updated":"2024-03-14T11:35:08Z","published":"2024-03-14T11:35:08Z","title":"Seed-based information retrieval in networks of research publications:\n  Evaluation of direct citations, bibliographic coupling, co-citations and\n  PubMed related article score","summary":"  In this contribution, we deal with seed-based information retrieval in\nnetworks of research publications. Using systematic reviews as a baseline, and\npublication data from the NIH Open Citation Collection, we compare the\nperformance of the three citation-based approaches direct citation,\nco-citation, and bibliographic coupling with respect to recall and precision\nmeasures. In addition, we include the PubMed Related Article score as well as\ncombined approaches in the comparison. We also provide a fairly comprehensive\nreview of earlier research in which citation relations have been used for\ninformation retrieval purposes. The results show an advantage for co-citation\nover bibliographic coupling and direct citation. However, combining the three\napproaches outperforms the exclusive use of co-citation in the study. The\nresults further indicate, in line with previous research, that combining\ncitation-based approaches with textual approaches enhances the performance of\nseed-based information retrieval. The results from the study may guide\napproaches combining citation-based and textual approaches in their choice of\ncitation similarity measures. We suggest that future research use more\nstructured approaches to evaluate methods for seed-based retrieval of\npublications, including comparative approaches as well as the elaboration of\ncommon data sets and baselines for evaluation.\n","authors":["Peter Sjögårde","Per Ahlgren"],"pdf_url":"https://arxiv.org/pdf/2403.09295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09180v1","updated":"2024-03-14T08:47:32Z","published":"2024-03-14T08:47:32Z","title":"Online and Offline Evaluation in Search Clarification","summary":"  The effectiveness of clarification question models in engaging users within\nsearch systems is currently constrained, casting doubt on their overall\nusefulness. To improve the performance of these models, it is crucial to employ\nassessment approaches that encompass both real-time feedback from users (online\nevaluation) and the characteristics of clarification questions evaluated\nthrough human assessment (offline evaluation). However, the relationship\nbetween online and offline evaluations has been debated in information\nretrieval. This study aims to investigate how this discordance holds in search\nclarification. We use user engagement as ground truth and employ several\noffline labels to investigate to what extent the offline ranked lists of\nclarification resemble the ideal ranked lists based on online user engagement.\n","authors":["Leila Tavakoli","Johanne R. Trippas","Hamed Zamani","Falk Scholer","Mark Sanderson"],"pdf_url":"https://arxiv.org/pdf/2403.09180v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2305.13068v3","updated":"2024-03-14T08:15:27Z","published":"2023-05-22T14:37:05Z","title":"Making Language Models Better Tool Learners with Execution Feedback","summary":"  Tools serve as pivotal interfaces that enable humans to understand and\nreshape the environment. With the advent of foundation models, AI systems can\nutilize tools to expand their capabilities and interact with the real world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce large language models to utilize\ntools indiscriminately, as complex tasks often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the large language model\nselectively use tools by improving the accuracy of tool usage while enhancing\ninsufficient tool learning and mitigating excessive reliance on tools. Code is\navailable at https://github.com/zjunlp/TRICE.\n","authors":["Shuofei Qiao","Honghao Gui","Chengfei Lv","Qianghuai Jia","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13068v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09148v1","updated":"2024-03-14T07:58:27Z","published":"2024-03-14T07:58:27Z","title":"Evaluating LLMs for Gender Disparities in Notable Persons","summary":"  This study examines the use of Large Language Models (LLMs) for retrieving\nfactual information, addressing concerns over their propensity to produce\nfactually incorrect \"hallucinated\" responses or to altogether decline to even\nanswer prompt at all. Specifically, it investigates the presence of\ngender-based biases in LLMs' responses to factual inquiries. This paper takes a\nmulti-pronged approach to evaluating GPT models by evaluating fairness across\nmultiple dimensions of recall, hallucinations and declinations. Our findings\nreveal discernible gender disparities in the responses generated by GPT-3.5.\nWhile advancements in GPT-4 have led to improvements in performance, they have\nnot fully eradicated these gender disparities, notably in instances where\nresponses are declined. The study further explores the origins of these\ndisparities by examining the influence of gender associations in prompts and\nthe homogeneity in the responses.\n","authors":["Lauren Rhue","Sofie Goethals","Arun Sundararajan"],"pdf_url":"https://arxiv.org/pdf/2403.09148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09142v1","updated":"2024-03-14T07:40:54Z","published":"2024-03-14T07:40:54Z","title":"USimAgent: Large Language Models for Simulating Search Users","summary":"  Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators.\n","authors":["Erhan Zhang","Xingzhu Wang","Peiyuan Gong","Yankai Lin","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2403.09142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09031v1","updated":"2024-03-14T01:46:56Z","published":"2024-03-14T01:46:56Z","title":"Projected Gradient Descent for Spectral Compressed Sensing via Symmetric\n  Hankel Factorization","summary":"  Current spectral compressed sensing methods via Hankel matrix completion\nemploy symmetric factorization to demonstrate the low-rank property of the\nHankel matrix. However, previous non-convex gradient methods only utilize\nasymmetric factorization to achieve spectral compressed sensing. In this paper,\nwe propose a novel nonconvex projected gradient descent method for spectral\ncompressed sensing via symmetric factorization named Symmetric Hankel Projected\nGradient Descent (SHGD), which updates only one matrix and avoids a balancing\nregularization term. SHGD reduces about half of the computation and storage\ncosts compared to the prior gradient method based on asymmetric factorization.\n{Besides, the symmetric factorization employed in our work is completely novel\nto the prior low-rank factorization model, introducing a new factorization\nambiguity under complex orthogonal transformation}. Novel distance metrics are\ndesigned for our factorization method and a linear convergence guarantee to the\ndesired signal is established with $O(r^2\\log(n))$ observations. Numerical\nsimulations demonstrate the superior performance of the proposed SHGD method in\nphase transitions and computation efficiency compared to state-of-the-art\nmethods.\n","authors":["Jinsheng Li","Wei Cui","Xu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.09031v1.pdf","comment":"accepted in IEEE Transactions on Signal Processing"},{"id":"http://arxiv.org/abs/2403.00895v2","updated":"2024-03-14T01:28:25Z","published":"2024-03-01T15:32:44Z","title":"End-to-end Graph-Sequential Representation Learning for Accurate\n  Recommendations","summary":"  Recent recommender system advancements have focused on developing\nsequence-based and graph-based approaches. Both approaches proved useful in\nmodeling intricate relationships within behavioral data, leading to promising\noutcomes in personalized ranking and next-item recommendation tasks while\nmaintaining good scalability. However, they capture very different signals from\ndata. While the former approach represents users directly through ordered\ninteractions with recent items, the latter aims to capture indirect\ndependencies across the interactions graph. This paper presents a novel\nmulti-representational learning framework exploiting these two paradigms'\nsynergies. Our empirical evaluation on several datasets demonstrates that\nmutual training of sequential and graph components with the proposed framework\nsignificantly improves recommendations performance.\n","authors":["Vladimir Baikalov","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2403.00895v2.pdf","comment":"4 pages, 1 figure, submitted to WWW'24, short-paper track"},{"id":"http://arxiv.org/abs/2402.13897v2","updated":"2024-03-14T00:21:09Z","published":"2024-02-21T16:09:25Z","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and\n  Logical Reasoning","summary":"  Information retrieval is a rapidly evolving field. However it still faces\nsignificant limitations in the scientific and industrial vast amounts of\ninformation, such as semantic divergence and vocabulary gaps in sparse\nretrieval, low precision and lack of interpretability in semantic search, or\nhallucination and outdated information in generative models. In this paper, we\nintroduce a two-block approach to tackle these hurdles for long documents. The\nfirst block enhances language understanding in sparse retrieval by query\nexpansion to retrieve relevant documents. The second block deepens the result\nby providing comprehensive and informative answers to the complex question\nusing only the information spread in the long document, enabling bidirectional\nengagement. At various stages of the pipeline, intermediate results are\npresented to users to facilitate understanding of the system's reasoning. We\nbelieve this bidirectional approach brings significant advancements in terms of\ntransparency, logical thinking, and comprehensive understanding in the field of\nscientific information retrieval.\n","authors":["Loïc Rakotoson","Sylvain Massip","Fréjus A. A. Laleye"],"pdf_url":"https://arxiv.org/pdf/2402.13897v2.pdf","comment":"6 pages, 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.09635v1","updated":"2024-03-14T17:59:14Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v1.pdf","comment":"Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.\n  Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable"},{"id":"http://arxiv.org/abs/2310.07240v3","updated":"2024-03-14T17:58:52Z","published":"2023-10-11T07:08:20Z","title":"CacheGen: Fast Context Loading for Language Model Applications via KV\n  Cache Streaming","summary":"  As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge or\nuser-specific information. Yet using long contexts poses a challenge for\nresponsive LLM systems, as nothing can be generated until the whole context is\nprocessed by the LLM. While the context-processing delay can be reduced by\nreusing the KV cache of a context across different inputs, fetching the KV\ncache, which contains large tensors, over the network can cause extra network\ndelays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, which embraces KV cache's distributional\nproperties, to encode a KV cache into more compact bitstream representations\nwith negligible encoding/decoding overhead. This reduces the bandwidth demand\nto fetch the KV cache. Second, to maintain low context-loading delay and high\ngeneration quality, CacheGen adapts the streaming strategies to cope with\nchanges in available bandwidth. When available bandwidth drops, CacheGen may\nraise the compression level for a part of the context or choose to recompute\nits KV cache on the fly. We test CacheGen on four popular LLMs of various sizes\nand four datasets (662 contexts in total). Compared to the recent systems that\nreuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the\ntotal delay in fetching and processing contexts by 2.7-3.2x while having\nnegligible impact on the LLM response quality in accuracy or perplexity.\n","authors":["Yuhan Liu","Hanchen Li","Yihua Cheng","Siddhant Ray","Yuyang Huang","Qizheng Zhang","Kuntai Du","Jiayi Yao","Shan Lu","Ganesh Ananthanarayanan","Michael Maire","Henry Hoffmann","Ari Holtzman","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.07240v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09629v1","updated":"2024-03-14T17:58:16Z","published":"2024-03-14T17:58:16Z","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking","summary":"  When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.\n","authors":["Eric Zelikman","Georges Harik","Yijia Shao","Varuna Jayasiri","Nick Haber","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.09629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09625v1","updated":"2024-03-14T17:57:04Z","published":"2024-03-14T17:57:04Z","title":"Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation","summary":"  Recent years have witnessed the strong power of 3D generation models, which\noffer a new level of creative flexibility by allowing users to guide the 3D\ncontent generation process through a single image or natural language. However,\nit remains challenging for existing 3D generation methods to create\nsubject-driven 3D content across diverse prompts. In this paper, we introduce a\nnovel 3D customization method, dubbed Make-Your-3D that can personalize\nhigh-fidelity and consistent 3D content from only a single image of a subject\nwith text description within 5 minutes. Our key insight is to harmonize the\ndistributions of a multi-view diffusion model and an identity-specific 2D\ngenerative model, aligning them with the distribution of the desired 3D\nsubject. Specifically, we design a co-evolution framework to reduce the\nvariance of distributions, where each model undergoes a process of learning\nfrom the other through identity-aware optimization and subject-prior\noptimization, respectively. Extensive experiments demonstrate that our method\ncan produce high-quality, consistent, and subject-specific 3D content with\ntext-driven modifications that are unseen in subject image.\n","authors":["Fangfu Liu","Hanyang Wang","Weiliang Chen","Haowen Sun","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2403.09625v1.pdf","comment":"Project page: https://liuff19.github.io/Make-Your-3D"},{"id":"http://arxiv.org/abs/2403.09621v1","updated":"2024-03-14T17:55:10Z","published":"2024-03-14T17:55:10Z","title":"Minimax Optimal and Computationally Efficient Algorithms for\n  Distributionally Robust Offline Reinforcement Learning","summary":"  Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a variety of new techniques, involving\na novel function approximation mechanism incorporating variance information, a\nnew procedure of suboptimality and estimation uncertainty decomposition, a\nquantification of the robust value function shrinkage, and a meticulously\ndesigned family of hard instances, which might be of independent interest.\n","authors":["Zhishuai Liu","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.09621v1.pdf","comment":"53 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2310.17467v2","updated":"2024-03-14T17:51:56Z","published":"2023-10-26T15:15:01Z","title":"The statistical thermodynamics of generative diffusion models: Phase\n  transitions, symmetry breaking and critical instability","summary":"  Generative diffusion models have achieved spectacular performance in many\nareas of generative modeling. While the fundamental ideas behind these models\ncome from non-equilibrium physics, variational inference and stochastic\ncalculus, in this paper we show that many aspects of these models can be\nunderstood using the tools of equilibrium statistical mechanics. Using this\nreformulation, we show that generative diffusion models undergo second-order\nphase transitions corresponding to symmetry breaking phenomena. We show that\nthese phase-transitions are always in a mean-field universality class, as they\nare the result of a self-consistency condition in the generative dynamics. We\nargue that the critical instability that arises from the phase transitions lies\nat the heart of their generative capabilities, which are characterized by a set\nof mean field critical exponents. Furthermore, using the statistical physics of\ndisordered systems, we show that memorization can be understood as a form of\ncritical condensation corresponding to a disordered phase transition. Finally,\nwe show that the dynamic equation of the generative process can be interpreted\nas a stochastic adiabatic transformation that minimizes the free energy while\nkeeping the system in thermal equilibrium.\n","authors":["Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2310.17467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09613v1","updated":"2024-03-14T17:51:54Z","published":"2024-03-14T17:51:54Z","title":"Reawakening knowledge: Anticipatory recovery from catastrophic\n  interference via structured training","summary":"  We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.\n","authors":["Yanlai Yang","Matt Jones","Michael C. Mozer","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09613v1.pdf","comment":"19 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.09612v1","updated":"2024-03-14T17:51:38Z","published":"2024-03-14T17:51:38Z","title":"Compute-first optical detection for noise-resilient visual perception","summary":"  In the context of visual perception, the optical signal from a scene is\ntransferred into the electronic domain by detectors in the form of image data,\nwhich are then processed for the extraction of visual information. In noisy and\nweak-signal environments such as thermal imaging for night vision applications,\nhowever, the performance of neural computing tasks faces a significant\nbottleneck due to the inherent degradation of data quality upon noisy\ndetection. Here, we propose a concept of optical signal processing before\ndetection to address this issue. We demonstrate that spatially redistributing\noptical signals through a properly designed linear transformer can enhance the\ndetection noise resilience of visual perception tasks, as benchmarked with the\nMNIST classification. Our idea is supported by a quantitative analysis\ndetailing the relationship between signal concentration and noise robustness,\nas well as its practical implementation in an incoherent imaging system. This\ncompute-first detection scheme can pave the way for advancing infrared machine\nvision technologies widely used for industrial and defense applications.\n","authors":["Jungmin Kim","Nanfang Yu","Zongfu Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09612v1.pdf","comment":"Main 9 pages, 5 figures, Supplementary information 5 pages"},{"id":"http://arxiv.org/abs/2403.09611v1","updated":"2024-03-14T17:51:32Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, consisting of\nboth dense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09603v1","updated":"2024-03-14T17:44:35Z","published":"2024-03-14T17:44:35Z","title":"Optimistic Verifiable Training by Controlling Hardware Nondeterminism","summary":"  The increasing compute demands of AI systems has led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning, poses challenges. Existing works\non verifiable training largely fall into two classes: proof-based systems,\nwhich struggle to scale due to requiring cryptographic techniques, and\n\"optimistic\" methods that consider a trusted third-party auditor who replicates\nthe training process. A key challenge with the latter is that hardware\nnondeterminism between GPU types during training prevents an auditor from\nreplicating the training process exactly, and such schemes are therefore\nnon-robust. We propose a method that combines training in a higher precision\nthan the target model, rounding after intermediate computation steps, and\nstoring rounding decisions based on an adaptive thresholding procedure, to\nsuccessfully control for nondeterminism. Across three different NVIDIA GPUs\n(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32\nprecision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2\n(117M) models. Our verifiable training scheme significantly decreases the\nstorage and time costs compared to proof-based systems.\n","authors":["Megha Srivastava","Simran Arora","Dan Boneh"],"pdf_url":"https://arxiv.org/pdf/2403.09603v1.pdf","comment":"11 pages, 5 figures, preprint"},{"id":"http://arxiv.org/abs/2403.09598v1","updated":"2024-03-14T17:39:14Z","published":"2024-03-14T17:39:14Z","title":"Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds","summary":"  Multi-label imbalanced classification poses a significant challenge in\nmachine learning, particularly evident in bioacoustics where animal sounds\noften co-occur, and certain sounds are much less frequent than others. This\npaper focuses on the specific case of classifying anuran species sounds using\nthe dataset AnuraSet, that contains both class imbalance and multi-label\nexamples. To address these challenges, we introduce Mixture of Mixups (Mix2), a\nframework that leverages mixing regularization methods Mixup, Manifold Mixup,\nand MultiMix. Experimental results show that these methods, individually, may\nlead to suboptimal results; however, when applied randomly, with one selected\nat each training iteration, they prove effective in addressing the mentioned\nchallenges, particularly for rare classes with few occurrences. Further\nanalysis reveals that Mix2 is also proficient in classifying sounds across\nvarious levels of class co-occurrences.\n","authors":["Ilyass Moummad","Nicolas Farrugia","Romain Serizel","Jeremy Froidevaux","Vincent Lostanlen"],"pdf_url":"https://arxiv.org/pdf/2403.09598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14496v4","updated":"2024-03-14T17:31:06Z","published":"2023-09-25T19:45:45Z","title":"Era Splitting -- Invariant Learning for Decision Trees","summary":"  Real-life machine learning problems exhibit distributional shifts in the data\nfrom one time to another or from one place to another. This behavior is beyond\nthe scope of the traditional empirical risk minimization paradigm, which\nassumes i.i.d. distribution of data over time and across locations. The\nemerging field of out-of-distribution (OOD) generalization addresses this\nreality with new theory and algorithms which incorporate environmental, or\nera-wise information into the algorithms. So far, most research has been\nfocused on linear models and/or neural networks. In this research we develop\ntwo new splitting criteria for decision trees, which allow us to apply ideas\nfrom OOD generalization research to decision tree models, namely, gradient\nboosting decision trees (GBDT). The new splitting criteria use era-wise\ninformation associated with the data to grow tree-based models that are optimal\nacross all disjoint eras in the data, instead of optimal over the entire data\nset pooled together, which is the default setting. In this paper, two new\nsplitting criteria are defined and analyzed theoretically. Effectiveness is\ntested on four experiments, ranging from simple, synthetic to complex,\nreal-world applications. In particular we cast the OOD domain-adaptation\nproblem in the context of financial markets, where the new models out-perform\nstate-of-the-art GBDT models on the Numerai data set. The new criteria are\nincorporated into the Scikit-Learn code base and made freely available online.\n","authors":["Timothy DeLise"],"pdf_url":"https://arxiv.org/pdf/2309.14496v4.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.09588v1","updated":"2024-03-14T17:26:00Z","published":"2024-03-14T17:26:00Z","title":"Iterative Forgetting: Online Data Stream Regression Using\n  Database-Inspired Adaptive Granulation","summary":"  Many modern systems, such as financial, transportation, and\ntelecommunications systems, are time-sensitive in the sense that they demand\nlow-latency predictions for real-time decision-making. Such systems often have\nto contend with continuous unbounded data streams as well as concept drift,\nwhich are challenging requirements that traditional regression techniques are\nunable to cater to. There exists a need to create novel data stream regression\nmethods that can handle these scenarios. We present a database-inspired\ndatastream regression model that (a) uses inspiration from R*-trees to create\ngranules from incoming datastreams such that relevant information is retained,\n(b) iteratively forgets granules whose information is deemed to be outdated,\nthus maintaining a list of only recent, relevant granules, and (c) uses the\nrecent data and granules to provide low-latency predictions. The\nR*-tree-inspired approach also makes the algorithm amenable to integration with\ndatabase systems. Our experiments demonstrate that the ability of this method\nto discard data produces a significant order-of-magnitude improvement in\nlatency and training time when evaluated against the most accurate\nstate-of-the-art algorithms, while the R*-tree-inspired granulation technique\nprovides competitively accurate predictions\n","authors":["Niket Kathiriya","Hossein Haeri","Cindy Chen","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.09588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00860v3","updated":"2024-03-14T17:21:37Z","published":"2023-11-01T21:28:24Z","title":"Zero Coordinate Shift: Whetted Automatic Differentiation for\n  Physics-informed Operator Learning","summary":"  Automatic differentiation (AD) is a critical step in physics-informed machine\nlearning, required for computing the high-order derivatives of network output\nw.r.t. coordinates of collocation points. In this paper, we present a novel and\nlightweight algorithm to conduct AD for physics-informed operator learning,\nwhich we call the trick of Zero Coordinate Shift (ZCS). Instead of making all\nsampled coordinates as leaf variables, ZCS introduces only one scalar-valued\nleaf variable for each spatial or temporal dimension, simplifying the wanted\nderivatives from \"many-roots-many-leaves\" to \"one-root-many-leaves\" whereby\nreverse-mode AD becomes directly utilisable. It has led to an outstanding\nperformance leap by avoiding the duplication of the computational graph along\nthe dimension of functions (physical parameters). ZCS is easy to implement with\ncurrent deep learning libraries; our own implementation is achieved by\nextending the DeepXDE package. We carry out a comprehensive benchmark analysis\nand several case studies, training physics-informed DeepONets to solve partial\ndifferential equations (PDEs) without data. The results show that ZCS has\npersistently reduced GPU memory consumption and wall time for training by an\norder of magnitude, and such reduction factor scales with the number of\nfunctions. As a low-level optimisation technique, ZCS imposes no restrictions\non data, physics (PDE) or network architecture and does not compromise training\nresults from any aspect.\n","authors":["Kuangdai Leng","Mallikarjun Shankar","Jeyan Thiyagalingam"],"pdf_url":"https://arxiv.org/pdf/2311.00860v3.pdf","comment":"Published in Journal of Computational Physics.\n  https://doi.org/10.1016/j.jcp.2024.112904"},{"id":"http://arxiv.org/abs/2403.09580v1","updated":"2024-03-14T17:14:53Z","published":"2024-03-14T17:14:53Z","title":"Algorithmic syntactic causal identification","summary":"  Causal identification in causal Bayes nets (CBNs) is an important tool in\ncausal inference allowing the derivation of interventional distributions from\nobservational distributions where this is possible in principle. However, most\nexisting formulations of causal identification using techniques such as\nd-separation and do-calculus are expressed within the mathematical language of\nclassical probability theory on CBNs. However, there are many causal settings\nwhere probability theory and hence current causal identification techniques are\ninapplicable such as relational databases, dataflow programs such as hardware\ndescription languages, distributed systems and most modern machine learning\nalgorithms. We show that this restriction can be lifted by replacing the use of\nclassical probability theory with the alternative axiomatic foundation of\nsymmetric monoidal categories. In this alternative axiomatization, we show how\nan unambiguous and clean distinction can be drawn between the general syntax of\ncausal models and any specific semantic implementation of that causal model.\nThis allows a purely syntactic algorithmic description of general causal\nidentification by a translation of recent formulations of the general ID\nalgorithm through fixing. Our description is given entirely in terms of the\nnon-parametric ADMG structure specifying a causal model and the algebraic\nsignature of the corresponding monoidal category, to which a sequence of\nmanipulations is then applied so as to arrive at a modified monoidal category\nin which the desired, purely syntactic interventional causal model, is\nobtained. We use this idea to derive purely syntactic analogues of classical\nback-door and front-door causal adjustment, and illustrate an application to a\nmore complex causal model.\n","authors":["Dhurim Cakiqi","Max A. Little"],"pdf_url":"https://arxiv.org/pdf/2403.09580v1.pdf","comment":"11 pages, 2 TikZ figures"},{"id":"http://arxiv.org/abs/2403.09579v1","updated":"2024-03-14T17:13:37Z","published":"2024-03-14T17:13:37Z","title":"uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with\n  Unsupervised Audio Mixtures","summary":"  Masked Autoencoders (MAEs) learn rich low-level representations from\nunlabeled data but require substantial labeled data to effectively adapt to\ndownstream tasks. Conversely, Instance Discrimination (ID) emphasizes\nhigh-level semantics, offering a potential solution to alleviate annotation\nrequirements in MAEs. Although combining these two approaches can address\ndownstream tasks with limited labeled data, naively integrating ID into MAEs\nleads to extended training times and high computational costs. To address this\nchallenge, we introduce uaMix-MAE, an efficient ID tuning strategy that\nleverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE\naligns the representations of pretrained MAEs, thereby facilitating effective\nadaptation to task-specific semantics. To optimize the model with small amounts\nof unlabeled data, we propose an audio mixing technique that manipulates audio\nsamples in both input and virtual label spaces. Experiments in low/few-shot\nsettings demonstrate that \\modelname achieves 4-6% accuracy improvements over\nvarious benchmarks when tuned with limited unlabeled data, such as\nAudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE\n","authors":["Afrina Tabassum","Dung Tran","Trung Dang","Ismini Lourentzou","Kazuhito Koishida"],"pdf_url":"https://arxiv.org/pdf/2403.09579v1.pdf","comment":"5 pages, 6 figures, 4 tables. To appear in ICASSP'2024"},{"id":"http://arxiv.org/abs/2403.02524v2","updated":"2024-03-14T17:04:37Z","published":"2024-03-04T22:28:20Z","title":"Koopman operators with intrinsic observables in rigged reproducing\n  kernel Hilbert spaces","summary":"  This paper presents a novel approach for estimating the Koopman operator\ndefined on a reproducing kernel Hilbert space (RKHS) and its spectra. We\npropose an estimation method, what we call Jet Dynamic Mode Decomposition\n(JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion\nknown as jets to enhance the estimation of the Koopman operator. This method\nrefines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy,\nespecially in the numerical estimation of eigenvalues. This paper proves\nJetDMD's superiority through explicit error bounds and convergence rate for\nspecial positive definite kernels, offering a solid theoretical foundation for\nits performance. We also delve into the spectral analysis of the Koopman\noperator, proposing the notion of extended Koopman operator within a framework\nof rigged Hilbert space. This notion leads to a deeper understanding of\nestimated Koopman eigenfunctions and capturing them outside the original\nfunction space. Through the theory of rigged Hilbert space, our study provides\na principled methodology to analyze the estimated spectrum and eigenfunctions\nof Koopman operators, and enables eigendecomposition within a rigged RKHS. We\nalso propose a new effective method for reconstructing the dynamical system\nfrom temporally-sampled trajectory data of the dynamical system with solid\ntheoretical guarantee. We conduct several numerical simulations using the van\nder Pol oscillator, the Duffing oscillator, the H\\'enon map, and the Lorenz\nattractor, and illustrate the performance of JetDMD with clear numerical\ncomputations of eigenvalues and accurate predictions of the dynamical systems.\n","authors":["Isao Ishikawa","Yuka Hashimoto","Masahiro Ikeda","Yoshinobu Kawahara"],"pdf_url":"https://arxiv.org/pdf/2403.02524v2.pdf","comment":"We correct several typos. We have released the code for the numerical\n  simulation at https://github.com/1sa014kawa/JetDMD"},{"id":"http://arxiv.org/abs/2403.09571v1","updated":"2024-03-14T17:00:29Z","published":"2024-03-14T17:00:29Z","title":"Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis","summary":"  The tremendous hype around autonomous driving is eagerly calling for emerging\nand novel technologies to support advanced mobility use cases. As car\nmanufactures keep developing SAE level 3+ systems to improve the safety and\ncomfort of passengers, traffic authorities need to establish new procedures to\nmanage the transition from human-driven to fully-autonomous vehicles while\nproviding a feedback-loop mechanism to fine-tune envisioned autonomous systems.\nThus, a way to automatically profile autonomous vehicles and differentiate\nthose from human-driven ones is a must. In this paper, we present a\nfully-fledged framework that monitors active vehicles using camera images and\nstate information in order to determine whether vehicles are autonomous,\nwithout requiring any active notification from the vehicles themselves.\nEssentially, it builds on the cooperation among vehicles, which share their\ndata acquired on the road feeding a machine learning model to identify\nautonomous cars. We extensively tested our solution and created the NexusStreet\ndataset, by means of the CARLA simulator, employing an autonomous driving\ncontrol agent and a steering wheel maneuvered by licensed drivers. Experiments\nshow it is possible to discriminate the two behaviors by analyzing video clips\nwith an accuracy of 80%, which improves up to 93% when the target state\ninformation is available. Lastly, we deliberately degraded the state to observe\nhow the framework performs under non-ideal data collection conditions.\n","authors":["Fabio Maresca","Filippo Grazioli","Antonio Albanese","Vincenzo Sciancalepore","Gianpiero Negri","Xavier Costa-Perez"],"pdf_url":"https://arxiv.org/pdf/2403.09571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09570v1","updated":"2024-03-14T17:00:01Z","published":"2024-03-14T17:00:01Z","title":"Multi-Fidelity Bayesian Optimization With Across-Task Transferable\n  Max-Value Entropy Search","summary":"  In many applications, ranging from logistics to engineering, a designer is\nfaced with a sequence of optimization tasks for which the objectives are in the\nform of black-box functions that are costly to evaluate. For example, the\ndesigner may need to tune the hyperparameters of neural network models for\ndifferent learning tasks over time. Rather than evaluating the objective\nfunction for each candidate solution, the designer may have access to\napproximations of the objective functions, for which higher-fidelity\nevaluations entail a larger cost. Existing multi-fidelity black-box\noptimization strategies select candidate solutions and fidelity levels with the\ngoal of maximizing the information accrued about the optimal value or solution\nfor the current task. Assuming that successive optimization tasks are related,\nthis paper introduces a novel information-theoretic acquisition function that\nbalances the need to acquire information about the current task with the goal\nof collecting information transferable to future tasks. The proposed method\nincludes shared inter-task latent variables, which are transferred across tasks\nby implementing particle-based variational Bayesian updates. Experimental\nresults across synthetic and real-world examples reveal that the proposed\nprovident acquisition strategy that caters to future tasks can significantly\nimprove the optimization efficiency as soon as a sufficient number of tasks is\nprocessed.\n","authors":["Yunchuan Zhang","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2403.09570v1.pdf","comment":"submitted to IEEE for review"},{"id":"http://arxiv.org/abs/2403.07865v2","updated":"2024-03-14T16:57:37Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Yu Qiao","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09560v1","updated":"2024-03-14T16:52:57Z","published":"2024-03-14T16:52:57Z","title":"Self-Consistency Training for Hamiltonian Prediction","summary":"  Hamiltonian prediction is a versatile formulation to leverage machine\nlearning for solving molecular science problems. Yet, its applicability is\nlimited by insufficient labeled data for training. In this work, we highlight\nthat Hamiltonian prediction possesses a self-consistency principle, based on\nwhich we propose an exact training method that does not require labeled data.\nThis merit addresses the data scarcity difficulty, and distinguishes the task\nfrom other property prediction formulations with unique benefits: (1)\nself-consistency training enables the model to be trained on a large amount of\nunlabeled data, hence substantially enhances generalization; (2)\nself-consistency training is more efficient than labeling data with DFT for\nsupervised training, since it is an amortization of DFT calculation over a set\nof molecular structures. We empirically demonstrate the better generalization\nin data-scarce and out-of-distribution scenarios, and the better efficiency\nfrom the amortization. These benefits push forward the applicability of\nHamiltonian prediction to an ever larger scale.\n","authors":["He Zhang","Chang Liu","Zun Wang","Xinran Wei","Siyuan Liu","Nanning Zheng","Bin Shao","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.09560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08399v2","updated":"2024-03-14T16:49:24Z","published":"2022-12-16T10:46:20Z","title":"Assessing the Impact of Sequence Length Learning on Classification Tasks\n  for Transformer Encoder Models","summary":"  Classification algorithms using Transformer architectures can be affected by\nthe sequence length learning problem whenever observations from different\nclasses have a different length distribution. This problem causes models to use\nsequence length as a predictive feature instead of relying on important textual\ninformation. Although most public datasets are not affected by this problem,\nprivately owned corpora for fields such as medicine and insurance may carry\nthis data bias. The exploitation of this sequence length feature poses\nchallenges throughout the value chain as these machine learning models can be\nused in critical applications. In this paper, we empirically expose this\nproblem and present approaches to minimize its impacts.\n","authors":["Jean-Thomas Baillargeon","Luc Lamontagne"],"pdf_url":"https://arxiv.org/pdf/2212.08399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08757v2","updated":"2024-03-14T16:40:51Z","published":"2024-03-13T17:55:34Z","title":"Efficient Combinatorial Optimization via Heat Diffusion","summary":"  Combinatorial optimization problems are widespread but inherently challenging\ndue to their discrete nature.The primary limitation of existing methods is that\nthey can only access a small fraction of the solution space at each iteration,\nresulting in limited efficiency for searching the global optimal. To overcome\nthis challenge, diverging from conventional efforts of expanding the solver's\nsearch scope, we focus on enabling information to actively propagate to the\nsolver through heat diffusion. By transforming the target function while\npreserving its optima, heat diffusion facilitates information flow from distant\nregions to the solver, providing more efficient navigation. Utilizing heat\ndiffusion, we propose a framework for solving general combinatorial\noptimization problems. The proposed methodology demonstrates superior\nperformance across a range of the most challenging and widely encountered\ncombinatorial optimizations. Echoing recent advancements in harnessing\nthermodynamics for generative artificial intelligence, our study further\nreveals its significant potential in advancing combinatorial optimization.\n","authors":["Hengyuan Ma","Wenlian Lu","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2403.08757v2.pdf","comment":"Code is available in https://github.com/AwakerMhy/HeO"},{"id":"http://arxiv.org/abs/2305.10424v8","updated":"2024-03-14T16:38:36Z","published":"2023-05-17T17:56:59Z","title":"ZeroFlow: Scalable Scene Flow via Distillation","summary":"  Scene flow estimation is the task of describing the 3D motion field between\ntemporally successive point clouds. State-of-the-art methods use strong priors\nand test-time optimization techniques, but require on the order of tens of\nseconds to process full-size point clouds, making them unusable as computer\nvision primitives for real-time applications such as open world object\ndetection. Feedforward methods are considerably faster, running on the order of\ntens to hundreds of milliseconds for full-size point clouds, but require\nexpensive human supervision. To address both limitations, we propose Scene Flow\nvia Distillation, a simple, scalable distillation framework that uses a\nlabel-free optimization method to produce pseudo-labels to supervise a\nfeedforward model. Our instantiation of this framework, ZeroFlow, achieves\nstate-of-the-art performance on the Argoverse 2 Self-Supervised Scene Flow\nChallenge while using zero human labels by simply training on large-scale,\ndiverse unlabeled data. At test-time, ZeroFlow is over 1000x faster than\nlabel-free state-of-the-art optimization-based methods on full-size point\nclouds (34 FPS vs 0.028 FPS) and over 1000x cheaper to train on unlabeled data\ncompared to the cost of human annotation (\\$394 vs ~\\$750,000). To facilitate\nfurther research, we release our code, trained model weights, and high quality\npseudo-labels for the Argoverse 2 and Waymo Open datasets at\nhttps://vedder.io/zeroflow.html\n","authors":["Kyle Vedder","Neehar Peri","Nathaniel Chodosh","Ishan Khatri","Eric Eaton","Dinesh Jayaraman","Yang Liu","Deva Ramanan","James Hays"],"pdf_url":"https://arxiv.org/pdf/2305.10424v8.pdf","comment":"Accepted to ICLR 2024. 9 pages, 4 pages of citations, 6 pages of\n  Supplemental. Project page with data releases is at\n  http://vedder.io/zeroflow.html"},{"id":"http://arxiv.org/abs/2403.09549v1","updated":"2024-03-14T16:38:02Z","published":"2024-03-14T16:38:02Z","title":"Generalizing Denoising to Non-Equilibrium Structures Improves\n  Equivariant Force Fields","summary":"  Understanding the interactions of atoms such as forces in 3D atomistic\nsystems is fundamental to many applications like molecular dynamics and\ncatalyst design. However, simulating these interactions requires\ncompute-intensive ab initio calculations and thus results in limited data for\ntraining neural networks. In this paper, we propose to use denoising\nnon-equilibrium structures (DeNS) as an auxiliary task to better leverage\ntraining data and improve performance. For training with DeNS, we first corrupt\na 3D structure by adding noise to its 3D coordinates and then predict the\nnoise. Different from previous works on denoising, which are limited to\nequilibrium structures, the proposed method generalizes denoising to a much\nlarger set of non-equilibrium structures. The main difference is that a\nnon-equilibrium structure does not correspond to local energy minima and has\nnon-zero forces, and therefore it can have many possible atomic positions\ncompared to an equilibrium structure. This makes denoising non-equilibrium\nstructures an ill-posed problem since the target of denoising is not uniquely\ndefined. Our key insight is to additionally encode the forces of the original\nnon-equilibrium structure to specify which non-equilibrium structure we are\ndenoising. Concretely, given a corrupted non-equilibrium structure and the\nforces of the original one, we predict the non-equilibrium structure satisfying\nthe input forces instead of any arbitrary structures. Since DeNS requires\nencoding forces, DeNS favors equivariant networks, which can easily incorporate\nforces and other higher-order tensors in node embeddings. We study the\neffectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17\ndatasets and demonstrate that DeNS can achieve new state-of-the-art results on\nOC20 and OC22 and significantly improve training efficiency on MD17.\n","authors":["Yi-Lun Liao","Tess Smidt","Abhishek Das"],"pdf_url":"https://arxiv.org/pdf/2403.09549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01308v2","updated":"2024-03-14T16:37:37Z","published":"2024-03-02T20:40:11Z","title":"VBART: The Turkish LLM","summary":"  We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.\n","authors":["Meliksah Turker","Mehmet Erdi Ari","Aydin Han"],"pdf_url":"https://arxiv.org/pdf/2403.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09548v1","updated":"2024-03-14T16:35:43Z","published":"2024-03-14T16:35:43Z","title":"Breast Cancer Classification Using Gradient Boosting Algorithms Focusing\n  on Reducing the False Negative and SHAP for Explainability","summary":"  Cancer is one of the diseases that kill the most women in the world, with\nbreast cancer being responsible for the highest number of cancer cases and\nconsequently deaths. However, it can be prevented by early detection and,\nconsequently, early treatment. Any development for detection or perdition this\nkind of cancer is important for a better healthy life. Many studies focus on a\nmodel with high accuracy in cancer prediction, but sometimes accuracy alone may\nnot always be a reliable metric. This study implies an investigative approach\nto studying the performance of different machine learning algorithms based on\nboosting to predict breast cancer focusing on the recall metric. Boosting\nmachine learning algorithms has been proven to be an effective tool for\ndetecting medical diseases. The dataset of the University of California, Irvine\n(UCI) repository has been utilized to train and test the model classifier that\ncontains their attributes. The main objective of this study is to use\nstate-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and\nLightGBM to predict and diagnose breast cancer and to find the most effective\nmetric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study\nis the first to use these four boosting algorithms with Optuna, a library for\nhyperparameter optimization, and the SHAP method to improve the\ninterpretability of our model, which can be used as a support to identify and\npredict breast cancer. We were able to improve AUC or recall for all the models\nand reduce the False Negative for AdaBoost and LigthGBM the final AUC were more\nthan 99.41\\% for all models.\n","authors":["João Manoel Herrera Pinheiro","Marcelo Becker"],"pdf_url":"https://arxiv.org/pdf/2403.09548v1.pdf","comment":"9 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.06726v2","updated":"2024-03-14T16:35:41Z","published":"2024-03-11T13:44:49Z","title":"Probabilistic Contrastive Learning for Long-Tailed Visual Recognition","summary":"  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n","authors":["Chaoqun Du","Yulin Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.06726v2.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)"},{"id":"http://arxiv.org/abs/2403.09547v1","updated":"2024-03-14T16:35:39Z","published":"2024-03-14T16:35:39Z","title":"How do Machine Learning Projects use Continuous Integration Practices?\n  An Empirical Study on GitHub Actions","summary":"  Continuous Integration (CI) is a well-established practice in traditional\nsoftware development, but its nuances in the domain of Machine Learning (ML)\nprojects remain relatively unexplored. Given the distinctive nature of ML\ndevelopment, understanding how CI practices are adopted in this context is\ncrucial for tailoring effective approaches. In this study, we conduct a\ncomprehensive analysis of 185 open-source projects on GitHub (93 ML and 92\nnon-ML projects). Our investigation comprises both quantitative and qualitative\ndimensions, aiming to uncover differences in CI adoption between ML and non-ML\nprojects. Our findings indicate that ML projects often require longer build\ndurations, and medium-sized ML projects exhibit lower test coverage compared to\nnon-ML projects. Moreover, small and medium-sized ML projects show a higher\nprevalence of increasing build duration trends compared to their non-ML\ncounterparts. Additionally, our qualitative analysis illuminates the\ndiscussions around CI in both ML and non-ML projects, encompassing themes like\nCI Build Execution and Status, CI Testing, and CI Infrastructure. These\ninsights shed light on the unique challenges faced by ML projects in adopting\nCI practices effectively.\n","authors":["João Helis Bernardo","Daniel Alencar da Costa","Sérgio Queiroz de Medeiros","Uirá Kulesza"],"pdf_url":"https://arxiv.org/pdf/2403.09547v1.pdf","comment":"10 pages, Mining Software Repositories, MSR 2024"},{"id":"http://arxiv.org/abs/2403.09543v1","updated":"2024-03-14T16:30:52Z","published":"2024-03-14T16:30:52Z","title":"Explorations in Texture Learning","summary":"  In this work, we investigate \\textit{texture learning}: the identification of\ntextures learned by object classification models, and the extent to which they\nrely on these textures. We build texture-object associations that uncover new\ninsights about the relationships between texture and object classes in CNNs and\nfind three classes of results: associations that are strong and expected,\nstrong and not expected, and expected but not present. Our analysis\ndemonstrates that investigations in texture learning enable new methods for\ninterpretability and have the potential to uncover unexpected biases.\n","authors":["Blaine Hoak","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2403.09543v1.pdf","comment":"Accepted to ICLR 2024, Tiny Papers Track"},{"id":"http://arxiv.org/abs/2309.10639v4","updated":"2024-03-14T16:29:56Z","published":"2023-09-19T14:20:55Z","title":"Geometric structure of Deep Learning networks and construction of global\n  ${\\mathcal L}^2$ minimizers","summary":"  In this paper, we explicitly determine local and global minimizers of the\n$\\mathcal{L}^2$ cost function in underparametrized Deep Learning (DL) networks;\nour main goal is to shed light on their geometric structure and properties. We\naccomplish this by a direct construction, without invoking the gradient descent\nflow at any point of this work. We specifically consider $L$ hidden layers, a\nReLU ramp activation function, an $\\mathcal{L}^2$ Schatten class (or\nHilbert-Schmidt) cost function, input and output spaces $\\mathbb{R}^Q$ with\nequal dimension $Q\\geq1$, and hidden layers also defined on $\\mathbb{R}^{Q}$;\nthe training inputs are assumed to be sufficiently clustered. The training\ninput size $N$ can be arbitrarily large - thus, we are considering the\nunderparametrized regime. More general settings are left to future work. We\nconstruct an explicit family of minimizers for the global minimum of the cost\nfunction in the case $L\\geq Q$, which we show to be degenerate. Moreover, we\ndetermine a set of $2^Q-1$ distinct degenerate local minima of the cost\nfunction. In the context presented here, the concatenation of hidden layers of\nthe DL network is reinterpreted as a recursive application of a {\\em truncation\nmap} which \"curates\" the training inputs by minimizing their noise to signal\nratio.\n","authors":["Thomas Chen","Patricia Muñoz Ewald"],"pdf_url":"https://arxiv.org/pdf/2309.10639v4.pdf","comment":"AMS Latex, 22 pages. Typos corrected, slightly extended"},{"id":"http://arxiv.org/abs/2403.09539v1","updated":"2024-03-14T16:27:49Z","published":"2024-03-14T16:27:49Z","title":"Logits of API-Protected LLMs Leak Proprietary Information","summary":"  The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.\n","authors":["Matthew Finlayson","Swabha Swayamdipta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13991v2","updated":"2024-03-14T16:20:50Z","published":"2023-05-23T12:20:29Z","title":"Expressive Losses for Verified Robustness via Convex Combinations","summary":"  In order to train networks for verified adversarial robustness, it is common\nto over-approximate the worst-case loss over perturbation regions, resulting in\nnetworks that attain verifiability at the expense of standard performance. As\nshown in recent work, better trade-offs between accuracy and robustness can be\nobtained by carefully coupling adversarial training with over-approximations.\nWe hypothesize that the expressivity of a loss function, which we formalize as\nthe ability to span a range of trade-offs between lower and upper bounds to the\nworst-case loss through a single parameter (the over-approximation\ncoefficient), is key to attaining state-of-the-art performance. To support our\nhypothesis, we show that trivial expressive losses, obtained via convex\ncombinations between adversarial attacks and IBP bounds, yield state-of-the-art\nresults across a variety of settings in spite of their conceptual simplicity.\nWe provide a detailed analysis of the relationship between the\nover-approximation coefficient and performance profiles across different\nexpressive losses, showing that, while expressivity is essential, better\napproximations of the worst-case loss are not necessarily linked to superior\nrobustness-accuracy trade-offs.\n","authors":["Alessandro De Palma","Rudy Bunel","Krishnamurthy Dvijotham","M. Pawan Kumar","Robert Stanforth","Alessio Lomuscio"],"pdf_url":"https://arxiv.org/pdf/2305.13991v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09516v1","updated":"2024-03-14T15:58:36Z","published":"2024-03-14T15:58:36Z","title":"Leveraging Prototypical Representations for Mitigating Social Bias\n  without Demographic Information","summary":"  Mitigating social biases typically requires identifying the social groups\nassociated with each data sample. In this paper, we present DAFair, a novel\napproach to address social bias in language models. Unlike traditional methods\nthat rely on explicit demographic labels, our approach does not require any\nsuch information. Instead, we leverage predefined prototypical demographic\ntexts and incorporate a regularization term during the fine-tuning process to\nmitigate bias in the model's representations. Our empirical results across two\ntasks and two models demonstrate the effectiveness of our method compared to\nprevious approaches that do not rely on labeled data. Moreover, with limited\ndemographic-annotated data, our approach outperforms common debiasing\napproaches.\n","authors":["Shadi Iskander","Kira Radinsky","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.09516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v2","updated":"2024-03-14T15:57:59Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.09509v1","updated":"2024-03-14T15:56:02Z","published":"2024-03-14T15:56:02Z","title":"On STPA for Distributed Development of Safe Autonomous Driving: An\n  Interview Study","summary":"  Safety analysis is used to identify hazards and build knowledge during the\ndesign phase of safety-relevant functions. This is especially true for complex\nAI-enabled and software intensive systems such as Autonomous Drive (AD).\nSystem-Theoretic Process Analysis (STPA) is a novel method applied in\nsafety-related fields like defense and aerospace, which is also becoming\npopular in the automotive industry. However, STPA assumes prerequisites that\nare not fully valid in the automotive system engineering with distributed\nsystem development and multi-abstraction design levels. This would inhibit\nsoftware developers from using STPA to analyze their software as part of a\nbigger system, resulting in a lack of traceability. This can be seen as a\nmaintainability challenge in continuous development and deployment (DevOps). In\nthis paper, STPA's different guidelines for the automotive industry, e.g.\nJ31887/ISO21448/STPA handbook, are firstly compared to assess their\napplicability to the distributed development of complex AI-enabled systems like\nAD. Further, an approach to overcome the challenges of using STPA in a\nmulti-level design context is proposed. By conducting an interview study with\nautomotive industry experts for the development of AD, the challenges are\nvalidated and the effectiveness of the proposed approach is evaluated.\n","authors":["Ali Nouri","Christian Berger","Fredrik Törner"],"pdf_url":"https://arxiv.org/pdf/2403.09509v1.pdf","comment":"Accepted at SEAA. 8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2305.03571v2","updated":"2024-03-14T15:54:10Z","published":"2023-05-05T14:27:58Z","title":"Model-free Reinforcement Learning of Semantic Communication by\n  Stochastic Policy Gradient","summary":"  Following the recent success of Machine Learning tools in wireless\ncommunications, the idea of semantic communication by Weaver from 1949 has\ngained attention. It breaks with Shannon's classic design paradigm by aiming to\ntransmit the meaning, i.e., semantics, of a message instead of its exact\nversion, allowing for information rate savings. In this work, we apply the\nStochastic Policy Gradient (SPG) to design a semantic communication system by\nreinforcement learning, separating transmitter and receiver, and not requiring\na known or differentiable channel model -- a crucial step towards deployment in\npractice. Further, we derive the use of SPG for both classic and semantic\ncommunication from the maximization of the mutual information between received\nand target variables. Numerical results show that our approach achieves\ncomparable performance to a model-aware approach based on the reparametrization\ntrick, albeit with a decreased convergence rate.\n","authors":["Edgar Beck","Carsten Bockelmann","Armin Dekorsy"],"pdf_url":"https://arxiv.org/pdf/2305.03571v2.pdf","comment":"Accepted for publication in IEEE International Conference on Machine\n  Learning for Communication and Networking (ICMLCN 2024), Source Code:\n  https://github.com/ant-uni-bremen/SINFONY"},{"id":"http://arxiv.org/abs/2403.09506v1","updated":"2024-03-14T15:53:04Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: A Motion Coherent Augmentation for Video\n  Recognition","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video recognition and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video recognition, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v1.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2310.13459v4","updated":"2024-03-14T15:52:16Z","published":"2023-10-20T12:45:12Z","title":"Stable Nonconvex-Nonconcave Training via Linear Interpolation","summary":"  This paper presents a theoretical analysis of linear interpolation as a\nprincipled method for stabilizing (large-scale) neural network training. We\nargue that instabilities in the optimization process are often caused by the\nnonmonotonicity of the loss landscape and show how linear interpolation can\nhelp by leveraging the theory of nonexpansive operators. We construct a new\noptimization scheme called relaxed approximate proximal point (RAPP), which is\nthe first explicit method without anchoring to achieve last iterate convergence\nrates for $\\rho$-comonotone problems while only requiring $\\rho >\n-\\tfrac{1}{2L}$. The construction extends to constrained and regularized\nsettings. By replacing the inner optimizer in RAPP we rediscover the family of\nLookahead algorithms for which we establish convergence in cohypomonotone\nproblems even when the base optimizer is taken to be gradient descent ascent.\nThe range of cohypomonotone problems in which Lookahead converges is further\nexpanded by exploiting that Lookahead inherits the properties of the base\noptimizer. We corroborate the results with experiments on generative\nadversarial networks which demonstrates the benefits of the linear\ninterpolation present in both RAPP and Lookahead.\n","authors":["Thomas Pethick","Wanyun Xie","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2310.13459v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07011v2","updated":"2024-03-14T15:45:56Z","published":"2024-02-10T18:14:57Z","title":"FedImpro: Measuring and Improving Client Update in Federated Learning","summary":"  Federated Learning (FL) models often experience client drift caused by\nheterogeneous data, where the distribution of data differs across clients. To\naddress this issue, advanced research primarily focuses on manipulating the\nexisting gradients to achieve more consistent client models. In this paper, we\npresent an alternative perspective on client drift and aim to mitigate it by\ngenerating improved local models. First, we analyze the generalization\ncontribution of local training and conclude that this generalization\ncontribution is bounded by the conditional Wasserstein distance between the\ndata distribution of different clients. Then, we propose FedImpro, to construct\nsimilar conditional distributions for local training. Specifically, FedImpro\ndecouples the model into high-level and low-level components, and trains the\nhigh-level portion on reconstructed feature distributions. This approach\nenhances the generalization contribution and reduces the dissimilarity of\ngradients in FL. Experimental results show that FedImpro can help FL defend\nagainst data heterogeneity and enhance the generalization performance of the\nmodel.\n","authors":["Zhenheng Tang","Yonggang Zhang","Shaohuai Shi","Xinmei Tian","Tongliang Liu","Bo Han","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2402.07011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09502v1","updated":"2024-03-14T15:44:19Z","published":"2024-03-14T15:44:19Z","title":"EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning","summary":"  Recent advancements in self-supervised audio-visual representation learning\nhave demonstrated its potential to capture rich and comprehensive\nrepresentations. However, despite the advantages of data augmentation verified\nin many learning methods, audio-visual learning has struggled to fully harness\nthese benefits, as augmentations can easily disrupt the correspondence between\ninput pairs. To address this limitation, we introduce EquiAV, a novel framework\nthat leverages equivariance for audio-visual contrastive learning. Our approach\nbegins with extending equivariance to audio-visual learning, facilitated by a\nshared attention-based transformation predictor. It enables the aggregation of\nfeatures from diverse augmentations into a representative embedding, providing\nrobust supervision. Notably, this is achieved with minimal computational\noverhead. Extensive ablation studies and qualitative results verify the\neffectiveness of our method. EquiAV outperforms previous works across various\naudio-visual benchmarks.\n","authors":["Jongsuk Kim","Hyeongkeun Lee","Kyeongha Rho","Junmo Kim","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2403.09502v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.09499v1","updated":"2024-03-14T15:42:26Z","published":"2024-03-14T15:42:26Z","title":"A Reinforcement Learning Approach to Dairy Farm Battery Management using\n  Q Learning","summary":"  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and\n24.49\\% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n","authors":["Nawazish Ali","Abdul Wahid","Rachael Shaw","Karl Mason"],"pdf_url":"https://arxiv.org/pdf/2403.09499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05342v2","updated":"2024-03-14T15:40:01Z","published":"2023-11-29T12:58:42Z","title":"Most discriminative stimuli for functional cell type clustering","summary":"  Identifying cell types and understanding their functional properties is\ncrucial for unraveling the mechanisms underlying perception and cognition. In\nthe retina, functional types can be identified by carefully selected stimuli,\nbut this requires expert domain knowledge and biases the procedure towards\npreviously known cell types. In the visual cortex, it is still unknown what\nfunctional types exist and how to identify them. Thus, for unbiased\nidentification of the functional cell types in retina and visual cortex, new\napproaches are needed. Here we propose an optimization-based clustering\napproach using deep predictive models to obtain functional clusters of neurons\nusing Most Discriminative Stimuli (MDS). Our approach alternates between\nstimulus optimization with cluster reassignment akin to an\nexpectation-maximization algorithm. The algorithm recovers functional clusters\nin mouse retina, marmoset retina and macaque visual area V4. This demonstrates\nthat our approach can successfully find discriminative stimuli across species,\nstages of the visual system and recording techniques. The resulting most\ndiscriminative stimuli can be used to assign functional cell types fast and on\nthe fly, without the need to train complex predictive models or show a large\nnatural scene dataset, paving the way for experiments that were previously\nlimited by experimental time. Crucially, MDS are interpretable: they visualize\nthe distinctive stimulus patterns that most unambiguously identify a specific\ntype of neuron.\n","authors":["Max F. Burg","Thomas Zenkel","Michaela Vystrčilová","Jonathan Oesterle","Larissa Höfling","Konstantin F. Willeke","Jan Lause","Sarah Müller","Paul G. Fahey","Zhiwei Ding","Kelli Restivo","Shashwat Sridhar","Tim Gollisch","Philipp Berens","Andreas S. Tolias","Thomas Euler","Matthias Bethge","Alexander S. Ecker"],"pdf_url":"https://arxiv.org/pdf/2401.05342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05930v2","updated":"2024-03-14T15:39:55Z","published":"2023-12-10T16:33:41Z","title":"A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary\n  Analysis","summary":"  Nailfold capillaroscopy is widely used in assessing health conditions,\nhighlighting the pressing need for an automated nailfold capillary analysis\nsystem. In this study, we present a pioneering effort in constructing a\ncomprehensive nailfold capillary dataset-321 images, 219 videos from 68\nsubjects, with clinic reports and expert annotations-that serves as a crucial\nresource for training deep-learning models. Leveraging this dataset, we\nfinetuned three deep learning models with expert annotations as supervised\nlabels and integrated them into a novel end-to-end nailfold capillary analysis\npipeline. This pipeline excels in automatically detecting and measuring a wide\nrange of size factors, morphological features, and dynamic aspects of nailfold\ncapillaries. We compared our outcomes with clinical reports. Experiment results\nshowed that our automated pipeline achieves an average of sub-pixel level\nprecision in measurements and 89.9% accuracy in identifying morphological\nabnormalities. These results underscore its potential for advancing\nquantitative medical research and enabling pervasive computing in healthcare.\nOur data and code are available at\nhttps://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.\n","authors":["Linxi Zhao","Jiankai Tang","Dongyu Chen","Xiaohong Liu","Yong Zhou","Yuanchun Shi","Guangyu Wang","Yuntao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.05930v2.pdf","comment":"Dataset, code, pretrained models:\n  https://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary"},{"id":"http://arxiv.org/abs/2309.11093v2","updated":"2024-03-14T15:36:17Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2211.11612v2","updated":"2024-03-14T15:35:59Z","published":"2022-11-21T16:13:23Z","title":"Plug and Play Active Learning for Object Detection","summary":"  Annotating datasets for object detection is an expensive and time-consuming\nendeavor. To minimize this burden, active learning (AL) techniques are employed\nto select the most informative samples for annotation within a constrained\n\"annotation budget\". Traditional AL strategies typically rely on model\nuncertainty or sample diversity for query sampling, while more advanced methods\nhave focused on developing AL-specific object detector architectures to enhance\nperformance. However, these specialized approaches are not readily adaptable to\ndifferent object detectors due to the significant engineering effort required\nfor integration. To overcome this challenge, we introduce Plug and Play Active\nLearning (PPAL), a simple and effective AL strategy for object detection. PPAL\nis a two-stage method comprising uncertainty-based and diversity-based sampling\nphases. In the first stage, our Difficulty Calibrated Uncertainty Sampling\nleverage a category-wise difficulty coefficient that combines both\nclassification and localisation difficulties to re-weight instance\nuncertainties, from which we sample a candidate pool for the subsequent\ndiversity-based sampling. In the second stage, we propose Category Conditioned\nMatching Similarity to better compute the similarities of multi-instance images\nas ensembles of their instance similarities, which is used by the k-Means++\nalgorithm to sample the final AL queries. PPAL makes no change to model\narchitectures or detector training pipelines; hence it can be easily\ngeneralized to different object detectors. We benchmark PPAL on the MS-COCO and\nPascal VOC datasets using different detector architectures and show that our\nmethod outperforms prior work by a large margin. Code is available at\nhttps://github.com/ChenhongyiYang/PPAL\n","authors":["Chenhongyi Yang","Lichao Huang","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2211.11612v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09491v1","updated":"2024-03-14T15:32:25Z","published":"2024-03-14T15:32:25Z","title":"On using Machine Learning Algorithms for Motorcycle Collision Detection","summary":"  Globally, motorcycles attract vast and varied users. However, since the rate\nof severe injury and fatality in motorcycle accidents far exceeds passenger car\naccidents, efforts have been directed toward increasing passive safety systems.\nImpact simulations show that the risk of severe injury or death in the event of\na motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped\nwith passive safety measures such as airbags and seat belts. For the passive\nsafety systems to be activated, a collision must be detected within\nmilliseconds for a wide variety of impact configurations, but under no\ncircumstances may it be falsely triggered. For the challenge of reliably\ndetecting impending collisions, this paper presents an investigation towards\nthe applicability of machine learning algorithms. First, a series of\nsimulations of accidents and driving operation is introduced to collect data to\ntrain machine learning classification models. Their performance is henceforth\nassessed and compared via multiple representative and application-oriented\ncriteria.\n","authors":["Philipp Rodegast","Steffen Maier","Jonas Kneifl","Jörg Fehr"],"pdf_url":"https://arxiv.org/pdf/2403.09491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.03668v6","updated":"2024-03-14T15:25:16Z","published":"2022-03-04T14:16:50Z","title":"A Typology for Exploring the Mitigation of Shortcut Behavior","summary":"  As machine learning models become increasingly larger, trained weakly\nsupervised on large, possibly uncurated data sets, it becomes increasingly\nimportant to establish mechanisms for inspecting, interacting, and revising\nmodels to mitigate learning shortcuts and guarantee their learned knowledge is\naligned with human knowledge. The recently proposed XIL framework was developed\nfor this purpose, and several such methods have been introduced, each with\nindividual motivations and methodological details. In this work, we provide a\nunification of various XIL methods into a single typology by establishing a\ncommon set of basic modules. In doing so, we pave the way for a principled\ncomparison of existing, but, importantly, also future XIL approaches. In\naddition, we discuss existing and introduce novel measures and benchmarks for\nevaluating the overall abilities of a XIL method. Given this extensive toolbox,\nincluding our typology, measures, and benchmarks, we finally compare several\nrecent XIL methods methodologically and quantitatively. In our evaluations, all\nmethods prove to revise a model successfully. However, we found remarkable\ndifferences in individual benchmark tasks, revealing valuable\napplication-relevant aspects for integrating these benchmarks in developing\nfuture methods.\n","authors":["Felix Friedrich","Wolfgang Stammer","Patrick Schramowski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2203.03668v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09479v1","updated":"2024-03-14T15:20:54Z","published":"2024-03-14T15:20:54Z","title":"Laying the Foundation First? Investigating the Generalization from\n  Atomic Skills to Complex Reasoning Tasks","summary":"  Current language models have demonstrated their capability to develop basic\nreasoning, but struggle in more complicated reasoning tasks that require a\ncombination of atomic skills, such as math word problem requiring skills like\narithmetic and unit conversion. Previous methods either do not improve the\ninherent atomic skills of models or not attempt to generalize the atomic skills\nto complex reasoning tasks. In this paper, we first propose a probing framework\nto investigate whether the atomic skill can spontaneously generalize to complex\nreasoning tasks. Then, we introduce a hierarchical curriculum learning training\nstrategy to achieve better skill generalization. In our experiments, we find\nthat atomic skills can not spontaneously generalize to compositional tasks. By\nleveraging hierarchical curriculum learning, we successfully induce\ngeneralization, significantly improve the performance of open-source LMs on\ncomplex reasoning tasks. Promisingly, the skill generalization exhibit\neffective in cross-dataset and cross-domain scenarios. Complex reasoning can\nalso help enhance atomic skills. Our findings offer valuable guidance for\ndesigning better training strategies for complex reasoning tasks.\n","authors":["Yuncheng Huang","Qianyu He","Yipei Xu","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09477v1","updated":"2024-03-14T15:19:19Z","published":"2024-03-14T15:19:19Z","title":"VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance\n  Fields","summary":"  Autonomous mobile robots are an increasingly integral part of modern factory\nand warehouse operations. Obstacle detection, avoidance and path planning are\ncritical safety-relevant tasks, which are often solved using expensive LiDAR\nsensors and depth cameras. We propose to use cost-effective low-resolution\nranging sensors, such as ultrasonic and infrared time-of-flight sensors by\ndeveloping VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance\nFields. Building upon Instant Neural Graphics Primitives with a Multiresolution\nHash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from\nultrasonic and infrared sensors and utilizes them to update the occupancy grid\nused for ray marching. Experimental evaluation in 2D demonstrates that\nVIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds\nregarding coverage. Notably, in small environments, its accuracy aligns with\nthat of LiDAR measurements, while in larger ones, it is bounded by the utilized\nultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic\nand infrared sensors is highly effective when dealing with sparse data and low\nview variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the\nmapping capabilities and increases the training speed by 46% compared to\nInstant-NGP. Overall, VIRUS-NeRF presents a promising approach for\ncost-effective local mapping in mobile robotics, with potential applications in\nsafety and navigation tasks. The code can be found at\nhttps://github.com/ethz-asl/virus nerf.\n","authors":["Nicolaj Schmid","Cornelius von Einem","Cesar Cadena","Roland Siegwart","Lorenz Hruby","Florian Tschopp"],"pdf_url":"https://arxiv.org/pdf/2403.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08760v2","updated":"2024-03-14T15:16:05Z","published":"2022-11-16T08:46:10Z","title":"SVD-PINNs: Transfer Learning of Physics-Informed Neural Networks via\n  Singular Value Decomposition","summary":"  Physics-informed neural networks (PINNs) have attracted significant attention\nfor solving partial differential equations (PDEs) in recent years because they\nalleviate the curse of dimensionality that appears in traditional methods.\nHowever, the most disadvantage of PINNs is that one neural network corresponds\nto one PDE. In practice, we usually need to solve a class of PDEs, not just\none. With the explosive growth of deep learning, many useful techniques in\ngeneral deep learning tasks are also suitable for PINNs. Transfer learning\nmethods may reduce the cost for PINNs in solving a class of PDEs. In this\npaper, we proposed a transfer learning method of PINNs via keeping singular\nvectors and optimizing singular values (namely SVD-PINNs). Numerical\nexperiments on high dimensional PDEs (10-d linear parabolic equations and 10-d\nAllen-Cahn equations) show that SVD-PINNs work for solving a class of PDEs with\ndifferent but close right-hand-side functions.\n","authors":["Yihang Gao","Ka Chun Cheung","Michael K. Ng"],"pdf_url":"https://arxiv.org/pdf/2211.08760v2.pdf","comment":"Accepted to The 2022 IEEE Symposium Series on Computational\n  Intelligence (IEEE SSCI 2022)"},{"id":"http://arxiv.org/abs/2403.09472v1","updated":"2024-03-14T15:12:38Z","published":"2024-03-14T15:12:38Z","title":"Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision","summary":"  Current AI alignment methodologies rely on human-provided demonstrations or\njudgments, and the learned capabilities of AI systems would be upper-bounded by\nhuman capabilities as a result. This raises a challenging research question:\nHow can we keep improving the systems when their capabilities have surpassed\nthe levels of humans? This paper answers this question in the context of\ntackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from\nhuman annotations on easier tasks (e.g., level 1-3 MATH problems), which we\nterm as \\textit{easy-to-hard generalization}. Our key insight is that an\nevaluator (reward model) trained on supervisions for easier tasks can be\neffectively used for scoring candidate solutions of harder tasks and hence\nfacilitating easy-to-hard generalization over different levels of tasks. Based\non this insight, we propose a novel approach to scalable alignment, which\nfirstly trains the process-supervised reward models on easy problems (e.g.,\nlevel 1-3), and then uses them to evaluate the performance of policy models on\nhard problems. We show that such \\textit{easy-to-hard generalization from\nevaluators} can enable \\textit{easy-to-hard generalizations in generators}\neither through re-ranking or reinforcement learning (RL). Notably, our\nprocess-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500,\ndespite only using human supervision on easy problems. Our approach suggests a\npromising path toward AI systems that advance beyond the frontier of human\nsupervision.\n","authors":["Zhiqing Sun","Longhui Yu","Yikang Shen","Weiyang Liu","Yiming Yang","Sean Welleck","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10517v2","updated":"2024-03-14T15:09:08Z","published":"2023-11-17T13:40:10Z","title":"Mind the map! Accounting for existing map information when estimating\n  online HDMaps from sensor","summary":"  While HDMaps are a crucial component of autonomous driving, they are\nexpensive to acquire and maintain. Estimating these maps from sensors therefore\npromises to significantly lighten costs. These estimations however overlook\nexisting HDMaps, with current methods at most geolocalizing low quality maps or\nconsidering a general database of known maps. In this paper, we propose to\naccount for existing maps of the precise situation studied when estimating\nHDMaps. We identify 3 reasonable types of useful existing maps (minimalist,\nnoisy, and outdated). We also introduce MapEX, a novel online HDMap estimation\nframework that accounts for existing maps. MapEX achieves this by encoding map\nelements into query tokens and by refining the matching algorithm used to train\nclassic query based map estimation models. We demonstrate that MapEX brings\nsignificant improvements on the nuScenes dataset. For instance, MapEX - given\nnoisy maps - improves by 38% over the MapTRv2 detector it is based on and by 8%\nover the current SOTA.\n","authors":["Rémy Sun","Li Yang","Diane Lingrand","Frédéric Precioso"],"pdf_url":"https://arxiv.org/pdf/2311.10517v2.pdf","comment":"23 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2211.16648v2","updated":"2024-03-14T15:06:02Z","published":"2022-11-30T00:32:37Z","title":"COMET: A Comprehensive Cluster Design Methodology for Distributed Deep\n  Learning Training","summary":"  Modern Deep Learning (DL) models have grown to sizes requiring massive\nclusters of specialized, high-end nodes to train. Designing such clusters to\nmaximize both performance and utilization--to amortize their steep cost--is a\nchallenging task requiring careful balance of compute, memory, and network\nresources. Moreover, a plethora of each model's tuning knobs drastically affect\nthe performance, with optimal values often depending on the underlying\ncluster's characteristics, which necessitates a complex cluster-workload\nco-design process. To facilitate the design space exploration of such massive\nDL training clusters, we introduce COMET, a holistic cluster design methodology\nand workflow to jointly study the impact of parallelization strategies and key\ncluster resource provisioning on the performance of distributed DL training. We\ndevelop a step-by-step process to establish a reusable and flexible\nmethodology, and demonstrate its application with case studies of training\nlarge models on cluster configurations of variable compute, memory, and network\nresources. Our case studies demonstrate COMET's utility in identifying\npromising architectural optimization directions and guiding system designers in\nconfiguring key model and cluster parameters. To illustrate, cluster\nconfiguration comparisons identify performance differences of up to 7.7x and\nhighlight performance optimization opportunities of up to 1.4x when employing\nmemory expansion as an optimization technique.\n","authors":["Divya Kiran Kadiyala","Saeed Rashidi","Taekyung Heo","Abhimanyu Rajeshkumar Bambhaniya","Tushar Krishna","Alexandros Daglis"],"pdf_url":"https://arxiv.org/pdf/2211.16648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09465v1","updated":"2024-03-14T15:04:45Z","published":"2024-03-14T15:04:45Z","title":"Outlier Robust Multivariate Polynomial Regression","summary":"  We study the problem of robust multivariate polynomial regression: let\n$p\\colon\\mathbb{R}^n\\to\\mathbb{R}$ be an unknown $n$-variate polynomial of\ndegree at most $d$ in each variable. We are given as input a set of random\nsamples $(\\mathbf{x}_i,y_i) \\in [-1,1]^n \\times \\mathbb{R}$ that are noisy\nversions of $(\\mathbf{x}_i,p(\\mathbf{x}_i))$. More precisely, each\n$\\mathbf{x}_i$ is sampled independently from some distribution $\\chi$ on\n$[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an\noutlier) with probability at most $\\rho < 1/2$, and otherwise satisfies\n$|y_i-p(\\mathbf{x}_i)|\\leq\\sigma$. The goal is to output a polynomial\n$\\hat{p}$, of degree at most $d$ in each variable, within an\n$\\ell_\\infty$-distance of at most $O(\\sigma)$ from $p$.\n  Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We\ngeneralize their results to the $n$-variate setting, showing an algorithm that\nachieves a sample complexity of $O_n(d^n\\log d)$, where the hidden constant\ndepends on $n$, if $\\chi$ is the $n$-dimensional Chebyshev distribution. The\nsample complexity is $O_n(d^{2n}\\log d)$, if the samples are drawn from the\nuniform distribution instead. The approximation error is guaranteed to be at\nmost $O(\\sigma)$, and the run-time depends on $\\log(1/\\sigma)$. In the setting\nwhere each $\\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the\nrun-time's dependence on $N$ is linear. We also show that our sample\ncomplexities are optimal in terms of $d^n$. Furthermore, we show that it is\npossible to have the run-time be independent of $1/\\sigma$, at the cost of a\nhigher sample complexity.\n","authors":["Vipul Arora","Arnab Bhattacharyya","Mathews Boban","Venkatesan Guruswami","Esty Kelman"],"pdf_url":"https://arxiv.org/pdf/2403.09465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17437v3","updated":"2024-03-14T14:55:48Z","published":"2022-10-31T16:06:48Z","title":"Learning New Tasks from a Few Examples with Soft-Label Prototypes","summary":"  Existing approaches to few-shot learning in NLP rely on large language models\nand fine-tuning of these to generalise on out-of-distribution data. In this\nwork, we propose a simple yet powerful approach to \"extreme\" few-shot learning,\nwherein models are exposed to as little as 4 examples per class, based on\nsoft-label prototypes that collectively capture the distribution of different\nclasses across the input domain space. Inspired by previous work (Sucholutsky\net al., 2021) on univariate or simple multivariate (synthetic) data, we propose\na novel approach that is effective on large, high-dimensional and real-world\ndatasets. We learn soft-label prototypes within a neural framework (DeepSLP)\nand we experimentally demonstrate that it achieves superior performance on\n31/48 tested tasks and few-shot settings while closely matching the performance\nof strong baselines on the rest. We focus on learning previously unseen NLP\ntasks from very few examples (4, 8, 16) per label and present an in-depth\nanalysis of the effectiveness of our approach.\n","authors":["Avyav Kumar Singh","Ekaterina Shutova","Helen Yannakoudakis"],"pdf_url":"https://arxiv.org/pdf/2210.17437v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19791v3","updated":"2024-03-14T14:54:54Z","published":"2023-10-30T17:55:02Z","title":"LILO: Learning Interpretable Libraries by Compressing and Documenting\n  Code","summary":"  While large language models (LLMs) now excel at code generation, a key aspect\nof software development is the art of refactoring: consolidating code into\nlibraries of reusable and readable programs. In this paper, we introduce LILO,\na neurosymbolic framework that iteratively synthesizes, compresses, and\ndocuments code to build libraries tailored to particular problem domains. LILO\ncombines LLM-guided program synthesis with recent algorithmic advances in\nautomated refactoring from Stitch: a symbolic compression system that\nefficiently identifies optimal lambda abstractions across large code corpora.\nTo make these abstractions interpretable, we introduce an auto-documentation\n(AutoDoc) procedure that infers natural language names and docstrings based on\ncontextual examples of usage. In addition to improving human readability, we\nfind that AutoDoc boosts performance by helping LILO's synthesizer to interpret\nand deploy learned abstractions. We evaluate LILO on three inductive program\nsynthesis benchmarks for string editing, scene reasoning, and graphics\ncomposition. Compared to existing neural and symbolic methods - including the\nstate-of-the-art library learning algorithm DreamCoder - LILO solves more\ncomplex tasks and learns richer libraries that are grounded in linguistic\nknowledge.\n","authors":["Gabriel Grand","Lionel Wong","Matthew Bowers","Theo X. Olausson","Muxin Liu","Joshua B. Tenenbaum","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2310.19791v3.pdf","comment":"ICLR 2024 camera-ready"},{"id":"http://arxiv.org/abs/2403.09454v1","updated":"2024-03-14T14:53:18Z","published":"2024-03-14T14:53:18Z","title":"Machine learning for structural design models of continuous beam systems\n  via influence zones","summary":"  This work develops a machine learned structural design model for continuous\nbeam systems from the inverse problem perspective. After demarcating between\nforward, optimisation and inverse machine learned operators, the investigation\nproposes a novel methodology based on the recently developed influence zone\nconcept which represents a fundamental shift in approach compared to\ntraditional structural design methods. The aim of this approach is to\nconceptualise a non-iterative structural design model that predicts\ncross-section requirements for continuous beam systems of arbitrary system\nsize. After generating a dataset of known solutions, an appropriate neural\nnetwork architecture is identified, trained, and tested against unseen data.\nThe results show a mean absolute percentage testing error of 1.6% for\ncross-section property predictions, along with a good ability of the neural\nnetwork to generalise well to structural systems of variable size. The CBeamXP\ndataset generated in this work and an associated python-based neural network\ntraining script are available at an open-source data repository to allow for\nthe reproducibility of results and to encourage further investigations.\n","authors":["Adrien Gallet","Andrew Liew","Iman Hajirasouliha","Danny Smyl"],"pdf_url":"https://arxiv.org/pdf/2403.09454v1.pdf","comment":"30 pages, 16 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.09450v1","updated":"2024-03-14T14:48:37Z","published":"2024-03-14T14:48:37Z","title":"Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative\n  Privacy Risk","summary":"  While diffusion models have recently demonstrated remarkable progress in\ngenerating realistic images, privacy risks also arise: published models or APIs\ncould generate training images and thus leak privacy-sensitive training\ninformation. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that\nfine-tuning the pre-trained models with manipulated data can amplify the\nexisting privacy risks. We demonstrate that S2L could occur in various standard\nfine-tuning strategies for diffusion models, including concept-injection\nmethods (DreamBooth and Textual Inversion) and parameter-efficient methods\n(LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L\ncan amplify the state-of-the-art membership inference attack (MIA) on diffusion\nmodels by $5.4\\%$ (absolute difference) AUC and can increase extracted private\nsamples from almost $0$ samples to $16.3$ samples on average per target domain.\nThis discovery underscores that the privacy risk with diffusion models is even\nmore severe than previously recognized. Codes are available at\nhttps://github.com/VITA-Group/Shake-to-Leak.\n","authors":["Zhangheng Li","Junyuan Hong","Bo Li","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19078v2","updated":"2024-03-14T14:47:54Z","published":"2024-02-29T12:03:05Z","title":"Smooth Tchebycheff Scalarization for Multi-Objective Optimization","summary":"  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent different optimal\ntrade-offs among the objectives for a given problem. However, these existing\nmethods could have high computational complexity or may not have good\ntheoretical properties for solving a general differentiable multi-objective\noptimization problem. In this work, by leveraging the smooth optimization\ntechnique, we propose a novel and lightweight smooth Tchebycheff scalarization\napproach for gradient-based multi-objective optimization. It has good\ntheoretical properties for finding all Pareto solutions with valid trade-off\npreferences, while enjoying significantly lower computational complexity\ncompared to other methods. Experimental results on various real-world\napplication problems fully demonstrate the effectiveness of our proposed\nmethod.\n","authors":["Xi Lin","Xiaoyuan Zhang","Zhiyuan Yang","Fei Liu","Zhenkun Wang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.19078v2.pdf","comment":"fix some typos"},{"id":"http://arxiv.org/abs/2403.09441v1","updated":"2024-03-14T14:34:25Z","published":"2024-03-14T14:34:25Z","title":"Adversarial Fine-tuning of Compressed Neural Networks for Joint\n  Improvement of Robustness and Efficiency","summary":"  As deep learning (DL) models are increasingly being integrated into our\neveryday lives, ensuring their safety by making them robust against adversarial\nattacks has become increasingly critical. DL models have been found to be\nsusceptible to adversarial attacks which can be achieved by introducing small,\ntargeted perturbations to disrupt the input data. Adversarial training has been\npresented as a mitigation strategy which can result in more robust models. This\nadversarial robustness comes with additional computational costs required to\ndesign adversarial attacks during training. The two objectives -- adversarial\nrobustness and computational efficiency -- then appear to be in conflict of\neach other. In this work, we explore the effects of two different model\ncompression methods -- structured weight pruning and quantization -- on\nadversarial robustness. We specifically explore the effects of fine-tuning on\ncompressed models, and present the trade-off between standard fine-tuning and\nadversarial fine-tuning. Our results show that compression does not inherently\nlead to loss in model robustness and adversarial fine-tuning of a compressed\nmodel can yield large improvement to the robustness performance of models. We\npresent experiments on two benchmark datasets showing that adversarial\nfine-tuning of compressed models can achieve robustness performance comparable\nto adversarially trained models, while also improving computational efficiency.\n","authors":["Hallgrimur Thorsteinsson","Valdemar J Henriksen","Tong Chen","Raghavendra Selvan"],"pdf_url":"https://arxiv.org/pdf/2403.09441v1.pdf","comment":"22 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2310.03556v2","updated":"2024-03-14T14:24:43Z","published":"2023-10-05T14:08:42Z","title":"Stable Training of Probabilistic Models Using the Leave-One-Out Maximum\n  Log-Likelihood Objective","summary":"  Probabilistic modelling of power systems operation and planning processes\ndepends on data-driven methods, which require sufficiently large datasets. When\nhistorical data lacks this, it is desired to model the underlying data\ngeneration mechanism as a probability distribution to assess the data quality\nand generate more data, if needed. Kernel density estimation (KDE) based models\nare popular choices for this task, but they fail to adapt to data regions with\nvarying densities. In this paper, an adaptive KDE model is employed to\ncircumvent this, where each kernel in the model has an individual bandwidth.\nThe leave-one-out maximum log-likelihood (LOO-MLL) criterion is proposed to\nprevent the singular solutions that the regular MLL criterion gives rise to,\nand it is proven that LOO-MLL prevents these. Relying on this guaranteed\nrobustness, the model is extended by adjustable weights for the kernels. In\naddition, a modified expectation-maximization algorithm is employed to\naccelerate the optimization speed reliably. The performance of the proposed\nmethod and models are exhibited on two power systems datasets using different\nstatistical tests and by comparison with Gaussian mixture models. Results show\nthat the proposed models have promising performance, in addition to their\nsingularity prevention guarantees.\n","authors":["Kutay Bölat","Simon H. Tindemans","Peter Palensky"],"pdf_url":"https://arxiv.org/pdf/2310.03556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09429v1","updated":"2024-03-14T14:20:22Z","published":"2024-03-14T14:20:22Z","title":"Variational Inference with Sequential Sample-Average Approximations","summary":"  We present variational inference with sequential sample-average approximation\n(VISA), a method for approximate inference in computationally intensive models,\nsuch as those based on numerical simulations. VISA extends importance-weighted\nforward-KL variational inference by employing a sequence of sample-average\napproximations, which are considered valid inside a trust region. This makes it\npossible to reuse model evaluations across multiple gradient steps, thereby\nreducing computational cost. We perform experiments on high-dimensional\nGaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate\nthat VISA can achieve comparable approximation accuracy to standard\nimportance-weighted forward-KL variational inference with computational savings\nof a factor two or more for conservatively chosen learning rates.\n","authors":["Heiko Zimmermann","Christian A. Naesseth","Jan-Willem van de Meent"],"pdf_url":"https://arxiv.org/pdf/2403.09429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09428v1","updated":"2024-03-14T14:19:48Z","published":"2024-03-14T14:19:48Z","title":"Borrowing Treasures from Neighbors: In-Context Learning for Multimodal\n  Learning with Missing Modalities and Data Scarcity","summary":"  Multimodal machine learning with missing modalities is an increasingly\nrelevant challenge arising in various applications such as healthcare. This\npaper extends the current research into missing modalities to the low-data\nregime, i.e., a downstream task has both missing modalities and limited sample\nsize issues. This problem setting is particularly challenging and also\npractical as it is often expensive to get full-modality data and sufficient\nannotated training samples. We propose to use retrieval-augmented in-context\nlearning to address these two crucial issues by unleashing the potential of a\ntransformer's in-context learning ability. Diverging from existing methods,\nwhich primarily belong to the parametric paradigm and often require sufficient\ntraining samples, our work exploits the value of the available full-modality\ndata, offering a novel perspective on resolving the challenge. The proposed\ndata-dependent framework exhibits a higher degree of sample efficiency and is\nempirically demonstrated to enhance the classification model's performance on\nboth full- and missing-modality data in the low-data regime across various\nmultimodal learning tasks. When only 1% of the training data are available, our\nproposed method demonstrates an average improvement of 6.1% over a recent\nstrong baseline across various datasets and missing states. Notably, our method\nalso reduces the performance gap between full-modality and missing-modality\ndata compared with the baseline.\n","authors":["Zhuo Zhi","Ziquan Liu","Moe Elbadawi","Adam Daneshmend","Mine Orlu","Abdul Basit","Andreas Demosthenous","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2403.09428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09415v1","updated":"2024-03-14T14:04:37Z","published":"2024-03-14T14:04:37Z","title":"User Identification via Free Roaming Eye Tracking Data","summary":"  We present a new dataset of \"free roaming\" (FR) and \"targeted roaming\" (TR):\na pool of 41 participants is asked to walk around a university campus (FR) or\nis asked to find a particular room within a library (TR). Eye movements are\nrecorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz). On\nthis dataset we investigate the accuracy of user identification using a\npreviously known machine learning pipeline where a Radial Basis Function\nNetwork (RBFN) is used as classifier. Our highest accuracies are 87.3% for FR\nand 89.4% for TR. This should be compared to 95.3% which is the (corresponding)\nhighest accuracy we are aware of (achieved in a laboratory setting using the\n\"RAN\" stimulus of the BioEye 2015 competition dataset). To the best of our\nknowledge, our results are the first that study user identification in a non\nlaboratory setting; such settings are often more feasible than laboratory\nsettings and may include further advantages. The minimum duration of each\nrecording is 263s for FR and 154s for TR. Our best accuracies are obtained when\nrestricting to 120s and 140s for FR and TR respectively, always cut from the\nend of the trajectories (both for the training and testing sessions). If we cut\nthe same length from the beginning, then accuracies are 12.2% lower for FR and\naround 6.4% lower for TR. On the full trajectories accuracies are lower by 5%\nand 52% for FR and TR. We also investigate the impact of including higher order\nvelocity derivatives (such as acceleration, jerk, or jounce).\n","authors":["Rishabh Vallabh Varsha Haria","Amin El Abed","Sebastian Maneth"],"pdf_url":"https://arxiv.org/pdf/2403.09415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09407v1","updated":"2024-03-14T13:59:04Z","published":"2024-03-14T13:59:04Z","title":"LM2D: Lyrics- and Music-Driven Dance Synthesis","summary":"  Dance typically involves professional choreography with complex movements\nthat follow a musical rhythm and can also be influenced by lyrical content. The\nintegration of lyrics in addition to the auditory dimension, enriches the\nfoundational tone and makes motion generation more amenable to its semantic\nmeanings. However, existing dance synthesis methods tend to model motions only\nconditioned on audio signals. In this work, we make two contributions to bridge\nthis gap. First, we propose LM2D, a novel probabilistic architecture that\nincorporates a multimodal diffusion model with consistency distillation,\ndesigned to create dance conditioned on both music and lyrics in one diffusion\ngeneration step. Second, we introduce the first 3D dance-motion dataset that\nencompasses both music and lyrics, obtained with pose estimation technologies.\nWe evaluate our model against music-only baseline models with objective metrics\nand human evaluations, including dancers and choreographers. The results\ndemonstrate LM2D is able to produce realistic and diverse dance matching both\nlyrics and music. A video summary can be accessed at:\nhttps://youtu.be/4XCgvYookvA.\n","authors":["Wenjie Yin","Xuejiao Zhao","Yi Yu","Hang Yin","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2403.09407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12191v5","updated":"2024-03-14T13:46:29Z","published":"2022-01-28T15:45:13Z","title":"Kernelized Concept Erasure","summary":"  The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n","authors":["Shauli Ravfogel","Francisco Vargas","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12191v5.pdf","comment":"Accepted as a long paper in EMNLP22"},{"id":"http://arxiv.org/abs/2311.08364v2","updated":"2024-03-14T13:43:52Z","published":"2023-11-14T18:14:56Z","title":"Plum: Prompt Learning using Metaheuristic","summary":"  Since the emergence of large language models, prompt learning has become a\npopular method for optimizing and customizing these models. Special prompts,\nsuch as Chain-of-Thought, have even revealed previously unknown reasoning\ncapabilities within these models. However, the progress of discovering\neffective prompts has been slow, driving a desire for general prompt\noptimization methods. Unfortunately, few existing prompt learning methods\nsatisfy the criteria of being truly \"general\", i.e., automatic, discrete,\nblack-box, gradient-free, and interpretable all at once. In this paper, we\nintroduce metaheuristics, a branch of discrete non-convex optimization methods\nwith over 100 options, as a promising approach to prompt learning. Within our\nparadigm, we test six typical methods: hill climbing, simulated annealing,\ngenetic algorithms with/without crossover, tabu search, and harmony search,\ndemonstrating their effectiveness in white-box and black-box prompt learning.\nFurthermore, we show that these methods can be used to discover more\nhuman-understandable prompts that were previously unknown in both reasoning and\nimage generation tasks, opening the door to a cornucopia of possibilities in\nprompt optimization. We release all the codes in\n\\url{https://github.com/research4pan/Plum}.\n","authors":["Rui Pan","Shuo Xing","Shizhe Diao","Wenhe Sun","Xiang Liu","Kashun Shum","Renjie Pi","Jipeng Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.08364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09389v1","updated":"2024-03-14T13:40:26Z","published":"2024-03-14T13:40:26Z","title":"Learning to optimize with convergence guarantees using nonlinear system\n  theory","summary":"  The increasing reliance on numerical methods for controlling dynamical\nsystems and training machine learning models underscores the need to devise\nalgorithms that dependably and efficiently navigate complex optimization\nlandscapes. Classical gradient descent methods offer strong theoretical\nguarantees for convex problems; however, they demand meticulous hyperparameter\ntuning for non-convex ones. The emerging paradigm of learning to optimize (L2O)\nautomates the discovery of algorithms with optimized performance leveraging\nlearning models and data - yet, it lacks a theoretical framework to analyze\nconvergence and robustness of the learned algorithms. In this paper, we fill\nthis gap by harnessing nonlinear system theory. Specifically, we propose an\nunconstrained parametrization of all convergent algorithms for smooth\nnon-convex objective functions. Notably, our framework is directly compatible\nwith automatic differentiation tools, ensuring convergence by design while\nlearning to optimize.\n","authors":["Andrea Martin","Luca Furieri"],"pdf_url":"https://arxiv.org/pdf/2403.09389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10012v4","updated":"2024-03-14T13:39:35Z","published":"2022-10-18T17:30:02Z","title":"Log-linear Guardedness and its Implications","summary":"  Methods for erasing human-interpretable concepts from neural representations\nthat assume linearity have been found to be tractable and useful. However, the\nimpact of this removal on the behavior of downstream classifiers trained on the\nmodified representations is not fully understood. In this work, we formally\ndefine the notion of log-linear guardedness as the inability of an adversary to\npredict the concept directly from the representation, and study its\nimplications. We show that, in the binary case, under certain assumptions, a\ndownstream log-linear model cannot recover the erased concept. However, we\ndemonstrate that a multiclass log-linear model \\emph{can} be constructed that\nindirectly recovers the concept in some cases, pointing to the inherent\nlimitations of log-linear guardedness as a downstream bias mitigation\ntechnique. These findings shed light on the theoretical limitations of linear\nerasure methods and highlight the need for further research on the connections\nbetween intrinsic and extrinsic bias in neural models.\n","authors":["Shauli Ravfogel","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2210.10012v4.pdf","comment":"Accepted as a long paper in ACL 2023"},{"id":"http://arxiv.org/abs/2306.07745v3","updated":"2024-03-14T13:36:01Z","published":"2023-06-13T13:01:42Z","title":"Kernelized Reinforcement Learning with Order Optimal Regret Bounds","summary":"  Reinforcement learning (RL) has shown empirical success in various real world\nsettings with complex models and large state-action spaces. The existing\nanalytical results, however, typically focus on settings with a small number of\nstate-actions or simple models such as linearly modeled state-action value\nfunctions. To derive RL policies that efficiently handle large state-action\nspaces with more general value functions, some recent works have considered\nnonlinear function approximation using kernel ridge regression. We propose\n$\\pi$-KRVI, an optimistic modification of least-squares value iteration, when\nthe state-action value function is represented by a reproducing kernel Hilbert\nspace (RKHS). We prove the first order-optimal regret guarantees under a\ngeneral setting. Our results show a significant polynomial in the number of\nepisodes improvement over the state of the art. In particular, with highly\nnon-smooth kernels (such as Neural Tangent kernel or some Mat\\'ern kernels) the\nexisting results lead to trivial (superlinear in the number of episodes) regret\nbounds. We show a sublinear regret bound that is order optimal in the case of\nMat\\'ern kernels where a lower bound on regret is known.\n","authors":["Sattar Vakili","Julia Olkhovskaya"],"pdf_url":"https://arxiv.org/pdf/2306.07745v3.pdf","comment":"Advances in Neural Information Processing Systems (NeurIPS), 2023. In\n  the previous version, we utilized Lemma C.1 from Yang et al., 2020a to bound\n  the RKHS norm of the kernel ridge predictor. In the current version, this is\n  proven in Lemma 5"},{"id":"http://arxiv.org/abs/2403.09383v1","updated":"2024-03-14T13:34:30Z","published":"2024-03-14T13:34:30Z","title":"Pantypes: Diverse Representatives for Self-Explainable Models","summary":"  Prototypical self-explainable classifiers have emerged to meet the growing\ndemand for interpretable AI systems. These classifiers are designed to\nincorporate high transparency in their decisions by basing inference on\nsimilarity with learned prototypical objects. While these models are designed\nwith diversity in mind, the learned prototypes often do not sufficiently\nrepresent all aspects of the input distribution, particularly those in low\ndensity regions. Such lack of sufficient data representation, known as\nrepresentation bias, has been associated with various detrimental properties\nrelated to machine learning diversity and fairness. In light of this, we\nintroduce pantypes, a new family of prototypical objects designed to capture\nthe full diversity of the input distribution through a sparse set of objects.\nWe show that pantypes can empower prototypical self-explainable models by\noccupying divergent regions of the latent space and thus fostering high\ndiversity, interpretability and fairness.\n","authors":["Rune Kjærsgaard","Ahcène Boubekki","Line Clemmensen"],"pdf_url":"https://arxiv.org/pdf/2403.09383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03244v2","updated":"2024-03-14T13:08:20Z","published":"2023-09-06T08:50:04Z","title":"EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by\n  Semantic Segmentation","summary":"  We introduce EGIC, an enhanced generative image compression method that\nallows traversing the distortion-perception curve efficiently from a single\nmodel. EGIC is based on two novel building blocks: i) OASIS-C, a conditional\npre-trained semantic segmentation-guided discriminator, which provides both\nspatially and semantically-aware gradient feedback to the generator,\nconditioned on the latent image distribution, and ii) Output Residual\nPrediction (ORP), a retrofit solution for multi-realism image compression that\nallows control over the synthesis process by adjusting the impact of the\nresidual between an MSE-optimized and GAN-optimized decoder output on the\nGAN-based reconstruction. Together, EGIC forms a powerful codec, outperforming\nstate-of-the-art diffusion and GAN-based methods (e.g., HiFiC, MS-ILLM, and\nDIRAC-100), while performing almost on par with VTM-20.0 on the distortion end.\nEGIC is simple to implement, very lightweight, and provides excellent\ninterpolation characteristics, which makes it a promising candidate for\npractical applications targeting the low bit range.\n","authors":["Nikolai Körber","Eduard Kromer","Andreas Siebert","Sascha Hauke","Daniel Mueller-Gritschneder","Björn Schuller"],"pdf_url":"https://arxiv.org/pdf/2309.03244v2.pdf","comment":"revised version"},{"id":"http://arxiv.org/abs/2403.07917v2","updated":"2024-03-14T13:00:37Z","published":"2024-02-27T15:59:15Z","title":"A Neural-Evolutionary Algorithm for Autonomous Transit Network Design","summary":"  Planning a public transit network is a challenging optimization problem, but\nessential in order to realize the benefits of autonomous buses. We propose a\nnovel algorithm for planning networks of routes for autonomous buses. We first\ntrain a graph neural net model as a policy for constructing route networks, and\nthen use the policy as one of several mutation operators in a evolutionary\nalgorithm. We evaluate this algorithm on a standard set of benchmarks for\ntransit network design, and find that it outperforms the learned policy alone\nby up to 20% and a plain evolutionary algorithm approach by up to 53% on\nrealistic benchmark instances.\n","authors":["Andrew Holliday","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2403.07917v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. arXiv admin note: text overlap with arXiv:2306.00720"},{"id":"http://arxiv.org/abs/2403.09347v1","updated":"2024-03-14T12:51:58Z","published":"2024-03-14T12:51:58Z","title":"BurstAttention: An Efficient Distributed Attention Framework for\n  Extremely Long Sequences","summary":"  Effective attention modules have played a crucial role in the success of\nTransformer-based large language models (LLMs), but the quadratic time and\nmemory complexities of these attention modules also pose a challenge when\nprocessing long sequences. One potential solution for the long sequence problem\nis to utilize distributed clusters to parallelize the computation of attention\nmodules across multiple devices (e.g., GPUs). However, adopting a distributed\napproach inevitably introduces extra memory overheads to store local attention\nresults and incurs additional communication costs to aggregate local results\ninto global ones. In this paper, we propose a distributed attention framework\nnamed ``BurstAttention'' to optimize memory access and communication operations\nat both the global cluster and local device levels. In our experiments, we\ncompare BurstAttention with other competitive distributed attention solutions\nfor long sequence processing. The experimental results under different length\nsettings demonstrate that BurstAttention offers significant advantages for\nprocessing long sequences compared with these competitive baselines, reducing\n40% communication overheads and achieving 2 X speedup during training 32K\nsequence length on 8 X A100.\n","authors":["Sun Ao","Weilin Zhao","Xu Han","Cheng Yang","Zhiyuan Liu","Chuan Shi","Maosong Sun","Shengnan Wang","Teng Su"],"pdf_url":"https://arxiv.org/pdf/2403.09347v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.06870v2","updated":"2024-03-14T12:27:04Z","published":"2024-03-11T16:23:38Z","title":"Semantic Residual Prompts for Continual Learning","summary":"  Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained\nmodel and focus training on a few parameter vectors termed prompts. Most of\nthese methods organize these vectors in a pool of key-value pairs, and use the\ninput image as query to retrieve the prompts (values). However, as keys are\nlearned while tasks progress, the prompting selection strategy is itself\nsubject to catastrophic forgetting, an issue often overlooked by existing\napproaches. For instance, prompts introduced to accommodate new tasks might end\nup interfering with previously learned prompts. To make the selection strategy\nmore stable, we ask a foundational model (CLIP) to select our prompt within a\ntwo-level adaptation mechanism. Specifically, the first level leverages\nstandard textual prompts for the CLIP textual encoder, leading to stable class\nprototypes. The second level, instead, uses these prototypes along with the\nquery image as keys to index a second pool. The retrieved prompts serve to\nadapt a pre-trained ViT, granting plasticity. In doing so, we also propose a\nnovel residual mechanism to transfer CLIP semantics to the ViT layers. Through\nextensive analysis on established CL benchmarks, we show that our method\nsignificantly outperforms both state-of-the-art CL approaches and the zero-shot\nCLIP test. Notably, our findings hold true even for datasets with a substantial\ndomain gap w.r.t. the pre-training knowledge of the backbone model, as\nshowcased by experiments on satellite imagery and medical datasets.\n","authors":["Martin Menabue","Emanuele Frascaroli","Matteo Boschini","Enver Sangineto","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2403.06870v2.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.08568v2","updated":"2024-03-14T12:26:17Z","published":"2024-03-13T14:24:09Z","title":"Consistent Prompting for Rehearsal-Free Continual Learning","summary":"  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n","authors":["Zhanxin Gao","Jun Cen","Xiaobin Chang"],"pdf_url":"https://arxiv.org/pdf/2403.08568v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2310.12921v2","updated":"2024-03-14T12:16:00Z","published":"2023-10-19T17:17:06Z","title":"Vision-Language Models are Zero-Shot Reward Models for Reinforcement\n  Learning","summary":"  Reinforcement learning (RL) requires either manually specifying a reward\nfunction, which is often infeasible, or learning a reward model from a large\namount of human feedback, which is often very expensive. We study a more\nsample-efficient alternative: using pretrained vision-language models (VLMs) as\nzero-shot reward models (RMs) to specify tasks via natural language. We propose\na natural and general approach to using VLMs as reward models, which we call\nVLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn\ncomplex tasks without a manually specified reward function, such as kneeling,\ndoing the splits, and sitting in a lotus position. For each of these tasks, we\nonly provide a single sentence text prompt describing the desired task with\nminimal prompt engineering. We provide videos of the trained agents at:\nhttps://sites.google.com/view/vlm-rm. We can improve performance by providing a\nsecond \"baseline\" prompt and projecting out parts of the CLIP embedding space\nirrelevant to distinguish between goal and baseline. Further, we find a strong\nscaling effect for VLM-RMs: larger VLMs trained with more compute and data are\nbetter reward models. The failure modes of VLM-RMs we encountered are all\nrelated to known capability limitations of current VLMs, such as limited\nspatial reasoning ability or visually unrealistic environments that are far\noff-distribution for the VLM. We find that VLM-RMs are remarkably robust as\nlong as the VLM is large enough. This suggests that future VLMs will become\nmore and more useful reward models for a wide range of RL applications.\n","authors":["Juan Rocamonde","Victoriano Montesinos","Elvis Nava","Ethan Perez","David Lindner"],"pdf_url":"https://arxiv.org/pdf/2310.12921v2.pdf","comment":"Presented at International Conference on Learning Representations\n  (ICLR) 2024"},{"id":"http://arxiv.org/abs/2310.19812v3","updated":"2024-03-14T12:15:38Z","published":"2023-10-18T09:51:38Z","title":"Brain decoding: toward real-time reconstruction of visual perception","summary":"  In the past five years, the use of generative and foundational AI systems has\ngreatly improved the decoding of brain activity. Visual perception, in\nparticular, can now be decoded from functional Magnetic Resonance Imaging\n(fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers\nfrom a limited temporal resolution ($\\approx$0.5 Hz) and thus fundamentally\nconstrains its real-time usage. Here, we propose an alternative approach based\non magnetoencephalography (MEG), a neuroimaging device capable of measuring\nbrain activity with high temporal resolution ($\\approx$5,000 Hz). For this, we\ndevelop an MEG decoding model trained with both contrastive and regression\nobjectives and consisting of three modules: i) pretrained embeddings obtained\nfrom the image, ii) an MEG module trained end-to-end and iii) a pretrained\nimage generator. Our results are threefold: Firstly, our MEG decoder shows a 7X\nimprovement of image-retrieval over classic linear decoders. Second, late brain\nresponses to images are best decoded with DINOv2, a recent foundational image\nmodel. Third, image retrievals and generations both suggest that high-level\nvisual features can be decoded from MEG signals, although the same approach\napplied to 7T fMRI also recovers better low-level features. Overall, these\nresults, while preliminary, provide an important step towards the decoding --\nin real-time -- of the visual processes continuously unfolding within the human\nbrain.\n","authors":["Yohann Benchetrit","Hubert Banville","Jean-Rémi King"],"pdf_url":"https://arxiv.org/pdf/2310.19812v3.pdf","comment":"25 pages, 13 figures, updated and reformatted version following\n  acceptance at ICLR 2024"},{"id":"http://arxiv.org/abs/2402.18603v4","updated":"2024-03-14T12:10:43Z","published":"2024-02-28T08:29:42Z","title":"MMSR: Symbolic Regression is a Multimodal Task","summary":"  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n","authors":["Yanjie Li","Jingyi Liu","Weijun Li","Lina Yu","Min Wu","Wenqiang Li","Meilan Hao","Su Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2402.18603v4.pdf","comment":"12 page"},{"id":"http://arxiv.org/abs/2403.09318v1","updated":"2024-03-14T12:09:36Z","published":"2024-03-14T12:09:36Z","title":"A Hierarchical Fused Quantum Fuzzy Neural Network for Image\n  Classification","summary":"  Neural network is a powerful learning paradigm for data feature learning in\nthe era of big data. However, most neural network models are deterministic\nmodels that ignore the uncertainty of data. Fuzzy neural networks are proposed\nto address this problem. FDNN is a hierarchical deep neural network that\nderives information from both fuzzy and neural representations, the\nrepresentations are then fused to form representation to be classified. FDNN\nperform well on uncertain data classification tasks. In this paper, we proposed\na novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from\nclassical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership\nfunctions in fuzzy neural network. We conducted simulated experiment on two\ntypes of datasets (Dirty-MNIST and 15-Scene), the results show that the\nproposed model can outperform several existing methods. In addition, we\ndemonstrate the robustness of the proposed quantum circuit.\n","authors":["Sheng-Yao Wu","Run-Ze Li","Yan-Qi Song","Su-Juan Qin","Qiao-Yan Wen","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.09318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09912v3","updated":"2024-03-14T12:05:08Z","published":"2023-07-19T11:32:24Z","title":"Learning invariant representations of time-homogeneous stochastic\n  dynamical systems","summary":"  We consider the general class of time-homogeneous stochastic dynamical\nsystems, both discrete and continuous, and study the problem of learning a\nrepresentation of the state that faithfully captures its dynamics. This is\ninstrumental to learning the transfer operator or the generator of the system,\nwhich in turn can be used for numerous tasks, such as forecasting and\ninterpreting the system dynamics. We show that the search for a good\nrepresentation can be cast as an optimization problem over neural networks. Our\napproach is supported by recent results in statistical learning theory,\nhighlighting the role of approximation error and metric distortion in the\nlearning problem. The objective function we propose is associated with\nprojection operators from the representation space to the data space, overcomes\nmetric distortion, and can be empirically estimated from data. In the\ndiscrete-time setting, we further derive a relaxed objective function that is\ndifferentiable and numerically well-conditioned. We compare our method against\nstate-of-the-art approaches on different datasets, showing better performance\nacross the board.\n","authors":["Vladimir R. Kostic","Pietro Novelli","Riccardo Grazzi","Karim Lounici","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2307.09912v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09303v1","updated":"2024-03-14T11:51:01Z","published":"2024-03-14T11:51:01Z","title":"Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical\n  Perspective","summary":"  Medical anomaly detection aims to identify abnormal findings using only\nnormal training data, playing a crucial role in health screening and\nrecognizing rare diseases. Reconstruction-based methods, particularly those\nutilizing autoencoders (AEs), are dominant in this field. They work under the\nassumption that AEs trained on only normal data cannot reconstruct unseen\nabnormal regions well, thereby enabling the anomaly detection based on\nreconstruction errors. However, this assumption does not always hold due to the\nmismatch between the reconstruction training objective and the anomaly\ndetection task objective, rendering these methods theoretically unsound. This\nstudy focuses on providing a theoretical foundation for AE-based reconstruction\nmethods in anomaly detection. By leveraging information theory, we elucidate\nthe principles of these methods and reveal that the key to improving AE in\nanomaly detection lies in minimizing the information entropy of latent vectors.\nExperiments on four datasets with two image modalities validate the\neffectiveness of our theory. To the best of our knowledge, this is the first\neffort to theoretically clarify the principles and design philosophy of AE for\nanomaly detection. Code will be available upon acceptance.\n","authors":["Yu Cai","Hao Chen","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01436v2","updated":"2024-03-14T11:50:51Z","published":"2023-09-30T08:05:59Z","title":"Graph Neural Architecture Search with GPT-4","summary":"  Graph Neural Architecture Search (GNAS) has shown promising results in\nautomatically designing graph neural networks. However, GNAS still requires\nintensive human labor with rich domain knowledge to design the search space and\nsearch strategy. In this paper, we integrate GPT-4 into GNAS and propose a new\nGPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The\nbasic idea of our method is to design a new class of prompts for GPT-4 to guide\nGPT-4 toward the generative task of graph neural architectures. The prompts\nconsist of descriptions of the search space, search strategy, and search\nfeedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS\ngenerates more accurate graph neural networks with fast convergence.\nExperimental results show that embedding GPT-4 into GNAS outperforms the\nstate-of-the-art GNAS methods.\n","authors":["Haishuai Wang","Yang Gao","Xin Zheng","Peng Zhang","Hongyang Chen","Jiajun Bu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2310.01436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09302v1","updated":"2024-03-14T11:49:43Z","published":"2024-03-14T11:49:43Z","title":"StainFuser: Controlling Diffusion for Faster Neural Style Transfer in\n  Multi-Gigapixel Histology Images","summary":"  Stain normalization algorithms aim to transform the color and intensity\ncharacteristics of a source multi-gigapixel histology image to match those of a\ntarget image, mitigating inconsistencies in the appearance of stains used to\nhighlight cellular components in the images. We propose a new approach,\nStainFuser, which treats this problem as a style transfer task using a novel\nConditional Latent Diffusion architecture, eliminating the need for handcrafted\ncolor components. With this method, we curate SPI-2M the largest stain\nnormalization dataset to date of over 2 million histology images with neural\nstyle transfer for high-quality transformations. Trained on this data,\nStainFuser outperforms current state-of-the-art GAN and handcrafted methods in\nterms of the quality of normalized images. Additionally, compared to existing\napproaches, it improves the performance of nuclei instance segmentation and\nclassification models when used as a test time augmentation method on the\nchallenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel\nWhole Slide Images (WSIs) and demonstrate improved performance in terms of\ncomputational efficiency, image quality and consistency across tiles over\ncurrent methods.\n","authors":["Robert Jewsbury","Ruoyu Wang","Abhir Bhalerao","Nasir Rajpoot","Quoc Dang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.09302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09300v1","updated":"2024-03-14T11:46:25Z","published":"2024-03-14T11:46:25Z","title":"Recursive Causal Discovery","summary":"  Causal discovery, i.e., learning the causal graph from data, is often the\nfirst step toward the identification and estimation of causal effects, a key\nrequirement in numerous scientific domains. Causal discovery is hampered by two\nmain challenges: limited data results in errors in statistical testing and the\ncomputational complexity of the learning task is daunting. This paper builds\nupon and extends four of our prior publications (Mokhtarian et al., 2021;\nAkbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced\nthe concept of removable variables, which are the only variables that can be\nremoved recursively for the purpose of causal discovery. Presence and\nidentification of removable variables allow recursive approaches for causal\ndiscovery, a promising solution that helps to address the aforementioned\nchallenges by reducing the problem size successively. This reduction not only\nminimizes conditioning sets in each conditional independence (CI) test, leading\nto fewer errors but also significantly decreases the number of required CI\ntests. The worst-case performances of these methods nearly match the lower\nbound. In this paper, we present a unified framework for the proposed\nalgorithms, refined with additional details and enhancements for a coherent\npresentation. A comprehensive literature review is also included, comparing the\ncomputational complexity of our methods with existing approaches, showcasing\ntheir state-of-the-art efficiency. Another contribution of this paper is the\nrelease of RCD, a Python package that efficiently implements these algorithms.\nThis package is designed for practitioners and researchers interested in\napplying these methods in practical scenarios. The package is available at\ngithub.com/ban-epfl/rcd, with comprehensive documentation provided at\nrcdpackage.com.\n","authors":["Ehsan Mokhtarian","Sepehr Elahi","Sina Akbari","Negar Kiyavash"],"pdf_url":"https://arxiv.org/pdf/2403.09300v1.pdf","comment":"50 pages, 5 tables, 11 algorithms, 5 figures"},{"id":"http://arxiv.org/abs/2310.06625v4","updated":"2024-03-14T11:45:57Z","published":"2023-10-10T13:44:09Z","title":"iTransformer: Inverted Transformers Are Effective for Time Series\n  Forecasting","summary":"  The recent boom of linear forecasting models questions the ongoing passion\nfor architectural modifications of Transformer-based forecasters. These\nforecasters leverage Transformers to model the global dependencies over\ntemporal tokens of time series, with each token formed by multiple variates of\nthe same timestamp. However, Transformers are challenged in forecasting series\nwith larger lookback windows due to performance degradation and computation\nexplosion. Besides, the embedding for each temporal token fuses multiple\nvariates that represent potential delayed events and distinct physical\nmeasurements, which may fail in learning variate-centric representations and\nresult in meaningless attention maps. In this work, we reflect on the competent\nduties of Transformer components and repurpose the Transformer architecture\nwithout any modification to the basic components. We propose iTransformer that\nsimply applies the attention and feed-forward network on the inverted\ndimensions. Specifically, the time points of individual series are embedded\ninto variate tokens which are utilized by the attention mechanism to capture\nmultivariate correlations; meanwhile, the feed-forward network is applied for\neach variate token to learn nonlinear representations. The iTransformer model\nachieves state-of-the-art on challenging real-world datasets, which further\nempowers the Transformer family with promoted performance, generalization\nability across different variates, and better utilization of arbitrary lookback\nwindows, making it a nice alternative as the fundamental backbone of time\nseries forecasting. Code is available at this repository:\nhttps://github.com/thuml/iTransformer.\n","authors":["Yong Liu","Tengge Hu","Haoran Zhang","Haixu Wu","Shiyu Wang","Lintao Ma","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2310.06625v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09298v1","updated":"2024-03-14T11:37:02Z","published":"2024-03-14T11:37:02Z","title":"More than words: Advancements and challenges in speech recognition for\n  singing","summary":"  This paper addresses the challenges and advancements in speech recognition\nfor singing, a domain distinctly different from standard speech recognition.\nSinging encompasses unique challenges, including extensive pitch variations,\ndiverse vocal styles, and background music interference. We explore key areas\nsuch as phoneme recognition, language identification in songs, keyword\nspotting, and full lyrics transcription. I will describe some of my own\nexperiences when performing research on these tasks just as they were starting\nto gain traction, but will also show how recent developments in deep learning\nand large-scale datasets have propelled progress in this field. My goal is to\nilluminate the complexities of applying speech recognition to singing, evaluate\ncurrent capabilities, and outline future research directions.\n","authors":["Anna Kruspe"],"pdf_url":"https://arxiv.org/pdf/2403.09298v1.pdf","comment":"Conference on Electronic Speech Signal Processing (ESSV) 2024,\n  Keynote"},{"id":"http://arxiv.org/abs/2403.09290v1","updated":"2024-03-14T11:23:39Z","published":"2024-03-14T11:23:39Z","title":"SELECTOR: Heterogeneous graph network with convolutional masked\n  autoencoder for multimodal robust prediction of cancer survival","summary":"  Accurately predicting the survival rate of cancer patients is crucial for\naiding clinicians in planning appropriate treatment, reducing cancer-related\nmedical expenses, and significantly enhancing patients' quality of life.\nMultimodal prediction of cancer patient survival offers a more comprehensive\nand precise approach. However, existing methods still grapple with challenges\nrelated to missing multimodal data and information interaction within\nmodalities. This paper introduces SELECTOR, a heterogeneous graph-aware network\nbased on convolutional mask encoders for robust multimodal prediction of cancer\npatient survival. SELECTOR comprises feature edge reconstruction, convolutional\nmask encoder, feature cross-fusion, and multimodal survival prediction modules.\nInitially, we construct a multimodal heterogeneous graph and employ the\nmeta-path method for feature edge reconstruction, ensuring comprehensive\nincorporation of feature information from graph edges and effective embedding\nof nodes. To mitigate the impact of missing features within the modality on\nprediction accuracy, we devised a convolutional masked autoencoder (CMAE) to\nprocess the heterogeneous graph post-feature reconstruction. Subsequently, the\nfeature cross-fusion module facilitates communication between modalities,\nensuring that output features encompass all features of the modality and\nrelevant information from other modalities. Extensive experiments and analysis\non six cancer datasets from TCGA demonstrate that our method significantly\noutperforms state-of-the-art methods in both modality-missing and\nintra-modality information-confirmed cases. Our codes are made available at\nhttps://github.com/panliangrui/Selector.\n","authors":["Liangrui Pan","Yijun Peng","Yan Li","Xiang Wang","Wenjuan Liu","Liwen Xu","Qingchun Liang","Shaoliang Peng"],"pdf_url":"https://arxiv.org/pdf/2403.09290v1.pdf","comment":"Accepted on Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2403.09284v1","updated":"2024-03-14T11:12:10Z","published":"2024-03-14T11:12:10Z","title":"DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning","summary":"  Personalized federated learning becomes a hot research topic that can learn a\npersonalized learning model for each client. Existing personalized federated\nlearning models prefer to aggregate similar clients with similar data\ndistribution to improve the performance of learning models. However,\nsimilaritybased personalized federated learning methods may exacerbate the\nclass imbalanced problem. In this paper, we propose a novel Dynamic\nAffinity-based Personalized Federated Learning model (DA-PFL) to alleviate the\nclass imbalanced problem during federated learning. Specifically, we build an\naffinity metric from a complementary perspective to guide which clients should\nbe aggregated. Then we design a dynamic aggregation strategy to dynamically\naggregate clients based on the affinity metric in each round to reduce the\nclass imbalanced risk. Extensive experiments show that the proposed DA-PFL\nmodel can significantly improve the accuracy of each client in three real-world\ndatasets with state-of-the-art comparison methods.\n","authors":["Xu Yang","Jiyuan Feng","Songyue Guo","Ye Wang","Ye Ding","Binxing Fang","Qing Liao"],"pdf_url":"https://arxiv.org/pdf/2403.09284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01282v2","updated":"2024-03-14T11:08:36Z","published":"2024-02-02T10:16:10Z","title":"Differentiable and accelerated wavelet transforms on the sphere and ball","summary":"  Directional wavelet dictionaries are hierarchical representations which\nefficiently capture and segment information across scale, location and\norientation. Such representations demonstrate a particular affinity to physical\nsignals, which often exhibit highly anisotropic, localised multiscale\nstructure. Many physically important signals are observed over spherical\ndomains, such as the celestial sky in cosmology. Leveraging recent advances in\ncomputational harmonic analysis, we design new highly distributable and\nautomatically differentiable directional wavelet transforms on the\n$2$-dimensional sphere $\\mathbb{S}^2$ and $3$-dimensional ball $\\mathbb{B}^3 =\n\\mathbb{R}^+ \\times \\mathbb{S}^2$ (the space formed by augmenting the sphere\nwith the radial half-line). We observe up to a $300$-fold and $21800$-fold\nacceleration for signals on the sphere and ball, respectively, compared to\nexisting software, whilst maintaining 64-bit machine precision. Not only do\nthese algorithms dramatically accelerate existing spherical wavelet transforms,\nthe gradient information afforded by automatic differentiation unlocks many\ndata-driven analysis techniques previously not possible for these spaces. We\npublicly release both S2WAV and S2BALL, open-sourced JAX libraries for our\ntransforms that are automatically differentiable and readily deployable both on\nand over clusters of hardware accelerators (e.g. GPUs & TPUs).\n","authors":["Matthew A. Price","Alicja Polanska","Jessica Whitney","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2402.01282v2.pdf","comment":"code available on the sphere at\n  https://github.com/astro-informatics/s2wav and on the ball at\n  https://github.com/astro-informatics/s2ball"},{"id":"http://arxiv.org/abs/2403.07059v2","updated":"2024-03-14T11:02:26Z","published":"2024-03-11T18:00:06Z","title":"Better than classical? The subtle art of benchmarking quantum machine\n  learning models","summary":"  Benchmarking models via classical simulations is one of the main ways to\njudge ideas in quantum machine learning before noise-free hardware is\navailable. However, the huge impact of the experimental design on the results,\nthe small scales within reach today, as well as narratives influenced by the\ncommercialisation of quantum technologies make it difficult to gain robust\ninsights. To facilitate better decision-making we develop an open-source\npackage based on the PennyLane software framework and use it to conduct a\nlarge-scale study that systematically tests 12 popular quantum machine learning\nmodels on 6 binary classification tasks used to create 160 individual datasets.\nWe find that overall, out-of-the-box classical machine learning models\noutperform the quantum classifiers. Moreover, removing entanglement from a\nquantum model often results in as good or better performance, suggesting that\n\"quantumness\" may not be the crucial ingredient for the small learning tasks\nconsidered here. Our benchmarks also unlock investigations beyond simplistic\nleaderboard comparisons, and we identify five important questions for quantum\nmodel design that follow from our results.\n","authors":["Joseph Bowles","Shahnawaz Ahmed","Maria Schuld"],"pdf_url":"https://arxiv.org/pdf/2403.07059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01334v2","updated":"2024-03-14T11:01:15Z","published":"2023-10-02T16:51:32Z","title":"Merge, Then Compress: Demystify Efficient SMoE with Hints from Its\n  Routing Policy","summary":"  Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up\nthe learning capacity of neural networks, however, they have issues like (a)\nHigh Memory Usage, due to duplication of the network layers into multiple\ncopies as experts; and (b) Redundancy in Experts, as common learning-based\nrouting policies suffer from representational collapse. Therefore, vanilla SMoE\nmodels are memory inefficient and non-scalable, especially for\nresource-constrained downstream scenarios. In this paper, we ask: Can we craft\na compact SMoE model by consolidating expert information? What is the best\nrecipe to merge multiple experts into fewer but more knowledgeable experts? Our\npilot investigation reveals that conventional model merging methods fail to be\neffective in such expert merging for SMoE. The potential reasons are: (1)\nredundant information overshadows critical experts; (2) appropriate neuron\npermutation for each expert is missing to bring all of them in alignment. To\naddress this, we propose M-SMoE, which leverages routing statistics to guide\nexpert merging. Specifically, it starts with neuron permutation alignment for\nexperts; then, dominant experts and their \"group members\" are formed; lastly,\nevery expert group is merged into a single expert by utilizing each expert's\nactivation frequency as their weight for merging, thus diminishing the impact\nof insignificant experts. Moreover, we observed that our proposed merging\npromotes a low dimensionality in the merged expert's weight space, naturally\npaving the way for additional compression. Hence, our final method, MC-SMoE\n(i.e., Merge, then Compress SMoE), further decomposes the merged experts into\nlow-rank and structural sparse alternatives. Extensive experiments across 8\nbenchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE\nachieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in\nperformance.\n","authors":["Pingzhi Li","Zhenyu Zhang","Prateek Yadav","Yi-Lin Sung","Yu Cheng","Mohit Bansal","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2310.01334v2.pdf","comment":"This paper is accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2311.04744v2","updated":"2024-03-14T10:55:46Z","published":"2023-11-08T15:12:31Z","title":"Euclidean, Projective, Conformal: Choosing a Geometric Algebra for\n  Equivariant Transformers","summary":"  The Geometric Algebra Transformer (GATr) is a versatile architecture for\ngeometric deep learning based on projective geometric algebra. We generalize\nthis architecture into a blueprint that allows one to construct a scalable\ntransformer architecture given any geometric (or Clifford) algebra. We study\nversions of this architecture for Euclidean, projective, and conformal\nalgebras, all of which are suited to represent 3D data, and evaluate them in\ntheory and practice. The simplest Euclidean architecture is computationally\ncheap, but has a smaller symmetry group and is not as sample-efficient, while\nthe projective model is not sufficiently expressive. Both the conformal algebra\nand an improved version of the projective algebra define powerful, performant\narchitectures.\n","authors":["Pim de Haan","Taco Cohen","Johann Brehmer"],"pdf_url":"https://arxiv.org/pdf/2311.04744v2.pdf","comment":"Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.09267v1","updated":"2024-03-14T10:44:10Z","published":"2024-03-14T10:44:10Z","title":"Deep Limit Order Book Forecasting","summary":"  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base, to efficiently process\nlarge-scale Limit Order Book data and quantitatively assess state-of-the-art\ndeep learning models' forecasting capabilities. Our results are twofold. We\ndemonstrate that the stocks' microstructural characteristics influence the\nefficacy of deep learning methods and that their high forecasting power does\nnot necessarily correspond to actionable trading signals. We argue that\ntraditional machine learning metrics fail to adequately assess the quality of\nforecasts in the Limit Order Book context. As an alternative, we propose an\ninnovative operational framework that assesses predictions' practicality by\nfocusing on the probability of accurately forecasting complete transactions.\nThis work offers academics and practitioners an avenue to make informed and\nrobust decisions on the application of deep learning techniques, their scope\nand limitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n","authors":["Antonio Briola","Silvia Bartolucci","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2403.09267v1.pdf","comment":"43 pages, 14 figures, 12 Tables"},{"id":"http://arxiv.org/abs/2207.14000v2","updated":"2024-03-14T10:35:57Z","published":"2022-07-28T10:44:46Z","title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation","summary":"  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gate attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention\ncan achieve higher test accuracy than DeepLogic and other RNN baseline models.\nOur model achieves better out-of-distribution generalisation than RoBERTa-Large\nwhen the rules have been shuffled. Furthermore, to address the issue of\nunbalanced distribution of reasoning depths in the current multi-step reasoning\ndatasets, we develop PARARULE-Plus, a large dataset with more examples that\nrequire deeper reasoning steps. Experimental results show that the addition of\nPARARULE-Plus can increase the model's performance on examples requiring deeper\nreasoning depths. The source code and data are available at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Tim Hartill","Neset Tan","Zhenyun Deng","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2207.14000v2.pdf","comment":"10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)"},{"id":"http://arxiv.org/abs/2403.09259v1","updated":"2024-03-14T10:33:28Z","published":"2024-03-14T10:33:28Z","title":"To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation","summary":"  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines\nuncertainty and diversity for sentence selection. HUDS computes uncertainty\nscores for unlabeled sentences and subsequently stratifies them. It then\nclusters sentence embeddings within each stratum using k-MEANS and computes\ndiversity scores by distance to the centroid. A weighted hybrid score that\ncombines uncertainty and diversity is then used to select the top instances for\nannotation in each AL iteration. Experiments on multi-domain German-English\ndatasets demonstrate the better performance of HUDS over other strong AL\nbaselines. We analyze the sentence selection with HUDS and show that it\nprioritizes diverse instances having high model uncertainty for annotation in\nearly AL iterations.\n","authors":["Abdul Hameed Azeemi","Ihsan Ayyub Qazi","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2403.09259v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2309.07030v3","updated":"2024-03-14T10:23:57Z","published":"2023-09-13T15:36:39Z","title":"Optimal transport distances for directed, weighted graphs: a case study\n  with cell-cell communication networks","summary":"  Comparing graphs by means of optimal transport has recently gained\nsignificant attention, as the distances induced by optimal transport provide\nboth a principled metric between graphs as well as an interpretable description\nof the associated changes between graphs in terms of a transport plan. As the\nlack of symmetry introduces challenges in the typically considered\nformulations, optimal transport distances for graphs have mostly been developed\nfor undirected graphs. Here, we propose two distance measures to compare\ndirected graphs based on variants of optimal transport: (i) an earth movers\ndistance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate\nthese two distances and discuss their relative performance for both simulated\ngraph data and real-world directed cell-cell communication graphs, inferred\nfrom single-cell RNA-seq data.\n","authors":["James S. Nagai","Ivan G. Costa","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2309.07030v3.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.08609v2","updated":"2024-03-14T10:01:45Z","published":"2024-03-13T15:21:14Z","title":"On the Convergence of Locally Adaptive and Scalable Diffusion-Based\n  Sampling Methods for Deep Bayesian Neural Network Posteriors","summary":"  Achieving robust uncertainty quantification for deep neural networks\nrepresents an important requirement in many real-world applications of deep\nlearning such as medical imaging where it is necessary to assess the\nreliability of a neural network's prediction. Bayesian neural networks are a\npromising approach for modeling uncertainties in deep neural networks.\nUnfortunately, generating samples from the posterior distribution of neural\nnetworks is a major challenge. One significant advance in that direction would\nbe the incorporation of adaptive step sizes, similar to modern neural network\noptimizers, into Monte Carlo Markov chain sampling algorithms without\nsignificantly increasing computational demand. Over the past years, several\npapers have introduced sampling algorithms with claims that they achieve this\nproperty. However, do they indeed converge to the correct distribution? In this\npaper, we demonstrate that these methods can have a substantial bias in the\ndistribution they sample, even in the limit of vanishing step sizes and at full\nbatch size.\n","authors":["Tim Rensmeyer","Oliver Niggemann"],"pdf_url":"https://arxiv.org/pdf/2403.08609v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10112v2","updated":"2024-03-14T09:56:35Z","published":"2023-12-15T09:09:25Z","title":"NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on\n  Normalizing Flows and Generative Adversarial Networks","summary":"  Modeling and synthesizing real sRGB noise is crucial for various low-level\nvision tasks, such as building datasets for training image denoising systems.\nThe distribution of real sRGB noise is highly complex and affected by a\nmultitude of factors, making its accurate modeling extremely challenging.\nTherefore, recent studies have proposed methods that employ data-driven\ngenerative models, such as generative adversarial networks (GAN) and\nNormalizing Flows. These studies achieve more accurate modeling of sRGB noise\ncompared to traditional noise modeling methods. However, there are performance\nlimitations due to the inherent characteristics of each generative model. To\naddress this issue, we propose NM-FlowGAN, a hybrid approach that exploits the\nstrengths of both GAN and Normalizing Flows. We simultaneously employ a\npixel-wise noise modeling network based on Normalizing Flows, and spatial\ncorrelation modeling networks based on GAN. In our experiments, our NM-FlowGAN\noutperforms other baselines on the sRGB noise synthesis task. Moreover, the\ndenoising neural network, trained with synthesized image pairs from our model,\nalso shows superior performance compared to other baselines. Our code is\navailable at: \\url{https://github.com/YoungJooHan/NM-FlowGAN}.\n","authors":["Young Joo Han","Ha-Jin Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10112v2.pdf","comment":"25 pages, 11 figures, 7 tables"},{"id":"http://arxiv.org/abs/2309.17167v3","updated":"2024-03-14T09:52:16Z","published":"2023-09-29T12:04:14Z","title":"DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks","summary":"  Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns are raised about potential data\ncontamination in their considerable volume of training corpus. Moreover, the\nstatic nature and fixed complexity of current benchmarks may inadequately gauge\nthe advancing capabilities of LLMs. In this paper, we introduce DyVal, a\ngeneral and flexible protocol for dynamic evaluation of LLMs. Based on our\nframework, we build graph-informed DyVal by leveraging the structural advantage\nof directed acyclic graphs to dynamically generate evaluation samples with\ncontrollable complexities. DyVal generates challenging evaluation sets on\nreasoning tasks including mathematics, logical reasoning, and algorithm\nproblems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo\nand GPT-4. Experiments show that LLMs perform worse in DyVal-generated\nevaluation samples with different complexities, highlighting the significance\nof dynamic evaluation. We also analyze the failure cases and results of\ndifferent prompting methods. Moreover, DyVal-generated samples are not only\nevaluation sets, but also helpful data for fine-tuning to improve the\nperformance of LLMs on existing benchmarks. We hope that DyVal can shed light\non future evaluation research of LLMs. Code is available at:\nhttps://github.com/microsoft/promptbench.\n","authors":["Kaijie Zhu","Jiaao Chen","Jindong Wang","Neil Zhenqiang Gong","Diyi Yang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2309.17167v3.pdf","comment":"ICLR 2024 spotlight; 38 pages; code is at aka.ms/dyval"},{"id":"http://arxiv.org/abs/2403.09228v1","updated":"2024-03-14T09:48:48Z","published":"2024-03-14T09:48:48Z","title":"Uncertainty Quantification for cross-subject Motor Imagery\n  classification","summary":"  Uncertainty Quantification aims to determine when the prediction from a\nMachine Learning model is likely to be wrong. Computer Vision research has\nexplored methods for determining epistemic uncertainty (also known as model\nuncertainty), which should correspond with generalisation error. These methods\ntheoretically allow to predict misclassifications due to inter-subject\nvariability. We applied a variety of Uncertainty Quantification methods to\npredict misclassifications for a Motor Imagery Brain Computer Interface. Deep\nEnsembles performed best, both in terms of classification performance and\ncross-subject Uncertainty Quantification performance. However, we found that\nstandard CNNs with Softmax output performed better than some of the more\nadvanced methods.\n","authors":["Prithviraj Manivannan","Ivo Pascal de Jong","Matias Valdenegro-Toro","Andreea Ioana Sburlea"],"pdf_url":"https://arxiv.org/pdf/2403.09228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09223v1","updated":"2024-03-14T09:43:07Z","published":"2024-03-14T09:43:07Z","title":"MCformer: Multivariate Time Series Forecasting with Mixed-Channels\n  Transformer","summary":"  The massive generation of time-series data by largescale Internet of Things\n(IoT) devices necessitates the exploration of more effective models for\nmultivariate time-series forecasting. In previous models, there was a\npredominant use of the Channel Dependence (CD) strategy (where each channel\nrepresents a univariate sequence). Current state-of-the-art (SOTA) models\nprimarily rely on the Channel Independence (CI) strategy. The CI strategy\ntreats all channels as a single channel, expanding the dataset to improve\ngeneralization performance and avoiding inter-channel correlation that disrupts\nlong-term features. However, the CI strategy faces the challenge of\ninterchannel correlation forgetting. To address this issue, we propose an\ninnovative Mixed Channels strategy, combining the data expansion advantages of\nthe CI strategy with the ability to counteract inter-channel correlation\nforgetting. Based on this strategy, we introduce MCformer, a multivariate\ntime-series forecasting model with mixed channel features. The model blends a\nspecific number of channels, leveraging an attention mechanism to effectively\ncapture inter-channel correlation information when modeling long-term features.\nExperimental results demonstrate that the Mixed Channels strategy outperforms\npure CI strategy in multivariate time-series forecasting tasks.\n","authors":["Wenyong Han","Tao Zhu Member","Liming Chen","Huansheng Ning","Yang Luo","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2403.09223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09215v1","updated":"2024-03-14T09:28:28Z","published":"2024-03-14T09:28:28Z","title":"On the Laplace Approximation as Model Selection Criterion for Gaussian\n  Processes","summary":"  Model selection aims to find the best model in terms of accuracy,\ninterpretability or simplicity, preferably all at once. In this work, we focus\non evaluating model performance of Gaussian process models, i.e. finding a\nmetric that provides the best trade-off between all those criteria. While\nprevious work considers metrics like the likelihood, AIC or dynamic nested\nsampling, they either lack performance or have significant runtime issues,\nwhich severely limits applicability. We address these challenges by introducing\nmultiple metrics based on the Laplace approximation, where we overcome a severe\ninconsistency occuring during naive application of the Laplace approximation.\nExperiments show that our metrics are comparable in quality to the gold\nstandard dynamic nested sampling without compromising for computational speed.\nOur model selection criteria allow significantly faster and high quality model\nselection of Gaussian process models.\n","authors":["Andreas Besginow","Jan David Hüwel","Thomas Pawellek","Christian Beecks","Markus Lange-Hegermann"],"pdf_url":"https://arxiv.org/pdf/2403.09215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05811v2","updated":"2024-03-14T09:24:51Z","published":"2024-03-09T06:19:53Z","title":"Near Minimax-Optimal Distributional Temporal Difference Algorithms and\n  The Freedman Inequality in Hilbert Spaces","summary":"  Distributional reinforcement learning (DRL) has achieved empirical success in\nvarious domains. One of the core tasks in the field of DRL is distributional\npolicy evaluation, which involves estimating the return distribution $\\eta^\\pi$\nfor a given policy $\\pi$. The distributional temporal difference (TD) algorithm\nhas been accordingly proposed, which is an extension of the temporal difference\nalgorithm in the classic RL literature. In the tabular case,\n\\citet{rowland2018analysis} and \\citet{rowland2023analysis} proved the\nasymptotic convergence of two instances of distributional TD, namely\ncategorical temporal difference algorithm (CTD) and quantile temporal\ndifference algorithm (QTD), respectively. In this paper, we go a step further\nand analyze the finite-sample performance of distributional TD. To facilitate\ntheoretical analysis, we propose a non-parametric distributional TD algorithm\n(NTD). For a $\\gamma$-discounted infinite-horizon tabular Markov decision\nprocess, we show that for NTD we need\n$\\tilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1-\\gamma)^{2p+1}}\\right)$ iterations\nto achieve an $\\varepsilon$-optimal estimator with high probability, when the\nestimation error is measured by the $p$-Wasserstein distance. This sample\ncomplexity bound is minimax optimal (up to logarithmic factors) in the case of\nthe $1$-Wasserstein distance. To achieve this, we establish a novel Freedman's\ninequality in Hilbert spaces, which would be of independent interest. In\naddition, we revisit CTD, showing that the same non-asymptotic convergence\nbounds hold for CTD in the case of the $p$-Wasserstein distance.\n","authors":["Yang Peng","Liangyu Zhang","Zhihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09209v1","updated":"2024-03-14T09:22:17Z","published":"2024-03-14T09:22:17Z","title":"LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection","summary":"  Enterprises and organizations are faced with potential threats from insider\nemployees that may lead to serious consequences. Previous studies on insider\nthreat detection (ITD) mainly focus on detecting abnormal users or abnormal\ntime periods (e.g., a week or a day). However, a user may have hundreds of\nthousands of activities in the log, and even within a day there may exist\nthousands of activities for a user, requiring a high investigation budget to\nverify abnormal users or activities given the detection results. On the other\nhand, existing works are mainly post-hoc methods rather than real-time\ndetection, which can not report insider threats in time before they cause loss.\nIn this paper, we conduct the first study towards real-time ITD at activity\nlevel, and present a fine-grained and efficient framework LAN. Specifically,\nLAN simultaneously learns the temporal dependencies within an activity sequence\nand the relationships between activities across sequences with graph structure\nlearning. Moreover, to mitigate the data imbalance problem in ITD, we propose a\nnovel hybrid prediction loss, which integrates self-supervision signals {from\nnormal activities} and supervision signals from abnormal activities into a\nunified loss for anomaly detection. We evaluate the performance of LAN on two\nwidely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative\nexperiments demonstrate the superiority of LAN, outperforming 9\nstate-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD\non CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to\npost-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in\nAUC on two datasets. Finally, the ablation study, parameter analysis, and\ncompatibility analysis evaluate the impact of each module and hyper-parameter\nin LAN.\n","authors":["Xiangrui Cai","Yang Wang","Sihan Xu","Hao Li","Ying Zhang","Xiaojie Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.09209v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.09206v1","updated":"2024-03-14T09:19:50Z","published":"2024-03-14T09:19:50Z","title":"Upper Bound of Bayesian Generalization Error in Partial Concept\n  Bottleneck Model (CBM): Partial CBM outperforms naive CBM","summary":"  Concept Bottleneck Model (CBM) is a methods for explaining neural networks.\nIn CBM, concepts which correspond to reasons of outputs are inserted in the\nlast intermediate layer as observed values. It is expected that we can\ninterpret the relationship between the output and concept similar to linear\nregression. However, this interpretation requires observing all concepts and\ndecreases the generalization performance of neural networks. Partial CBM\n(PCBM), which uses partially observed concepts, has been devised to resolve\nthese difficulties. Although some numerical experiments suggest that the\ngeneralization performance of PCBMs is almost as high as that of the original\nneural networks, the theoretical behavior of its generalization error has not\nbeen yet clarified since PCBM is singular statistical model. In this paper, we\nreveal the Bayesian generalization error in PCBM with a three-layered and\nlinear architecture. The result indcates that the structure of partially\nobserved concepts decreases the Bayesian generalization error compared with\nthat of CBM (full-observed concepts).\n","authors":["Naoki Hayashi","Yoshihide Sawada"],"pdf_url":"https://arxiv.org/pdf/2403.09206v1.pdf","comment":"17 pages, 1 figure, submitted to TMLR"},{"id":"http://arxiv.org/abs/2311.11642v3","updated":"2024-03-14T09:13:55Z","published":"2023-11-20T10:01:13Z","title":"Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging","summary":"  Video face re-aging deals with altering the apparent age of a person to the\ntarget age in videos. This problem is challenging due to the lack of paired\nvideo datasets maintaining temporal consistency in identity and age. Most\nre-aging methods process each image individually without considering the\ntemporal consistency of videos. While some existing works address the issue of\ntemporal coherence through video facial attribute manipulation in latent space,\nthey often fail to deliver satisfactory performance in age transformation. To\ntackle the issues, we propose (1) a novel synthetic video dataset that features\nsubjects across a diverse range of age groups; (2) a baseline architecture\ndesigned to validate the effectiveness of our proposed dataset, and (3) the\ndevelopment of novel metrics tailored explicitly for evaluating the temporal\nconsistency of video re-aging techniques. Our comprehensive experiments on\npublic datasets, including VFHQ and CelebA-HQ, show that our method outperforms\nexisting approaches in age transformation accuracy and temporal consistency.\nNotably, in user studies, our method was preferred for temporal consistency by\n48.1\\% of participants for the older direction and by 39.3\\% for the younger\ndirection.\n","authors":["Abdul Muqeet","Kyuchul Lee","Bumsoo Kim","Yohan Hong","Hyungrae Lee","Woonggon Kim","KwangHee Lee"],"pdf_url":"https://arxiv.org/pdf/2311.11642v3.pdf","comment":"28 pages, 11 figures, 11 tables, Project page:\n  https://video-reaging.github.io/"},{"id":"http://arxiv.org/abs/2403.09193v1","updated":"2024-03-14T09:07:14Z","published":"2024-03-14T09:07:14Z","title":"Are Vision Language Models Texture or Shape Biased and Can We Steer\n  Them?","summary":"  Vision language models (VLMs) have drastically changed the computer vision\nmodel landscape in only a few years, opening an exciting array of new\napplications from zero-shot image classification, over to image captioning, and\nvisual question answering. Unlike pure vision models, they offer an intuitive\nway to access visual content through language prompting. The wide applicability\nof such models encourages us to ask whether they also align with human vision -\nspecifically, how far they adopt human-induced visual biases through multimodal\nfusion, or whether they simply inherit biases from pure vision models. One\nimportant visual bias is the texture vs. shape bias, or the dominance of local\nover global information. In this paper, we study this bias in a wide range of\npopular VLMs. Interestingly, we find that VLMs are often more shape-biased than\ntheir vision encoders, indicating that visual biases are modulated to some\nextent through text in multimodal models. If text does indeed influence visual\nbiases, this suggests that we may be able to steer visual biases not just\nthrough visual input but also through language: a hypothesis that we confirm\nthrough extensive experiments. For instance, we are able to steer shape bias\nfrom as low as 49% to as high as 72% through prompting alone. For now, the\nstrong human bias towards shape (96%) remains out of reach for all tested VLMs.\n","authors":["Paul Gavrikov","Jovita Lukasik","Steffen Jung","Robert Geirhos","Bianca Lamm","Muhammad Jehanzeb Mirza","Margret Keuper","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.09193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09188v1","updated":"2024-03-14T09:03:51Z","published":"2024-03-14T09:03:51Z","title":"Design of an basis-projected layer for sparse datasets in deep learning\n  training using gc-ms spectra as a case study","summary":"  Deep learning (DL) models encompass millions or even billions of parameters\nand learn complex patterns from big data. However, not all data are initially\nstored in a suitable formation to effectively train a DL model, e.g., gas\nchromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These\ndatasets commonly contain many zero values, and the sparse data formation\ncauses difficulties in optimizing DL models. A DL module called the\nbasis-projected layer (BPL) was proposed to mitigate the issue by transforming\nthe sparse data into a dense representation. The transformed data is expected\nto facilitate the gradient calculation and finetuned process in a DL training\nprocess. The dataset, example of a sparse dataset, contained 362 specialty\ncoffee odorant spectra detected from GC-MS. The BPL layer was placed at the\nbeginning of the DL model. The tunable parameters in the layer were learnable\nprojected axes that were the bases of a new representation space. The layer\nrotated these bases when its parameters were updated. When the number of the\nbases was the same as the original dimension, the increasing percentage of the\nF1 scores was 8.56%. Furthermore, when the number was set as 768 (the original\ndimension was 490), the increasing percentage of the F1 score was 11.49%. The\nlayer not only maintained the model performance and even constructed a better\nrepresentation space in analyzing sparse datasets.\n","authors":["Yu Tang Chang","Shih Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.09188v1.pdf","comment":"5 pages, 2 figures, 2 tables, conference"},{"id":"http://arxiv.org/abs/2403.09183v1","updated":"2024-03-14T08:53:01Z","published":"2024-03-14T08:53:01Z","title":"Generalized Relevance Learning Grassmann Quantization","summary":"  Due to advancements in digital cameras, it is easy to gather multiple images\n(or videos) from an object under different conditions. Therefore, image-set\nclassification has attracted more attention, and different solutions were\nproposed to model them. A popular way to model image sets is subspaces, which\nform a manifold called the Grassmann manifold. In this contribution, we extend\nthe application of Generalized Relevance Learning Vector Quantization to deal\nwith Grassmann manifold. The proposed model returns a set of prototype\nsubspaces and a relevance vector. While prototypes model typical behaviours\nwithin classes, the relevance factors specify the most discriminative principal\nvectors (or images) for the classification task. They both provide insights\ninto the model's decisions by highlighting influential images and pixels for\npredictions. Moreover, due to learning prototypes, the model complexity of the\nnew method during inference is independent of dataset size, unlike previous\nworks. We applied it to several recognition tasks including handwritten digit\nrecognition, face recognition, activity recognition, and object recognition.\nExperiments demonstrate that it outperforms previous works with lower\ncomplexity and can successfully model the variation, such as handwritten style\nor lighting conditions. Moreover, the presence of relevances makes the model\nrobust to the selection of subspaces' dimensionality.\n","authors":["M. Mohammadi","M. Babai","M. H. F. Wilkinson"],"pdf_url":"https://arxiv.org/pdf/2403.09183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03018v2","updated":"2024-03-14T08:50:35Z","published":"2023-01-08T10:59:44Z","title":"Energy Disaggregation & Appliance Identification in a Smart Home:\n  Transfer Learning enables Edge Computing","summary":"  Non-intrusive load monitoring (NILM) or energy disaggregation aims to extract\nthe load profiles of individual consumer electronic appliances, given an\naggregate load profile of the mains of a smart home. This work proposes a novel\ndeep-learning and edge computing approach to solve the NILM problem and a few\nrelated problems as follows. 1) We build upon the reputed seq2-point\nconvolutional neural network (CNN) model to come up with the proposed\nseq2-[3]-point CNN model to solve the (home) NILM problem and site-NILM problem\n(basically, NILM at a smaller scale). 2) We solve the related problem of\nappliance identification by building upon the state-of-the-art (pre-trained)\n2D-CNN models, i.e., AlexNet, ResNet-18, and DenseNet-121, which are fine-tuned\ntwo custom datasets that consist of Wavelets and short-time Fourier transform\n(STFT)-based 2D electrical signatures of the appliances. 3) Finally, we do some\nbasic qualitative inference about an individual appliance's health by comparing\nthe power consumption of the same appliance across multiple homes.\nLow-frequency REDD dataset is used for all problems, except site-NILM where\nREFIT dataset has been used. As for the results, we achieve a maximum accuracy\nof 94.6\\% for home-NILM, 81\\% for site-NILM, and 88.9\\% for appliance\nidentification (with Resnet-based model).\n","authors":["M. Hashim Shahab","Hasan Mujtaba Buttar","Ahsan Mehmood","Waqas Aman","M. Mahboob Ur Rahman","M. Wasim Nawaz","Haris Pervaiz","Qammer H. Abbasi"],"pdf_url":"https://arxiv.org/pdf/2301.03018v2.pdf","comment":"10 pages, 4 figures, 3 tables, under review with a journal"},{"id":"http://arxiv.org/abs/2311.17717v2","updated":"2024-03-14T08:35:10Z","published":"2023-11-29T15:19:49Z","title":"Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via\n  Lightweight Erasers","summary":"  Concept erasure in text-to-image diffusion models aims to disable pre-trained\ndiffusion models from generating images related to a target concept. To perform\nreliable concept erasure, the properties of robustness and locality are\ndesirable. The former refrains the model from producing images associated with\nthe target concept for any paraphrased or learned prompts, while the latter\npreserves its ability in generating images with non-target concepts. In this\npaper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler).\nIt learns a lightweight Eraser to perform concept erasing while satisfying the\nabove desirable properties by proposed concept-localized regularization and\nadversarial prompt learning schemes. Comprehensive experiments with various\nconcepts verify the superiority of Receler over previous methods. Our code will\nbe available upon acceptance.\n","authors":["Chi-Pin Huang","Kai-Po Chang","Chung-Ting Tsai","Yung-Hsuan Lai","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09171v1","updated":"2024-03-14T08:31:39Z","published":"2024-03-14T08:31:39Z","title":"ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks","summary":"  Although Graph Neural Networks (GNNs) have exhibited the powerful ability to\ngather graph-structured information from neighborhood nodes via various\nmessage-passing mechanisms, the performance of GNNs is limited by poor\ngeneralization and fragile robustness caused by noisy and redundant graph data.\nAs a prominent solution, Graph Augmentation Learning (GAL) has recently\nreceived increasing attention. Among prior GAL approaches, edge-dropping\nmethods that randomly remove edges from a graph during training are effective\ntechniques to improve the robustness of GNNs. However, randomly dropping edges\noften results in bypassing critical edges, consequently weakening the\neffectiveness of message passing. In this paper, we propose a novel adversarial\nedge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor\nguiding the removal of edges, which can be flexibly incorporated into diverse\nGNN backbones. Employing an adversarial training framework, the edge predictor\nutilizes the line graph transformed from the original graph to estimate the\nedges to be dropped, which improves the interpretability of the edge-dropping\nmethod. The proposed ADEdgeDrop is optimized alternately by stochastic gradient\ndescent and projected gradient descent. Comprehensive experiments on six graph\nbenchmark datasets demonstrate that the proposed ADEdgeDrop outperforms\nstate-of-the-art baselines across various GNN backbones, demonstrating improved\ngeneralization and robustness.\n","authors":["Zhaoliang Chen","Zhihao Wu","Ylli Sadikaj","Claudia Plant","Hong-Ning Dai","Shiping Wang","Wenzhong Guo"],"pdf_url":"https://arxiv.org/pdf/2403.09171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08381v3","updated":"2024-03-14T08:19:15Z","published":"2023-08-16T14:09:48Z","title":"Precision and Recall Reject Curves for Classification","summary":"  For some classification scenarios, it is desirable to use only those\nclassification instances that a trained model associates with a high certainty.\nTo obtain such high-certainty instances, previous work has proposed\naccuracy-reject curves. Reject curves allow to evaluate and compare the\nperformance of different certainty measures over a range of thresholds for\naccepting or rejecting classifications. However, the accuracy may not be the\nmost suited evaluation metric for all applications, and instead precision or\nrecall may be preferable. This is the case, for example, for data with\nimbalanced class distributions. We therefore propose reject curves that\nevaluate precision and recall, the recall-reject curve and the precision-reject\ncurve. Using prototype-based classifiers from learning vector quantization, we\nfirst validate the proposed curves on artificial benchmark data against the\naccuracy reject curve as a baseline. We then show on imbalanced benchmarks and\nmedical, real-world data that for these scenarios, the proposed precision- and\nrecall-curves yield more accurate insights into classifier performance than\naccuracy reject curves.\n","authors":["Lydia Fischer","Patricia Wollstadt"],"pdf_url":"https://arxiv.org/pdf/2308.08381v3.pdf","comment":"10 pages, 3 figures. Updated figure labels. Included reviewer\n  remarks. Accepted at International Workshop on Self-Organizing Maps and\n  Learning Vector Quantization, Clustering and Data Visualization (WSOM+) 2024"},{"id":"http://arxiv.org/abs/2305.13068v3","updated":"2024-03-14T08:15:27Z","published":"2023-05-22T14:37:05Z","title":"Making Language Models Better Tool Learners with Execution Feedback","summary":"  Tools serve as pivotal interfaces that enable humans to understand and\nreshape the environment. With the advent of foundation models, AI systems can\nutilize tools to expand their capabilities and interact with the real world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce large language models to utilize\ntools indiscriminately, as complex tasks often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the large language model\nselectively use tools by improving the accuracy of tool usage while enhancing\ninsufficient tool learning and mitigating excessive reliance on tools. Code is\navailable at https://github.com/zjunlp/TRICE.\n","authors":["Shuofei Qiao","Honghao Gui","Chengfei Lv","Qianghuai Jia","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13068v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2310.20703v3","updated":"2024-03-14T08:05:18Z","published":"2023-10-31T17:59:05Z","title":"Vanishing Gradients in Reinforcement Finetuning of Language Models","summary":"  Pretrained language models are commonly aligned with human preferences and\ndownstream tasks via reinforcement finetuning (RFT), which refers to maximizing\na (possibly learned) reward function using policy gradient algorithms. This\nwork identifies a fundamental optimization obstacle in RFT: we prove that the\nexpected gradient for an input vanishes when its reward standard deviation\nunder the model is small, even if the expected reward is far from optimal.\nThrough experiments on an RFT benchmark and controlled environments, as well as\na theoretical analysis, we then demonstrate that vanishing gradients due to\nsmall reward standard deviation are prevalent and detrimental, leading to\nextremely slow reward maximization. Lastly, we explore ways to overcome\nvanishing gradients in RFT. We find the common practice of an initial\nsupervised finetuning (SFT) phase to be the most promising candidate, which\nsheds light on its importance in an RFT pipeline. Moreover, we show that a\nrelatively small number of SFT optimization steps on as few as 1% of the input\nsamples can suffice, indicating that the initial SFT phase need not be\nexpensive in terms of compute and data labeling efforts. Overall, our results\nemphasize that being mindful for inputs whose expected gradient vanishes, as\nmeasured by the reward standard deviation, is crucial for successful execution\nof RFT.\n","authors":["Noam Razin","Hattie Zhou","Omid Saremi","Vimal Thilak","Arwen Bradley","Preetum Nakkiran","Joshua Susskind","Etai Littwin"],"pdf_url":"https://arxiv.org/pdf/2310.20703v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2401.11792v5","updated":"2024-03-14T07:47:03Z","published":"2024-01-22T09:44:16Z","title":"Safe and Generalized end-to-end Autonomous Driving System with\n  Reinforcement Learning and Demonstrations","summary":"  An intelligent driving system should be capable of dynamically formulating\nappropriate driving strategies based on the current environment and vehicle\nstatus, while ensuring the security and reliability of the system. However,\nexisting methods based on reinforcement learning and imitation learning suffer\nfrom low safety, poor generalization, and inefficient sampling. Additionally,\nthey cannot accurately predict future driving trajectories, and the accurate\nprediction of future driving trajectories is a precondition for making optimal\ndecisions. To solve these problems, in this paper, we introduce a Safe and\nGeneralized end-to-end Autonomous Driving System (SGADS) for complex and\nvarious scenarios. Our SGADS incorporates variational inference with\nnormalizing flows, enabling the intelligent vehicle to accurately predict\nfuture driving trajectories. Moreover, we propose the formulation of robust\nsafety constraints. Furthermore, we combine reinforcement learning with\ndemonstrations to augment search process of the agent. The experimental results\ndemonstrate that our SGADS can significantly improve safety performance,\nexhibit strong generalization, and enhance the training efficiency of\nintelligent vehicles in complex urban scenarios compared to existing methods.\n","authors":["Zuojin Tang","Xiaoyu Chen","YongQiang Li","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11792v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02254v2","updated":"2024-03-14T07:42:52Z","published":"2023-12-04T18:45:28Z","title":"Innovations in Agricultural Forecasting: A Multivariate Regression Study\n  on Global Crop Yield Prediction","summary":"  The prediction of crop yields internationally is a crucial objective in\nagricultural research. Thus, this study implements 6 regression models (Linear,\nTree, Gradient Descent, Gradient Boosting, K Nearest Neighbors, and Random\nForest) to predict crop yields in 37 developing countries over 27 years. Given\n4 key training parameters, insecticides (tonnes), rainfall (mm), temperature\n(Celsius), and yield (hg/ha), it was found that our Random Forest Regression\nmodel achieved a determination coefficient (r2) of 0.94, with a margin of error\n(ME) of .03. The models were trained and tested using the Food and Agricultural\nOrganization of the United Nations data, along with the World Bank Climate\nChange Data Catalog. Furthermore, each parameter was analyzed to understand how\nvarying factors could impact overall yield. We used unconventional models,\ncontrary to generally used Deep Learning (DL) and Machine Learning (ML) models,\ncombined with recently collected data to implement a unique approach in our\nresearch. Existing scholarship would benefit from understanding the most\noptimal model for agricultural research, specifically using the United Nations\ndata.\n","authors":["Ishaan Gupta","Samyutha Ayalasomayajula","Yashas Shashidhara","Anish Kataria","Shreyas Shashidhara","Krishita Kataria","Aditya Undurti"],"pdf_url":"https://arxiv.org/pdf/2312.02254v2.pdf","comment":"12 pages, 8 figures, 1 table, Guided by Dr. Aditya Undurti"},{"id":"http://arxiv.org/abs/2403.09141v1","updated":"2024-03-14T07:40:32Z","published":"2024-03-14T07:40:32Z","title":"Uncertainty Estimation in Multi-Agent Distributed Learning for\n  AI-Enabled Edge Devices","summary":"  Initially considered as low-power units with limited autonomous processing,\nEdge IoT devices have seen a paradigm shift with the introduction of FPGAs and\nAI accelerators. This advancement has vastly amplified their computational\ncapabilities, emphasizing the practicality of edge AI. Such progress introduces\nnew challenges of optimizing AI tasks for the limitations of energy and network\nresources typical in Edge computing environments. Our study explores methods\nthat enable distributed data processing through AI-enabled edge devices,\nenhancing collaborative learning capabilities. A key focus of our research is\nthe challenge of determining confidence levels in learning outcomes,\nconsidering the spatial and temporal variability of data sets encountered by\nindependent agents. To address this issue, we investigate the application of\nBayesian neural networks, proposing a novel approach to manage uncertainty in\ndistributed learning environments.\n","authors":["Gleb Radchenko","Victoria Andrea Fill"],"pdf_url":"https://arxiv.org/pdf/2403.09141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08268v2","updated":"2024-03-14T07:17:03Z","published":"2024-02-13T07:47:36Z","title":"World Model on Million-Length Video And Language With Blockwise\n  RingAttention","summary":"  Current language models fall short in understanding aspects of the world not\neasily described in words, and struggle with complex, long-form tasks. Video\nsequences offer valuable temporal information absent in language and static\nimages, making them attractive for joint modeling with language. Such models\ncould develop a understanding of both human textual knowledge and the physical\nworld, enabling broader AI capabilities for assisting humans. However, learning\nfrom millions of tokens of video and language sequences poses challenges due to\nmemory constraints, computational complexity, and limited datasets. To address\nthese challenges, we curate a large dataset of diverse videos and books,\nutilize the Blockwise RingAttention technique to scalably train on long\nsequences, and gradually increase context size from 4K to 1M tokens. This paper\nmakes the following contributions: (a) Largest context size neural network: We\ntrain one of the largest context size transformers on long video and language\nsequences, setting new benchmarks in difficult retrieval tasks and long video\nunderstanding. (b) Solutions for overcoming vision-language training\nchallenges, including using masked sequence packing for mixing different\nsequence lengths, loss weighting to balance language and vision, and\nmodel-generated QA dataset for long sequence chat. (c) A highly-optimized\nimplementation with RingAttention, Blockwise Transformers, masked sequence\npacking, and other key features for training on millions-length multimodal\nsequences. (d) Fully open-sourced a family of 7B parameter models capable of\nprocessing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM,\nLWM-Chat) of over 1M tokens. This work paves the way for training on massive\ndatasets of long video and language to develop understanding of both human\nknowledge and the multimodal world, and broader capabilities.\n","authors":["Hao Liu","Wilson Yan","Matei Zaharia","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2402.08268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01742v2","updated":"2024-03-14T06:35:34Z","published":"2024-03-04T05:39:23Z","title":"Diffusion-TS: Interpretable Diffusion for General Time Series Generation","summary":"  Denoising diffusion probabilistic models (DDPMs) are becoming the leading\nparadigm for generative models. It has recently shown breakthroughs in audio\nsynthesis, time series imputation and forecasting. In this paper, we propose\nDiffusion-TS, a novel diffusion-based framework that generates multivariate\ntime series samples of high quality by using an encoder-decoder transformer\nwith disentangled temporal representations, in which the decomposition\ntechnique guides Diffusion-TS to capture the semantic meaning of time series\nwhile transformers mine detailed sequential information from the noisy model\ninput. Different from existing diffusion-based approaches, we train the model\nto directly reconstruct the sample instead of the noise in each diffusion step,\ncombining a Fourier-based loss term. Diffusion-TS is expected to generate time\nseries satisfying both interpretablity and realness. In addition, it is shown\nthat the proposed Diffusion-TS can be easily extended to conditional generation\ntasks, such as forecasting and imputation, without any model changes. This also\nmotivates us to further explore the performance of Diffusion-TS under irregular\nsettings. Finally, through qualitative and quantitative experiments, results\nshow that Diffusion-TS achieves the state-of-the-art results on various\nrealistic analyses of time series.\n","authors":["Xinyu Yuan","Yan Qiao"],"pdf_url":"https://arxiv.org/pdf/2403.01742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09123v1","updated":"2024-03-14T06:14:07Z","published":"2024-03-14T06:14:07Z","title":"Optimal Top-Two Method for Best Arm Identification and Fluid Analysis","summary":"  Top-$2$ methods have become popular in solving the best arm identification\n(BAI) problem. The best arm, or the arm with the largest mean amongst finitely\nmany, is identified through an algorithm that at any sequential step\nindependently pulls the empirical best arm, with a fixed probability $\\beta$,\nand pulls the best challenger arm otherwise. The probability of incorrect\nselection is guaranteed to lie below a specified $\\delta >0$. Information\ntheoretic lower bounds on sample complexity are well known for BAI problem and\nare matched asymptotically as $\\delta \\rightarrow 0$ by computationally\ndemanding plug-in methods. The above top 2 algorithm for any $\\beta \\in (0,1)$\nhas sample complexity within a constant of the lower bound. However,\ndetermining the optimal $\\beta$ that matches the lower bound has proven\ndifficult. In this paper, we address this and propose an optimal top-2 type\nalgorithm. We consider a function of allocations anchored at a threshold. If it\nexceeds the threshold then the algorithm samples the empirical best arm.\nOtherwise, it samples the challenger arm. We show that the proposed algorithm\nis optimal as $\\delta \\rightarrow 0$. Our analysis relies on identifying a\nlimiting fluid dynamics of allocations that satisfy a series of ordinary\ndifferential equations pasted together and that describe the asymptotic path\nfollowed by our algorithm. We rely on the implicit function theorem to show\nexistence and uniqueness of these fluid ode's and to show that the proposed\nalgorithm remains close to the ode solution.\n","authors":["Agniv Bandyopadhyay","Sandeep Juneja","Shubhada Agrawal"],"pdf_url":"https://arxiv.org/pdf/2403.09123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09117v1","updated":"2024-03-14T05:40:23Z","published":"2024-03-14T05:40:23Z","title":"Randomized Principal Component Analysis for Hyperspectral Image\n  Classification","summary":"  The high-dimensional feature space of the hyperspectral imagery poses major\nchallenges to the processing and analysis of the hyperspectral data sets. In\nsuch a case, dimensionality reduction is necessary to decrease the\ncomputational complexity. The random projections open up new ways of\ndimensionality reduction, especially for large data sets. In this paper, the\nprincipal component analysis (PCA) and randomized principal component analysis\n(R-PCA) for the classification of hyperspectral images using support vector\nmachines (SVM) and light gradient boosting machines (LightGBM) have been\ninvestigated. In this experimental research, the number of features was reduced\nto 20 and 30 for classification of two hyperspectral datasets (Indian Pines and\nPavia University). The experimental results demonstrated that PCA outperformed\nR-PCA for SVM for both datasets, but received close accuracy values for\nLightGBM. The highest classification accuracies were obtained as 0.9925 and\n0.9639 by LightGBM with original features for the Pavia University and Indian\nPines, respectively.\n","authors":["Mustafa Ustuner"],"pdf_url":"https://arxiv.org/pdf/2403.09117v1.pdf","comment":"5 pages, I have submitted this paper to M2GARSS 2024, 2024 IEEE\n  Mediterranean and Middle-East Geoscience and Remote Sensing Symposium"},{"id":"http://arxiv.org/abs/2403.09113v1","updated":"2024-03-14T05:29:35Z","published":"2024-03-14T05:29:35Z","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based\n  on Meta Learning","summary":"  Large-scale pretraining followed by task-specific finetuning has achieved\ngreat success in various NLP tasks. Since finetuning all parameters of large\npretrained models poses substantial computational and memory challenges,\nseveral efficient finetuning methods have been developed. Among them, low-rank\nadaptation (LoRA), which finetunes low-rank incremental update matrices on top\nof frozen pretrained weights, has proven particularly effective. Nonetheless,\nLoRA's uniform rank assignment across all layers, along with its reliance on an\nexhaustive search to find the best rank, leads to high computation costs and\nsuboptimal finetuning performance. To address these limitations, we introduce\nAutoLoRA, a meta learning based framework for automatically identifying the\noptimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a\nlow-rank update matrix with a selection variable, which determines whether the\nrank-1 matrix should be discarded. A meta learning based method is developed to\nlearn these selection variables. The optimal rank is determined by thresholding\nthe values of these variables. Our comprehensive experiments on natural\nlanguage understanding, generation, and sequence labeling demonstrate the\neffectiveness of AutoLoRA.\n","authors":["Ruiyi Zhang","Rushi Qiang","Sai Ashish Somayajula","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2403.09113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.14464v3","updated":"2024-03-14T05:28:04Z","published":"2022-06-29T08:40:55Z","title":"SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations","summary":"  Score-based generative models (SGMs) show the state-of-the-art sampling\nquality and diversity. However, their training/sampling complexity is\nnotoriously high due to the highly complicated forward/reverse processes, so\nthey are not suitable for resource-limited settings. To solving this problem,\nlearning a simpler process is gathering much attention currently. We present an\nenhanced GAN-based denoising method, called SPI-GAN, using our proposed\nstraight-path interpolation definition. To this end, we propose a GAN\narchitecture i) denoising through the straight-path and ii) characterized by a\ncontinuous mapping neural network for imitating the denoising path. This\napproach drastically reduces the sampling time while achieving as high sampling\nquality and diversity as SGMs. As a result, SPI-GAN is one of the best-balanced\nmodels among the sampling quality, diversity, and time for CIFAR-10, and\nCelebA-HQ-256.\n","authors":["Jinsung Jeon","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2206.14464v3.pdf","comment":"Accepted at ICLR 2024 Practical ML for Developing Countries Workshop\n  (PML4DC)"},{"id":"http://arxiv.org/abs/2403.08265v2","updated":"2024-03-14T05:18:08Z","published":"2024-03-13T05:32:13Z","title":"Random Search as a Baseline for Sparse Neural Network Architecture\n  Search","summary":"  Sparse neural networks have shown similar or better generalization\nperformance than their dense counterparts while having higher parameter\nefficiency. This has motivated a number of works to learn or search for high\nperforming sparse networks. While reports of task performance or efficiency\ngains are impressive, standard baselines are lacking leading to poor\ncomparability and unreliable reproducibility across methods. In this work, we\npropose Random Search as a baseline algorithm for finding good sparse\nconfigurations and study its performance. We apply Random Search on the node\nspace of an overparameterized network with the goal of finding better\ninitialized sparse sub-networks that are positioned more advantageously in the\nloss landscape. We record the post-training performances of the found sparse\nnetworks and at various levels of sparsity, and compare against both their\nfully connected parent networks and random sparse configurations at the same\nsparsity levels. First, we demonstrate performance at different levels of\nsparsity and highlight that a significant level of performance can still be\npreserved even when the network is highly sparse. Second, we observe that for\nthis sparse architecture search task, initialized sparse networks found by\nRandom Search neither perform better nor converge more efficiently than their\nrandom counterparts. Thus we conclude that Random Search may be viewed as a\nreasonable neutral baseline for sparsity search methods.\n","authors":["Rezsa Farahani"],"pdf_url":"https://arxiv.org/pdf/2403.08265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09110v1","updated":"2024-03-14T05:17:39Z","published":"2024-03-14T05:17:39Z","title":"SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning","summary":"  Deep reinforcement learning (DRL) has shown significant promise for\nuncovering sophisticated control policies that interact in environments with\ncomplicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak\nfusion reactor or minimizing the drag force exerted on an object in a fluid\nflow. However, these algorithms require an abundance of training examples and\nmay become prohibitively expensive for many applications. In addition, the\nreliance on deep neural networks often results in an uninterpretable, black-box\npolicy that may be too computationally expensive to use with certain embedded\nsystems. Recent advances in sparse dictionary learning, such as the sparse\nidentification of nonlinear dynamics (SINDy), have shown promise for creating\nefficient and interpretable data-driven models in the low-data regime. In this\nwork we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to\ncreate efficient, interpretable, and trustworthy representations of the\ndynamics model, reward function, and control policy. We demonstrate the\neffectiveness of our approaches on benchmark control environments and\nchallenging fluids problems. SINDy-RL achieves comparable performance to\nstate-of-the-art DRL algorithms using significantly fewer interactions in the\nenvironment and results in an interpretable control policy orders of magnitude\nsmaller than a deep neural network policy.\n","authors":["Nicholas Zolman","Urban Fasel","J. Nathan Kutz","Steven L. Brunton"],"pdf_url":"https://arxiv.org/pdf/2403.09110v1.pdf","comment":"24 pages + 14 appendices (45 pages total). 25 figures, 7 tables. For\n  code, see https://github.com/nzolman/sindy-rl"},{"id":"http://arxiv.org/abs/2309.03179v4","updated":"2024-03-14T05:15:10Z","published":"2023-09-06T17:39:05Z","title":"SLiMe: Segment Like Me","summary":"  Significant strides have been made using large vision-language models, like\nStable Diffusion (SD), for a variety of downstream tasks, including image\nediting, image correspondence, and 3D shape generation. Inspired by these\nadvancements, we explore leveraging these extensive vision-language models for\nsegmenting images at any desired granularity using as few as one annotated\nsample by proposing SLiMe. SLiMe frames this problem as an optimization task.\nSpecifically, given a single training image and its segmentation mask, we first\nextract attention maps, including our novel \"weighted accumulated\nself-attention map\" from the SD prior. Then, using the extracted attention\nmaps, the text embeddings of Stable Diffusion are optimized such that, each of\nthem, learn about a single segmented region from the training image. These\nlearned embeddings then highlight the segmented region in the attention maps,\nwhich in turn can then be used to derive the segmentation map. This enables\nSLiMe to segment any real-world image during inference with the granularity of\nthe segmented region in the training image, using just one example. Moreover,\nleveraging additional training data when available, i.e. few-shot, improves the\nperformance of SLiMe. We carried out a knowledge-rich set of experiments\nexamining various design factors and showed that SLiMe outperforms other\nexisting one-shot and few-shot segmentation methods.\n","authors":["Aliasghar Khani","Saeid Asgari Taghanaki","Aditya Sanghi","Ali Mahdavi Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2309.03179v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07630v2","updated":"2024-03-14T05:04:40Z","published":"2024-02-12T13:13:04Z","title":"G-Retriever: Retrieval-Augmented Generation for Textual Graph\n  Understanding and Question Answering","summary":"  Given a graph with textual attributes, we enable users to `chat with their\ngraph': that is, to ask questions about the graph using a conversational\ninterface. In response to a user's questions, our method provides textual\nreplies and highlights the relevant parts of the graph. While existing works\nintegrate large language models (LLMs) and graph neural networks (GNNs) in\nvarious ways, they mostly focus on either conventional graph tasks (such as\nnode, edge, and graph classification), or on answering simple graph queries on\nsmall or synthetic graphs. In contrast, we develop a flexible\nquestion-answering framework targeting real-world textual graphs, applicable to\nmultiple applications including scene graph understanding, common sense\nreasoning, and knowledge graph reasoning. Toward this goal, we first develop\nour Graph Question Answering (GraphQA) benchmark with data collected from\ndifferent tasks. Then, we propose our G-Retriever approach, which integrates\nthe strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can\nbe fine-tuned to enhance graph understanding via soft prompting. To resist\nhallucination and to allow for textual graphs that greatly exceed the LLM's\ncontext window size, G-Retriever performs RAG over a graph by formulating this\ntask as a Prize-Collecting Steiner Tree optimization problem. Empirical\nevaluations show that our method outperforms baselines on textual graph tasks\nfrom multiple domains, scales well with larger graph sizes, and resists\nhallucination. (Our codes and datasets are available at:\nhttps://github.com/XiaoxinHe/G-Retriever.)\n","authors":["Xiaoxin He","Yijun Tian","Yifei Sun","Nitesh V. Chawla","Thomas Laurent","Yann LeCun","Xavier Bresson","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2402.07630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09107v1","updated":"2024-03-14T05:00:29Z","published":"2024-03-14T05:00:29Z","title":"S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering","summary":"  Anchor-based large-scale multi-view clustering has attracted considerable\nattention for its effectiveness in handling massive datasets. However, current\nmethods mainly seek the consensus embedding feature for clustering by exploring\nglobal correlations between anchor graphs or projection matrices.In this paper,\nwe propose a simple yet efficient scalable multi-view tensor clustering\n(S^2MVTC) approach, where our focus is on learning correlations of embedding\nfeatures within and across views. Specifically, we first construct the\nembedding feature tensor by stacking the embedding features of different views\ninto a tensor and rotating it. Additionally, we build a novel tensor\nlow-frequency approximation (TLFA) operator, which incorporates graph\nsimilarity into embedding feature learning, efficiently achieving smooth\nrepresentation of embedding features within different views. Furthermore,\nconsensus constraints are applied to embedding features to ensure inter-view\nsemantic consistency. Experimental results on six large-scale multi-view\ndatasets demonstrate that S^2MVTC significantly outperforms state-of-the-art\nalgorithms in terms of clustering performance and CPU execution time,\nespecially when handling massive data. The code of S^2MVTC is publicly\navailable at https://github.com/longzhen520/S2MVTC.\n","authors":["Zhen Long","Qiyuan Wang","Yazhou Ren","Yipeng Liu","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.09107v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2310.01794v3","updated":"2024-03-14T04:52:05Z","published":"2023-10-03T04:42:44Z","title":"GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers\n  through In-depth Benchmarking","summary":"  Numerous explainability methods have been proposed to shed light on the inner\nworkings of GNNs. Despite the inclusion of empirical evaluations in all the\nproposed algorithms, the interrogative aspects of these evaluations lack\ndiversity. As a result, various facets of explainability pertaining to GNNs,\nsuch as a comparative analysis of counterfactual reasoners, their stability to\nvariational factors such as different GNN architectures, noise, stochasticity\nin non-convex loss surfaces, feasibility amidst domain constraints, and so\nforth, have yet to be formally investigated. Motivated by this need, we present\na benchmarking study on perturbation-based explainability methods for GNNs,\naiming to systematically evaluate and compare a wide range of explainability\ntechniques. Among the key findings of our study, we identify the Pareto-optimal\nmethods that exhibit superior efficacy and stability in the presence of noise.\nNonetheless, our study reveals that all algorithms are affected by stability\nissues when faced with noisy data. Furthermore, we have established that the\ncurrent generation of counterfactual explainers often fails to provide feasible\nrecourses due to violations of topological constraints encoded by\ndomain-specific considerations. Overall, this benchmarking study empowers\nstakeholders in the field of GNNs with a comprehensive understanding of the\nstate-of-the-art explainability methods, potential research problems for\nfurther enhancement, and the implications of their application in real-world\nscenarios.\n","authors":["Mert Kosan","Samidha Verma","Burouj Armgaan","Khushbu Pahwa","Ambuj Singh","Sourav Medya","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2310.01794v3.pdf","comment":"Accepted at ICLR 2024"},{"id":"http://arxiv.org/abs/2401.10474v2","updated":"2024-03-14T04:49:05Z","published":"2024-01-19T03:50:19Z","title":"LDReg: Local Dimensionality Regularized Self-Supervised Learning","summary":"  Representations learned via self-supervised learning (SSL) can be susceptible\nto dimensional collapse, where the learned representation subspace is of\nextremely low dimensionality and thus fails to represent the full data\ndistribution and modalities. Dimensional collapse also known as the\n\"underfilling\" phenomenon is one of the major causes of degraded performance on\ndownstream tasks. Previous work has investigated the dimensional collapse\nproblem of SSL at a global level. In this paper, we demonstrate that\nrepresentations can span over high dimensional space globally, but collapse\nlocally. To address this, we propose a method called $\\textit{local\ndimensionality regularization (LDReg)}$. Our formulation is based on the\nderivation of the Fisher-Rao metric to compare and optimize local distance\ndistributions at an asymptotically small radius for each data point. By\nincreasing the local intrinsic dimensionality, we demonstrate through a range\nof experiments that LDReg improves the representation quality of SSL. The\nresults also show that LDReg can regularize dimensionality at both local and\nglobal levels.\n","authors":["Hanxun Huang","Ricardo J. G. B. Campello","Sarah Monazam Erfani","Xingjun Ma","Michael E. Houle","James Bailey"],"pdf_url":"https://arxiv.org/pdf/2401.10474v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09101v1","updated":"2024-03-14T04:48:31Z","published":"2024-03-14T04:48:31Z","title":"Soften to Defend: Towards Adversarial Robustness via Self-Guided Label\n  Refinement","summary":"  Adversarial training (AT) is currently one of the most effective ways to\nobtain the robustness of deep neural networks against adversarial attacks.\nHowever, most AT methods suffer from robust overfitting, i.e., a significant\ngeneralization gap in adversarial robustness between the training and testing\ncurves. In this paper, we first identify a connection between robust\noverfitting and the excessive memorization of noisy labels in AT from a view of\ngradient norm. As such label noise is mainly caused by a distribution mismatch\nand improper label assignments, we are motivated to propose a label refinement\napproach for AT. Specifically, our Self-Guided Label Refinement first\nself-refines a more accurate and informative label distribution from\nover-confident hard labels, and then it calibrates the training by dynamically\nincorporating knowledge from self-distilled models into the current model and\nthus requiring no external teachers. Empirical results demonstrate that our\nmethod can simultaneously boost the standard accuracy and robust performance\nacross multiple benchmark datasets, attack types, and architectures. In\naddition, we also provide a set of analyses from the perspectives of\ninformation theory to dive into our method and suggest the importance of soft\nlabels for robust generalization.\n","authors":["Daiwei Yu","Zhuorong Li","Lina Wei","Canghong Jin","Yun Zhang","Sixian Chan"],"pdf_url":"https://arxiv.org/pdf/2403.09101v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.09100v1","updated":"2024-03-14T04:48:06Z","published":"2024-03-14T04:48:06Z","title":"Virtual birefringence imaging and histological staining of amyloid\n  deposits in label-free tissue using autofluorescence microscopy and deep\n  learning","summary":"  Systemic amyloidosis is a group of diseases characterized by the deposition\nof misfolded proteins in various organs and tissues, leading to progressive\norgan dysfunction and failure. Congo red stain is the gold standard chemical\nstain for the visualization of amyloid deposits in tissue sections, as it forms\ncomplexes with the misfolded proteins and shows a birefringence pattern under\npolarized light microscopy. However, Congo red staining is tedious and costly\nto perform, and prone to false diagnoses due to variations in the amount of\namyloid, staining quality and expert interpretation through manual examination\nof tissue under a polarization microscope. Here, we report the first\ndemonstration of virtual birefringence imaging and virtual Congo red staining\nof label-free human tissue to show that a single trained neural network can\nrapidly transform autofluorescence images of label-free tissue sections into\nbrightfield and polarized light microscopy equivalent images, matching the\nhistochemically stained versions of the same samples. We demonstrate the\nefficacy of our method with blind testing and pathologist evaluations on\ncardiac tissue where the virtually stained images agreed well with the\nhistochemically stained ground truth images. Our virtually stained polarization\nand brightfield images highlight amyloid birefringence patterns in a\nconsistent, reproducible manner while mitigating diagnostic challenges due to\nvariations in the quality of chemical staining and manual imaging processes as\npart of the clinical workflow.\n","authors":["Xilin Yang","Bijie Bai","Yijie Zhang","Musa Aydin","Sahan Yoruc Selcuk","Zhen Guo","Gregory A. Fishbein","Karine Atlan","William Dean Wallace","Nir Pillar","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2403.09100v1.pdf","comment":"20 Pages, 5 Figures"},{"id":"http://arxiv.org/abs/2403.09090v1","updated":"2024-03-14T04:26:00Z","published":"2024-03-14T04:26:00Z","title":"Dissipative Gradient Descent Ascent Method: A Control Theory Inspired\n  Algorithm for Min-max Optimization","summary":"  Gradient Descent Ascent (GDA) methods for min-max optimization problems\ntypically produce oscillatory behavior that can lead to instability, e.g., in\nbilinear settings. To address this problem, we introduce a dissipation term\ninto the GDA updates to dampen these oscillations. The proposed Dissipative GDA\n(DGDA) method can be seen as performing standard GDA on a state-augmented and\nregularized saddle function that does not strictly introduce additional\nconvexity/concavity. We theoretically show the linear convergence of DGDA in\nthe bilinear and strongly convex-strongly concave settings and assess its\nperformance by comparing DGDA with other methods such as GDA, Extra-Gradient\n(EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these\nmethods, achieving superior convergence rates. We support our claims with two\nnumerical examples that showcase DGDA's effectiveness in solving saddle point\nproblems.\n","authors":["Tianqi Zheng","Nicolas Loizou","Pengcheng You","Enrique Mallada"],"pdf_url":"https://arxiv.org/pdf/2403.09090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08309v2","updated":"2024-03-14T04:24:41Z","published":"2024-03-13T07:38:20Z","title":"HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain\n  Reinforcement Learning From AI Feedback","summary":"  Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter\nannotation cycles and lower costs over Reinforcement Learning from Human\nFeedback (RLHF), making it highly efficient during the rapid strategy iteration\nperiods of large language model (LLM) training. Using ChatGPT as a labeler to\nprovide feedback on open-domain prompts in RLAIF training, we observe an\nincrease in human evaluators' preference win ratio for model responses, but a\ndecrease in evaluators' satisfaction rate. Analysis suggests that the decrease\nin satisfaction rate is mainly due to some responses becoming less helpful,\nparticularly in terms of correctness and truthfulness, highlighting practical\nlimitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement\nLearning from AI Feedback (HRLAIF). This method enhances the accuracy of AI\nannotations for responses, making the model's helpfulness more robust in\ntraining process. Additionally, it employs AI for Red Teaming, further\nimproving the model's harmlessness. Human evaluation results show that HRLAIF\ninherits the ability of RLAIF to enhance human preference for outcomes at a low\ncost while also improving the satisfaction rate of responses. Compared to the\npolicy model before Reinforcement Learning (RL), it achieves an increase of\n2.08\\% in satisfaction rate, effectively addressing the issue of a decrease of\n4.58\\% in satisfaction rate after basic RLAIF.\n","authors":["Ang Li","Qiugen Xiao","Peng Cao","Jian Tang","Yi Yuan","Zijie Zhao","Xiaoyuan Chen","Liang Zhang","Xiangyang Li","Kaitong Yang","Weidong Guo","Yukang Gan","Xu Yu","Daniell Wang","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2403.08309v2.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.09086v1","updated":"2024-03-14T04:06:45Z","published":"2024-03-14T04:06:45Z","title":"Learning from straggler clients in federated learning","summary":"  How well do existing federated learning algorithms learn from client devices\nthat return model updates with a significant time delay? Is it even possible to\nlearn effectively from clients that report back minutes, hours, or days after\nbeing scheduled? We answer these questions by developing Monte Carlo\nsimulations of client latency that are guided by real-world applications. We\nstudy synchronous optimization algorithms like FedAvg and FedAdam as well as\nthe asynchronous FedBuff algorithm, and observe that all these existing\napproaches struggle to learn from severely delayed clients. To improve upon\nthis situation, we experiment with modifications, including distillation\nregularization and exponential moving averages of model weights. Finally, we\nintroduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation\nand averaging, respectively. Experiments with the EMNIST, CIFAR-100, and\nStackOverflow benchmark federated learning tasks demonstrate that our new\nalgorithms outperform existing ones in terms of accuracy for straggler clients,\nwhile also providing better trade-offs between training time and total\naccuracy.\n","authors":["Andrew Hard","Antonious M. Girgis","Ehsan Amid","Sean Augenstein","Lara McConnaughey","Rajiv Mathews","Rohan Anil"],"pdf_url":"https://arxiv.org/pdf/2403.09086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03195v2","updated":"2024-03-14T03:59:19Z","published":"2024-01-06T11:37:20Z","title":"Efficient Bitrate Ladder Construction using Transfer Learning and\n  Spatio-Temporal Features","summary":"  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n","authors":["Ali Falahati","Mohammad Karim Safavi","Ardavan Elahi","Farhad Pakdaman","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2401.03195v2.pdf","comment":"7 pages, 9 figures, 7 tables, Copyright 2024 IEEE - Presented in IEEE\n  MVIP 2024"},{"id":"http://arxiv.org/abs/2403.06398v2","updated":"2024-03-14T03:57:43Z","published":"2024-03-11T03:19:45Z","title":"On the Diminishing Returns of Width for Continual Learning","summary":"  While deep neural networks have demonstrated groundbreaking performance in\nvarious settings, these models often suffer from \\emph{catastrophic forgetting}\nwhen trained on new tasks in sequence. Several works have empirically\ndemonstrated that increasing the width of a neural network leads to a decrease\nin catastrophic forgetting but have yet to characterize the exact relationship\nbetween width and continual learning. We design one of the first frameworks to\nanalyze Continual Learning Theory and prove that width is directly related to\nforgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that\nincreasing network widths to reduce forgetting yields diminishing returns. We\nempirically verify our claims at widths hitherto unexplored in prior studies\nwhere the diminishing returns are clearly observed as predicted by our theory.\n","authors":["Etash Guha","Vihan Lakshman"],"pdf_url":"https://arxiv.org/pdf/2403.06398v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.09066v1","updated":"2024-03-14T03:13:01Z","published":"2024-03-14T03:13:01Z","title":"Hyperparameters in Continual Learning: a Reality Check","summary":"  Various algorithms for continual learning (CL) have been designed with the\ngoal of effectively alleviating the trade-off between stability and plasticity\nduring the CL process. To achieve this goal, tuning appropriate hyperparameters\nfor each algorithm is essential. As an evaluation protocol, it has been common\npractice to train a CL algorithm using diverse hyperparameter values on a CL\nscenario constructed with a benchmark dataset. Subsequently, the best\nperformance attained with the optimal hyperparameter value serves as the\ncriterion for evaluating the CL algorithm. In this paper, we contend that this\nevaluation protocol is not only impractical but also incapable of effectively\nassessing the CL capability of a CL algorithm. Returning to the fundamental\nprinciples of model evaluation in machine learning, we propose an evaluation\nprotocol that involves Hyperparameter Tuning and Evaluation phases. Those\nphases consist of different datasets but share the same CL scenario. In the\nHyperparameter Tuning phase, each algorithm is iteratively trained with\ndifferent hyperparameter values to find the optimal hyperparameter values.\nSubsequently, in the Evaluation phase, the optimal hyperparameter values is\ndirectly applied for training each algorithm, and their performance in the\nEvaluation phase serves as the criterion for evaluating them. Through\nexperiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in\nclass-incremental learning, we not only observed that the existing evaluation\nmethod fail to properly assess the CL capability of each algorithm but also\nobserve that some recently proposed state-of-the-art algorithms, which reported\nsuperior performance, actually exhibit inferior performance compared to the\nprevious algorithm.\n","authors":["Sungmin Cha","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2403.09066v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.07969v2","updated":"2024-03-14T02:47:41Z","published":"2024-03-12T14:56:34Z","title":"KnowCoder: Coding Structured Knowledge into LLMs for Universal\n  Information Extraction","summary":"  In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct\nUniversal Information Extraction (UIE) via code generation. KnowCoder aims to\ndevelop a kind of unified schema representation that LLMs can easily understand\nand an effective learning framework that encourages LLMs to follow schemas and\nextract structured knowledge accurately. To achieve these, KnowCoder introduces\na code-style schema representation method to uniformly transform different\nschemas into Python classes, with which complex schema information, such as\nconstraints among tasks in UIE, can be captured in an LLM-friendly manner. We\nfurther construct a code-style schema library covering over $\\textbf{30,000}$\ntypes of knowledge, which is the largest one for UIE, to the best of our\nknowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase\nlearning framework that enhances its schema understanding ability via code\npretraining and its schema following ability via instruction tuning. After code\npretraining on around $1.5$B automatically constructed data, KnowCoder already\nattains remarkable generalization ability and achieves relative improvements by\n$\\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting. After\ninstruction tuning, KnowCoder further exhibits strong generalization ability on\nunseen schemas and achieves up to $\\textbf{12.5%}$ and $\\textbf{21.9%}$,\ncompared to sota baselines, under the zero-shot setting and the low resource\nsetting, respectively. Additionally, based on our unified schema\nrepresentations, various human-annotated datasets can simultaneously be\nutilized to refine KnowCoder, which achieves significant improvements up to\n$\\textbf{7.5%}$ under the supervised setting.\n","authors":["Zixuan Li","Yutao Zeng","Yuxin Zuo","Weicheng Ren","Wenxuan Liu","Miao Su","Yucan Guo","Yantao Liu","Xiang Li","Zhilei Hu","Long Bai","Wei Li","Yidan Liu","Pan Yang","Xiaolong Jin","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.07969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09054v1","updated":"2024-03-14T02:42:42Z","published":"2024-03-14T02:42:42Z","title":"Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient\n  Generative Inference","summary":"  Transformers have emerged as the underpinning architecture for Large Language\nModels (LLMs). In generative language models, the inference process involves\ntwo primary phases: prompt processing and token generation. Token generation,\nwhich constitutes the majority of the computational workload, primarily entails\nvector-matrix multiplications and interactions with the Key-Value (KV) Cache.\nThis phase is constrained by memory bandwidth due to the overhead of\ntransferring weights and KV cache values from the memory system to the\ncomputing units. This memory bottleneck becomes particularly pronounced in\napplications that require long-context and extensive text generation, both of\nwhich are increasingly crucial for LLMs.\n  This paper introduces \"Keyformer\", an innovative inference-time approach, to\nmitigate the challenges associated with KV cache size and memory bandwidth\nutilization. Keyformer leverages the observation that approximately 90% of the\nattention weight in generative inference focuses on a specific subset of\ntokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in\nthe KV cache by identifying these crucial tokens using a novel score function.\nThis approach effectively reduces both the KV cache size and memory bandwidth\nusage without compromising model accuracy. We evaluate Keyformer's performance\nacross three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ\nvarious positional embedding algorithms. Our assessment encompasses a variety\nof tasks, with a particular emphasis on summarization and conversation tasks\ninvolving extended contexts. Keyformer's reduction of KV cache reduces\ninference latency by 2.1x and improves token generation throughput by 2.4x,\nwhile preserving the model's accuracy.\n","authors":["Muhammad Adnan","Akhil Arunkumar","Gaurav Jain","Prashant J. Nair","Ilya Soloveychik","Purushotham Kamath"],"pdf_url":"https://arxiv.org/pdf/2403.09054v1.pdf","comment":"A collaborative effort by d-matrix and the University of British\n  Columbia"},{"id":"http://arxiv.org/abs/2403.09053v1","updated":"2024-03-14T02:42:19Z","published":"2024-03-14T02:42:19Z","title":"Towards a theory of model distillation","summary":"  Distillation is the task of replacing a complicated machine learning model\nwith a simpler model that approximates the original [BCNM06,HVD15]. Despite\nmany practical applications, basic questions about the extent to which models\ncan be distilled, and the runtime and amount of data needed to distill, remain\nlargely open.\n  To study these questions, we initiate a general theory of distillation,\ndefining PAC-distillation in an analogous way to PAC-learning [Val84]. As\napplications of this theory: (1) we propose new algorithms to extract the\nknowledge stored in the trained weights of neural networks -- we show how to\nefficiently distill neural networks into succinct, explicit decision tree\nrepresentations when possible by using the ``linear representation\nhypothesis''; and (2) we prove that distillation can be much cheaper than\nlearning from scratch, and make progress on characterizing its complexity.\n","authors":["Enric Boix-Adsera"],"pdf_url":"https://arxiv.org/pdf/2403.09053v1.pdf","comment":"47 pages, 5 figures. Please reach out with comments! Feedback is\n  welcome"},{"id":"http://arxiv.org/abs/2403.09048v1","updated":"2024-03-14T02:36:16Z","published":"2024-03-14T02:36:16Z","title":"Taming Cross-Domain Representation Variance in Federated Prototype\n  Learning with Heterogeneous Data Domains","summary":"  Federated learning (FL) allows collaborative machine learning training\nwithout sharing private data. While most FL methods assume identical data\ndomains across clients, real-world scenarios often involve heterogeneous data\ndomains. Federated Prototype Learning (FedPL) addresses this issue, using mean\nfeature vectors as prototypes to enhance model generalization. However,\nexisting FedPL methods create the same number of prototypes for each client,\nleading to cross-domain performance gaps and disparities for clients with\nvaried data distributions. To mitigate cross-domain feature representation\nvariance, we introduce FedPLVM, which establishes variance-aware dual-level\nprototypes clustering and employs a novel $\\alpha$-sparsity prototype loss. The\ndual-level prototypes clustering strategy creates local clustered prototypes\nbased on private data features, then performs global prototypes clustering to\nreduce communication complexity and preserve local data privacy. The\n$\\alpha$-sparsity prototype loss aligns samples from underrepresented domains,\nenhancing intra-class similarity and reducing inter-class similarity.\nEvaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our\nmethod's superiority over existing approaches.\n","authors":["Lei Wang","Jieming Bian","Letian Zhang","Chen Chen","Jie Xu"],"pdf_url":"https://arxiv.org/pdf/2403.09048v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2308.10792v5","updated":"2024-03-14T02:28:22Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v5.pdf","comment":"V2; Last update: March 12, 2024"},{"id":"http://arxiv.org/abs/2403.09039v1","updated":"2024-03-14T02:26:10Z","published":"2024-03-14T02:26:10Z","title":"Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly\n  Detection in Dynamic Graphs","summary":"  Anomaly detection in dynamic graphs presents a significant challenge due to\nthe temporal evolution of graph structures and attributes. The conventional\napproaches that tackle this problem typically employ an unsupervised learning\nframework, capturing normality patterns with exclusive normal data during\ntraining and identifying deviations as anomalies during testing. However, these\nmethods face critical drawbacks: they either only depend on proxy tasks for\ngeneral representation without directly pinpointing normal patterns, or they\nneglect to differentiate between spatial and temporal normality patterns,\nleading to diminished efficacy in anomaly detection. To address these\nchallenges, we introduce a novel Spatial-Temporal memories-enhanced graph\nautoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs)\nand gated temporal convolution layers to extract spatial features and temporal\nfeatures, respectively. Then STRIPE incorporates separate spatial and temporal\nmemory networks, which capture and store prototypes of normal patterns, thereby\npreserving the uniqueness of spatial and temporal normality. After that,\nthrough a mutual attention mechanism, these stored patterns are then retrieved\nand integrated with encoded graph embeddings. Finally, the integrated features\nare fed into the decoder to reconstruct the graph streams which serve as the\nproxy task for anomaly detection. This comprehensive approach not only\nminimizes reconstruction errors but also refines the model by emphasizing the\ncompactness and distinctiveness of the embeddings in relation to the nearest\nmemory prototypes. Through extensive testing, STRIPE has demonstrated a\nsuperior capability to discern anomalies by effectively leveraging the distinct\nspatial and temporal dynamics of dynamic graphs, significantly outperforming\nexisting methodologies, with an average improvement of 15.39% on AUC values.\n","authors":["Jie Liu","Xuequn Shang","Xiaolin Han","Wentao Zhang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2403.09039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09035v1","updated":"2024-03-14T02:11:38Z","published":"2024-03-14T02:11:38Z","title":"DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers","summary":"  Enabling efficient and accurate deep neural network (DNN) inference on\nmicrocontrollers is non-trivial due to the constrained on-chip resources.\nCurrent methodologies primarily focus on compressing larger models yet at the\nexpense of model accuracy. In this paper, we rethink the problem from the\ninverse perspective by constructing small/weak models directly and improving\ntheir accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference\nframework with a selector-classifiers architecture, where the selector routes\neach input sample to the appropriate classifier for classification. DiTMoS is\ngrounded on a key insight: a composition of weak models can exhibit high\ndiversity and the union of them can significantly boost the accuracy upper\nbound. To approach the upper bound, DiTMoS introduces three strategies\nincluding diverse training data splitting to increase the classifiers'\ndiversity, adversarial selector-classifiers training to ensure synergistic\ninteractions thereby maximizing their complementarity, and heterogeneous\nfeature aggregation to improve the capacity of classifiers. We further propose\na network slicing technique to alleviate the extra memory overhead incurred by\nfeature aggregation. We deploy DiTMoS on the Neucleo STM32F767ZI board and\nevaluate it based on three time-series datasets for human activity recognition,\nkeywords spotting, and emotion recognition, respectively. The experiment\nresults manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement\ncompared to the best baseline; (b) network slicing almost completely eliminates\nthe memory overhead incurred by feature aggregation with a marginal increase of\nlatency.\n","authors":["Xiao Ma","Shengfeng He","Hezhe Qiao","Dong Ma"],"pdf_url":"https://arxiv.org/pdf/2403.09035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09032v1","updated":"2024-03-14T01:51:35Z","published":"2024-03-14T01:51:35Z","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences","summary":"  Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires assessing intricate\ntextual LLMs' outputs. By relying on automated metrics and static analysis\ntools, existing benchmarks fail to assess nuances in user instructions and LLM\noutputs, highlighting the need for large-scale datasets and benchmarks for LLM\npreference alignment. In this paper, we introduce CodeUltraFeedback, a\npreference dataset of 10,000 complex instructions to tune and align LLMs to\ncoding preferences through AI feedback. We generate responses to the\ninstructions using a pool of 14 diverse LLMs, which we then annotate according\nto their alignment with five coding preferences using the LLM-as-a-Judge\napproach with GPT-3.5, producing both numerical and textual feedback. We also\npresent CODAL-Bench, a benchmark for assessing LLM alignment with these coding\npreferences. Our results show that CodeLlama-7B-Instruct, aligned through\nreinforcement learning from AI feedback (RLAIF) with direct preference\noptimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B\nLLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference\ntuning. Furthermore, we show our DPO-aligned CodeLlama model improves\nfunctional correctness on HumanEval+ compared to the unaligned base model.\nTherefore, our contributions bridge the gap in preference tuning of LLMs for\ncode and set the stage for further advancements in model alignment and RLAIF\nfor code intelligence. Our code and data are available at\nhttps://github.com/martin-wey/CodeUltraFeedback.\n","authors":["Martin Weyssow","Aton Kamanda","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2403.09032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09030v1","updated":"2024-03-14T01:46:30Z","published":"2024-03-14T01:46:30Z","title":"An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from\n  Acoustic Signals","summary":"  This study aimed to develop a deep learning model for the classification of\nbearing faults in wind turbine generators from acoustic signals. A\nconvolutional LSTM model was successfully constructed and trained by using\naudio data from five predefined fault types for both training and validation.\nTo create the dataset, raw audio signal data was collected and processed in\nframes to capture time and frequency domain information. The model exhibited\noutstanding accuracy on training samples and demonstrated excellent\ngeneralization ability during validation, indicating its proficiency of\ngeneralization capability. On the test samples, the model achieved remarkable\nclassification performance, with an overall accuracy exceeding 99.5%, and a\nfalse positive rate of less than 1% for normal status. The findings of this\nstudy provide essential support for the diagnosis and maintenance of bearing\nfaults in wind turbine generators, with the potential to enhance the\nreliability and efficiency of wind power generation.\n","authors":["Zhao Wang","Xiaomeng Li","Na Li","Longlong Shu"],"pdf_url":"https://arxiv.org/pdf/2403.09030v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.11093v2","updated":"2024-03-14T15:36:17Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09451v1","updated":"2024-03-14T14:49:40Z","published":"2024-03-14T14:49:40Z","title":"M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in\n  Cognitive Load Assessment","summary":"  This paper introduces the M&M model, a novel multimodal-multitask learning\nframework, applied to the AVCAffe dataset for cognitive load assessment (CLA).\nM&M uniquely integrates audiovisual cues through a dual-pathway architecture,\nfeaturing specialized streams for audio and video inputs. A key innovation lies\nin its cross-modality multihead attention mechanism, fusing the different\nmodalities for synchronized multitasking. Another notable feature is the\nmodel's three specialized branches, each tailored to a specific cognitive load\nlabel, enabling nuanced, task-specific analysis. While it shows modest\nperformance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a\npromising framework for integrated multimodal processing. This work paves the\nway for future enhancements in multimodal-multitask learning systems,\nemphasizing the fusion of diverse data types for complex task handling.\n","authors":["Long Nguyen-Phuoc","Renald Gaboriau","Dimitri Delacroix","Laurent Navarro"],"pdf_url":"https://arxiv.org/pdf/2403.09451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09407v1","updated":"2024-03-14T13:59:04Z","published":"2024-03-14T13:59:04Z","title":"LM2D: Lyrics- and Music-Driven Dance Synthesis","summary":"  Dance typically involves professional choreography with complex movements\nthat follow a musical rhythm and can also be influenced by lyrical content. The\nintegration of lyrics in addition to the auditory dimension, enriches the\nfoundational tone and makes motion generation more amenable to its semantic\nmeanings. However, existing dance synthesis methods tend to model motions only\nconditioned on audio signals. In this work, we make two contributions to bridge\nthis gap. First, we propose LM2D, a novel probabilistic architecture that\nincorporates a multimodal diffusion model with consistency distillation,\ndesigned to create dance conditioned on both music and lyrics in one diffusion\ngeneration step. Second, we introduce the first 3D dance-motion dataset that\nencompasses both music and lyrics, obtained with pose estimation technologies.\nWe evaluate our model against music-only baseline models with objective metrics\nand human evaluations, including dancers and choreographers. The results\ndemonstrate LM2D is able to produce realistic and diverse dance matching both\nlyrics and music. A video summary can be accessed at:\nhttps://youtu.be/4XCgvYookvA.\n","authors":["Wenjie Yin","Xuejiao Zhao","Yi Yu","Hang Yin","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2403.09407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12751v2","updated":"2024-03-14T13:38:53Z","published":"2023-11-21T17:52:30Z","title":"Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with\n  Spatial Relation Matching","summary":"  Navigating drones through natural language commands remains challenging due\nto the dearth of accessible multi-modal datasets and the stringent precision\nrequirements for aligning visual and textual data. To address this pressing\nneed, we introduce GeoText-1652, a new natural language-guided geo-localization\nbenchmark. This dataset is systematically constructed through an interactive\nhuman-computer process leveraging Large Language Model (LLM) driven annotation\ntechniques in conjunction with pre-trained vision models. GeoText-1652 extends\nthe established University-1652 image dataset with spatial-aware text\nannotations, thereby establishing one-to-one correspondences between image,\ntext, and bounding box elements. We further introduce a new optimization\nobjective to leverage fine-grained spatial associations, called blending\nspatial matching, for region-level spatial relation matching. Extensive\nexperiments reveal that our approach maintains a competitive recall rate\ncomparing other prevailing cross-modality methods. This underscores the\npromising potential of our approach in elevating drone control and navigation\nthrough the seamless integration of natural language commands in real-world\nscenarios.\n","authors":["Meng Chu","Zhedong Zheng","Wei Ji","Tingyu Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.12751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15230v2","updated":"2024-03-14T11:49:40Z","published":"2023-11-26T08:04:43Z","title":"GAIA: Zero-shot Talking Avatar Generation","summary":"  Zero-shot talking avatar generation aims at synthesizing natural talking\nvideos from speech and a single portrait image. Previous methods have relied on\ndomain-specific heuristics such as warping-based motion representation and 3D\nMorphable Models, which limit the naturalness and diversity of the generated\navatars. In this work, we introduce GAIA (Generative AI for Avatar), which\neliminates the domain priors in talking avatar generation. In light of the\nobservation that the speech only drives the motion of the avatar while the\nappearance of the avatar and the background typically remain the same\nthroughout the entire video, we divide our approach into two stages: 1)\ndisentangling each frame into motion and appearance representations; 2)\ngenerating motion sequences conditioned on the speech and reference portrait\nimage. We collect a large-scale high-quality talking avatar dataset and train\nthe model on it with different scales (up to 2B parameters). Experimental\nresults verify the superiority, scalability, and flexibility of GAIA as 1) the\nresulting model beats previous baseline models in terms of naturalness,\ndiversity, lip-sync quality, and visual quality; 2) the framework is scalable\nsince larger models yield better results; 3) it is general and enables\ndifferent applications like controllable talking avatar generation and\ntext-instructed avatar generation.\n","authors":["Tianyu He","Junliang Guo","Runyi Yu","Yuchi Wang","Jialiang Zhu","Kaikai An","Leyi Li","Xu Tan","Chunyu Wang","Han Hu","HsiangTao Wu","Sheng Zhao","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2311.15230v2.pdf","comment":"ICLR 2024. Project page: https://microsoft.github.io/GAIA/"},{"id":"http://arxiv.org/abs/2403.05428v2","updated":"2024-03-14T09:54:00Z","published":"2024-03-08T16:30:39Z","title":"Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker\n  Recognition","summary":"  In real-world conversations, the diversity and ambiguity of stickers often\nlead to varied interpretations based on the context, necessitating the\nrequirement for comprehensively understanding stickers and supporting\nmulti-tagging. To address this challenge, we introduce StickerTAG, the first\nmulti-tag sticker dataset comprising a collected tag set with 461 tags and\n13,571 sticker-tag pairs, designed to provide a deeper understanding of\nstickers. Recognizing multiple tags for stickers becomes particularly\nchallenging due to sticker tags usually are fine-grained attribute aware.\nHence, we propose an Attentive Attribute-oriented Prompt Learning method, ie,\nAtt$^2$PL, to capture informative features of stickers in a fine-grained manner\nto better differentiate tags. Specifically, we first apply an\nAttribute-oriented Description Generation (ADG) module to obtain the\ndescription for stickers from four attributes. Then, a Local Re-attention (LoR)\nmodule is designed to perceive the importance of local information. Finally, we\nuse prompt learning to guide the recognition process and adopt confidence\npenalty optimization to penalize the confident output distribution. Extensive\nexperiments show that our method achieves encouraging results for all commonly\nused metrics.\n","authors":["Bingbing Wang","Bin Liang","Chun-Mei Feng","Wangmeng Zuo","Zhixin Bai","Shijue Huang","Kam-Fai Wong","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.05428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07338v3","updated":"2024-03-14T09:28:24Z","published":"2024-03-12T05:43:16Z","title":"D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic\n  Communications","summary":"  Semantic communications (SemCom) have emerged as a new paradigm for\nsupporting sixth-generation applications, where semantic features of data are\ntransmitted using artificial intelligence algorithms to attain high\ncommunication efficiencies. Most existing SemCom techniques utilize deep neural\nnetworks (DNNs) to implement analog source-channel mappings, which are\nincompatible with existing digital communication architectures. To address this\nissue, this paper proposes a novel framework of digital deep joint\nsource-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The\nframework features digital source and channel codings that are jointly\noptimized to reduce the end-to-end (E2E) distortion. First, deep source coding\nwith an adaptive density model is designed to encode semantic features\naccording to their distributions. Second, digital channel coding is employed to\nprotect encoded features against channel distortion. To facilitate their joint\ndesign, the E2E distortion is characterized as a function of the source and\nchannel rates via the analysis of the Bayesian model and Lipschitz assumption\non the DNNs. Then to minimize the E2E distortion, a two-step algorithm is\nproposed to control the source-channel rates for a given channel\nsignal-to-noise ratio. Simulation results reveal that the proposed framework\noutperforms classic deep JSCC and mitigates the cliff and leveling-off effects,\nwhich commonly exist for separation-based approaches.\n","authors":["Jianhao Huang","Kai Yuan","Chuan Huang","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.07338v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11642v3","updated":"2024-03-14T09:13:55Z","published":"2023-11-20T10:01:13Z","title":"Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging","summary":"  Video face re-aging deals with altering the apparent age of a person to the\ntarget age in videos. This problem is challenging due to the lack of paired\nvideo datasets maintaining temporal consistency in identity and age. Most\nre-aging methods process each image individually without considering the\ntemporal consistency of videos. While some existing works address the issue of\ntemporal coherence through video facial attribute manipulation in latent space,\nthey often fail to deliver satisfactory performance in age transformation. To\ntackle the issues, we propose (1) a novel synthetic video dataset that features\nsubjects across a diverse range of age groups; (2) a baseline architecture\ndesigned to validate the effectiveness of our proposed dataset, and (3) the\ndevelopment of novel metrics tailored explicitly for evaluating the temporal\nconsistency of video re-aging techniques. Our comprehensive experiments on\npublic datasets, including VFHQ and CelebA-HQ, show that our method outperforms\nexisting approaches in age transformation accuracy and temporal consistency.\nNotably, in user studies, our method was preferred for temporal consistency by\n48.1\\% of participants for the older direction and by 39.3\\% for the younger\ndirection.\n","authors":["Abdul Muqeet","Kyuchul Lee","Bumsoo Kim","Yohan Hong","Hyungrae Lee","Woonggon Kim","KwangHee Lee"],"pdf_url":"https://arxiv.org/pdf/2311.11642v3.pdf","comment":"28 pages, 11 figures, 11 tables, Project page:\n  https://video-reaging.github.io/"},{"id":"http://arxiv.org/abs/2403.04321v2","updated":"2024-03-14T08:02:29Z","published":"2024-03-07T08:37:33Z","title":"Discriminative Probing and Tuning for Text-to-Image Generation","summary":"  Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.\n","authors":["Leigang Qu","Wenjie Wang","Yongqi Li","Hanwang Zhang","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2403.04321v2.pdf","comment":"CVPR 2024; project page: https://dpt-t2i.github.io/"},{"id":"http://arxiv.org/abs/2403.08551v2","updated":"2024-03-14T06:32:00Z","published":"2024-03-13T14:02:54Z","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting","summary":"  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 1000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding.\n","authors":["Xinjie Zhang","Xingtong Ge","Tongda Xu","Dailan He","Yan Wang","Hongwei Qin","Guo Lu","Jing Geng","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03195v2","updated":"2024-03-14T03:59:19Z","published":"2024-01-06T11:37:20Z","title":"Efficient Bitrate Ladder Construction using Transfer Learning and\n  Spatio-Temporal Features","summary":"  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n","authors":["Ali Falahati","Mohammad Karim Safavi","Ardavan Elahi","Farhad Pakdaman","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2401.03195v2.pdf","comment":"7 pages, 9 figures, 7 tables, Copyright 2024 IEEE - Presented in IEEE\n  MVIP 2024"},{"id":"http://arxiv.org/abs/2312.05730v2","updated":"2024-03-14T01:01:00Z","published":"2023-12-10T02:26:56Z","title":"AFL-Net: Integrating Audio, Facial, and Lip Modalities with a Two-step\n  Cross-attention for Robust Speaker Diarization in the Wild","summary":"  Speaker diarization in real-world videos presents significant challenges due\nto varying acoustic conditions, diverse scenes, the presence of off-screen\nspeakers, etc. This paper builds upon a previous study (AVR-Net) and introduces\na novel multi-modal speaker diarization system, AFL-Net. The proposed AFL-Net\nincorporates dynamic lip movement as an additional modality to enhance the\nidentity distinction. Besides, unlike AVR-Net which extracts high-level\nrepresentations from each modality independently, AFL-Net employs a two-step\ncross-attention mechanism to sufficiently fuse different modalities, resulting\nin more comprehensive information to enhance the performance. Moreover, we also\nincorporated a masking strategy during training, where the face and lip\nmodalities are randomly obscured. This strategy enhances the impact of the\naudio modality on the system outputs. Experimental results demonstrate that\nAFL-Net outperforms state-of-the-art baselines, such as the AVR-Net and DyViSE.\n","authors":["Yongkang Yin","Xu Li","Ying Shan","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2312.05730v2.pdf","comment":null}]}}